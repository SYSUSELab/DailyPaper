[
  {
    "source_paper": "2511.20403_output/content.md",
    "benchmark_name": "CLASSES2TEST",
    "benchmark_name_quote": "We introduce the CLASSES2TEST dataset, which maps Java classes under test to their corresponding test classes",
    "is_original_proposal": "Yesï¼Œæœ¬æ–‡æ˜¯è¯¥æ•°æ®é›†çš„åŸå§‹å‘å¸ƒè®ºæ–‡",
    "is_original_proposal_quote": "We introduce the CLASSES2TEST dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics",
    "dataset_url": "https://anonymous.4open.science/r/classes2test",
    "dataset_url_quote": "1https://anonymous.4open.science/r/classes2test",
    "task_description": "ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„Javaå•å…ƒæµ‹è¯•çš„è´¨é‡ï¼Œæ”¯æŒç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ¯”è¾ƒä¸åŒLLMå’Œæç¤ºç­–ç•¥",
    "task_description_quote": "AGONETEST does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions.",
    "dimension": "å•å…ƒæµ‹è¯•è´¨é‡è¯„ä¼°ï¼ŒåŒ…æ‹¬ç¼–è¯‘æˆåŠŸç‡ã€ä»£ç è¦†ç›–ç‡ã€ç¼ºé™·æ£€æµ‹èƒ½åŠ›ã€æµ‹è¯•å¼‚å‘³ç­‰",
    "dimension_quote": "a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",
    "evaluation_method": "é›†æˆé«˜çº§è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚å˜å¼‚åˆ†æ•°å’Œæµ‹è¯•å¼‚å‘³ï¼Œè¿›è¡Œç»¼åˆè¯„ä¼°",
    "evaluation_method_quote": "a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",
    "context_dependency": "ç±»çº§åˆ«æµ‹è¯•ï¼Œæ¶µç›–æ–¹æ³•äº¤äº’å’Œå…±äº«çŠ¶æ€",
    "context_dependency_quote": "Our approach focuses on class-level test code evaluation, which is closer to real-world practices as it covers method interactions and shared state",
    "problem_domain": "è½¯ä»¶æµ‹è¯•ï¼ŒJavaå•å…ƒæµ‹è¯•ç”Ÿæˆ",
    "problem_domain_quote": "Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly.",
    "problem_difficulty": "ç°å®ä¸–ç•Œè½¯ä»¶é¡¹ç›®çº§åˆ«ï¼Œæ¯”å•æ–¹æ³•æµ‹è¯•æ›´å¤æ‚",
    "problem_difficulty_quote": "This extended dataset makes it possible to assess the test performance of an LLM on a more complex scope (the entire class) than the single method.",
    "language": "Java",
    "language_quote": "An annotated open source Java project dataset extending METHODS2TEST [9]",
    "data_size": "åŸºäº9,410ä¸ªGitHubä»“åº“çš„æ•°æ®é›†",
    "data_size_quote": "AGONETEST offers far broader applicability by using a dataset of 9,410 GitHub repositories",
    "source_type": "æ‰©å±•è‡ªMETHODS2TESTæ•°æ®é›†çš„Javaå¼€æºé¡¹ç›®",
    "source_type_quote": "An annotated open source Java project dataset extending METHODS2TEST [9]",
    "last_updated": 2025,
    "last_updated_quote": "arXiv:2511.20403v1  [cs.SE]  25 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ŒåŸºäºç°æœ‰æ•°æ®é›†æ‰©å±•",
    "build_type_quote": "Leveraging the METHODS2TEST dataset [9], we developed a new dataset specifically aimed at comparing human-written tests with those produced by LLMs.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ç±»çº§åˆ«å•å…ƒæµ‹è¯•ç”Ÿæˆ",
    "task_granularity_quote": "Our approach focuses on class-level test code evaluation, which is closer to real-world practices as it covers method interactions and shared state",
    "evaluation_metrics": "å˜å¼‚åˆ†æ•°ã€æµ‹è¯•å¼‚å‘³ã€ä»£ç è¦†ç›–ç‡",
    "evaluation_metrics_quote": "a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",
    "input_modality": "Javaç±»ä»£ç ",
    "input_modality_quote": "which maps classes under test to their related test classes",
    "output_modality": "å•å…ƒæµ‹è¯•ä»£ç ",
    "output_modality_quote": "for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "which maps Java classes under test to their corresponding test classes",
    "execution_environment": "æ”¯æŒæ‰€æœ‰Java LTSç‰ˆæœ¬çš„é¡¹ç›®ç¯å¢ƒ",
    "execution_environment_quote": "AGONETEST overcomes this barrier by supporting all Java LTS versions.",
    "unique_features": "ä¸“æ³¨äºç±»çº§åˆ«æµ‹è¯•è¯„ä¼°ï¼Œæ”¯æŒå¤šç§LLMå’Œæç¤ºç­–ç•¥çš„æ¯”è¾ƒï¼Œæä¾›ç«¯åˆ°ç«¯çš„è‡ªåŠ¨åŒ–è¯„ä¼°æµæ°´çº¿",
    "unique_features_quote": "AGONETEST shifts the focus to the generation of class-level tests. Our approach makes it possible to use up-to-date LLMs and not constrain prompt design (our prompts can be customized), thereby handling more complex, real-world scenarios.",
    "data_size_quantity": 9410,
    "data_size_unit": "ä¸ªGitHubä»“åº“",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java']",
    "dimension_normalized": "['å•å…ƒæµ‹è¯•è´¨é‡è¯„ä¼°', 'ç¼–è¯‘æˆåŠŸç‡', 'ä»£ç è¦†ç›–ç‡', 'ç¼ºé™·æ£€æµ‹èƒ½åŠ›', 'æµ‹è¯•å¼‚å‘³']",
    "evaluation_method_normalized": "['å˜å¼‚åˆ†æ•°', 'æµ‹è¯•å¼‚å‘³', 'ä»£ç è¦†ç›–ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶æµ‹è¯•', 'Javaå•å…ƒæµ‹è¯•ç”Ÿæˆ']",
    "source_type_normalized": "['æ‰©å±•è‡ªMETHODS2TESTæ•°æ®é›†çš„Javaå¼€æºé¡¹ç›®']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•æ–‡ä»¶",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.21380_output/content.md",
    "benchmark_name": "ROCODE, LogHub2.0",
    "benchmark_name_quote": "Two projects (i.e., ROCODE [15] and LogHub2.0 [16]) are selected for the following experiments.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ•°æ®é›†é€‚åº”ä»»åŠ¡ - å°†è½¯ä»¶å·¥ç¨‹ç ”ç©¶å·¥å…·è‡ªåŠ¨é€‚é…åˆ°æ–°çš„æ•°æ®é›†ï¼Œæ„å»ºå¯è¿è¡Œçš„å®éªŒå¹¶è·å–æ‰§è¡Œç»“æœ",
    "task_description_quote": "Our objective is to automatically modify ğ‘…ğ·or ğ‘…ğ‘‡, construct a runnable experiment, and obtain its execution results.",
    "dimension": "å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ•°æ®é›†é€‚åº”ä»»åŠ¡ä¸­çš„èƒ½åŠ›è¯„ä¼°",
    "dimension_quote": "This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks.",
    "evaluation_method": "äº”é˜¶æ®µè¯„ä¼°æµç¨‹ï¼šæ–‡ä»¶ç†è§£ã€ä»£ç ç¼–è¾‘ã€å‘½ä»¤ç”Ÿæˆã€éªŒè¯å’Œæœ€ç»ˆæ‰§è¡Œï¼Œæµ‹é‡æˆåŠŸç‡å¹¶åˆ†æå¤±è´¥æ¨¡å¼",
    "evaluation_method_quote": "Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®ç¯å¢ƒï¼Œéœ€è¦ç†è§£æ•´ä¸ªä»£ç ä»“åº“çš„æ¶æ„",
    "context_dependency_quote": "Before modifying code or generating commands for a repository, a multi-agent system must browse the essential files in a repository to understand its architecture.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ç ”ç©¶ï¼ŒåŒ…æ‹¬ä»£ç ç”Ÿæˆã€bugä¿®å¤ã€æ€§èƒ½åˆ†æå’Œå®‰å…¨ç­‰é¢†åŸŸ",
    "problem_domain_quote": "In areas ranging from code generation and bug fixing to performance analysis and security, researchers are constantly proposing new techniques",
    "problem_difficulty": "å¤æ‚è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œéœ€è¦å¤šæ­¥éª¤åè°ƒå’Œè¿­ä»£ä¿®å¤",
    "problem_difficulty_quote": "Most of the evaluated multi-agent systems failed to complete the assigned tasks. Under such circumstances, nearly all results would be classified as failures",
    "language": "Python",
    "language_quote": "To ensure a consistent and manageable experimental environment, we retained only artifacts implemented in Python.",
    "data_size": "ä»é¡¶çº§è½¯ä»¶å·¥ç¨‹ä¼šè®®ï¼ˆFSE, ICSE, ASE, ISSTAï¼‰2024-2025å¹´æ¥å—çš„è®ºæ–‡ä¸­ç­›é€‰çš„å¯é‡ç”¨ç ”ç©¶æ„ä»¶",
    "data_size_quote": "We began by collecting all papers accepted by the top four SE (FSE, ICSE, ASE and ISSTA) in 2024-2025 that were either (i) awarded a Reusable Artifact badge or (ii) explicitly identified by the authors as providing reusable artifacts.",
    "source_type": "å­¦æœ¯ç ”ç©¶æ„ä»¶ï¼Œæ¥è‡ªé¡¶çº§è½¯ä»¶å·¥ç¨‹ä¼šè®®çš„å¯é‡ç”¨ç ”ç©¶é¡¹ç›®",
    "source_type_quote": "To construct a representative set of high-quality SE research artifacts, we followed a systematic multi-stage selection process",
    "last_updated": 2025,
    "last_updated_quote": "arXiv:2511.21380v1  [cs.SE]  26 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºçš„ç ”ç©¶æ„ä»¶é›†åˆ",
    "build_type_quote": "We began by collecting all papers accepted by the top four SE (FSE, ICSE, ASE and ISSTA) in 2024-2025",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç¼–è¾‘ã€æ–‡ä»¶åˆ›å»ºã€å‘½ä»¤ç”Ÿæˆã€éªŒè¯ä¿®å¤",
    "task_granularity_quote": "Edit and create necessary files... Generate and execute the commands... Validating and repairing",
    "evaluation_metrics": "æˆåŠŸç‡ã€ç»“æ„ç›¸ä¼¼åº¦ï¼ˆä»7.25%åˆ°67.14%ï¼‰ã€å®ŒæˆçŠ¶æ€è®°å½•",
    "evaluation_metrics_quote": "Results show that... substantially improve structural similarity to ground truth (from 7.25% to 67.14%)",
    "input_modality": "ä»£ç ä»“åº“ã€è‡ªç„¶è¯­è¨€æç¤º",
    "input_modality_quote": "Each multi-agent system is provided with a processed repository... along with a simple prompt that specifies the adaptation task",
    "output_modality": "ä¿®æ”¹åçš„ä»£ç ã€ç”Ÿæˆçš„è„šæœ¬å‘½ä»¤ã€æ‰§è¡Œç»“æœ",
    "output_modality_quote": "the code adaptations performed to ensure compatibility, the executable scripts or commands derived for running the experiment, and the results produced by executing those scripts or commands",
    "task_io_type": "ä»£ç åˆ°ä»£ç çš„é€‚åº”ä»»åŠ¡",
    "task_io_type_quote": "automatically modify ğ‘…ğ·or ğ‘…ğ‘‡, construct a runnable experiment, and obtain its execution results",
    "execution_environment": "Pythonç¯å¢ƒï¼Œéœ€è¦å¯é çš„éƒ¨ç½²ç¯å¢ƒ",
    "execution_environment_quote": "This choice allows for reliable environment deployment and isolates our investigation from environment setup challenges",
    "unique_features": "ä¸“æ³¨äºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ•°æ®é›†é€‚åº”ä»»åŠ¡ä¸­çš„è¡¨ç°è¯„ä¼°ï¼Œé‡‡ç”¨äº”é˜¶æ®µè¯„ä¼°æµç¨‹ï¼Œç ”ç©¶æç¤ºçº§å¹²é¢„å¯¹æ€§èƒ½çš„å½±å“",
    "unique_features_quote": "This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. Through a five-stage evaluation pipeline... we measure success rates, analyze failure patterns, and assess prompt-based interventions",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ•°æ®é›†é€‚åº”ä»»åŠ¡ä¸­çš„èƒ½åŠ›è¯„ä¼°']",
    "evaluation_method_normalized": "['æˆåŠŸç‡', 'ç»“æ„ç›¸ä¼¼åº¦', 'å®ŒæˆçŠ¶æ€è®°å½•']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹ç ”ç©¶', 'ä»£ç ç”Ÿæˆ', 'bugä¿®å¤', 'æ€§èƒ½åˆ†æ', 'å®‰å…¨']",
    "source_type_normalized": "['å­¦æœ¯ç ”ç©¶æ„ä»¶', 'é¡¶çº§è½¯ä»¶å·¥ç¨‹ä¼šè®®', 'å¯é‡ç”¨ç ”ç©¶é¡¹ç›®']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.21022_output/content.md",
    "benchmark_name": "EDAPIBench",
    "benchmark_name_quote": "We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances.",
    "is_original_proposal": "Yesï¼Œæœ¬æ–‡æ˜¯è¯¥æ•°æ®é›†çš„åŸå§‹å‘å¸ƒè®ºæ–‡",
    "is_original_proposal_quote": "We introduce EDAPIBench, a dedicated benchmark... We construct EDAPIBenchâ€”the first dedicated benchmark for evaluating deprecated API knowledge editing in LLMs",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ä¸­åºŸå¼ƒAPIçŸ¥è¯†ç¼–è¾‘çš„æ€§èƒ½ï¼Œä¸“é—¨ç”¨äºæµ‹è¯•æ¨¡å‹ç¼–è¾‘æŠ€æœ¯èƒ½å¦æœ‰æ•ˆæ›´æ–°åºŸå¼ƒAPIçŸ¥è¯†å¹¶ç”Ÿæˆæœ€æ–°çš„API",
    "task_description_quote": "a dedicated benchmark for evaluating deprecated API knowledge editing in LLMs... whether existing model editing methods can effectively update deprecated API knowledge within LLMs, and enable the edited models to correctly replace deprecated APIs with up-to-date ones",
    "dimension": "æœ‰æ•ˆæ€§ã€æ³›åŒ–æ€§ã€å¯ç§»æ¤æ€§ã€ç‰¹å¼‚æ€§",
    "dimension_quote": "comprehensively assess model editing performance across four key dimensions: Effectiveness, Generalization, Portability, Specificity",
    "evaluation_method": "åŸºäºå››ä¸ªç»´åº¦çš„è¯„ä¼°ï¼šæœ‰æ•ˆæ€§ï¼ˆç¼–è¾‘åæ¨¡å‹æ˜¯å¦ç”Ÿæˆæœ€æ–°APIï¼‰ã€æ³›åŒ–æ€§ï¼ˆè¯­ä¹‰ç­‰ä»·ä½†è¯­æ³•å˜åŒ–çš„è¾“å…¥ï¼‰ã€å¯ç§»æ¤æ€§ï¼ˆä¸åŒè¾“å…¥ä½†æ¶‰åŠç›¸åŒåºŸå¼ƒAPIï¼‰ã€ç‰¹å¼‚æ€§ï¼ˆä¿æŒä¸ç¼–è¾‘ä»»åŠ¡æ— å…³è¾“å…¥çš„è¡Œä¸ºä¸€è‡´æ€§ï¼‰",
    "evaluation_method_quote": "Effectiveness: Measuring whether the edited model generates the up-to-date API for the original inputs; Generalization: Measuring whether it generates the up-to-date API on semantically equivalent but syntactically varied inputs; Portability: Measuring whether it generates the up-to-date API across different inputs... Specificity: Measuring whether it preserves consistent pre-editing behavior on inputs unrelated to the editing task",
    "context_dependency": "å•å‡½æ•°çº§åˆ«çš„ä»£ç è¡¥å…¨ä¸Šä¸‹æ–‡",
    "context_dependency_quote": "Our study frames the model editing scenario as a code completion task... we extract lines preceding the API invocation as candidate editing inputs (to prompt LLM API completion) and the invocation line as the target API (ground truth)",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ã€APIç»´æŠ¤ã€ç¬¬ä¸‰æ–¹åº“æ›´æ–°",
    "problem_domain_quote": "deprecated APIs from 8 popular Python libraries... covering eight popular Python libraries such as PyTorch and TensorFlow",
    "problem_difficulty": "å®é™…å·¥ç¨‹çº§éš¾åº¦ï¼Œæ¶‰åŠçœŸå®ä¸–ç•ŒåºŸå¼ƒAPIçš„è¯†åˆ«å’Œæ›´æ–°",
    "problem_difficulty_quote": "Using these mappings, we extract 65,596 real-world functions from GitHub that call the up-to-date APIs",
    "language": "Python",
    "language_quote": "deprecated APIs from 8 popular Python libraries... covering eight popular Python libraries such as PyTorch and TensorFlow",
    "data_size": "åŒ…å«70å¤šä¸ªåºŸå¼ƒAPIï¼Œè¶…è¿‡3000ä¸ªç¼–è¾‘å®ä¾‹ï¼Œä»GitHubæå–äº†65,596ä¸ªçœŸå®ä¸–ç•Œå‡½æ•°",
    "data_size_quote": "featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances... we extract 65,596 real-world functions from GitHub",
    "source_type": "ä»GitHubçœŸå®é¡¹ç›®ä»£ç ä¸­æå–ï¼ŒåŸºäºå·²éªŒè¯çš„APIæ˜ å°„å…³ç³»",
    "source_type_quote": "We begin with 145 verified API mappings (deprecated â†’up-to-date) from Wang et al. [49]... Using these mappings, we extract 65,596 real-world functions from GitHub that call the up-to-date APIs",
    "last_updated": "2025-2026",
    "last_updated_quote": "Publication date: November 2026. arXiv:2511.21022v1 [cs.SE] 26 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œå…¨è‡ªåŠ¨æ„å»º",
    "build_type_quote": "which can be fully automatically constructed... with fully automated construction",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "APIçº§åˆ«çš„ä»£ç è¡¥å…¨",
    "task_granularity_quote": "code completion tasks... during code completion, LLMs frequently suggest deprecated API invocations",
    "evaluation_metrics": "åŸºäºå››ä¸ªç»´åº¦çš„å®šæ€§è¯„ä¼°ï¼šæœ‰æ•ˆæ€§ã€æ³›åŒ–æ€§ã€å¯ç§»æ¤æ€§ã€ç‰¹å¼‚æ€§",
    "evaluation_metrics_quote": "Effectiveness, Generalization, Portability, Specificity",
    "input_modality": "ä»£ç ç‰‡æ®µ",
    "input_modality_quote": "the editing input (a code snippet to be completed)",
    "output_modality": "APIè°ƒç”¨ä»£ç ",
    "output_modality_quote": "the specific correct, up-to-date API that should replace the deprecated one",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "code completion task... generating code",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªä¸“é—¨é’ˆå¯¹åºŸå¼ƒAPIçŸ¥è¯†ç¼–è¾‘çš„åŸºå‡†æµ‹è¯•ï¼Œæ”¯æŒå…¨è‡ªåŠ¨æ„å»ºï¼ŒåŒ…å«å››ä¸ªç»´åº¦çš„ç»¼åˆè¯„ä¼°",
    "unique_features_quote": "the first dedicated benchmark for evaluating deprecated API knowledge editing in LLMs, with fully automated construction, serving as a standardized, rigorous platform",
    "data_size_quantity": 65596,
    "data_size_unit": "ä¸ªå‡½æ•°",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['æœ‰æ•ˆæ€§', 'æ³›åŒ–æ€§', 'å¯ç§»æ¤æ€§', 'ç‰¹å¼‚æ€§']",
    "evaluation_method_normalized": "['æœ‰æ•ˆæ€§', 'æ³›åŒ–æ€§', 'å¯ç§»æ¤æ€§', 'ç‰¹å¼‚æ€§']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'APIç»´æŠ¤', 'ç¬¬ä¸‰æ–¹åº“æ›´æ–°']",
    "source_type_normalized": "['ä»GitHubçœŸå®é¡¹ç›®ä»£ç ä¸­æå–', 'åŸºäºå·²éªŒè¯çš„APIæ˜ å°„å…³ç³»']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç è¡¥å…¨",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.19875_output/content.md",
    "benchmark_name": "CodeFuse-CommitEval",
    "benchmark_name_quote": "We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models.",
    "is_original_proposal": "Yesï¼Œæœ¬æ–‡æ˜¯è¯¥æ•°æ®é›†çš„åŸå§‹å‘å¸ƒè®ºæ–‡",
    "is_original_proposal_quote": "We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models.",
    "dataset_url": "https://figshare.com/s/21fe4ec9cb960b52bffe",
    "dataset_url_quote": "The dataset is publicly available at https://figshare.com/s/21fe4ec9cb960b52bffe.",
    "task_description": "æ£€æµ‹æäº¤ä¿¡æ¯ä¸ä»£ç å˜æ›´ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ï¼ˆMessage-Code Inconsistency, MCIï¼‰",
    "task_description_quote": "A Message-Code Inconsistency (MCI) occurs when the natural language description in a commit message does not accurately reflect the actual modifications in the associated code diff.",
    "dimension": "æäº¤ä¿¡æ¯ä¸ä»£ç å˜æ›´çš„ä¸€è‡´æ€§æ£€æµ‹èƒ½åŠ›",
    "dimension_quote": "evaluate models for MCI detection",
    "evaluation_method": "ä½¿ç”¨Recallã€Precisionã€Specificityç­‰æŒ‡æ ‡è¯„ä¼°æ¨¡å‹æ£€æµ‹ç»“æœ",
    "evaluation_method_quote": "Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%)",
    "context_dependency": "æäº¤ä¿¡æ¯ä¸å¯¹åº”çš„ä»£ç å·®å¼‚ï¼ˆdiffï¼‰",
    "context_dependency_quote": "Each item of the verified dataset contains a commit message, the corresponding code diff, and the ground truth label",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ã€ç‰ˆæœ¬æ§åˆ¶ã€ä»£ç å®¡æŸ¥",
    "problem_domain_quote": "Version control relies on commit messages to convey the rationale for code changes",
    "problem_difficulty": "éœ€è¦è¯­ä¹‰ç†è§£å’Œä¸Šä¸‹æ–‡æ¨ç†çš„å¤æ‚ä»»åŠ¡",
    "problem_difficulty_quote": "purpose inconsistencies require deeper semantic understanding and contextual reasoning",
    "language": "å¤šç§ç¼–ç¨‹è¯­è¨€ï¼ˆåŸºäºApacheCMæ•°æ®é›†çš„å¤šæ ·æ€§ï¼‰",
    "language_quote": "because of its diversity in programming languages and the high-quality commits",
    "data_size": "åŒ…å«æ­£è´Ÿæ ·æœ¬çš„å¹³è¡¡æ•°æ®é›†",
    "data_size_quote": "we generate a balanced dataset with both positive and negative samples",
    "source_type": "åŸºäºApacheCMæ•°æ®é›†ï¼Œé€šè¿‡è§„åˆ™å¼•å¯¼çš„çªå˜ç”Ÿæˆä¸ä¸€è‡´æäº¤ä¿¡æ¯",
    "source_type_quote": "Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2511.19875v1 [cs.SE] 25 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç»“åˆLLMæ•°æ®åˆæˆèƒ½åŠ›ä¸ç°æœ‰æäº¤è¯­æ–™åº“",
    "build_type_quote": "We present a comprehensive pipeline for constructing MCI datasets. By combining the data synthesis capabilities of LLMs with existing commit corpora",
    "contamination_status": "é€šè¿‡åŒé‡éªŒè¯ç¡®ä¿æ•°æ®è´¨é‡",
    "contamination_status_quote": "apply two-fold validation to verify both positive and negative samples",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä¸€è‡´æ€§æ£€æµ‹",
    "task_granularity_quote": "detecting inconsistencies between commit messages and code",
    "evaluation_metrics": "Recall, Precision, Specificity",
    "evaluation_metrics_quote": "average Recall 85.95%, Precision 80.28%, Specificity 63.8%",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆæäº¤ä¿¡æ¯ï¼‰ä¸ä»£ç å·®å¼‚",
    "input_modality_quote": "Each item of the verified dataset contains a commit message, the corresponding code diff",
    "output_modality": "äºŒå…ƒåˆ†ç±»ï¼ˆä¸€è‡´æˆ–ä¸ä¸€è‡´ï¼‰",
    "output_modality_quote": "allowing the models to detect whether the commit is consistent or not",
    "task_io_type": "æ–‡æœ¬ä¸ä»£ç åˆ°åˆ†ç±»",
    "task_io_type_quote": "detect whether the commit is consistent or not",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªä¸“é—¨ç”¨äºMCIæ£€æµ‹çš„åŸºå‡†ï¼ŒåŒ…å«ä¸ƒç§ä¸ä¸€è‡´ç±»å‹ï¼Œæ”¯æŒå¤šç§å¢å¼ºç­–ç•¥è¯„ä¼°",
    "unique_features_quote": "CODEFUSE-COMMITEVAL is the first benchmarking work tailored for comprehensively evaluating LLM's MCI detection ability. We define seven mutation rules to guide powerful LLMs in generating various types of inconsistent commit messages",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['å¤šç§ç¼–ç¨‹è¯­è¨€']",
    "dimension_normalized": "['æäº¤ä¿¡æ¯ä¸ä»£ç å˜æ›´çš„ä¸€è‡´æ€§æ£€æµ‹èƒ½åŠ›']",
    "evaluation_method_normalized": "['Recall', 'Precision', 'Specificity']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ç‰ˆæœ¬æ§åˆ¶', 'ä»£ç å®¡æŸ¥']",
    "source_type_normalized": "['åŸºäºApacheCMæ•°æ®é›†', 'é€šè¿‡è§„åˆ™å¼•å¯¼çš„çªå˜ç”Ÿæˆä¸ä¸€è‡´æäº¤ä¿¡æ¯']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2511.20709_output/content.md",
    "benchmark_name": "DUALGAUGE-BENCH",
    "benchmark_name_quote": "we further curated DUALGAUGE-BENCH, a comprehensive benchmark suite of 154 programming tasks",
    "is_original_proposal": "Yesï¼Œæœ¬æ–‡æ˜¯è¯¥æ•°æ®é›†çš„åŸå§‹å‘å¸ƒè®ºæ–‡",
    "is_original_proposal_quote": "We present DUALGAUGE, the first fully automated benchmarking framework... we also present DUALGAUGE-BENCH, a curated benchmark suite",
    "dataset_url": "https://anonymous.4open.science/r/DualBench-6D1D",
    "dataset_url_quote": "Our system source code, benchmark, and evaluation/study data can all be found at https://anonymous.4open.science/r/DualBench-6D1D",
    "task_description": "è”åˆè¯„ä¼°ä»£ç ç”Ÿæˆçš„å®‰å…¨æ€§å’ŒåŠŸèƒ½æ€§ï¼Œç¡®ä¿ç”Ÿæˆçš„ä»£ç æ—¢æ»¡è¶³åŠŸèƒ½è§„èŒƒåˆä¸ä¼šå¼•å…¥å®‰å…¨æ¼æ´",
    "task_description_quote": "rigorously evaluate the security and correctness of LLM-generated code in unison... ensuring that generated programs fulfill their specifications without introducing vulnerabilities",
    "dimension": "ä»£ç ç”Ÿæˆçš„å®‰å…¨æ€§å’ŒåŠŸèƒ½æ€§è”åˆè¯„ä¼°",
    "dimension_quote": "designed to rigorously evaluate the security and correctness of LLM-generated code in unison",
    "evaluation_method": "åœ¨æ²™ç›’ç¯å¢ƒä¸­æ‰§è¡Œç¨‹åºï¼Œé’ˆå¯¹åŠŸèƒ½å’Œå®‰å…¨æ€§æµ‹è¯•å¥—ä»¶è¿è¡Œï¼ŒåŸºäºæ‰§è¡Œç»“æœè¯„ä¼°æµ‹è¯•é€šè¿‡ç‡å’Œè”åˆæ­£ç¡®æ€§-å®‰å…¨æ€§æŒ‡æ ‡",
    "evaluation_method_quote": "executes it in a sandboxed environment against functional and security test suites, and evaluates the execution results against both test suites to report test pass rates and joint correctness-security metrics",
    "context_dependency": "å•ä»»åŠ¡ä»£ç ç”Ÿæˆï¼ŒåŸºäºè‡ªç„¶è¯­è¨€æç¤ºç”Ÿæˆå®Œæ•´ç¨‹åº",
    "context_dependency_quote": "Given an pure-natural-language prompt as functional specification... generates the model's code output for the prompt",
    "problem_domain": "è·¨é¢†åŸŸç¼–ç¨‹ä»»åŠ¡ï¼Œæ¶µç›–å¤šæ ·åŒ–åŠŸèƒ½é¢†åŸŸ",
    "problem_domain_quote": "spanning diverse functionality domains",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "è¯­è¨€æ— å…³ï¼Œæ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€",
    "language_quote": "This dataset design and curation process allow it to be agnostic to programming languages",
    "data_size": "åŒ…å«154ä¸ªç¼–ç¨‹ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½é…æœ‰åŠŸèƒ½å’Œå®‰å…¨æ€§æµ‹è¯•å¥—ä»¶",
    "data_size_quote": "a comprehensive benchmark suite of 154 programming tasks... Each task/prompt is paired with both a functional test suite... and a security test suite",
    "source_type": "äººå·¥ä¸LLMååŒåˆ›å»ºï¼Œé€šè¿‡å¤šè¯„åˆ†è€…çš„äººç±»ä¸“ä¸šçŸ¥è¯†è¿›è¡Œç²¾ç‚¼å’Œä¿®æ­£",
    "source_type_quote": "constructed these test suites through a human-and-LLM co-creation processâ€”leveraging multiple LLMs to generate candidate tests, then refining and amending these with human expertise of multiple raters",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2511.20709v1 [cs.SE] 24 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ŒåŸºäºè§„èŒƒæµ‹è¯•èŒƒå¼",
    "build_type_quote": "following a specification-based testing paradigm",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "code generation... translating natural language promptsâ€”typically serving as functional specificationsâ€”into programs",
    "evaluation_metrics": "æµ‹è¯•é€šè¿‡ç‡ã€è”åˆæ­£ç¡®æ€§-å®‰å…¨æ€§æŒ‡æ ‡",
    "evaluation_metrics_quote": "report test pass rates and joint correctness-security metrics",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "Given an pure-natural-language prompt as functional specification",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "generates the model's code output for the prompt",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "translating natural language prompts... into programs",
    "execution_environment": "æ²™ç›’éš”ç¦»å®¹å™¨ç¯å¢ƒï¼Œæ”¯æŒä¾èµ–è§£æå’Œç¯å¢ƒé…ç½®",
    "execution_environment_quote": "executes it in a sandboxed environment... The execution engine compiles and runs the model-generated code within isolated containers",
    "unique_features": "é¦–ä¸ªæ”¯æŒå®‰å…¨æ€§å’ŒåŠŸèƒ½æ€§è”åˆè¯„ä¼°çš„åŸºå‡†å¥—ä»¶ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½é…æœ‰è¦†ç›–é©±åŠ¨çš„åŠŸèƒ½å’Œå®‰å…¨æ€§æµ‹è¯•å¥—ä»¶ï¼Œé‡‡ç”¨äººå·¥ä¸LLMååŒåˆ›å»ºè¿‡ç¨‹",
    "unique_features_quote": "the first benchmark suite that pairs each code-generation prompt with dual (functional and security), coverage-enforced test suites... constructed through a human-and-LLM co-creation process",
    "data_size_quantity": 154,
    "data_size_unit": "ä¸ªç¼–ç¨‹ä»»åŠ¡",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['è¯­è¨€æ— å…³', 'å¤šç§ç¼–ç¨‹è¯­è¨€']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„å®‰å…¨æ€§å’ŒåŠŸèƒ½æ€§è”åˆè¯„ä¼°']",
    "evaluation_method_normalized": "['æµ‹è¯•é€šè¿‡ç‡', 'è”åˆæ­£ç¡®æ€§-å®‰å…¨æ€§æŒ‡æ ‡']",
    "problem_domain_normalized": "['è·¨é¢†åŸŸç¼–ç¨‹ä»»åŠ¡', 'å¤šæ ·åŒ–åŠŸèƒ½é¢†åŸŸ']",
    "source_type_normalized": "['äººå·¥ä¸LLMååŒåˆ›å»º', 'å¤šè¯„åˆ†è€…çš„äººç±»ä¸“ä¸šçŸ¥è¯†è¿›è¡Œç²¾ç‚¼å’Œä¿®æ­£']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.23408_output/content.md",
    "benchmark_name": "Vul4J",
    "benchmark_name_quote": "Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects and covering distinct Common Weakness Enumeration (CWE) classes.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–æ¼æ´ä¿®å¤æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå…·ä½“é’ˆå¯¹çœŸå®æ¼æ´å’Œäººå·¥ç”Ÿæˆçš„æ¼æ´ã€‚",
    "task_description_quote": "In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs... using both real and artificial vulnerabilities.",
    "dimension": "æ¼æ´ä¿®å¤çš„æœ‰æ•ˆæ€§ã€æ¨¡å‹é—´çš„äº’è¡¥æ€§ä¸é‡å æ€§",
    "dimension_quote": "Our paper empirically investigates the effectiveness and complementarity of several prominent LLMs in automated vulnerability patching...",
    "evaluation_method": "ä½¿ç”¨æ¼æ´è¯æ˜ï¼ˆProof-of-Vulnerability, PoVï¼‰æµ‹è¯•æ‰§è¡Œæ¥éªŒè¯ç”Ÿæˆçš„è¡¥ä¸æ˜¯å¦æˆåŠŸä¿®å¤æ¼æ´ã€‚",
    "evaluation_method_quote": "Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities.",
    "context_dependency": "å•å‡½æ•°/ä»£ç ç‰‡æ®µ",
    "context_dependency_quote": "Given a vulnerable code snippet, these models generate a patched version that aims to eliminate security flaws while preserving functionality.",
    "problem_domain": "è½¯ä»¶å®‰å…¨ã€æ¼æ´ä¿®å¤",
    "problem_domain_quote": "Automated vulnerability patching is crucial for software security...",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Java",
    "language_quote": "Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities...",
    "data_size": "15ä¸ªçœŸå®æ¼æ´åŠå…¶41ä¸ªäººå·¥ç”Ÿæˆçš„å¯¹åº”æ¼æ´",
    "data_size_quote": "To perform this evaluation, we employed 15 real vulnerabilities and their 41 artificial counterparts (vulnerabilities).",
    "source_type": "çœŸå®æ¼æ´æ¥è‡ªå¼€æºé¡¹ç›®å’Œå…¬å…±æ¼æ´æ•°æ®åº“ï¼ˆå¦‚CVEï¼‰ï¼Œäººå·¥æ¼æ´ç”±CodeBERTç”Ÿæˆå¹¶ç»è¿‡éªŒè¯ã€‚",
    "source_type_quote": "Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects... Secondly, we augmented our evaluation with artificially generated vulnerabilities... we incorporated artificial vulnerabilities derived from the work of Garg et al.[15]. Garg et al. used CodeBERT[11] to generate thousands of candidate artificial vulnerabilities...",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆVul4Jæ•°æ®é›†ï¼‰",
    "build_type_quote": "Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ä¿®å¤ï¼ˆæ¼æ´è¡¥ä¸ç”Ÿæˆï¼‰",
    "task_granularity_quote": "Given a vulnerable code snippet, these models generate a patched version that aims to eliminate security flaws while preserving functionality.",
    "evaluation_metrics": "PoVæµ‹è¯•é€šè¿‡ç‡ï¼ˆåŸºäºæ‰§è¡Œçš„éªŒè¯ï¼‰",
    "evaluation_metrics_quote": "Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities.",
    "input_modality": "ä»£ç ï¼ˆåŒ…å«æ¼æ´çš„ä»£ç ç‰‡æ®µï¼‰",
    "input_modality_quote": "Given a vulnerable code snippet, these models generate a patched version...",
    "output_modality": "ä»£ç ï¼ˆä¿®å¤åçš„ä»£ç ç‰‡æ®µï¼‰",
    "output_modality_quote": "Given a vulnerable code snippet, these models generate a patched version...",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "Given a vulnerable code snippet, these models generate a patched version...",
    "execution_environment": "Maven/Gradleæ„å»ºç¯å¢ƒï¼ŒåŒ…å«PoV JUnitæµ‹è¯•ç”¨ä¾‹",
    "execution_environment_quote": "When no suitable test existed in the original project, Bui et al. [2] manually wrote one by following the exploit steps in the CVE report, thereby guaranteeing that every vulnerability in the dataset can be triggered, reproduced, and validated in a Maven/Gradle build.",
    "unique_features": "è¯¥ç ”ç©¶ä¸ä»…è¯„ä¼°LLMå¯¹çœŸå®æ¼æ´çš„ä¿®å¤èƒ½åŠ›ï¼Œè¿˜è¯„ä¼°å…¶å¯¹äººå·¥ç”Ÿæˆæ¼æ´çš„ä¿®å¤èƒ½åŠ›ï¼Œä»¥æµ‹è¯•æ¨¡å‹çš„æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚æ•°æ®é›†ï¼ˆVul4Jï¼‰ä¸ºæ¯ä¸ªæ¼æ´æä¾›äº†å¯æ‰§è¡Œçš„æ¼æ´è¯æ˜ï¼ˆPoVï¼‰æµ‹è¯•ç”¨ä¾‹ï¼Œç”¨äºè‡ªåŠ¨åŒ–éªŒè¯è¡¥ä¸çš„æœ‰æ•ˆæ€§ã€‚",
    "unique_features_quote": "Our paper empirically investigates the effectiveness and complementarity of several prominent LLMs in automated vulnerability patching, specifically focusing on both real vulnerabilities and their corresponding artificial vulnerabilities... For every entry Vul4J provides... one or more Proof-of-Vulnerability (PoV) JUnit test cases. A PoV test is an executable exploit oracle...",
    "data_size_quantity": 15,
    "data_size_unit": "ä¸ªçœŸå®æ¼æ´",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java']",
    "dimension_normalized": "['æ¼æ´ä¿®å¤çš„æœ‰æ•ˆæ€§', 'æ¨¡å‹é—´çš„äº’è¡¥æ€§ä¸é‡å æ€§']",
    "evaluation_method_normalized": "['PoVæµ‹è¯•é€šè¿‡ç‡ï¼ˆåŸºäºæ‰§è¡Œçš„éªŒè¯ï¼‰']",
    "problem_domain_normalized": "['è½¯ä»¶å®‰å…¨', 'æ¼æ´ä¿®å¤']",
    "source_type_normalized": "['çœŸå®æ¼æ´æ¥è‡ªå¼€æºé¡¹ç›®å’Œå…¬å…±æ¼æ´æ•°æ®åº“ï¼ˆå¦‚CVEï¼‰', 'äººå·¥æ¼æ´ç”±CodeBERTç”Ÿæˆå¹¶ç»è¿‡éªŒè¯ã€‚']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2401.01062_output/content.md",
    "benchmark_name": "CAASD (Capability Assessment of Automatic Software Development)",
    "benchmark_name_quote": "we have developed a novel benchmark named CAASD (Capability Assessment of Automatic Software Development).",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we have developed a novel benchmark named CAASD (Capability Assessment of Automatic Software Development).",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°AIè¾…åŠ©è½¯ä»¶å¼€å‘ç³»ç»Ÿçš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ç³»ç»Ÿçº§å®ç°ä»»åŠ¡çš„èƒ½åŠ›",
    "task_description_quote": "all of them either are limited to simple function-level implementation tasks or lack detailed requirements specifications for system-level implementation tasks",
    "dimension": "ç³»ç»Ÿçº§è½¯ä»¶å¼€å‘èƒ½åŠ›è¯„ä¼°",
    "dimension_quote": "for assessing how well a software development task is completed",
    "evaluation_method": "é€šè¿‡å‚è€ƒç”¨ä¾‹è¯„ä¼°ç³»ç»Ÿå®ç°çš„è´¨é‡å’Œå®Œæ•´æ€§",
    "evaluation_method_quote": "Each task of CAASD is equipped with a list of reference use cases depicting the system requirements. The reference use cases are used to evaluate the quality and completeness of a system implementation.",
    "context_dependency": "ç³»ç»Ÿçº§å¼€å‘ï¼ˆå¤šæ–‡ä»¶é¡¹ç›®ï¼‰",
    "context_dependency_quote": "system-level implementation tasks",
    "problem_domain": "è½¯ä»¶å¼€å‘",
    "problem_domain_quote": "software development",
    "problem_difficulty": "éå¹³å‡¡è½¯ä»¶é¡¹ç›®",
    "problem_difficulty_quote": "non-trivial software projects",
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2024",
    "last_updated_quote": "arXiv:2401.01062v1 [cs.SE] 2 Jan 2024",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we have developed a novel benchmark named CAASD",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ç³»ç»Ÿçº§å®ç°ä»»åŠ¡",
    "task_granularity_quote": "system-level implementation tasks",
    "evaluation_metrics": "é€šè¿‡ç‡",
    "evaluation_metrics_quote": "AISD achieves an impressive pass rate of 75.2%",
    "input_modality": "è‡ªç„¶è¯­è¨€éœ€æ±‚æè¿°",
    "input_modality_quote": "high-level (potentially vague) user requirements as inputs",
    "output_modality": "ç³»ç»Ÿå®ç°",
    "output_modality_quote": "system implementation",
    "task_io_type": "éœ€æ±‚åˆ°ç³»ç»Ÿå®ç°",
    "task_io_type_quote": "taking high-level (potentially vague) user requirements as inputs, generates detailed use cases, prototype system designs, and subsequently system implementation",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªè¯„ä¼°è½¯ä»¶å¼€å‘ä»»åŠ¡å®Œæˆè´¨é‡çš„åŸºå‡†ï¼ŒåŒ…å«è¯¦ç»†çš„ç³»ç»Ÿéœ€æ±‚è§„èŒƒ",
    "unique_features_quote": "this is the first benchmark that offers criteria for assessing how well a software development task is completed",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2024,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ç³»ç»Ÿçº§è½¯ä»¶å¼€å‘èƒ½åŠ›è¯„ä¼°']",
    "evaluation_method_normalized": "['é€šè¿‡ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶å¼€å‘']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2401.12554_output/content.md",
    "benchmark_name": "ParEval",
    "benchmark_name_quote": "we propose the Parallel Code Generation Evaluation (ParEval) benchmark: a set of benchmarks (prompts) for evaluating how well LLMs generate parallel code.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we propose the Parallel Code Generation Evaluation (ParEval) benchmark",
    "dataset_url": "github.com/parallelcodefoundry/ParEval",
    "dataset_url_quote": "ParEval is available online at: github.com/parallelcodefoundry/ParEval.",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¹¶è¡Œä»£ç çš„èƒ½åŠ›",
    "task_description_quote": "we study the capabilities of state-of-the-art language models to generate parallel code.",
    "dimension": "å¹¶è¡Œä»£ç ç”Ÿæˆæ­£ç¡®æ€§å’Œæ€§èƒ½",
    "dimension_quote": "We evaluate several state-of-the-art open- and closed-source LLMs using these benchmarks, and report metrics that represent the correctness and performance of the generated code.",
    "evaluation_method": "æ–°é¢–çš„ä»£ç ç”Ÿæˆè¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬speedupğ‘›@kå’Œefficiencyğ‘›@kï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆä»£ç çš„æ€§èƒ½å’Œæ‰©å±•æ€§",
    "evaluation_method_quote": "We introduce novel code generation evaluation metrics that assess performance and parallel scaling.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "ç§‘å­¦å’Œå¹¶è¡Œè®¡ç®—",
    "problem_domain_quote": "consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "C/C++",
    "language_quote": "Traditional Python code generation benchmarks are tested by running eval on the generated code for a small number of small unit tests. On the other hand, in the case of parallel code â€” we must compile C/C++ code, link against one or more parallel libraries, and run the code in the proper parallel environment.",
    "data_size": "420ä¸ªä¸åŒçš„ç¼–ç ä»»åŠ¡",
    "data_size_quote": "consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.",
    "source_type": "æ‰‹åŠ¨è®¾è®¡",
    "source_type_quote": "These benchmarks are challenging to test. Traditional Python code generation benchmarks are tested by running eval on the generated code for a small number of small unit tests.",
    "last_updated": "2024",
    "last_updated_quote": "HPDC â€™24, June 3â€“7, 2024, Pisa, Italy",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we propose the Parallel Code Generation Evaluation (ParEval) benchmark: a set of benchmarks (prompts) for evaluating how well LLMs generate parallel code.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "we study the capabilities of state-of-the-art language models to generate parallel code.",
    "evaluation_metrics": "speedupğ‘›@kå’Œefficiencyğ‘›@k",
    "evaluation_metrics_quote": "We introduce two novel metrics, speedupğ‘›@k and efficiencyğ‘›@k, for evaluating the performance and scaling of LLM generated code.",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "we study the capabilities of state-of-the-art language models to generate parallel code.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.",
    "execution_environment": "éœ€è¦ç‰¹å®šä¾èµ–ï¼ˆå¹¶è¡Œåº“ï¼‰",
    "execution_environment_quote": "we must compile C/C++ code, link against one or more parallel libraries, and run the code in the proper parallel environment.",
    "unique_features": "ä¸“æ³¨äºå¹¶è¡Œä»£ç ç”Ÿæˆè¯„ä¼°ï¼Œè¦†ç›–12ç§è®¡ç®—é—®é¢˜ç±»å‹å’Œ7ç§æ‰§è¡Œæ¨¡å‹",
    "unique_features_quote": "These benchmarks cover twelve different computational problem types, and seven different execution models: serial, OpenMP, Kokkos, MPI, MPI+OpenMP, CUDA, and HIP.",
    "data_size_quantity": 420,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2024,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['C', 'C++']",
    "dimension_normalized": "['å¹¶è¡Œä»£ç ç”Ÿæˆæ­£ç¡®æ€§', 'æ€§èƒ½']",
    "evaluation_method_normalized": "['speedupğ‘›@k', 'efficiencyğ‘›@k']",
    "problem_domain_normalized": "['ç§‘å­¦è®¡ç®—', 'å¹¶è¡Œè®¡ç®—']",
    "source_type_normalized": "['æ‰‹åŠ¨è®¾è®¡']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.01010_output/content.md",
    "benchmark_name": "Chain of Unit-Physics",
    "benchmark_name_quote": "To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯¥è¯„æµ‹åŸºå‡†æ—¨åœ¨è§£å†³ç§‘å­¦è®¡ç®—ä¸­çš„ä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å…·æœ‰ä¸¥æ ¼ç‰©ç†çº¦æŸçš„é«˜é£é™©ç§‘å­¦é—®é¢˜ï¼Œå¦‚ç‡ƒçƒ§ç§‘å­¦ä¸­çš„è®¡ç®—æµä½“åŠ¨åŠ›å­¦ï¼ˆCFDï¼‰æ±‚è§£å™¨å¼€å‘ã€‚",
    "task_description_quote": "Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community.",
    "dimension": "ç‰©ç†ä¸€è‡´æ€§ã€æ•°å€¼ç¨³å®šæ€§ã€ç®—æ³•æ­£ç¡®æ€§",
    "dimension_quote": "The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",
    "evaluation_method": "åŸºäºå•å…ƒç‰©ç†æµ‹è¯•çš„éªŒè¯æ–¹æ³•ï¼ŒåŒ…æ‹¬ç‰©ç†çº¦æŸæ£€æŸ¥ã€æ•°å€¼ä¸€è‡´æ€§æ£€æŸ¥å’Œè¯Šæ–­åˆ†æ",
    "evaluation_method_quote": "The Verification Agent applies formalized unit-physics tests to assess physical and numerical consistency. These primitives yield physics-grounded verification even without reference datasets.",
    "context_dependency": "å¤šæ­¥éª¤ç§‘å­¦è®¡ç®—ä»»åŠ¡ï¼Œæ¶‰åŠå¤æ‚çš„ç‰©ç†çº¦æŸå’Œæ•°å€¼è®¡ç®—",
    "context_dependency_quote": "The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",
    "problem_domain": "è®¡ç®—ç§‘å­¦ï¼Œç‰¹åˆ«æ˜¯ç‡ƒçƒ§ç§‘å­¦å’Œè®¡ç®—æµä½“åŠ¨åŠ›å­¦",
    "problem_domain_quote": "To overcome these limitations, this work systematically applies an inverse code design methodology (see Fig. 1), formally known as test-driven development (TDD) [26], to scientific software in combustion science, one such domain where TDD approaches have not been thoroughly evaluated.",
    "problem_difficulty": "é«˜éš¾åº¦ï¼Œæ¶‰åŠ12è‡ªç”±åº¦çš„å¤æ‚ç‡ƒçƒ§ä»»åŠ¡",
    "problem_difficulty_quote": "The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "åŸºäºäººç±»ä¸“å®¶çŸ¥è¯†æ„å»ºçš„å•å…ƒç‰©ç†æµ‹è¯•",
    "source_type_quote": "This work proposes Chain of Unit-Physics, an approach that embeds human expert knowledge directly into distinct reasoning chains of the agentic system via 'unit-physics'â€”formalized, testable constraints that encode fundamental physics (e.g., energy conservation laws) and practitioner experience (e.g., dimensional / bound checks, and floating-point diagnostics).",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2512.01010v1 [cs.MA] 30 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ŒåŸºäºäººç±»ä¸“å®¶çŸ¥è¯†",
    "build_type_quote": "This work proposes Chain of Unit-Physics, an approach that embeds human expert knowledge directly into distinct reasoning chains of the agentic system via 'unit-physics'â€”formalized, testable constraints that encode fundamental physics (e.g., energy conservation laws) and practitioner experience (e.g., dimensional / bound checks, and floating-point diagnostics).",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ç«¯åˆ°ç«¯ç§‘å­¦ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "Closed-weight systems and code-focused agentic variants fail to produce correct end-to-end solvers, despite tool and web access, exhibiting four recurrent error classes: interface (syntax/API) hallucinations, overconfident assumptions, numerical/physical incoherence, and configuration fragility.",
    "evaluation_metrics": "ç‰©ç†ä¸€è‡´æ€§ã€æ•°å€¼è¯¯å·®ã€è¿è¡Œæ—¶é—´å’Œå†…å­˜ä½¿ç”¨æ•ˆç‡",
    "evaluation_metrics_quote": "On the benchmark task, the proposed framework converges within 5â€“6 iterations, matches the human-expert implementation (mean error of 3.1Ë†10Â´3%), with a â€33.4% faster runtime and a â€30% efficient memory usage at a cost comparable to mid-sized commercial APIs, yielding a practical template for physics-grounded scientific code generation.",
    "input_modality": "è‡ªç„¶è¯­è¨€æŸ¥è¯¢å’Œå•å…ƒç‰©ç†æµ‹è¯•",
    "input_modality_quote": "In the input, the userâ€™s scientific question is paired with 'basis prompts' that establish execution permissions, tool availability, coding language settings, and transfer protocols. Additional input scopes the unit-physics tests that concatenated with the scientific query to form the complete input to the framework.",
    "output_modality": "ç§‘å­¦è®¡ç®—ä»£ç å’Œå¯è§†åŒ–ç»“æœ",
    "output_modality_quote": "Finally, the agent consolidates the output into domain-relevant visualizations (for example, line graphs and contour graphs of key quantities of interest), closing the loop between natural-language inquiry and quantitative scientific insight.",
    "task_io_type": "è‡ªç„¶è¯­è¨€åˆ°ç§‘å­¦ä»£ç ",
    "task_io_type_quote": "Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community.",
    "execution_environment": "Pythonè™šæ‹Ÿæ²™ç®±ç¯å¢ƒï¼Œå…·æœ‰éš”ç¦»çš„æ‰§è¡Œæƒé™",
    "execution_environment_quote": "The framework runs inside a dedicated Python virtual sandbox environment that is not pre-configured with metadata on all available libraries. The agent is granted isolated code execution privileges within this sandbox, ensuring that any dependency installs or script executions cannot affect the system root directory or global files.",
    "unique_features": "åŸºäºç¬¬ä¸€æ€§åŸç†çš„å•å…ƒç‰©ç†æµ‹è¯•é©±åŠ¨ä»£ç ç”Ÿæˆï¼Œå¼ºè°ƒç‰©ç†ä¸€è‡´æ€§å’Œæ•°å€¼ç¨³å®šæ€§",
    "unique_features_quote": "This inverse-design method provides two key advantages in scientific software. First, human-authored tests embed deep domain expertise (first principles or primitives) into targeted validation checks, ensuring that each algorithmic component faithfully represents the underlying physics.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ç‰©ç†ä¸€è‡´æ€§', 'æ•°å€¼ç¨³å®šæ€§', 'ç®—æ³•æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['ç‰©ç†ä¸€è‡´æ€§', 'æ•°å€¼è¯¯å·®', 'è¿è¡Œæ—¶é—´', 'å†…å­˜ä½¿ç”¨æ•ˆç‡']",
    "problem_domain_normalized": "['è®¡ç®—ç§‘å­¦', 'ç‡ƒçƒ§ç§‘å­¦', 'è®¡ç®—æµä½“åŠ¨åŠ›å­¦']",
    "source_type_normalized": "['åŸºäºäººç±»ä¸“å®¶çŸ¥è¯†æ„å»ºçš„å•å…ƒç‰©ç†æµ‹è¯•']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.01396_output/content.md",
    "benchmark_name": "BackportBench",
    "benchmark_name_quote": "we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",
    "dataset_url": "https://github.com/BackportBench/BackportBench",
    "dataset_url_quote": "The BackportBench is available on https://github.com/BackportBench/BackportBench.",
    "task_description": "è‡ªåŠ¨åŒ–è¡¥ä¸å›ç§»æ¤ã€‚æ—¨åœ¨ä¸ºæœªæ‰“è¡¥ä¸çš„è½¯ä»¶ç‰ˆæœ¬ç”Ÿæˆè¡¥ä¸ï¼ŒåŸºäºå·²æœ‰çš„è¡¥ä¸å’Œä¸¤ä¸ªç‰ˆæœ¬ä¹‹é—´çš„ä»£ç å·®å¼‚ã€‚",
    "task_description_quote": "BackportBench defines the backporting problem as generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.",
    "dimension": "è‡ªåŠ¨åŒ–è¡¥ä¸å›ç§»æ¤çš„æœ‰æ•ˆæ€§ã€å¤šè¯­è¨€èƒ½åŠ›ã€å¤„ç†è·¨æ–‡ä»¶ä¸å…¼å®¹æ€§çš„èƒ½åŠ›",
    "dimension_quote": "To facilitate the development and evaluation of automated backporting techniques... BackportBench is a multilingual benchmark... This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges.",
    "evaluation_method": "åœ¨å¯æ‰§è¡Œçš„Dockerç¯å¢ƒä¸­è¿è¡Œç›¸å…³æµ‹è¯•ç”¨ä¾‹è¿›è¡ŒéªŒè¯",
    "evaluation_method_quote": "each task instance provides an executable Docker environment with scripts for running relevant test cases, thereby aligns with the criteria for a successful benchmark [5].",
    "context_dependency": "ä»“åº“çº§åˆ«ï¼Œæ¶‰åŠè·¨æ–‡ä»¶çš„ä¸å…¼å®¹æ€§å¤„ç†",
    "context_dependency_quote": "This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges.",
    "problem_domain": "è½¯ä»¶å®‰å…¨ã€è½¯ä»¶ç»´æŠ¤ã€æ¼æ´ä¿®å¤",
    "problem_domain_quote": "BackportBench contains 202 real-world vulnerability patch backporting task instances curated from three popular OSS ecosystems... To remove such vulnerabilities... patch backporting is a helpful practice that adapts patches for other versions and makes it easier to deliver patches to users on older branches.",
    "problem_difficulty": "ç°å®ä¸–ç•Œå·¥ç¨‹çº§ï¼Œæ¶‰åŠé€»è¾‘å’Œç»“æ„æ€§å˜æ›´",
    "problem_difficulty_quote": "the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes.",
    "language": "Python, Java, JavaScript",
    "language_quote": "BackportBench contains 202 patch backporting problems from PyPI, Maven, and npm, covering Python, Java, and JavaScript, respectively.",
    "data_size": "åŒ…å«202ä¸ªè¡¥ä¸å›ç§»æ¤ä»»åŠ¡å®ä¾‹",
    "data_size_quote": "BackportBench contains 202 real-world vulnerability patch backporting task instances",
    "source_type": "ä»ä¸‰ä¸ªæµè¡Œçš„å¼€æºè½¯ä»¶ç”Ÿæ€ç³»ç»Ÿï¼ˆPyPI, Maven, npmï¼‰ä¸­æ”¶é›†çš„çœŸå®ä¸–ç•Œæ¼æ´è¡¥ä¸å›ç§»æ¤ä»»åŠ¡",
    "source_type_quote": "BackportBench contains 202 real-world vulnerability patch backporting task instances curated from three popular OSS ecosystems, PyPI , Maven, and npm",
    "last_updated": "2018 (æ ¹æ®è®ºæ–‡ç‰ˆæƒæ—¥æœŸ)ï¼Œä½†arXivç‰ˆæœ¬ä¸º2025å¹´12æœˆ",
    "last_updated_quote": "Â© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ... arXiv:2512.01396v1  [cs.SE]  1 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": "ACMç‰ˆæƒï¼Œå…è®¸ä¸ªäººæˆ–è¯¾å ‚ä½¿ç”¨ï¼Œç¦æ­¢å•†ä¸šç”¨é€”",
    "dataset_license_quote": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.",
    "task_granularity": "ä»£ç ä¿®å¤/è¡¥ä¸ç”Ÿæˆ",
    "task_granularity_quote": "generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.",
    "evaluation_metrics": "é€šè¿‡æµ‹è¯•ç”¨ä¾‹éªŒè¯è¡¥ä¸æœ‰æ•ˆæ€§ï¼Œè€ŒéåŸºäºç­‰ä»·æ€§æˆ–ç›¸ä¼¼æ€§çš„æŒ‡æ ‡",
    "evaluation_metrics_quote": "each task instance provides an executable Docker environment with scripts for running relevant test cases... We argue that whether backported patches achieve equivalence is not the only way to prove the patch is effective.",
    "input_modality": "ä»£ç ï¼ˆä¸¤ä¸ªç‰ˆæœ¬çš„ä»£ç åº“åŠåŸå§‹è¡¥ä¸ï¼‰",
    "input_modality_quote": "generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.",
    "output_modality": "ä»£ç ï¼ˆå›ç§»æ¤åçš„è¡¥ä¸ï¼‰",
    "output_modality_quote": "generating a patch for the unpatched software version",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.",
    "execution_environment": "å¯æ‰§è¡Œçš„Dockerç¯å¢ƒï¼ŒåŒ…å«è¿è¡Œç›¸å…³æµ‹è¯•ç”¨ä¾‹çš„è„šæœ¬",
    "execution_environment_quote": "each task instance provides an executable Docker environment with scripts for running relevant test cases",
    "unique_features": "é¦–ä¸ªé’ˆå¯¹è¡¥ä¸å›ç§»æ¤é—®é¢˜çš„ç»¼åˆæ€§ã€å¤šè¯­è¨€åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼ŒåŒ…å«å¯æ‰§è¡Œç¯å¢ƒå’Œæµ‹è¯•ç”¨ä¾‹ï¼Œå°†é—®é¢˜ä»ä»£ç å—/å‡½æ•°çº§åˆ«æå‡åˆ°ä»“åº“çº§åˆ«ï¼Œæ›´è´´è¿‘ç°å®è½¯ä»¶ç»´æŠ¤æŒ‘æˆ˜ã€‚",
    "unique_features_quote": "the first comprehensive benchmark suite for patch backporting problem... a multilingual benchmark... contains executable Docker environments and test cases for validation... This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges.",
    "data_size_quantity": 202,
    "data_size_unit": "ä¸ªè¡¥ä¸å›ç§»æ¤ä»»åŠ¡å®ä¾‹",
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": null,
    "language_normalized": "['Python', 'Java', 'JavaScript']",
    "dimension_normalized": "['è‡ªåŠ¨åŒ–è¡¥ä¸å›ç§»æ¤çš„æœ‰æ•ˆæ€§', 'å¤šè¯­è¨€èƒ½åŠ›', 'å¤„ç†è·¨æ–‡ä»¶ä¸å…¼å®¹æ€§çš„èƒ½åŠ›']",
    "evaluation_method_normalized": "['é€šè¿‡æµ‹è¯•ç”¨ä¾‹éªŒè¯è¡¥ä¸æœ‰æ•ˆæ€§ï¼Œè€ŒéåŸºäºç­‰ä»·æ€§æˆ–ç›¸ä¼¼æ€§çš„æŒ‡æ ‡']",
    "problem_domain_normalized": "['è½¯ä»¶å®‰å…¨', 'è½¯ä»¶ç»´æŠ¤', 'æ¼æ´ä¿®å¤']",
    "source_type_normalized": "['ä»ä¸‰ä¸ªæµè¡Œçš„å¼€æºè½¯ä»¶ç”Ÿæ€ç³»ç»Ÿï¼ˆPyPI, Maven, npmï¼‰ä¸­æ”¶é›†çš„çœŸå®ä¸–ç•Œæ¼æ´è¡¥ä¸å›ç§»æ¤ä»»åŠ¡']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "ä»…ä¾›å­¦æœ¯ç ”ç©¶",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.01255_output/content.md",
    "benchmark_name": "ARENAJS",
    "benchmark_name_quote": "we construct ARENAJSâ€”the first systematic benchmark for LLM-based JavaScript vulnerability detection",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "In this paper, we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection... we propose FORGEJS... Then, we use FORGEJS to construct ARENAJSâ€”the first systematic benchmark for LLM-based JavaScript vulnerability detection",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨JavaScriptæ¼æ´æ£€æµ‹æ–¹é¢çš„èƒ½åŠ›",
    "task_description_quote": "evaluating LLMsâ€™ capability in JavaScript vulnerability detection",
    "dimension": "æ¼æ´æ£€æµ‹èƒ½åŠ›ã€æ¨ç†å……åˆ†æ€§ã€é²æ£’æ€§ã€ç°å®ä¸–ç•Œå¯ç”¨æ€§",
    "dimension_quote": "reveals key limitations in reasoning sufficiency, robustness, and real-world usability",
    "evaluation_method": "ä½¿ç”¨JUDGEJSè‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«æç¤ºæ¨¡æ¿ã€å“åº”è§£æã€æ ‡ç­¾å¯¹é½å’Œç¨³å¥è¯„åˆ†ï¼Œåˆ©ç”¨è¯­ä¹‰ç­‰ä»·å’Œæ¨¡ç³ŠåŒ¹é…ï¼Œåœ¨å‡½æ•°çº§å’Œé¡¹ç›®çº§ç”Ÿæˆç»Ÿä¸€çš„ã€ä¸¥æ ¼çš„ã€å¯è¿½æº¯çš„æŒ‡æ ‡å¥—ä»¶ï¼ŒåŒ…æ‹¬F1ã€å‡é˜³æ€§ç‡ï¼ˆFPRï¼‰ä»¥åŠå·¥ç¨‹çº¦æŸä¸‹çš„æ£€æµ‹æ•ˆèƒ½",
    "evaluation_method_quote": "we propose JUDGEJS, an automated evaluation framework that spans prompt templates, response parsing, label alignment, and robust scoring; leveraging semantic equivalence and fuzzy matching, it harmonizes heterogeneous outputs and yields a unified, rigorous, and traceable metric suite across function- and project-levels, including F1, false positive rate (FPR), and detection efficacy under engineering constraints.",
    "context_dependency": "å‡½æ•°çº§å’Œé¡¹ç›®çº§ï¼ˆå®Œæ•´é¡¹ç›®ï¼‰",
    "context_dependency_quote": "supports both function-level and project-level evaluation",
    "problem_domain": "JavaScriptå®‰å…¨æ¼æ´æ£€æµ‹ï¼Œæ¶µç›–å‰ç«¯ï¼ˆå¦‚DOM-based XSSï¼‰ã€åç«¯ï¼ˆå¦‚SQLæ³¨å…¥ã€å‘½ä»¤æ³¨å…¥ã€åŸå‹æ±¡æŸ“ï¼‰å’Œå…¨æ ˆç¯å¢ƒ",
    "problem_domain_quote": "JavaScript vulnerability patterns vary substantially across these environments: backend prototype pollution... while frontend prototype pollution predominantly causes DOM-based XSS or client-side denial of service.",
    "problem_difficulty": "ç°å®ä¸–ç•Œå·¥ä¸šçº§ï¼Œåæ˜ çœŸå®ä»“åº“çš„å¤æ‚æ€§",
    "problem_difficulty_quote": "fails to reflect the complexity of real-world repositories",
    "language": "JavaScript",
    "language_quote": "benchmark for JavaScript vulnerability detection",
    "data_size": "è¦†ç›–æ•°åƒä¸ªçœŸå®çš„JavaScripté¡¹ç›®",
    "data_size_quote": "A benchmark that spans thousands of real JavaScript projects",
    "source_type": "èšåˆå¼‚æ„æ¥æºï¼Œç»“åˆçœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®",
    "source_type_quote": "aggregates heterogeneous sources... combining real-world and synthetic data",
    "last_updated": "2025-12-01 (æ ¹æ®arXivç‰ˆæœ¬æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2512.01255v1  [cs.CR]  1 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œé€šè¿‡FORGEJSè‡ªåŠ¨ç”Ÿæˆæ¡†æ¶æ„å»º",
    "build_type_quote": "we propose FORGEJS, the first automatic benchmark generation framework... Then, we use FORGEJS to construct ARENAJS",
    "contamination_status": "é€šè¿‡ä½¿ç”¨å®Œæ•´é¡¹ç›®è€Œéå•æ–‡ä»¶ç‰‡æ®µã€æ„å»ºä¿®å¤å‰åé¡¹ç›®å¯¹æ¥ç›´æ¥é‡åŒ–å‡é˜³æ€§ã€ä»¥åŠç³»ç»Ÿå¼•å…¥å››ç§å¢å¼ºç­–ç•¥æ¥ç ´åå¯¹æ–‡ä»¶åã€å¯¼å…¥è·¯å¾„å’Œæ³¨é‡Šçš„ä¾èµ–ï¼Œæ—¨åœ¨é¿å…é«˜ä¼°",
    "contamination_status_quote": "To avoid overestimation, FORGEJS uses complete projects rather than single-file snippets, constructs pre- and post-fix project pairs to directly quantify false positives and localize error sources, and systematically introduces four augmentation strategies to disrupt reliance on filenames, import paths, and comments.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ¼æ´æ£€æµ‹ï¼ˆåŒ…æ‹¬å®šä½æ¼æ´æ–‡ä»¶ã€å‡½æ•°ã€CWEç±»å‹å’Œå…·ä½“ä»£ç è¡Œï¼‰",
    "task_granularity_quote": "evaluation protocols commonly assess only whether vulnerability is detected and ignore reasoning quality such as whether the model can correctly localize the vulnerable file, function, and CWE type.",
    "evaluation_metrics": "F1åˆ†æ•°ã€å‡é˜³æ€§ç‡ï¼ˆFPRï¼‰ã€VD-SæŒ‡æ ‡",
    "evaluation_metrics_quote": "including F1, false positive rate (FPR), and detection efficacy under engineering constraints... from the perspective of the VD-S metric [6]",
    "input_modality": "JavaScriptä»£ç ï¼ˆå‡½æ•°æˆ–å®Œæ•´é¡¹ç›®ï¼‰",
    "input_modality_quote": "supports both function-level and project-level evaluation",
    "output_modality": "æ¼æ´æ£€æµ‹ç»“æœï¼ˆåŒ…æ‹¬æ˜¯å¦å­˜åœ¨æ¼æ´ã€CWEç±»å‹ã€å®šä½ä¿¡æ¯ã€æ¨ç†ç­‰ï¼‰",
    "output_modality_quote": "evaluation protocols commonly assess only whether vulnerability is detected and ignore reasoning quality such as whether the model can correctly localize the vulnerable file, function, and CWE type.",
    "task_io_type": "ä»£ç åˆ°å®‰å…¨åˆ†æ",
    "task_io_type_quote": "evaluating LLMsâ€™ capability in JavaScript vulnerability detection",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªåŸºäºLLMçš„JavaScriptæ¼æ´æ£€æµ‹ç³»ç»ŸåŸºå‡†ï¼›éµå¾ªä¸‰ä¸ªæ„å»ºåŸåˆ™ï¼šå…¨é¢æ€§ã€ä¸ä½ä¼°ã€ä¸é«˜ä¼°ï¼›è¦†ç›–218ç§CWEç±»å‹ï¼›æ”¯æŒå‡½æ•°çº§å’Œé¡¹ç›®çº§è¯„ä¼°ï¼›é€šè¿‡FORGEJSè‡ªåŠ¨ç”Ÿæˆï¼›åŒ…å«JUDGEJSè‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼›æ—¨åœ¨å®¢è§‚å®šä¹‰LLMåœ¨å¯é‡å¤ã€å¯æ‰©å±•ã€éƒ¨ç½²å—é™æ¡ä»¶ä¸‹çš„ç°å®æ€§èƒ½ä¸Šä¸‹é™ã€‚",
    "unique_features_quote": "we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection that directly address these limitations: (1) comprehensiveness, (2) no underestimation, and (3) no overestimation... FORGEJS aggregates heterogeneous sources, covers 218 CWE types... supports both function-level and project-level evaluation... Taken together, these principles aim to objectively define the realistic upper and lower bounds of LLM performance under reproducible, scalable, deployment-constrained conditions.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": 1,
    "language_normalized": "['JavaScript']",
    "dimension_normalized": "['æ¼æ´æ£€æµ‹èƒ½åŠ›', 'æ¨ç†å……åˆ†æ€§', 'é²æ£’æ€§', 'ç°å®ä¸–ç•Œå¯ç”¨æ€§']",
    "evaluation_method_normalized": "['F1åˆ†æ•°', 'å‡é˜³æ€§ç‡ï¼ˆFPRï¼‰', 'VD-SæŒ‡æ ‡']",
    "problem_domain_normalized": "['JavaScriptå®‰å…¨æ¼æ´æ£€æµ‹', 'å‰ç«¯ï¼ˆå¦‚DOM-based XSSï¼‰', 'åç«¯ï¼ˆå¦‚SQLæ³¨å…¥ã€å‘½ä»¤æ³¨å…¥ã€åŸå‹æ±¡æŸ“ï¼‰', 'å…¨æ ˆç¯å¢ƒ']",
    "source_type_normalized": "['èšåˆå¼‚æ„æ¥æº', 'çœŸå®ä¸–ç•Œ', 'åˆæˆæ•°æ®']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2512.03421_output/content.md",
    "benchmark_name": "BugT",
    "benchmark_name_quote": "This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. ... We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMsâ€™ capabilities.",
    "dataset_url": "https://github.com/Xucranger/PLofLBFL",
    "dataset_url_quote": "To facilitate future study, we share our source code and experimental data in a GitHub repository 1. 1https://github.com/Xucranger/PLofLBFL",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆå­¦è€…ç¨‹åºä¸­è¿›è¡Œæ•…éšœå®šä½ï¼ˆFault Localizationï¼‰çš„æ€§èƒ½ã€‚",
    "task_description_quote": "This study evaluates the fault localization performance of six closed-source LLMs (e.g., OpenAI o3, GPT-3.5-Turbo, etc.) and seven open-source LLMs (e.g., DeepSeekR1, Llama3, etc.) across Codeflaws, Condefects, and BugT datasets.",
    "dimension": "æ•…éšœå®šä½çš„å‡†ç¡®æ€§ã€æ•ˆç‡ã€å¯ç”¨æ€§ï¼Œä»¥åŠå¯¹é—®é¢˜éš¾åº¦çš„é²æ£’æ€§ã€‚",
    "dimension_quote": "To investigate these issues, we aim to empirically assess the performance of different LLMs across various datasets, evaluating their accuracy, efficiency, and usability in fault localization for novice programs. ... We examine how problem difficulty levels across three datasets affect LLMsâ€™ fault localization performance, providing insights into model behavior at varying complexity levels.",
    "evaluation_method": "åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°LLMsçš„æ•…éšœå®šä½æ€§èƒ½ï¼Œå¹¶ä¸ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚SBFLã€MBFLï¼‰è¿›è¡Œæ¯”è¾ƒã€‚",
    "evaluation_method_quote": "This study evaluates the fault localization performance of six closed-source LLMs (e.g., OpenAI o3, GPT-3.5-Turbo, etc.) and seven open-source LLMs (e.g., DeepSeekR1, Llama3, etc.) across Codeflaws, Condefects, and BugT datasets. All closed-source LLMs outperform traditional SBFL and MBFL methods...",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "ç¼–ç¨‹æ•™è‚²ï¼Œåˆå­¦è€…ç¼–ç¨‹ã€‚",
    "problem_domain_quote": "Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. ... This study evaluates the fault localization performance ... for novice programs.",
    "problem_difficulty": "åŒ…å«ä¸åŒéš¾åº¦çº§åˆ«çš„é—®é¢˜ï¼Œä»ç®€å•åˆ°å¤æ‚ã€‚",
    "problem_difficulty_quote": "LLM accuracy decreases as problem difficulty increases in Codeflaws and Condefects, but top models maintain high accuracy even at peak difficulty in BugT, suggesting its lower complexity. ... We examine how problem difficulty levels across three datasets affect LLMsâ€™ fault localization performance...",
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "è‡ªå»ºçš„åŒ…å«çœŸå®ç¼–ç¨‹æ•…éšœçš„æ•°æ®é›†ã€‚",
    "source_type_quote": "We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMsâ€™ capabilities.",
    "last_updated": "2025-12-04",
    "last_updated_quote": "Preprint submitted to Journal of LATEX Templates December 4, 2025",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "We introduce a new self-created dataset with real programming faults...",
    "contamination_status": "ä¸“é—¨è®¾è®¡ä»¥å‡è½»æ•°æ®æ³„éœ²é—®é¢˜ã€‚",
    "contamination_status_quote": "BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ•…éšœå®šä½",
    "task_granularity_quote": "This study evaluates the fault localization performance...",
    "evaluation_metrics": "å‡†ç¡®æ€§ï¼ˆaccuracyï¼‰",
    "evaluation_metrics_quote": "LLM accuracy decreases as problem difficulty increases...",
    "input_modality": "åŒ…å«æ•…éšœçš„ä»£ç ",
    "input_modality_quote": "Novice Program refers to code written by novice programmers, usually in the early stages of learning programming. These programs usually contain multiple errors...",
    "output_modality": "æ•…éšœä½ç½®åŠè§£é‡Š",
    "output_modality_quote": "LLM generated fault explanations demonstrate significant value for novice programmer assistance...",
    "task_io_type": "ä»£ç åˆ°æ•…éšœä½ç½®/è§£é‡Š",
    "task_io_type_quote": "LLM-based fault localization methods leverage the modelsâ€™ understanding of program syntax and semantics to automatically localize faults in code, offering more precise and contextually relevant suggestions...",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“é—¨é’ˆå¯¹åˆå­¦è€…ç¨‹åºï¼ˆNovice Programsï¼‰æ„å»ºï¼Œæ—¨åœ¨å‡è½»LLMè¯„ä¼°ä¸­çš„æ•°æ®æ³„éœ²ï¼ˆdata leakageï¼‰é—®é¢˜ï¼ŒåŒ…å«çœŸå®çš„ç¼–ç¨‹æ•…éšœã€‚",
    "unique_features_quote": "BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. ... We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMsâ€™ capabilities.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": 4,
    "language_normalized": "[]",
    "dimension_normalized": "['æ•…éšœå®šä½çš„å‡†ç¡®æ€§', 'æ•ˆç‡', 'å¯ç”¨æ€§', 'å¯¹é—®é¢˜éš¾åº¦çš„é²æ£’æ€§']",
    "evaluation_method_normalized": "['å‡†ç¡®æ€§ï¼ˆaccuracyï¼‰']",
    "problem_domain_normalized": "['ç¼–ç¨‹æ•™è‚²', 'åˆå­¦è€…ç¼–ç¨‹']",
    "source_type_normalized": "['è‡ªå»ºçš„åŒ…å«çœŸå®ç¼–ç¨‹æ•…éšœçš„æ•°æ®é›†']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  }
]