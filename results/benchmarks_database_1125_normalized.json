[
  {
    "source_paper": "2511.20403_output/content.md",
    "benchmark_name": "CLASSES2TEST",
    "benchmark_name_quote": "We introduce the CLASSES2TEST dataset, which maps Java classes under test to their corresponding test classes",
    "is_original_proposal": "Yesï¼Œæœ¬æ–‡æ˜¯è¯¥æ•°æ®é›†çš„åŸå§‹å‘å¸ƒè®ºæ–‡",
    "is_original_proposal_quote": "We introduce the CLASSES2TEST dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics",
    "dataset_url": "https://anonymous.4open.science/r/classes2test",
    "dataset_url_quote": "1https://anonymous.4open.science/r/classes2test",
    "task_description": "ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„Javaå•å…ƒæµ‹è¯•çš„è´¨é‡ï¼Œæ”¯æŒç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ¯”è¾ƒä¸åŒLLMå’Œæç¤ºç­–ç•¥",
    "task_description_quote": "AGONETEST does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions.",
    "dimension": "å•å…ƒæµ‹è¯•è´¨é‡è¯„ä¼°ï¼ŒåŒ…æ‹¬ç¼–è¯‘æˆåŠŸç‡ã€ä»£ç è¦†ç›–ç‡ã€ç¼ºé™·æ£€æµ‹èƒ½åŠ›ã€æµ‹è¯•å¼‚å‘³ç­‰",
    "dimension_quote": "a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",
    "evaluation_method": "é›†æˆé«˜çº§è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚å˜å¼‚åˆ†æ•°å’Œæµ‹è¯•å¼‚å‘³ï¼Œè¿›è¡Œç»¼åˆè¯„ä¼°",
    "evaluation_method_quote": "a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",
    "context_dependency": "ç±»çº§åˆ«æµ‹è¯•ï¼Œæ¶µç›–æ–¹æ³•äº¤äº’å’Œå…±äº«çŠ¶æ€",
    "context_dependency_quote": "Our approach focuses on class-level test code evaluation, which is closer to real-world practices as it covers method interactions and shared state",
    "problem_domain": "è½¯ä»¶æµ‹è¯•ï¼ŒJavaå•å…ƒæµ‹è¯•ç”Ÿæˆ",
    "problem_domain_quote": "Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly.",
    "problem_difficulty": "ç°å®ä¸–ç•Œè½¯ä»¶é¡¹ç›®çº§åˆ«ï¼Œæ¯”å•æ–¹æ³•æµ‹è¯•æ›´å¤æ‚",
    "problem_difficulty_quote": "This extended dataset makes it possible to assess the test performance of an LLM on a more complex scope (the entire class) than the single method.",
    "language": "Java",
    "language_quote": "An annotated open source Java project dataset extending METHODS2TEST [9]",
    "data_size": "åŸºäº9,410ä¸ªGitHubä»“åº“çš„æ•°æ®é›†",
    "data_size_quote": "AGONETEST offers far broader applicability by using a dataset of 9,410 GitHub repositories",
    "source_type": "æ‰©å±•è‡ªMETHODS2TESTæ•°æ®é›†çš„Javaå¼€æºé¡¹ç›®",
    "source_type_quote": "An annotated open source Java project dataset extending METHODS2TEST [9]",
    "last_updated": 2025,
    "last_updated_quote": "arXiv:2511.20403v1  [cs.SE]  25 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ŒåŸºäºç°æœ‰æ•°æ®é›†æ‰©å±•",
    "build_type_quote": "Leveraging the METHODS2TEST dataset [9], we developed a new dataset specifically aimed at comparing human-written tests with those produced by LLMs.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ç±»çº§åˆ«å•å…ƒæµ‹è¯•ç”Ÿæˆ",
    "task_granularity_quote": "Our approach focuses on class-level test code evaluation, which is closer to real-world practices as it covers method interactions and shared state",
    "evaluation_metrics": "å˜å¼‚åˆ†æ•°ã€æµ‹è¯•å¼‚å‘³ã€ä»£ç è¦†ç›–ç‡",
    "evaluation_metrics_quote": "a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",
    "input_modality": "Javaç±»ä»£ç ",
    "input_modality_quote": "which maps classes under test to their related test classes",
    "output_modality": "å•å…ƒæµ‹è¯•ä»£ç ",
    "output_modality_quote": "for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "which maps Java classes under test to their corresponding test classes",
    "execution_environment": "æ”¯æŒæ‰€æœ‰Java LTSç‰ˆæœ¬çš„é¡¹ç›®ç¯å¢ƒ",
    "execution_environment_quote": "AGONETEST overcomes this barrier by supporting all Java LTS versions.",
    "unique_features": "ä¸“æ³¨äºç±»çº§åˆ«æµ‹è¯•è¯„ä¼°ï¼Œæ”¯æŒå¤šç§LLMå’Œæç¤ºç­–ç•¥çš„æ¯”è¾ƒï¼Œæä¾›ç«¯åˆ°ç«¯çš„è‡ªåŠ¨åŒ–è¯„ä¼°æµæ°´çº¿",
    "unique_features_quote": "AGONETEST shifts the focus to the generation of class-level tests. Our approach makes it possible to use up-to-date LLMs and not constrain prompt design (our prompts can be customized), thereby handling more complex, real-world scenarios.",
    "data_size_quantity": 9410,
    "data_size_unit": "ä¸ªGitHubä»“åº“",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java']",
    "dimension_normalized": "['å•å…ƒæµ‹è¯•è´¨é‡è¯„ä¼°', 'ç¼–è¯‘æˆåŠŸç‡', 'ä»£ç è¦†ç›–ç‡', 'ç¼ºé™·æ£€æµ‹èƒ½åŠ›', 'æµ‹è¯•å¼‚å‘³']",
    "evaluation_method_normalized": "['å˜å¼‚åˆ†æ•°', 'æµ‹è¯•å¼‚å‘³', 'ä»£ç è¦†ç›–ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶æµ‹è¯•', 'Javaå•å…ƒæµ‹è¯•ç”Ÿæˆ']",
    "source_type_normalized": "['æ‰©å±•è‡ªMETHODS2TESTæ•°æ®é›†çš„Javaå¼€æºé¡¹ç›®']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•æ–‡ä»¶",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.21380_output/content.md",
    "benchmark_name": "ROCODE, LogHub2.0",
    "benchmark_name_quote": "Two projects (i.e., ROCODE [15] and LogHub2.0 [16]) are selected for the following experiments.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ•°æ®é›†é€‚åº”ä»»åŠ¡ - å°†è½¯ä»¶å·¥ç¨‹ç ”ç©¶å·¥å…·è‡ªåŠ¨é€‚é…åˆ°æ–°çš„æ•°æ®é›†ï¼Œæ„å»ºå¯è¿è¡Œçš„å®éªŒå¹¶è·å–æ‰§è¡Œç»“æœ",
    "task_description_quote": "Our objective is to automatically modify ğ‘…ğ·or ğ‘…ğ‘‡, construct a runnable experiment, and obtain its execution results.",
    "dimension": "å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ•°æ®é›†é€‚åº”ä»»åŠ¡ä¸­çš„èƒ½åŠ›è¯„ä¼°",
    "dimension_quote": "This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks.",
    "evaluation_method": "äº”é˜¶æ®µè¯„ä¼°æµç¨‹ï¼šæ–‡ä»¶ç†è§£ã€ä»£ç ç¼–è¾‘ã€å‘½ä»¤ç”Ÿæˆã€éªŒè¯å’Œæœ€ç»ˆæ‰§è¡Œï¼Œæµ‹é‡æˆåŠŸç‡å¹¶åˆ†æå¤±è´¥æ¨¡å¼",
    "evaluation_method_quote": "Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®ç¯å¢ƒï¼Œéœ€è¦ç†è§£æ•´ä¸ªä»£ç ä»“åº“çš„æ¶æ„",
    "context_dependency_quote": "Before modifying code or generating commands for a repository, a multi-agent system must browse the essential files in a repository to understand its architecture.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ç ”ç©¶ï¼ŒåŒ…æ‹¬ä»£ç ç”Ÿæˆã€bugä¿®å¤ã€æ€§èƒ½åˆ†æå’Œå®‰å…¨ç­‰é¢†åŸŸ",
    "problem_domain_quote": "In areas ranging from code generation and bug fixing to performance analysis and security, researchers are constantly proposing new techniques",
    "problem_difficulty": "å¤æ‚è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œéœ€è¦å¤šæ­¥éª¤åè°ƒå’Œè¿­ä»£ä¿®å¤",
    "problem_difficulty_quote": "Most of the evaluated multi-agent systems failed to complete the assigned tasks. Under such circumstances, nearly all results would be classified as failures",
    "language": "Python",
    "language_quote": "To ensure a consistent and manageable experimental environment, we retained only artifacts implemented in Python.",
    "data_size": "ä»é¡¶çº§è½¯ä»¶å·¥ç¨‹ä¼šè®®ï¼ˆFSE, ICSE, ASE, ISSTAï¼‰2024-2025å¹´æ¥å—çš„è®ºæ–‡ä¸­ç­›é€‰çš„å¯é‡ç”¨ç ”ç©¶æ„ä»¶",
    "data_size_quote": "We began by collecting all papers accepted by the top four SE (FSE, ICSE, ASE and ISSTA) in 2024-2025 that were either (i) awarded a Reusable Artifact badge or (ii) explicitly identified by the authors as providing reusable artifacts.",
    "source_type": "å­¦æœ¯ç ”ç©¶æ„ä»¶ï¼Œæ¥è‡ªé¡¶çº§è½¯ä»¶å·¥ç¨‹ä¼šè®®çš„å¯é‡ç”¨ç ”ç©¶é¡¹ç›®",
    "source_type_quote": "To construct a representative set of high-quality SE research artifacts, we followed a systematic multi-stage selection process",
    "last_updated": 2025,
    "last_updated_quote": "arXiv:2511.21380v1  [cs.SE]  26 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºçš„ç ”ç©¶æ„ä»¶é›†åˆ",
    "build_type_quote": "We began by collecting all papers accepted by the top four SE (FSE, ICSE, ASE and ISSTA) in 2024-2025",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç¼–è¾‘ã€æ–‡ä»¶åˆ›å»ºã€å‘½ä»¤ç”Ÿæˆã€éªŒè¯ä¿®å¤",
    "task_granularity_quote": "Edit and create necessary files... Generate and execute the commands... Validating and repairing",
    "evaluation_metrics": "æˆåŠŸç‡ã€ç»“æ„ç›¸ä¼¼åº¦ï¼ˆä»7.25%åˆ°67.14%ï¼‰ã€å®ŒæˆçŠ¶æ€è®°å½•",
    "evaluation_metrics_quote": "Results show that... substantially improve structural similarity to ground truth (from 7.25% to 67.14%)",
    "input_modality": "ä»£ç ä»“åº“ã€è‡ªç„¶è¯­è¨€æç¤º",
    "input_modality_quote": "Each multi-agent system is provided with a processed repository... along with a simple prompt that specifies the adaptation task",
    "output_modality": "ä¿®æ”¹åçš„ä»£ç ã€ç”Ÿæˆçš„è„šæœ¬å‘½ä»¤ã€æ‰§è¡Œç»“æœ",
    "output_modality_quote": "the code adaptations performed to ensure compatibility, the executable scripts or commands derived for running the experiment, and the results produced by executing those scripts or commands",
    "task_io_type": "ä»£ç åˆ°ä»£ç çš„é€‚åº”ä»»åŠ¡",
    "task_io_type_quote": "automatically modify ğ‘…ğ·or ğ‘…ğ‘‡, construct a runnable experiment, and obtain its execution results",
    "execution_environment": "Pythonç¯å¢ƒï¼Œéœ€è¦å¯é çš„éƒ¨ç½²ç¯å¢ƒ",
    "execution_environment_quote": "This choice allows for reliable environment deployment and isolates our investigation from environment setup challenges",
    "unique_features": "ä¸“æ³¨äºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ•°æ®é›†é€‚åº”ä»»åŠ¡ä¸­çš„è¡¨ç°è¯„ä¼°ï¼Œé‡‡ç”¨äº”é˜¶æ®µè¯„ä¼°æµç¨‹ï¼Œç ”ç©¶æç¤ºçº§å¹²é¢„å¯¹æ€§èƒ½çš„å½±å“",
    "unique_features_quote": "This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. Through a five-stage evaluation pipeline... we measure success rates, analyze failure patterns, and assess prompt-based interventions",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ•°æ®é›†é€‚åº”ä»»åŠ¡ä¸­çš„èƒ½åŠ›è¯„ä¼°']",
    "evaluation_method_normalized": "['æˆåŠŸç‡', 'ç»“æ„ç›¸ä¼¼åº¦', 'å®ŒæˆçŠ¶æ€è®°å½•']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹ç ”ç©¶', 'ä»£ç ç”Ÿæˆ', 'bugä¿®å¤', 'æ€§èƒ½åˆ†æ', 'å®‰å…¨']",
    "source_type_normalized": "['å­¦æœ¯ç ”ç©¶æ„ä»¶', 'é¡¶çº§è½¯ä»¶å·¥ç¨‹ä¼šè®®', 'å¯é‡ç”¨ç ”ç©¶é¡¹ç›®']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.21022_output/content.md",
    "benchmark_name": "EDAPIBench",
    "benchmark_name_quote": "We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances.",
    "is_original_proposal": "Yesï¼Œæœ¬æ–‡æ˜¯è¯¥æ•°æ®é›†çš„åŸå§‹å‘å¸ƒè®ºæ–‡",
    "is_original_proposal_quote": "We introduce EDAPIBench, a dedicated benchmark... We construct EDAPIBenchâ€”the first dedicated benchmark for evaluating deprecated API knowledge editing in LLMs",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ä¸­åºŸå¼ƒAPIçŸ¥è¯†ç¼–è¾‘çš„æ€§èƒ½ï¼Œä¸“é—¨ç”¨äºæµ‹è¯•æ¨¡å‹ç¼–è¾‘æŠ€æœ¯èƒ½å¦æœ‰æ•ˆæ›´æ–°åºŸå¼ƒAPIçŸ¥è¯†å¹¶ç”Ÿæˆæœ€æ–°çš„API",
    "task_description_quote": "a dedicated benchmark for evaluating deprecated API knowledge editing in LLMs... whether existing model editing methods can effectively update deprecated API knowledge within LLMs, and enable the edited models to correctly replace deprecated APIs with up-to-date ones",
    "dimension": "æœ‰æ•ˆæ€§ã€æ³›åŒ–æ€§ã€å¯ç§»æ¤æ€§ã€ç‰¹å¼‚æ€§",
    "dimension_quote": "comprehensively assess model editing performance across four key dimensions: Effectiveness, Generalization, Portability, Specificity",
    "evaluation_method": "åŸºäºå››ä¸ªç»´åº¦çš„è¯„ä¼°ï¼šæœ‰æ•ˆæ€§ï¼ˆç¼–è¾‘åæ¨¡å‹æ˜¯å¦ç”Ÿæˆæœ€æ–°APIï¼‰ã€æ³›åŒ–æ€§ï¼ˆè¯­ä¹‰ç­‰ä»·ä½†è¯­æ³•å˜åŒ–çš„è¾“å…¥ï¼‰ã€å¯ç§»æ¤æ€§ï¼ˆä¸åŒè¾“å…¥ä½†æ¶‰åŠç›¸åŒåºŸå¼ƒAPIï¼‰ã€ç‰¹å¼‚æ€§ï¼ˆä¿æŒä¸ç¼–è¾‘ä»»åŠ¡æ— å…³è¾“å…¥çš„è¡Œä¸ºä¸€è‡´æ€§ï¼‰",
    "evaluation_method_quote": "Effectiveness: Measuring whether the edited model generates the up-to-date API for the original inputs; Generalization: Measuring whether it generates the up-to-date API on semantically equivalent but syntactically varied inputs; Portability: Measuring whether it generates the up-to-date API across different inputs... Specificity: Measuring whether it preserves consistent pre-editing behavior on inputs unrelated to the editing task",
    "context_dependency": "å•å‡½æ•°çº§åˆ«çš„ä»£ç è¡¥å…¨ä¸Šä¸‹æ–‡",
    "context_dependency_quote": "Our study frames the model editing scenario as a code completion task... we extract lines preceding the API invocation as candidate editing inputs (to prompt LLM API completion) and the invocation line as the target API (ground truth)",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ã€APIç»´æŠ¤ã€ç¬¬ä¸‰æ–¹åº“æ›´æ–°",
    "problem_domain_quote": "deprecated APIs from 8 popular Python libraries... covering eight popular Python libraries such as PyTorch and TensorFlow",
    "problem_difficulty": "å®é™…å·¥ç¨‹çº§éš¾åº¦ï¼Œæ¶‰åŠçœŸå®ä¸–ç•ŒåºŸå¼ƒAPIçš„è¯†åˆ«å’Œæ›´æ–°",
    "problem_difficulty_quote": "Using these mappings, we extract 65,596 real-world functions from GitHub that call the up-to-date APIs",
    "language": "Python",
    "language_quote": "deprecated APIs from 8 popular Python libraries... covering eight popular Python libraries such as PyTorch and TensorFlow",
    "data_size": "åŒ…å«70å¤šä¸ªåºŸå¼ƒAPIï¼Œè¶…è¿‡3000ä¸ªç¼–è¾‘å®ä¾‹ï¼Œä»GitHubæå–äº†65,596ä¸ªçœŸå®ä¸–ç•Œå‡½æ•°",
    "data_size_quote": "featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances... we extract 65,596 real-world functions from GitHub",
    "source_type": "ä»GitHubçœŸå®é¡¹ç›®ä»£ç ä¸­æå–ï¼ŒåŸºäºå·²éªŒè¯çš„APIæ˜ å°„å…³ç³»",
    "source_type_quote": "We begin with 145 verified API mappings (deprecated â†’up-to-date) from Wang et al. [49]... Using these mappings, we extract 65,596 real-world functions from GitHub that call the up-to-date APIs",
    "last_updated": "2025-2026",
    "last_updated_quote": "Publication date: November 2026. arXiv:2511.21022v1 [cs.SE] 26 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œå…¨è‡ªåŠ¨æ„å»º",
    "build_type_quote": "which can be fully automatically constructed... with fully automated construction",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "APIçº§åˆ«çš„ä»£ç è¡¥å…¨",
    "task_granularity_quote": "code completion tasks... during code completion, LLMs frequently suggest deprecated API invocations",
    "evaluation_metrics": "åŸºäºå››ä¸ªç»´åº¦çš„å®šæ€§è¯„ä¼°ï¼šæœ‰æ•ˆæ€§ã€æ³›åŒ–æ€§ã€å¯ç§»æ¤æ€§ã€ç‰¹å¼‚æ€§",
    "evaluation_metrics_quote": "Effectiveness, Generalization, Portability, Specificity",
    "input_modality": "ä»£ç ç‰‡æ®µ",
    "input_modality_quote": "the editing input (a code snippet to be completed)",
    "output_modality": "APIè°ƒç”¨ä»£ç ",
    "output_modality_quote": "the specific correct, up-to-date API that should replace the deprecated one",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "code completion task... generating code",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªä¸“é—¨é’ˆå¯¹åºŸå¼ƒAPIçŸ¥è¯†ç¼–è¾‘çš„åŸºå‡†æµ‹è¯•ï¼Œæ”¯æŒå…¨è‡ªåŠ¨æ„å»ºï¼ŒåŒ…å«å››ä¸ªç»´åº¦çš„ç»¼åˆè¯„ä¼°",
    "unique_features_quote": "the first dedicated benchmark for evaluating deprecated API knowledge editing in LLMs, with fully automated construction, serving as a standardized, rigorous platform",
    "data_size_quantity": 65596,
    "data_size_unit": "ä¸ªå‡½æ•°",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['æœ‰æ•ˆæ€§', 'æ³›åŒ–æ€§', 'å¯ç§»æ¤æ€§', 'ç‰¹å¼‚æ€§']",
    "evaluation_method_normalized": "['æœ‰æ•ˆæ€§', 'æ³›åŒ–æ€§', 'å¯ç§»æ¤æ€§', 'ç‰¹å¼‚æ€§']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'APIç»´æŠ¤', 'ç¬¬ä¸‰æ–¹åº“æ›´æ–°']",
    "source_type_normalized": "['ä»GitHubçœŸå®é¡¹ç›®ä»£ç ä¸­æå–', 'åŸºäºå·²éªŒè¯çš„APIæ˜ å°„å…³ç³»']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç è¡¥å…¨",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.19875_output/content.md",
    "benchmark_name": "CodeFuse-CommitEval",
    "benchmark_name_quote": "We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models.",
    "is_original_proposal": "Yesï¼Œæœ¬æ–‡æ˜¯è¯¥æ•°æ®é›†çš„åŸå§‹å‘å¸ƒè®ºæ–‡",
    "is_original_proposal_quote": "We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models.",
    "dataset_url": "https://figshare.com/s/21fe4ec9cb960b52bffe",
    "dataset_url_quote": "The dataset is publicly available at https://figshare.com/s/21fe4ec9cb960b52bffe.",
    "task_description": "æ£€æµ‹æäº¤ä¿¡æ¯ä¸ä»£ç å˜æ›´ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ï¼ˆMessage-Code Inconsistency, MCIï¼‰",
    "task_description_quote": "A Message-Code Inconsistency (MCI) occurs when the natural language description in a commit message does not accurately reflect the actual modifications in the associated code diff.",
    "dimension": "æäº¤ä¿¡æ¯ä¸ä»£ç å˜æ›´çš„ä¸€è‡´æ€§æ£€æµ‹èƒ½åŠ›",
    "dimension_quote": "evaluate models for MCI detection",
    "evaluation_method": "ä½¿ç”¨Recallã€Precisionã€Specificityç­‰æŒ‡æ ‡è¯„ä¼°æ¨¡å‹æ£€æµ‹ç»“æœ",
    "evaluation_method_quote": "Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%)",
    "context_dependency": "æäº¤ä¿¡æ¯ä¸å¯¹åº”çš„ä»£ç å·®å¼‚ï¼ˆdiffï¼‰",
    "context_dependency_quote": "Each item of the verified dataset contains a commit message, the corresponding code diff, and the ground truth label",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ã€ç‰ˆæœ¬æ§åˆ¶ã€ä»£ç å®¡æŸ¥",
    "problem_domain_quote": "Version control relies on commit messages to convey the rationale for code changes",
    "problem_difficulty": "éœ€è¦è¯­ä¹‰ç†è§£å’Œä¸Šä¸‹æ–‡æ¨ç†çš„å¤æ‚ä»»åŠ¡",
    "problem_difficulty_quote": "purpose inconsistencies require deeper semantic understanding and contextual reasoning",
    "language": "å¤šç§ç¼–ç¨‹è¯­è¨€ï¼ˆåŸºäºApacheCMæ•°æ®é›†çš„å¤šæ ·æ€§ï¼‰",
    "language_quote": "because of its diversity in programming languages and the high-quality commits",
    "data_size": "åŒ…å«æ­£è´Ÿæ ·æœ¬çš„å¹³è¡¡æ•°æ®é›†",
    "data_size_quote": "we generate a balanced dataset with both positive and negative samples",
    "source_type": "åŸºäºApacheCMæ•°æ®é›†ï¼Œé€šè¿‡è§„åˆ™å¼•å¯¼çš„çªå˜ç”Ÿæˆä¸ä¸€è‡´æäº¤ä¿¡æ¯",
    "source_type_quote": "Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2511.19875v1 [cs.SE] 25 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç»“åˆLLMæ•°æ®åˆæˆèƒ½åŠ›ä¸ç°æœ‰æäº¤è¯­æ–™åº“",
    "build_type_quote": "We present a comprehensive pipeline for constructing MCI datasets. By combining the data synthesis capabilities of LLMs with existing commit corpora",
    "contamination_status": "é€šè¿‡åŒé‡éªŒè¯ç¡®ä¿æ•°æ®è´¨é‡",
    "contamination_status_quote": "apply two-fold validation to verify both positive and negative samples",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä¸€è‡´æ€§æ£€æµ‹",
    "task_granularity_quote": "detecting inconsistencies between commit messages and code",
    "evaluation_metrics": "Recall, Precision, Specificity",
    "evaluation_metrics_quote": "average Recall 85.95%, Precision 80.28%, Specificity 63.8%",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆæäº¤ä¿¡æ¯ï¼‰ä¸ä»£ç å·®å¼‚",
    "input_modality_quote": "Each item of the verified dataset contains a commit message, the corresponding code diff",
    "output_modality": "äºŒå…ƒåˆ†ç±»ï¼ˆä¸€è‡´æˆ–ä¸ä¸€è‡´ï¼‰",
    "output_modality_quote": "allowing the models to detect whether the commit is consistent or not",
    "task_io_type": "æ–‡æœ¬ä¸ä»£ç åˆ°åˆ†ç±»",
    "task_io_type_quote": "detect whether the commit is consistent or not",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªä¸“é—¨ç”¨äºMCIæ£€æµ‹çš„åŸºå‡†ï¼ŒåŒ…å«ä¸ƒç§ä¸ä¸€è‡´ç±»å‹ï¼Œæ”¯æŒå¤šç§å¢å¼ºç­–ç•¥è¯„ä¼°",
    "unique_features_quote": "CODEFUSE-COMMITEVAL is the first benchmarking work tailored for comprehensively evaluating LLM's MCI detection ability. We define seven mutation rules to guide powerful LLMs in generating various types of inconsistent commit messages",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['å¤šç§ç¼–ç¨‹è¯­è¨€']",
    "dimension_normalized": "['æäº¤ä¿¡æ¯ä¸ä»£ç å˜æ›´çš„ä¸€è‡´æ€§æ£€æµ‹èƒ½åŠ›']",
    "evaluation_method_normalized": "['Recall', 'Precision', 'Specificity']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ç‰ˆæœ¬æ§åˆ¶', 'ä»£ç å®¡æŸ¥']",
    "source_type_normalized": "['åŸºäºApacheCMæ•°æ®é›†', 'é€šè¿‡è§„åˆ™å¼•å¯¼çš„çªå˜ç”Ÿæˆä¸ä¸€è‡´æäº¤ä¿¡æ¯']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2511.20709_output/content.md",
    "benchmark_name": "DUALGAUGE-BENCH",
    "benchmark_name_quote": "we further curated DUALGAUGE-BENCH, a comprehensive benchmark suite of 154 programming tasks",
    "is_original_proposal": "Yesï¼Œæœ¬æ–‡æ˜¯è¯¥æ•°æ®é›†çš„åŸå§‹å‘å¸ƒè®ºæ–‡",
    "is_original_proposal_quote": "We present DUALGAUGE, the first fully automated benchmarking framework... we also present DUALGAUGE-BENCH, a curated benchmark suite",
    "dataset_url": "https://anonymous.4open.science/r/DualBench-6D1D",
    "dataset_url_quote": "Our system source code, benchmark, and evaluation/study data can all be found at https://anonymous.4open.science/r/DualBench-6D1D",
    "task_description": "è”åˆè¯„ä¼°ä»£ç ç”Ÿæˆçš„å®‰å…¨æ€§å’ŒåŠŸèƒ½æ€§ï¼Œç¡®ä¿ç”Ÿæˆçš„ä»£ç æ—¢æ»¡è¶³åŠŸèƒ½è§„èŒƒåˆä¸ä¼šå¼•å…¥å®‰å…¨æ¼æ´",
    "task_description_quote": "rigorously evaluate the security and correctness of LLM-generated code in unison... ensuring that generated programs fulfill their specifications without introducing vulnerabilities",
    "dimension": "ä»£ç ç”Ÿæˆçš„å®‰å…¨æ€§å’ŒåŠŸèƒ½æ€§è”åˆè¯„ä¼°",
    "dimension_quote": "designed to rigorously evaluate the security and correctness of LLM-generated code in unison",
    "evaluation_method": "åœ¨æ²™ç›’ç¯å¢ƒä¸­æ‰§è¡Œç¨‹åºï¼Œé’ˆå¯¹åŠŸèƒ½å’Œå®‰å…¨æ€§æµ‹è¯•å¥—ä»¶è¿è¡Œï¼ŒåŸºäºæ‰§è¡Œç»“æœè¯„ä¼°æµ‹è¯•é€šè¿‡ç‡å’Œè”åˆæ­£ç¡®æ€§-å®‰å…¨æ€§æŒ‡æ ‡",
    "evaluation_method_quote": "executes it in a sandboxed environment against functional and security test suites, and evaluates the execution results against both test suites to report test pass rates and joint correctness-security metrics",
    "context_dependency": "å•ä»»åŠ¡ä»£ç ç”Ÿæˆï¼ŒåŸºäºè‡ªç„¶è¯­è¨€æç¤ºç”Ÿæˆå®Œæ•´ç¨‹åº",
    "context_dependency_quote": "Given an pure-natural-language prompt as functional specification... generates the model's code output for the prompt",
    "problem_domain": "è·¨é¢†åŸŸç¼–ç¨‹ä»»åŠ¡ï¼Œæ¶µç›–å¤šæ ·åŒ–åŠŸèƒ½é¢†åŸŸ",
    "problem_domain_quote": "spanning diverse functionality domains",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "è¯­è¨€æ— å…³ï¼Œæ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€",
    "language_quote": "This dataset design and curation process allow it to be agnostic to programming languages",
    "data_size": "åŒ…å«154ä¸ªç¼–ç¨‹ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½é…æœ‰åŠŸèƒ½å’Œå®‰å…¨æ€§æµ‹è¯•å¥—ä»¶",
    "data_size_quote": "a comprehensive benchmark suite of 154 programming tasks... Each task/prompt is paired with both a functional test suite... and a security test suite",
    "source_type": "äººå·¥ä¸LLMååŒåˆ›å»ºï¼Œé€šè¿‡å¤šè¯„åˆ†è€…çš„äººç±»ä¸“ä¸šçŸ¥è¯†è¿›è¡Œç²¾ç‚¼å’Œä¿®æ­£",
    "source_type_quote": "constructed these test suites through a human-and-LLM co-creation processâ€”leveraging multiple LLMs to generate candidate tests, then refining and amending these with human expertise of multiple raters",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2511.20709v1 [cs.SE] 24 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ŒåŸºäºè§„èŒƒæµ‹è¯•èŒƒå¼",
    "build_type_quote": "following a specification-based testing paradigm",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "code generation... translating natural language promptsâ€”typically serving as functional specificationsâ€”into programs",
    "evaluation_metrics": "æµ‹è¯•é€šè¿‡ç‡ã€è”åˆæ­£ç¡®æ€§-å®‰å…¨æ€§æŒ‡æ ‡",
    "evaluation_metrics_quote": "report test pass rates and joint correctness-security metrics",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "Given an pure-natural-language prompt as functional specification",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "generates the model's code output for the prompt",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "translating natural language prompts... into programs",
    "execution_environment": "æ²™ç›’éš”ç¦»å®¹å™¨ç¯å¢ƒï¼Œæ”¯æŒä¾èµ–è§£æå’Œç¯å¢ƒé…ç½®",
    "execution_environment_quote": "executes it in a sandboxed environment... The execution engine compiles and runs the model-generated code within isolated containers",
    "unique_features": "é¦–ä¸ªæ”¯æŒå®‰å…¨æ€§å’ŒåŠŸèƒ½æ€§è”åˆè¯„ä¼°çš„åŸºå‡†å¥—ä»¶ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½é…æœ‰è¦†ç›–é©±åŠ¨çš„åŠŸèƒ½å’Œå®‰å…¨æ€§æµ‹è¯•å¥—ä»¶ï¼Œé‡‡ç”¨äººå·¥ä¸LLMååŒåˆ›å»ºè¿‡ç¨‹",
    "unique_features_quote": "the first benchmark suite that pairs each code-generation prompt with dual (functional and security), coverage-enforced test suites... constructed through a human-and-LLM co-creation process",
    "data_size_quantity": 154,
    "data_size_unit": "ä¸ªç¼–ç¨‹ä»»åŠ¡",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['è¯­è¨€æ— å…³', 'å¤šç§ç¼–ç¨‹è¯­è¨€']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„å®‰å…¨æ€§å’ŒåŠŸèƒ½æ€§è”åˆè¯„ä¼°']",
    "evaluation_method_normalized": "['æµ‹è¯•é€šè¿‡ç‡', 'è”åˆæ­£ç¡®æ€§-å®‰å…¨æ€§æŒ‡æ ‡']",
    "problem_domain_normalized": "['è·¨é¢†åŸŸç¼–ç¨‹ä»»åŠ¡', 'å¤šæ ·åŒ–åŠŸèƒ½é¢†åŸŸ']",
    "source_type_normalized": "['äººå·¥ä¸LLMååŒåˆ›å»º', 'å¤šè¯„åˆ†è€…çš„äººç±»ä¸“ä¸šçŸ¥è¯†è¿›è¡Œç²¾ç‚¼å’Œä¿®æ­£']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  }
]