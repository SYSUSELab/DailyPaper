[
  {
    "source_paper": "2511.20403_output/content.md",
    "benchmark_name": "CLASSES2TEST",
    "benchmark_name_quote": "We introduce the CLASSES2TEST dataset, which maps Java classes under test to their corresponding test classes",
    "is_original_proposal": "Yesï¼Œæœ¬æ–‡æ˜¯è¯¥æ•°æ®é›†çš„åŸå§‹å‘å¸ƒè®ºæ–‡",
    "is_original_proposal_quote": "We introduce the CLASSES2TEST dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics",
    "dataset_url": "https://anonymous.4open.science/r/classes2test",
    "dataset_url_quote": "1https://anonymous.4open.science/r/classes2test",
    "task_description": "ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„Javaå•å…ƒæµ‹è¯•çš„è´¨é‡ï¼Œæ”¯æŒç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ¯”è¾ƒä¸åŒLLMå’Œæç¤ºç­–ç•¥",
    "task_description_quote": "AGONETEST does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions.",
    "dimension": "å•å…ƒæµ‹è¯•è´¨é‡è¯„ä¼°ï¼ŒåŒ…æ‹¬ç¼–è¯‘æˆåŠŸç‡ã€ä»£ç è¦†ç›–ç‡ã€ç¼ºé™·æ£€æµ‹èƒ½åŠ›ã€æµ‹è¯•å¼‚å‘³ç­‰",
    "dimension_quote": "a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",
    "evaluation_method": "é›†æˆé«˜çº§è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚å˜å¼‚åˆ†æ•°å’Œæµ‹è¯•å¼‚å‘³ï¼Œè¿›è¡Œç»¼åˆè¯„ä¼°",
    "evaluation_method_quote": "a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",
    "context_dependency": "ç±»çº§åˆ«æµ‹è¯•ï¼Œæ¶µç›–æ–¹æ³•äº¤äº’å’Œå…±äº«çŠ¶æ€",
    "context_dependency_quote": "Our approach focuses on class-level test code evaluation, which is closer to real-world practices as it covers method interactions and shared state",
    "problem_domain": "è½¯ä»¶æµ‹è¯•ï¼ŒJavaå•å…ƒæµ‹è¯•ç”Ÿæˆ",
    "problem_domain_quote": "Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly.",
    "problem_difficulty": "ç°å®ä¸–ç•Œè½¯ä»¶é¡¹ç›®çº§åˆ«ï¼Œæ¯”å•æ–¹æ³•æµ‹è¯•æ›´å¤æ‚",
    "problem_difficulty_quote": "This extended dataset makes it possible to assess the test performance of an LLM on a more complex scope (the entire class) than the single method.",
    "language": "Java",
    "language_quote": "An annotated open source Java project dataset extending METHODS2TEST [9]",
    "data_size": "åŸºäº9,410ä¸ªGitHubä»“åº“çš„æ•°æ®é›†",
    "data_size_quote": "AGONETEST offers far broader applicability by using a dataset of 9,410 GitHub repositories",
    "source_type": "æ‰©å±•è‡ªMETHODS2TESTæ•°æ®é›†çš„Javaå¼€æºé¡¹ç›®",
    "source_type_quote": "An annotated open source Java project dataset extending METHODS2TEST [9]",
    "last_updated": 2025,
    "last_updated_quote": "arXiv:2511.20403v1  [cs.SE]  25 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ŒåŸºäºç°æœ‰æ•°æ®é›†æ‰©å±•",
    "build_type_quote": "Leveraging the METHODS2TEST dataset [9], we developed a new dataset specifically aimed at comparing human-written tests with those produced by LLMs.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ç±»çº§åˆ«å•å…ƒæµ‹è¯•ç”Ÿæˆ",
    "task_granularity_quote": "Our approach focuses on class-level test code evaluation, which is closer to real-world practices as it covers method interactions and shared state",
    "evaluation_metrics": "å˜å¼‚åˆ†æ•°ã€æµ‹è¯•å¼‚å‘³ã€ä»£ç è¦†ç›–ç‡",
    "evaluation_metrics_quote": "a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",
    "input_modality": "Javaç±»ä»£ç ",
    "input_modality_quote": "which maps classes under test to their related test classes",
    "output_modality": "å•å…ƒæµ‹è¯•ä»£ç ",
    "output_modality_quote": "for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "which maps Java classes under test to their corresponding test classes",
    "execution_environment": "æ”¯æŒæ‰€æœ‰Java LTSç‰ˆæœ¬çš„é¡¹ç›®ç¯å¢ƒ",
    "execution_environment_quote": "AGONETEST overcomes this barrier by supporting all Java LTS versions.",
    "unique_features": "ä¸“æ³¨äºç±»çº§åˆ«æµ‹è¯•è¯„ä¼°ï¼Œæ”¯æŒå¤šç§LLMå’Œæç¤ºç­–ç•¥çš„æ¯”è¾ƒï¼Œæä¾›ç«¯åˆ°ç«¯çš„è‡ªåŠ¨åŒ–è¯„ä¼°æµæ°´çº¿",
    "unique_features_quote": "AGONETEST shifts the focus to the generation of class-level tests. Our approach makes it possible to use up-to-date LLMs and not constrain prompt design (our prompts can be customized), thereby handling more complex, real-world scenarios.",
    "data_size_quantity": 9410,
    "data_size_unit": "ä¸ªGitHubä»“åº“",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java']",
    "dimension_normalized": "['å•å…ƒæµ‹è¯•è´¨é‡è¯„ä¼°', 'ç¼–è¯‘æˆåŠŸç‡', 'ä»£ç è¦†ç›–ç‡', 'ç¼ºé™·æ£€æµ‹èƒ½åŠ›', 'æµ‹è¯•å¼‚å‘³']",
    "evaluation_method_normalized": "['å˜å¼‚åˆ†æ•°', 'æµ‹è¯•å¼‚å‘³', 'ä»£ç è¦†ç›–ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶æµ‹è¯•', 'Javaå•å…ƒæµ‹è¯•ç”Ÿæˆ']",
    "source_type_normalized": "['æ‰©å±•è‡ªMETHODS2TESTæ•°æ®é›†çš„Javaå¼€æºé¡¹ç›®']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•æ–‡ä»¶",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.21380_output/content.md",
    "benchmark_name": "ROCODE, LogHub2.0",
    "benchmark_name_quote": "Two projects (i.e., ROCODE [15] and LogHub2.0 [16]) are selected for the following experiments.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ•°æ®é›†é€‚åº”ä»»åŠ¡ - å°†è½¯ä»¶å·¥ç¨‹ç ”ç©¶å·¥å…·è‡ªåŠ¨é€‚é…åˆ°æ–°çš„æ•°æ®é›†ï¼Œæ„å»ºå¯è¿è¡Œçš„å®éªŒå¹¶è·å–æ‰§è¡Œç»“æœ",
    "task_description_quote": "Our objective is to automatically modify ğ‘…ğ·or ğ‘…ğ‘‡, construct a runnable experiment, and obtain its execution results.",
    "dimension": "å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ•°æ®é›†é€‚åº”ä»»åŠ¡ä¸­çš„èƒ½åŠ›è¯„ä¼°",
    "dimension_quote": "This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks.",
    "evaluation_method": "äº”é˜¶æ®µè¯„ä¼°æµç¨‹ï¼šæ–‡ä»¶ç†è§£ã€ä»£ç ç¼–è¾‘ã€å‘½ä»¤ç”Ÿæˆã€éªŒè¯å’Œæœ€ç»ˆæ‰§è¡Œï¼Œæµ‹é‡æˆåŠŸç‡å¹¶åˆ†æå¤±è´¥æ¨¡å¼",
    "evaluation_method_quote": "Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®ç¯å¢ƒï¼Œéœ€è¦ç†è§£æ•´ä¸ªä»£ç ä»“åº“çš„æ¶æ„",
    "context_dependency_quote": "Before modifying code or generating commands for a repository, a multi-agent system must browse the essential files in a repository to understand its architecture.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ç ”ç©¶ï¼ŒåŒ…æ‹¬ä»£ç ç”Ÿæˆã€bugä¿®å¤ã€æ€§èƒ½åˆ†æå’Œå®‰å…¨ç­‰é¢†åŸŸ",
    "problem_domain_quote": "In areas ranging from code generation and bug fixing to performance analysis and security, researchers are constantly proposing new techniques",
    "problem_difficulty": "å¤æ‚è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œéœ€è¦å¤šæ­¥éª¤åè°ƒå’Œè¿­ä»£ä¿®å¤",
    "problem_difficulty_quote": "Most of the evaluated multi-agent systems failed to complete the assigned tasks. Under such circumstances, nearly all results would be classified as failures",
    "language": "Python",
    "language_quote": "To ensure a consistent and manageable experimental environment, we retained only artifacts implemented in Python.",
    "data_size": "ä»é¡¶çº§è½¯ä»¶å·¥ç¨‹ä¼šè®®ï¼ˆFSE, ICSE, ASE, ISSTAï¼‰2024-2025å¹´æ¥å—çš„è®ºæ–‡ä¸­ç­›é€‰çš„å¯é‡ç”¨ç ”ç©¶æ„ä»¶",
    "data_size_quote": "We began by collecting all papers accepted by the top four SE (FSE, ICSE, ASE and ISSTA) in 2024-2025 that were either (i) awarded a Reusable Artifact badge or (ii) explicitly identified by the authors as providing reusable artifacts.",
    "source_type": "å­¦æœ¯ç ”ç©¶æ„ä»¶ï¼Œæ¥è‡ªé¡¶çº§è½¯ä»¶å·¥ç¨‹ä¼šè®®çš„å¯é‡ç”¨ç ”ç©¶é¡¹ç›®",
    "source_type_quote": "To construct a representative set of high-quality SE research artifacts, we followed a systematic multi-stage selection process",
    "last_updated": 2025,
    "last_updated_quote": "arXiv:2511.21380v1  [cs.SE]  26 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºçš„ç ”ç©¶æ„ä»¶é›†åˆ",
    "build_type_quote": "We began by collecting all papers accepted by the top four SE (FSE, ICSE, ASE and ISSTA) in 2024-2025",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç¼–è¾‘ã€æ–‡ä»¶åˆ›å»ºã€å‘½ä»¤ç”Ÿæˆã€éªŒè¯ä¿®å¤",
    "task_granularity_quote": "Edit and create necessary files... Generate and execute the commands... Validating and repairing",
    "evaluation_metrics": "æˆåŠŸç‡ã€ç»“æ„ç›¸ä¼¼åº¦ï¼ˆä»7.25%åˆ°67.14%ï¼‰ã€å®ŒæˆçŠ¶æ€è®°å½•",
    "evaluation_metrics_quote": "Results show that... substantially improve structural similarity to ground truth (from 7.25% to 67.14%)",
    "input_modality": "ä»£ç ä»“åº“ã€è‡ªç„¶è¯­è¨€æç¤º",
    "input_modality_quote": "Each multi-agent system is provided with a processed repository... along with a simple prompt that specifies the adaptation task",
    "output_modality": "ä¿®æ”¹åçš„ä»£ç ã€ç”Ÿæˆçš„è„šæœ¬å‘½ä»¤ã€æ‰§è¡Œç»“æœ",
    "output_modality_quote": "the code adaptations performed to ensure compatibility, the executable scripts or commands derived for running the experiment, and the results produced by executing those scripts or commands",
    "task_io_type": "ä»£ç åˆ°ä»£ç çš„é€‚åº”ä»»åŠ¡",
    "task_io_type_quote": "automatically modify ğ‘…ğ·or ğ‘…ğ‘‡, construct a runnable experiment, and obtain its execution results",
    "execution_environment": "Pythonç¯å¢ƒï¼Œéœ€è¦å¯é çš„éƒ¨ç½²ç¯å¢ƒ",
    "execution_environment_quote": "This choice allows for reliable environment deployment and isolates our investigation from environment setup challenges",
    "unique_features": "ä¸“æ³¨äºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ•°æ®é›†é€‚åº”ä»»åŠ¡ä¸­çš„è¡¨ç°è¯„ä¼°ï¼Œé‡‡ç”¨äº”é˜¶æ®µè¯„ä¼°æµç¨‹ï¼Œç ”ç©¶æç¤ºçº§å¹²é¢„å¯¹æ€§èƒ½çš„å½±å“",
    "unique_features_quote": "This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. Through a five-stage evaluation pipeline... we measure success rates, analyze failure patterns, and assess prompt-based interventions",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ•°æ®é›†é€‚åº”ä»»åŠ¡ä¸­çš„èƒ½åŠ›è¯„ä¼°']",
    "evaluation_method_normalized": "['æˆåŠŸç‡', 'ç»“æ„ç›¸ä¼¼åº¦', 'å®ŒæˆçŠ¶æ€è®°å½•']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹ç ”ç©¶', 'ä»£ç ç”Ÿæˆ', 'bugä¿®å¤', 'æ€§èƒ½åˆ†æ', 'å®‰å…¨']",
    "source_type_normalized": "['å­¦æœ¯ç ”ç©¶æ„ä»¶', 'é¡¶çº§è½¯ä»¶å·¥ç¨‹ä¼šè®®', 'å¯é‡ç”¨ç ”ç©¶é¡¹ç›®']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.21022_output/content.md",
    "benchmark_name": "EDAPIBench",
    "benchmark_name_quote": "We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances.",
    "is_original_proposal": "Yesï¼Œæœ¬æ–‡æ˜¯è¯¥æ•°æ®é›†çš„åŸå§‹å‘å¸ƒè®ºæ–‡",
    "is_original_proposal_quote": "We introduce EDAPIBench, a dedicated benchmark... We construct EDAPIBenchâ€”the first dedicated benchmark for evaluating deprecated API knowledge editing in LLMs",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ä¸­åºŸå¼ƒAPIçŸ¥è¯†ç¼–è¾‘çš„æ€§èƒ½ï¼Œä¸“é—¨ç”¨äºæµ‹è¯•æ¨¡å‹ç¼–è¾‘æŠ€æœ¯èƒ½å¦æœ‰æ•ˆæ›´æ–°åºŸå¼ƒAPIçŸ¥è¯†å¹¶ç”Ÿæˆæœ€æ–°çš„API",
    "task_description_quote": "a dedicated benchmark for evaluating deprecated API knowledge editing in LLMs... whether existing model editing methods can effectively update deprecated API knowledge within LLMs, and enable the edited models to correctly replace deprecated APIs with up-to-date ones",
    "dimension": "æœ‰æ•ˆæ€§ã€æ³›åŒ–æ€§ã€å¯ç§»æ¤æ€§ã€ç‰¹å¼‚æ€§",
    "dimension_quote": "comprehensively assess model editing performance across four key dimensions: Effectiveness, Generalization, Portability, Specificity",
    "evaluation_method": "åŸºäºå››ä¸ªç»´åº¦çš„è¯„ä¼°ï¼šæœ‰æ•ˆæ€§ï¼ˆç¼–è¾‘åæ¨¡å‹æ˜¯å¦ç”Ÿæˆæœ€æ–°APIï¼‰ã€æ³›åŒ–æ€§ï¼ˆè¯­ä¹‰ç­‰ä»·ä½†è¯­æ³•å˜åŒ–çš„è¾“å…¥ï¼‰ã€å¯ç§»æ¤æ€§ï¼ˆä¸åŒè¾“å…¥ä½†æ¶‰åŠç›¸åŒåºŸå¼ƒAPIï¼‰ã€ç‰¹å¼‚æ€§ï¼ˆä¿æŒä¸ç¼–è¾‘ä»»åŠ¡æ— å…³è¾“å…¥çš„è¡Œä¸ºä¸€è‡´æ€§ï¼‰",
    "evaluation_method_quote": "Effectiveness: Measuring whether the edited model generates the up-to-date API for the original inputs; Generalization: Measuring whether it generates the up-to-date API on semantically equivalent but syntactically varied inputs; Portability: Measuring whether it generates the up-to-date API across different inputs... Specificity: Measuring whether it preserves consistent pre-editing behavior on inputs unrelated to the editing task",
    "context_dependency": "å•å‡½æ•°çº§åˆ«çš„ä»£ç è¡¥å…¨ä¸Šä¸‹æ–‡",
    "context_dependency_quote": "Our study frames the model editing scenario as a code completion task... we extract lines preceding the API invocation as candidate editing inputs (to prompt LLM API completion) and the invocation line as the target API (ground truth)",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ã€APIç»´æŠ¤ã€ç¬¬ä¸‰æ–¹åº“æ›´æ–°",
    "problem_domain_quote": "deprecated APIs from 8 popular Python libraries... covering eight popular Python libraries such as PyTorch and TensorFlow",
    "problem_difficulty": "å®é™…å·¥ç¨‹çº§éš¾åº¦ï¼Œæ¶‰åŠçœŸå®ä¸–ç•ŒåºŸå¼ƒAPIçš„è¯†åˆ«å’Œæ›´æ–°",
    "problem_difficulty_quote": "Using these mappings, we extract 65,596 real-world functions from GitHub that call the up-to-date APIs",
    "language": "Python",
    "language_quote": "deprecated APIs from 8 popular Python libraries... covering eight popular Python libraries such as PyTorch and TensorFlow",
    "data_size": "åŒ…å«70å¤šä¸ªåºŸå¼ƒAPIï¼Œè¶…è¿‡3000ä¸ªç¼–è¾‘å®ä¾‹ï¼Œä»GitHubæå–äº†65,596ä¸ªçœŸå®ä¸–ç•Œå‡½æ•°",
    "data_size_quote": "featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances... we extract 65,596 real-world functions from GitHub",
    "source_type": "ä»GitHubçœŸå®é¡¹ç›®ä»£ç ä¸­æå–ï¼ŒåŸºäºå·²éªŒè¯çš„APIæ˜ å°„å…³ç³»",
    "source_type_quote": "We begin with 145 verified API mappings (deprecated â†’up-to-date) from Wang et al. [49]... Using these mappings, we extract 65,596 real-world functions from GitHub that call the up-to-date APIs",
    "last_updated": "2025-2026",
    "last_updated_quote": "Publication date: November 2026. arXiv:2511.21022v1 [cs.SE] 26 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œå…¨è‡ªåŠ¨æ„å»º",
    "build_type_quote": "which can be fully automatically constructed... with fully automated construction",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "APIçº§åˆ«çš„ä»£ç è¡¥å…¨",
    "task_granularity_quote": "code completion tasks... during code completion, LLMs frequently suggest deprecated API invocations",
    "evaluation_metrics": "åŸºäºå››ä¸ªç»´åº¦çš„å®šæ€§è¯„ä¼°ï¼šæœ‰æ•ˆæ€§ã€æ³›åŒ–æ€§ã€å¯ç§»æ¤æ€§ã€ç‰¹å¼‚æ€§",
    "evaluation_metrics_quote": "Effectiveness, Generalization, Portability, Specificity",
    "input_modality": "ä»£ç ç‰‡æ®µ",
    "input_modality_quote": "the editing input (a code snippet to be completed)",
    "output_modality": "APIè°ƒç”¨ä»£ç ",
    "output_modality_quote": "the specific correct, up-to-date API that should replace the deprecated one",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "code completion task... generating code",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªä¸“é—¨é’ˆå¯¹åºŸå¼ƒAPIçŸ¥è¯†ç¼–è¾‘çš„åŸºå‡†æµ‹è¯•ï¼Œæ”¯æŒå…¨è‡ªåŠ¨æ„å»ºï¼ŒåŒ…å«å››ä¸ªç»´åº¦çš„ç»¼åˆè¯„ä¼°",
    "unique_features_quote": "the first dedicated benchmark for evaluating deprecated API knowledge editing in LLMs, with fully automated construction, serving as a standardized, rigorous platform",
    "data_size_quantity": 65596,
    "data_size_unit": "ä¸ªå‡½æ•°",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['æœ‰æ•ˆæ€§', 'æ³›åŒ–æ€§', 'å¯ç§»æ¤æ€§', 'ç‰¹å¼‚æ€§']",
    "evaluation_method_normalized": "['æœ‰æ•ˆæ€§', 'æ³›åŒ–æ€§', 'å¯ç§»æ¤æ€§', 'ç‰¹å¼‚æ€§']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'APIç»´æŠ¤', 'ç¬¬ä¸‰æ–¹åº“æ›´æ–°']",
    "source_type_normalized": "['ä»GitHubçœŸå®é¡¹ç›®ä»£ç ä¸­æå–', 'åŸºäºå·²éªŒè¯çš„APIæ˜ å°„å…³ç³»']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç è¡¥å…¨",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.19875_output/content.md",
    "benchmark_name": "CodeFuse-CommitEval",
    "benchmark_name_quote": "We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models.",
    "is_original_proposal": "Yesï¼Œæœ¬æ–‡æ˜¯è¯¥æ•°æ®é›†çš„åŸå§‹å‘å¸ƒè®ºæ–‡",
    "is_original_proposal_quote": "We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models.",
    "dataset_url": "https://figshare.com/s/21fe4ec9cb960b52bffe",
    "dataset_url_quote": "The dataset is publicly available at https://figshare.com/s/21fe4ec9cb960b52bffe.",
    "task_description": "æ£€æµ‹æäº¤ä¿¡æ¯ä¸ä»£ç å˜æ›´ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ï¼ˆMessage-Code Inconsistency, MCIï¼‰",
    "task_description_quote": "A Message-Code Inconsistency (MCI) occurs when the natural language description in a commit message does not accurately reflect the actual modifications in the associated code diff.",
    "dimension": "æäº¤ä¿¡æ¯ä¸ä»£ç å˜æ›´çš„ä¸€è‡´æ€§æ£€æµ‹èƒ½åŠ›",
    "dimension_quote": "evaluate models for MCI detection",
    "evaluation_method": "ä½¿ç”¨Recallã€Precisionã€Specificityç­‰æŒ‡æ ‡è¯„ä¼°æ¨¡å‹æ£€æµ‹ç»“æœ",
    "evaluation_method_quote": "Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%)",
    "context_dependency": "æäº¤ä¿¡æ¯ä¸å¯¹åº”çš„ä»£ç å·®å¼‚ï¼ˆdiffï¼‰",
    "context_dependency_quote": "Each item of the verified dataset contains a commit message, the corresponding code diff, and the ground truth label",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ã€ç‰ˆæœ¬æ§åˆ¶ã€ä»£ç å®¡æŸ¥",
    "problem_domain_quote": "Version control relies on commit messages to convey the rationale for code changes",
    "problem_difficulty": "éœ€è¦è¯­ä¹‰ç†è§£å’Œä¸Šä¸‹æ–‡æ¨ç†çš„å¤æ‚ä»»åŠ¡",
    "problem_difficulty_quote": "purpose inconsistencies require deeper semantic understanding and contextual reasoning",
    "language": "å¤šç§ç¼–ç¨‹è¯­è¨€ï¼ˆåŸºäºApacheCMæ•°æ®é›†çš„å¤šæ ·æ€§ï¼‰",
    "language_quote": "because of its diversity in programming languages and the high-quality commits",
    "data_size": "åŒ…å«æ­£è´Ÿæ ·æœ¬çš„å¹³è¡¡æ•°æ®é›†",
    "data_size_quote": "we generate a balanced dataset with both positive and negative samples",
    "source_type": "åŸºäºApacheCMæ•°æ®é›†ï¼Œé€šè¿‡è§„åˆ™å¼•å¯¼çš„çªå˜ç”Ÿæˆä¸ä¸€è‡´æäº¤ä¿¡æ¯",
    "source_type_quote": "Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2511.19875v1 [cs.SE] 25 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç»“åˆLLMæ•°æ®åˆæˆèƒ½åŠ›ä¸ç°æœ‰æäº¤è¯­æ–™åº“",
    "build_type_quote": "We present a comprehensive pipeline for constructing MCI datasets. By combining the data synthesis capabilities of LLMs with existing commit corpora",
    "contamination_status": "é€šè¿‡åŒé‡éªŒè¯ç¡®ä¿æ•°æ®è´¨é‡",
    "contamination_status_quote": "apply two-fold validation to verify both positive and negative samples",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä¸€è‡´æ€§æ£€æµ‹",
    "task_granularity_quote": "detecting inconsistencies between commit messages and code",
    "evaluation_metrics": "Recall, Precision, Specificity",
    "evaluation_metrics_quote": "average Recall 85.95%, Precision 80.28%, Specificity 63.8%",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆæäº¤ä¿¡æ¯ï¼‰ä¸ä»£ç å·®å¼‚",
    "input_modality_quote": "Each item of the verified dataset contains a commit message, the corresponding code diff",
    "output_modality": "äºŒå…ƒåˆ†ç±»ï¼ˆä¸€è‡´æˆ–ä¸ä¸€è‡´ï¼‰",
    "output_modality_quote": "allowing the models to detect whether the commit is consistent or not",
    "task_io_type": "æ–‡æœ¬ä¸ä»£ç åˆ°åˆ†ç±»",
    "task_io_type_quote": "detect whether the commit is consistent or not",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªä¸“é—¨ç”¨äºMCIæ£€æµ‹çš„åŸºå‡†ï¼ŒåŒ…å«ä¸ƒç§ä¸ä¸€è‡´ç±»å‹ï¼Œæ”¯æŒå¤šç§å¢å¼ºç­–ç•¥è¯„ä¼°",
    "unique_features_quote": "CODEFUSE-COMMITEVAL is the first benchmarking work tailored for comprehensively evaluating LLM's MCI detection ability. We define seven mutation rules to guide powerful LLMs in generating various types of inconsistent commit messages",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['å¤šç§ç¼–ç¨‹è¯­è¨€']",
    "dimension_normalized": "['æäº¤ä¿¡æ¯ä¸ä»£ç å˜æ›´çš„ä¸€è‡´æ€§æ£€æµ‹èƒ½åŠ›']",
    "evaluation_method_normalized": "['Recall', 'Precision', 'Specificity']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ç‰ˆæœ¬æ§åˆ¶', 'ä»£ç å®¡æŸ¥']",
    "source_type_normalized": "['åŸºäºApacheCMæ•°æ®é›†', 'é€šè¿‡è§„åˆ™å¼•å¯¼çš„çªå˜ç”Ÿæˆä¸ä¸€è‡´æäº¤ä¿¡æ¯']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2511.20709_output/content.md",
    "benchmark_name": "DUALGAUGE-BENCH",
    "benchmark_name_quote": "we further curated DUALGAUGE-BENCH, a comprehensive benchmark suite of 154 programming tasks",
    "is_original_proposal": "Yesï¼Œæœ¬æ–‡æ˜¯è¯¥æ•°æ®é›†çš„åŸå§‹å‘å¸ƒè®ºæ–‡",
    "is_original_proposal_quote": "We present DUALGAUGE, the first fully automated benchmarking framework... we also present DUALGAUGE-BENCH, a curated benchmark suite",
    "dataset_url": "https://anonymous.4open.science/r/DualBench-6D1D",
    "dataset_url_quote": "Our system source code, benchmark, and evaluation/study data can all be found at https://anonymous.4open.science/r/DualBench-6D1D",
    "task_description": "è”åˆè¯„ä¼°ä»£ç ç”Ÿæˆçš„å®‰å…¨æ€§å’ŒåŠŸèƒ½æ€§ï¼Œç¡®ä¿ç”Ÿæˆçš„ä»£ç æ—¢æ»¡è¶³åŠŸèƒ½è§„èŒƒåˆä¸ä¼šå¼•å…¥å®‰å…¨æ¼æ´",
    "task_description_quote": "rigorously evaluate the security and correctness of LLM-generated code in unison... ensuring that generated programs fulfill their specifications without introducing vulnerabilities",
    "dimension": "ä»£ç ç”Ÿæˆçš„å®‰å…¨æ€§å’ŒåŠŸèƒ½æ€§è”åˆè¯„ä¼°",
    "dimension_quote": "designed to rigorously evaluate the security and correctness of LLM-generated code in unison",
    "evaluation_method": "åœ¨æ²™ç›’ç¯å¢ƒä¸­æ‰§è¡Œç¨‹åºï¼Œé’ˆå¯¹åŠŸèƒ½å’Œå®‰å…¨æ€§æµ‹è¯•å¥—ä»¶è¿è¡Œï¼ŒåŸºäºæ‰§è¡Œç»“æœè¯„ä¼°æµ‹è¯•é€šè¿‡ç‡å’Œè”åˆæ­£ç¡®æ€§-å®‰å…¨æ€§æŒ‡æ ‡",
    "evaluation_method_quote": "executes it in a sandboxed environment against functional and security test suites, and evaluates the execution results against both test suites to report test pass rates and joint correctness-security metrics",
    "context_dependency": "å•ä»»åŠ¡ä»£ç ç”Ÿæˆï¼ŒåŸºäºè‡ªç„¶è¯­è¨€æç¤ºç”Ÿæˆå®Œæ•´ç¨‹åº",
    "context_dependency_quote": "Given an pure-natural-language prompt as functional specification... generates the model's code output for the prompt",
    "problem_domain": "è·¨é¢†åŸŸç¼–ç¨‹ä»»åŠ¡ï¼Œæ¶µç›–å¤šæ ·åŒ–åŠŸèƒ½é¢†åŸŸ",
    "problem_domain_quote": "spanning diverse functionality domains",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "è¯­è¨€æ— å…³ï¼Œæ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€",
    "language_quote": "This dataset design and curation process allow it to be agnostic to programming languages",
    "data_size": "åŒ…å«154ä¸ªç¼–ç¨‹ä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½é…æœ‰åŠŸèƒ½å’Œå®‰å…¨æ€§æµ‹è¯•å¥—ä»¶",
    "data_size_quote": "a comprehensive benchmark suite of 154 programming tasks... Each task/prompt is paired with both a functional test suite... and a security test suite",
    "source_type": "äººå·¥ä¸LLMååŒåˆ›å»ºï¼Œé€šè¿‡å¤šè¯„åˆ†è€…çš„äººç±»ä¸“ä¸šçŸ¥è¯†è¿›è¡Œç²¾ç‚¼å’Œä¿®æ­£",
    "source_type_quote": "constructed these test suites through a human-and-LLM co-creation processâ€”leveraging multiple LLMs to generate candidate tests, then refining and amending these with human expertise of multiple raters",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2511.20709v1 [cs.SE] 24 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ŒåŸºäºè§„èŒƒæµ‹è¯•èŒƒå¼",
    "build_type_quote": "following a specification-based testing paradigm",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "code generation... translating natural language promptsâ€”typically serving as functional specificationsâ€”into programs",
    "evaluation_metrics": "æµ‹è¯•é€šè¿‡ç‡ã€è”åˆæ­£ç¡®æ€§-å®‰å…¨æ€§æŒ‡æ ‡",
    "evaluation_metrics_quote": "report test pass rates and joint correctness-security metrics",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "Given an pure-natural-language prompt as functional specification",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "generates the model's code output for the prompt",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "translating natural language prompts... into programs",
    "execution_environment": "æ²™ç›’éš”ç¦»å®¹å™¨ç¯å¢ƒï¼Œæ”¯æŒä¾èµ–è§£æå’Œç¯å¢ƒé…ç½®",
    "execution_environment_quote": "executes it in a sandboxed environment... The execution engine compiles and runs the model-generated code within isolated containers",
    "unique_features": "é¦–ä¸ªæ”¯æŒå®‰å…¨æ€§å’ŒåŠŸèƒ½æ€§è”åˆè¯„ä¼°çš„åŸºå‡†å¥—ä»¶ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½é…æœ‰è¦†ç›–é©±åŠ¨çš„åŠŸèƒ½å’Œå®‰å…¨æ€§æµ‹è¯•å¥—ä»¶ï¼Œé‡‡ç”¨äººå·¥ä¸LLMååŒåˆ›å»ºè¿‡ç¨‹",
    "unique_features_quote": "the first benchmark suite that pairs each code-generation prompt with dual (functional and security), coverage-enforced test suites... constructed through a human-and-LLM co-creation process",
    "data_size_quantity": 154,
    "data_size_unit": "ä¸ªç¼–ç¨‹ä»»åŠ¡",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['è¯­è¨€æ— å…³', 'å¤šç§ç¼–ç¨‹è¯­è¨€']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„å®‰å…¨æ€§å’ŒåŠŸèƒ½æ€§è”åˆè¯„ä¼°']",
    "evaluation_method_normalized": "['æµ‹è¯•é€šè¿‡ç‡', 'è”åˆæ­£ç¡®æ€§-å®‰å…¨æ€§æŒ‡æ ‡']",
    "problem_domain_normalized": "['è·¨é¢†åŸŸç¼–ç¨‹ä»»åŠ¡', 'å¤šæ ·åŒ–åŠŸèƒ½é¢†åŸŸ']",
    "source_type_normalized": "['äººå·¥ä¸LLMååŒåˆ›å»º', 'å¤šè¯„åˆ†è€…çš„äººç±»ä¸“ä¸šçŸ¥è¯†è¿›è¡Œç²¾ç‚¼å’Œä¿®æ­£']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.23408_output/content.md",
    "benchmark_name": "Vul4J",
    "benchmark_name_quote": "Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects and covering distinct Common Weakness Enumeration (CWE) classes.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨è‡ªåŠ¨åŒ–æ¼æ´ä¿®å¤æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œå…·ä½“é’ˆå¯¹çœŸå®æ¼æ´å’Œäººå·¥ç”Ÿæˆçš„æ¼æ´ã€‚",
    "task_description_quote": "In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs... using both real and artificial vulnerabilities.",
    "dimension": "æ¼æ´ä¿®å¤çš„æœ‰æ•ˆæ€§ã€æ¨¡å‹é—´çš„äº’è¡¥æ€§ä¸é‡å æ€§",
    "dimension_quote": "Our paper empirically investigates the effectiveness and complementarity of several prominent LLMs in automated vulnerability patching...",
    "evaluation_method": "ä½¿ç”¨æ¼æ´è¯æ˜ï¼ˆProof-of-Vulnerability, PoVï¼‰æµ‹è¯•æ‰§è¡Œæ¥éªŒè¯ç”Ÿæˆçš„è¡¥ä¸æ˜¯å¦æˆåŠŸä¿®å¤æ¼æ´ã€‚",
    "evaluation_method_quote": "Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities.",
    "context_dependency": "å•å‡½æ•°/ä»£ç ç‰‡æ®µ",
    "context_dependency_quote": "Given a vulnerable code snippet, these models generate a patched version that aims to eliminate security flaws while preserving functionality.",
    "problem_domain": "è½¯ä»¶å®‰å…¨ã€æ¼æ´ä¿®å¤",
    "problem_domain_quote": "Automated vulnerability patching is crucial for software security...",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Java",
    "language_quote": "Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities...",
    "data_size": "15ä¸ªçœŸå®æ¼æ´åŠå…¶41ä¸ªäººå·¥ç”Ÿæˆçš„å¯¹åº”æ¼æ´",
    "data_size_quote": "To perform this evaluation, we employed 15 real vulnerabilities and their 41 artificial counterparts (vulnerabilities).",
    "source_type": "çœŸå®æ¼æ´æ¥è‡ªå¼€æºé¡¹ç›®å’Œå…¬å…±æ¼æ´æ•°æ®åº“ï¼ˆå¦‚CVEï¼‰ï¼Œäººå·¥æ¼æ´ç”±CodeBERTç”Ÿæˆå¹¶ç»è¿‡éªŒè¯ã€‚",
    "source_type_quote": "Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects... Secondly, we augmented our evaluation with artificially generated vulnerabilities... we incorporated artificial vulnerabilities derived from the work of Garg et al.[15]. Garg et al. used CodeBERT[11] to generate thousands of candidate artificial vulnerabilities...",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆVul4Jæ•°æ®é›†ï¼‰",
    "build_type_quote": "Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ä¿®å¤ï¼ˆæ¼æ´è¡¥ä¸ç”Ÿæˆï¼‰",
    "task_granularity_quote": "Given a vulnerable code snippet, these models generate a patched version that aims to eliminate security flaws while preserving functionality.",
    "evaluation_metrics": "PoVæµ‹è¯•é€šè¿‡ç‡ï¼ˆåŸºäºæ‰§è¡Œçš„éªŒè¯ï¼‰",
    "evaluation_metrics_quote": "Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities.",
    "input_modality": "ä»£ç ï¼ˆåŒ…å«æ¼æ´çš„ä»£ç ç‰‡æ®µï¼‰",
    "input_modality_quote": "Given a vulnerable code snippet, these models generate a patched version...",
    "output_modality": "ä»£ç ï¼ˆä¿®å¤åçš„ä»£ç ç‰‡æ®µï¼‰",
    "output_modality_quote": "Given a vulnerable code snippet, these models generate a patched version...",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "Given a vulnerable code snippet, these models generate a patched version...",
    "execution_environment": "Maven/Gradleæ„å»ºç¯å¢ƒï¼ŒåŒ…å«PoV JUnitæµ‹è¯•ç”¨ä¾‹",
    "execution_environment_quote": "When no suitable test existed in the original project, Bui et al. [2] manually wrote one by following the exploit steps in the CVE report, thereby guaranteeing that every vulnerability in the dataset can be triggered, reproduced, and validated in a Maven/Gradle build.",
    "unique_features": "è¯¥ç ”ç©¶ä¸ä»…è¯„ä¼°LLMå¯¹çœŸå®æ¼æ´çš„ä¿®å¤èƒ½åŠ›ï¼Œè¿˜è¯„ä¼°å…¶å¯¹äººå·¥ç”Ÿæˆæ¼æ´çš„ä¿®å¤èƒ½åŠ›ï¼Œä»¥æµ‹è¯•æ¨¡å‹çš„æ³›åŒ–æ€§å’Œé²æ£’æ€§ã€‚æ•°æ®é›†ï¼ˆVul4Jï¼‰ä¸ºæ¯ä¸ªæ¼æ´æä¾›äº†å¯æ‰§è¡Œçš„æ¼æ´è¯æ˜ï¼ˆPoVï¼‰æµ‹è¯•ç”¨ä¾‹ï¼Œç”¨äºè‡ªåŠ¨åŒ–éªŒè¯è¡¥ä¸çš„æœ‰æ•ˆæ€§ã€‚",
    "unique_features_quote": "Our paper empirically investigates the effectiveness and complementarity of several prominent LLMs in automated vulnerability patching, specifically focusing on both real vulnerabilities and their corresponding artificial vulnerabilities... For every entry Vul4J provides... one or more Proof-of-Vulnerability (PoV) JUnit test cases. A PoV test is an executable exploit oracle...",
    "data_size_quantity": 15,
    "data_size_unit": "ä¸ªçœŸå®æ¼æ´",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java']",
    "dimension_normalized": "['æ¼æ´ä¿®å¤çš„æœ‰æ•ˆæ€§', 'æ¨¡å‹é—´çš„äº’è¡¥æ€§ä¸é‡å æ€§']",
    "evaluation_method_normalized": "['PoVæµ‹è¯•é€šè¿‡ç‡ï¼ˆåŸºäºæ‰§è¡Œçš„éªŒè¯ï¼‰']",
    "problem_domain_normalized": "['è½¯ä»¶å®‰å…¨', 'æ¼æ´ä¿®å¤']",
    "source_type_normalized": "['çœŸå®æ¼æ´æ¥è‡ªå¼€æºé¡¹ç›®å’Œå…¬å…±æ¼æ´æ•°æ®åº“ï¼ˆå¦‚CVEï¼‰', 'äººå·¥æ¼æ´ç”±CodeBERTç”Ÿæˆå¹¶ç»è¿‡éªŒè¯ã€‚']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2401.01062_output/content.md",
    "benchmark_name": "CAASD (Capability Assessment of Automatic Software Development)",
    "benchmark_name_quote": "we have developed a novel benchmark named CAASD (Capability Assessment of Automatic Software Development).",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we have developed a novel benchmark named CAASD (Capability Assessment of Automatic Software Development).",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°AIè¾…åŠ©è½¯ä»¶å¼€å‘ç³»ç»Ÿçš„èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹ç³»ç»Ÿçº§å®ç°ä»»åŠ¡çš„èƒ½åŠ›",
    "task_description_quote": "all of them either are limited to simple function-level implementation tasks or lack detailed requirements specifications for system-level implementation tasks",
    "dimension": "ç³»ç»Ÿçº§è½¯ä»¶å¼€å‘èƒ½åŠ›è¯„ä¼°",
    "dimension_quote": "for assessing how well a software development task is completed",
    "evaluation_method": "é€šè¿‡å‚è€ƒç”¨ä¾‹è¯„ä¼°ç³»ç»Ÿå®ç°çš„è´¨é‡å’Œå®Œæ•´æ€§",
    "evaluation_method_quote": "Each task of CAASD is equipped with a list of reference use cases depicting the system requirements. The reference use cases are used to evaluate the quality and completeness of a system implementation.",
    "context_dependency": "ç³»ç»Ÿçº§å¼€å‘ï¼ˆå¤šæ–‡ä»¶é¡¹ç›®ï¼‰",
    "context_dependency_quote": "system-level implementation tasks",
    "problem_domain": "è½¯ä»¶å¼€å‘",
    "problem_domain_quote": "software development",
    "problem_difficulty": "éå¹³å‡¡è½¯ä»¶é¡¹ç›®",
    "problem_difficulty_quote": "non-trivial software projects",
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2024",
    "last_updated_quote": "arXiv:2401.01062v1 [cs.SE] 2 Jan 2024",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we have developed a novel benchmark named CAASD",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ç³»ç»Ÿçº§å®ç°ä»»åŠ¡",
    "task_granularity_quote": "system-level implementation tasks",
    "evaluation_metrics": "é€šè¿‡ç‡",
    "evaluation_metrics_quote": "AISD achieves an impressive pass rate of 75.2%",
    "input_modality": "è‡ªç„¶è¯­è¨€éœ€æ±‚æè¿°",
    "input_modality_quote": "high-level (potentially vague) user requirements as inputs",
    "output_modality": "ç³»ç»Ÿå®ç°",
    "output_modality_quote": "system implementation",
    "task_io_type": "éœ€æ±‚åˆ°ç³»ç»Ÿå®ç°",
    "task_io_type_quote": "taking high-level (potentially vague) user requirements as inputs, generates detailed use cases, prototype system designs, and subsequently system implementation",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªè¯„ä¼°è½¯ä»¶å¼€å‘ä»»åŠ¡å®Œæˆè´¨é‡çš„åŸºå‡†ï¼ŒåŒ…å«è¯¦ç»†çš„ç³»ç»Ÿéœ€æ±‚è§„èŒƒ",
    "unique_features_quote": "this is the first benchmark that offers criteria for assessing how well a software development task is completed",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2024,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ç³»ç»Ÿçº§è½¯ä»¶å¼€å‘èƒ½åŠ›è¯„ä¼°']",
    "evaluation_method_normalized": "['é€šè¿‡ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶å¼€å‘']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2401.12554_output/content.md",
    "benchmark_name": "ParEval",
    "benchmark_name_quote": "we propose the Parallel Code Generation Evaluation (ParEval) benchmark: a set of benchmarks (prompts) for evaluating how well LLMs generate parallel code.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we propose the Parallel Code Generation Evaluation (ParEval) benchmark",
    "dataset_url": "github.com/parallelcodefoundry/ParEval",
    "dataset_url_quote": "ParEval is available online at: github.com/parallelcodefoundry/ParEval.",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆå¹¶è¡Œä»£ç çš„èƒ½åŠ›",
    "task_description_quote": "we study the capabilities of state-of-the-art language models to generate parallel code.",
    "dimension": "å¹¶è¡Œä»£ç ç”Ÿæˆæ­£ç¡®æ€§å’Œæ€§èƒ½",
    "dimension_quote": "We evaluate several state-of-the-art open- and closed-source LLMs using these benchmarks, and report metrics that represent the correctness and performance of the generated code.",
    "evaluation_method": "æ–°é¢–çš„ä»£ç ç”Ÿæˆè¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬speedupğ‘›@kå’Œefficiencyğ‘›@kï¼Œç”¨äºè¯„ä¼°ç”Ÿæˆä»£ç çš„æ€§èƒ½å’Œæ‰©å±•æ€§",
    "evaluation_method_quote": "We introduce novel code generation evaluation metrics that assess performance and parallel scaling.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "ç§‘å­¦å’Œå¹¶è¡Œè®¡ç®—",
    "problem_domain_quote": "consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "C/C++",
    "language_quote": "Traditional Python code generation benchmarks are tested by running eval on the generated code for a small number of small unit tests. On the other hand, in the case of parallel code â€” we must compile C/C++ code, link against one or more parallel libraries, and run the code in the proper parallel environment.",
    "data_size": "420ä¸ªä¸åŒçš„ç¼–ç ä»»åŠ¡",
    "data_size_quote": "consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.",
    "source_type": "æ‰‹åŠ¨è®¾è®¡",
    "source_type_quote": "These benchmarks are challenging to test. Traditional Python code generation benchmarks are tested by running eval on the generated code for a small number of small unit tests.",
    "last_updated": "2024",
    "last_updated_quote": "HPDC â€™24, June 3â€“7, 2024, Pisa, Italy",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we propose the Parallel Code Generation Evaluation (ParEval) benchmark: a set of benchmarks (prompts) for evaluating how well LLMs generate parallel code.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "we study the capabilities of state-of-the-art language models to generate parallel code.",
    "evaluation_metrics": "speedupğ‘›@kå’Œefficiencyğ‘›@k",
    "evaluation_metrics_quote": "We introduce two novel metrics, speedupğ‘›@k and efficiencyğ‘›@k, for evaluating the performance and scaling of LLM generated code.",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "we study the capabilities of state-of-the-art language models to generate parallel code.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.",
    "execution_environment": "éœ€è¦ç‰¹å®šä¾èµ–ï¼ˆå¹¶è¡Œåº“ï¼‰",
    "execution_environment_quote": "we must compile C/C++ code, link against one or more parallel libraries, and run the code in the proper parallel environment.",
    "unique_features": "ä¸“æ³¨äºå¹¶è¡Œä»£ç ç”Ÿæˆè¯„ä¼°ï¼Œè¦†ç›–12ç§è®¡ç®—é—®é¢˜ç±»å‹å’Œ7ç§æ‰§è¡Œæ¨¡å‹",
    "unique_features_quote": "These benchmarks cover twelve different computational problem types, and seven different execution models: serial, OpenMP, Kokkos, MPI, MPI+OpenMP, CUDA, and HIP.",
    "data_size_quantity": 420,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2024,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['C', 'C++']",
    "dimension_normalized": "['å¹¶è¡Œä»£ç ç”Ÿæˆæ­£ç¡®æ€§', 'æ€§èƒ½']",
    "evaluation_method_normalized": "['speedupğ‘›@k', 'efficiencyğ‘›@k']",
    "problem_domain_normalized": "['ç§‘å­¦è®¡ç®—', 'å¹¶è¡Œè®¡ç®—']",
    "source_type_normalized": "['æ‰‹åŠ¨è®¾è®¡']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.01010_output/content.md",
    "benchmark_name": "Chain of Unit-Physics",
    "benchmark_name_quote": "To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯¥è¯„æµ‹åŸºå‡†æ—¨åœ¨è§£å†³ç§‘å­¦è®¡ç®—ä¸­çš„ä»£ç ç”Ÿæˆä»»åŠ¡ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹å…·æœ‰ä¸¥æ ¼ç‰©ç†çº¦æŸçš„é«˜é£é™©ç§‘å­¦é—®é¢˜ï¼Œå¦‚ç‡ƒçƒ§ç§‘å­¦ä¸­çš„è®¡ç®—æµä½“åŠ¨åŠ›å­¦ï¼ˆCFDï¼‰æ±‚è§£å™¨å¼€å‘ã€‚",
    "task_description_quote": "Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community.",
    "dimension": "ç‰©ç†ä¸€è‡´æ€§ã€æ•°å€¼ç¨³å®šæ€§ã€ç®—æ³•æ­£ç¡®æ€§",
    "dimension_quote": "The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",
    "evaluation_method": "åŸºäºå•å…ƒç‰©ç†æµ‹è¯•çš„éªŒè¯æ–¹æ³•ï¼ŒåŒ…æ‹¬ç‰©ç†çº¦æŸæ£€æŸ¥ã€æ•°å€¼ä¸€è‡´æ€§æ£€æŸ¥å’Œè¯Šæ–­åˆ†æ",
    "evaluation_method_quote": "The Verification Agent applies formalized unit-physics tests to assess physical and numerical consistency. These primitives yield physics-grounded verification even without reference datasets.",
    "context_dependency": "å¤šæ­¥éª¤ç§‘å­¦è®¡ç®—ä»»åŠ¡ï¼Œæ¶‰åŠå¤æ‚çš„ç‰©ç†çº¦æŸå’Œæ•°å€¼è®¡ç®—",
    "context_dependency_quote": "The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",
    "problem_domain": "è®¡ç®—ç§‘å­¦ï¼Œç‰¹åˆ«æ˜¯ç‡ƒçƒ§ç§‘å­¦å’Œè®¡ç®—æµä½“åŠ¨åŠ›å­¦",
    "problem_domain_quote": "To overcome these limitations, this work systematically applies an inverse code design methodology (see Fig. 1), formally known as test-driven development (TDD) [26], to scientific software in combustion science, one such domain where TDD approaches have not been thoroughly evaluated.",
    "problem_difficulty": "é«˜éš¾åº¦ï¼Œæ¶‰åŠ12è‡ªç”±åº¦çš„å¤æ‚ç‡ƒçƒ§ä»»åŠ¡",
    "problem_difficulty_quote": "The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "åŸºäºäººç±»ä¸“å®¶çŸ¥è¯†æ„å»ºçš„å•å…ƒç‰©ç†æµ‹è¯•",
    "source_type_quote": "This work proposes Chain of Unit-Physics, an approach that embeds human expert knowledge directly into distinct reasoning chains of the agentic system via 'unit-physics'â€”formalized, testable constraints that encode fundamental physics (e.g., energy conservation laws) and practitioner experience (e.g., dimensional / bound checks, and floating-point diagnostics).",
    "last_updated": "2025",
    "last_updated_quote": "arXiv:2512.01010v1 [cs.MA] 30 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ŒåŸºäºäººç±»ä¸“å®¶çŸ¥è¯†",
    "build_type_quote": "This work proposes Chain of Unit-Physics, an approach that embeds human expert knowledge directly into distinct reasoning chains of the agentic system via 'unit-physics'â€”formalized, testable constraints that encode fundamental physics (e.g., energy conservation laws) and practitioner experience (e.g., dimensional / bound checks, and floating-point diagnostics).",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ç«¯åˆ°ç«¯ç§‘å­¦ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "Closed-weight systems and code-focused agentic variants fail to produce correct end-to-end solvers, despite tool and web access, exhibiting four recurrent error classes: interface (syntax/API) hallucinations, overconfident assumptions, numerical/physical incoherence, and configuration fragility.",
    "evaluation_metrics": "ç‰©ç†ä¸€è‡´æ€§ã€æ•°å€¼è¯¯å·®ã€è¿è¡Œæ—¶é—´å’Œå†…å­˜ä½¿ç”¨æ•ˆç‡",
    "evaluation_metrics_quote": "On the benchmark task, the proposed framework converges within 5â€“6 iterations, matches the human-expert implementation (mean error of 3.1Ë†10Â´3%), with a â€33.4% faster runtime and a â€30% efficient memory usage at a cost comparable to mid-sized commercial APIs, yielding a practical template for physics-grounded scientific code generation.",
    "input_modality": "è‡ªç„¶è¯­è¨€æŸ¥è¯¢å’Œå•å…ƒç‰©ç†æµ‹è¯•",
    "input_modality_quote": "In the input, the userâ€™s scientific question is paired with 'basis prompts' that establish execution permissions, tool availability, coding language settings, and transfer protocols. Additional input scopes the unit-physics tests that concatenated with the scientific query to form the complete input to the framework.",
    "output_modality": "ç§‘å­¦è®¡ç®—ä»£ç å’Œå¯è§†åŒ–ç»“æœ",
    "output_modality_quote": "Finally, the agent consolidates the output into domain-relevant visualizations (for example, line graphs and contour graphs of key quantities of interest), closing the loop between natural-language inquiry and quantitative scientific insight.",
    "task_io_type": "è‡ªç„¶è¯­è¨€åˆ°ç§‘å­¦ä»£ç ",
    "task_io_type_quote": "Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community.",
    "execution_environment": "Pythonè™šæ‹Ÿæ²™ç®±ç¯å¢ƒï¼Œå…·æœ‰éš”ç¦»çš„æ‰§è¡Œæƒé™",
    "execution_environment_quote": "The framework runs inside a dedicated Python virtual sandbox environment that is not pre-configured with metadata on all available libraries. The agent is granted isolated code execution privileges within this sandbox, ensuring that any dependency installs or script executions cannot affect the system root directory or global files.",
    "unique_features": "åŸºäºç¬¬ä¸€æ€§åŸç†çš„å•å…ƒç‰©ç†æµ‹è¯•é©±åŠ¨ä»£ç ç”Ÿæˆï¼Œå¼ºè°ƒç‰©ç†ä¸€è‡´æ€§å’Œæ•°å€¼ç¨³å®šæ€§",
    "unique_features_quote": "This inverse-design method provides two key advantages in scientific software. First, human-authored tests embed deep domain expertise (first principles or primitives) into targeted validation checks, ensuring that each algorithmic component faithfully represents the underlying physics.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ç‰©ç†ä¸€è‡´æ€§', 'æ•°å€¼ç¨³å®šæ€§', 'ç®—æ³•æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['ç‰©ç†ä¸€è‡´æ€§', 'æ•°å€¼è¯¯å·®', 'è¿è¡Œæ—¶é—´', 'å†…å­˜ä½¿ç”¨æ•ˆç‡']",
    "problem_domain_normalized": "['è®¡ç®—ç§‘å­¦', 'ç‡ƒçƒ§ç§‘å­¦', 'è®¡ç®—æµä½“åŠ¨åŠ›å­¦']",
    "source_type_normalized": "['åŸºäºäººç±»ä¸“å®¶çŸ¥è¯†æ„å»ºçš„å•å…ƒç‰©ç†æµ‹è¯•']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.01396_output/content.md",
    "benchmark_name": "BackportBench",
    "benchmark_name_quote": "we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",
    "dataset_url": "https://github.com/BackportBench/BackportBench",
    "dataset_url_quote": "The BackportBench is available on https://github.com/BackportBench/BackportBench.",
    "task_description": "è‡ªåŠ¨åŒ–è¡¥ä¸å›ç§»æ¤ã€‚æ—¨åœ¨ä¸ºæœªæ‰“è¡¥ä¸çš„è½¯ä»¶ç‰ˆæœ¬ç”Ÿæˆè¡¥ä¸ï¼ŒåŸºäºå·²æœ‰çš„è¡¥ä¸å’Œä¸¤ä¸ªç‰ˆæœ¬ä¹‹é—´çš„ä»£ç å·®å¼‚ã€‚",
    "task_description_quote": "BackportBench defines the backporting problem as generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.",
    "dimension": "è‡ªåŠ¨åŒ–è¡¥ä¸å›ç§»æ¤çš„æœ‰æ•ˆæ€§ã€å¤šè¯­è¨€èƒ½åŠ›ã€å¤„ç†è·¨æ–‡ä»¶ä¸å…¼å®¹æ€§çš„èƒ½åŠ›",
    "dimension_quote": "To facilitate the development and evaluation of automated backporting techniques... BackportBench is a multilingual benchmark... This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges.",
    "evaluation_method": "åœ¨å¯æ‰§è¡Œçš„Dockerç¯å¢ƒä¸­è¿è¡Œç›¸å…³æµ‹è¯•ç”¨ä¾‹è¿›è¡ŒéªŒè¯",
    "evaluation_method_quote": "each task instance provides an executable Docker environment with scripts for running relevant test cases, thereby aligns with the criteria for a successful benchmark [5].",
    "context_dependency": "ä»“åº“çº§åˆ«ï¼Œæ¶‰åŠè·¨æ–‡ä»¶çš„ä¸å…¼å®¹æ€§å¤„ç†",
    "context_dependency_quote": "This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges.",
    "problem_domain": "è½¯ä»¶å®‰å…¨ã€è½¯ä»¶ç»´æŠ¤ã€æ¼æ´ä¿®å¤",
    "problem_domain_quote": "BackportBench contains 202 real-world vulnerability patch backporting task instances curated from three popular OSS ecosystems... To remove such vulnerabilities... patch backporting is a helpful practice that adapts patches for other versions and makes it easier to deliver patches to users on older branches.",
    "problem_difficulty": "ç°å®ä¸–ç•Œå·¥ç¨‹çº§ï¼Œæ¶‰åŠé€»è¾‘å’Œç»“æ„æ€§å˜æ›´",
    "problem_difficulty_quote": "the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes.",
    "language": "Python, Java, JavaScript",
    "language_quote": "BackportBench contains 202 patch backporting problems from PyPI, Maven, and npm, covering Python, Java, and JavaScript, respectively.",
    "data_size": "åŒ…å«202ä¸ªè¡¥ä¸å›ç§»æ¤ä»»åŠ¡å®ä¾‹",
    "data_size_quote": "BackportBench contains 202 real-world vulnerability patch backporting task instances",
    "source_type": "ä»ä¸‰ä¸ªæµè¡Œçš„å¼€æºè½¯ä»¶ç”Ÿæ€ç³»ç»Ÿï¼ˆPyPI, Maven, npmï¼‰ä¸­æ”¶é›†çš„çœŸå®ä¸–ç•Œæ¼æ´è¡¥ä¸å›ç§»æ¤ä»»åŠ¡",
    "source_type_quote": "BackportBench contains 202 real-world vulnerability patch backporting task instances curated from three popular OSS ecosystems, PyPI , Maven, and npm",
    "last_updated": "2018 (æ ¹æ®è®ºæ–‡ç‰ˆæƒæ—¥æœŸ)ï¼Œä½†arXivç‰ˆæœ¬ä¸º2025å¹´12æœˆ",
    "last_updated_quote": "Â© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ... arXiv:2512.01396v1  [cs.SE]  1 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": "ACMç‰ˆæƒï¼Œå…è®¸ä¸ªäººæˆ–è¯¾å ‚ä½¿ç”¨ï¼Œç¦æ­¢å•†ä¸šç”¨é€”",
    "dataset_license_quote": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.",
    "task_granularity": "ä»£ç ä¿®å¤/è¡¥ä¸ç”Ÿæˆ",
    "task_granularity_quote": "generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.",
    "evaluation_metrics": "é€šè¿‡æµ‹è¯•ç”¨ä¾‹éªŒè¯è¡¥ä¸æœ‰æ•ˆæ€§ï¼Œè€ŒéåŸºäºç­‰ä»·æ€§æˆ–ç›¸ä¼¼æ€§çš„æŒ‡æ ‡",
    "evaluation_metrics_quote": "each task instance provides an executable Docker environment with scripts for running relevant test cases... We argue that whether backported patches achieve equivalence is not the only way to prove the patch is effective.",
    "input_modality": "ä»£ç ï¼ˆä¸¤ä¸ªç‰ˆæœ¬çš„ä»£ç åº“åŠåŸå§‹è¡¥ä¸ï¼‰",
    "input_modality_quote": "generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.",
    "output_modality": "ä»£ç ï¼ˆå›ç§»æ¤åçš„è¡¥ä¸ï¼‰",
    "output_modality_quote": "generating a patch for the unpatched software version",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.",
    "execution_environment": "å¯æ‰§è¡Œçš„Dockerç¯å¢ƒï¼ŒåŒ…å«è¿è¡Œç›¸å…³æµ‹è¯•ç”¨ä¾‹çš„è„šæœ¬",
    "execution_environment_quote": "each task instance provides an executable Docker environment with scripts for running relevant test cases",
    "unique_features": "é¦–ä¸ªé’ˆå¯¹è¡¥ä¸å›ç§»æ¤é—®é¢˜çš„ç»¼åˆæ€§ã€å¤šè¯­è¨€åŸºå‡†æµ‹è¯•å¥—ä»¶ï¼ŒåŒ…å«å¯æ‰§è¡Œç¯å¢ƒå’Œæµ‹è¯•ç”¨ä¾‹ï¼Œå°†é—®é¢˜ä»ä»£ç å—/å‡½æ•°çº§åˆ«æå‡åˆ°ä»“åº“çº§åˆ«ï¼Œæ›´è´´è¿‘ç°å®è½¯ä»¶ç»´æŠ¤æŒ‘æˆ˜ã€‚",
    "unique_features_quote": "the first comprehensive benchmark suite for patch backporting problem... a multilingual benchmark... contains executable Docker environments and test cases for validation... This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges.",
    "data_size_quantity": 202,
    "data_size_unit": "ä¸ªè¡¥ä¸å›ç§»æ¤ä»»åŠ¡å®ä¾‹",
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": null,
    "language_normalized": "['Python', 'Java', 'JavaScript']",
    "dimension_normalized": "['è‡ªåŠ¨åŒ–è¡¥ä¸å›ç§»æ¤çš„æœ‰æ•ˆæ€§', 'å¤šè¯­è¨€èƒ½åŠ›', 'å¤„ç†è·¨æ–‡ä»¶ä¸å…¼å®¹æ€§çš„èƒ½åŠ›']",
    "evaluation_method_normalized": "['é€šè¿‡æµ‹è¯•ç”¨ä¾‹éªŒè¯è¡¥ä¸æœ‰æ•ˆæ€§ï¼Œè€ŒéåŸºäºç­‰ä»·æ€§æˆ–ç›¸ä¼¼æ€§çš„æŒ‡æ ‡']",
    "problem_domain_normalized": "['è½¯ä»¶å®‰å…¨', 'è½¯ä»¶ç»´æŠ¤', 'æ¼æ´ä¿®å¤']",
    "source_type_normalized": "['ä»ä¸‰ä¸ªæµè¡Œçš„å¼€æºè½¯ä»¶ç”Ÿæ€ç³»ç»Ÿï¼ˆPyPI, Maven, npmï¼‰ä¸­æ”¶é›†çš„çœŸå®ä¸–ç•Œæ¼æ´è¡¥ä¸å›ç§»æ¤ä»»åŠ¡']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "ä»…ä¾›å­¦æœ¯ç ”ç©¶",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.01255_output/content.md",
    "benchmark_name": "ARENAJS",
    "benchmark_name_quote": "we construct ARENAJSâ€”the first systematic benchmark for LLM-based JavaScript vulnerability detection",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "In this paper, we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection... we propose FORGEJS... Then, we use FORGEJS to construct ARENAJSâ€”the first systematic benchmark for LLM-based JavaScript vulnerability detection",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨JavaScriptæ¼æ´æ£€æµ‹æ–¹é¢çš„èƒ½åŠ›",
    "task_description_quote": "evaluating LLMsâ€™ capability in JavaScript vulnerability detection",
    "dimension": "æ¼æ´æ£€æµ‹èƒ½åŠ›ã€æ¨ç†å……åˆ†æ€§ã€é²æ£’æ€§ã€ç°å®ä¸–ç•Œå¯ç”¨æ€§",
    "dimension_quote": "reveals key limitations in reasoning sufficiency, robustness, and real-world usability",
    "evaluation_method": "ä½¿ç”¨JUDGEJSè‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼ŒåŒ…å«æç¤ºæ¨¡æ¿ã€å“åº”è§£æã€æ ‡ç­¾å¯¹é½å’Œç¨³å¥è¯„åˆ†ï¼Œåˆ©ç”¨è¯­ä¹‰ç­‰ä»·å’Œæ¨¡ç³ŠåŒ¹é…ï¼Œåœ¨å‡½æ•°çº§å’Œé¡¹ç›®çº§ç”Ÿæˆç»Ÿä¸€çš„ã€ä¸¥æ ¼çš„ã€å¯è¿½æº¯çš„æŒ‡æ ‡å¥—ä»¶ï¼ŒåŒ…æ‹¬F1ã€å‡é˜³æ€§ç‡ï¼ˆFPRï¼‰ä»¥åŠå·¥ç¨‹çº¦æŸä¸‹çš„æ£€æµ‹æ•ˆèƒ½",
    "evaluation_method_quote": "we propose JUDGEJS, an automated evaluation framework that spans prompt templates, response parsing, label alignment, and robust scoring; leveraging semantic equivalence and fuzzy matching, it harmonizes heterogeneous outputs and yields a unified, rigorous, and traceable metric suite across function- and project-levels, including F1, false positive rate (FPR), and detection efficacy under engineering constraints.",
    "context_dependency": "å‡½æ•°çº§å’Œé¡¹ç›®çº§ï¼ˆå®Œæ•´é¡¹ç›®ï¼‰",
    "context_dependency_quote": "supports both function-level and project-level evaluation",
    "problem_domain": "JavaScriptå®‰å…¨æ¼æ´æ£€æµ‹ï¼Œæ¶µç›–å‰ç«¯ï¼ˆå¦‚DOM-based XSSï¼‰ã€åç«¯ï¼ˆå¦‚SQLæ³¨å…¥ã€å‘½ä»¤æ³¨å…¥ã€åŸå‹æ±¡æŸ“ï¼‰å’Œå…¨æ ˆç¯å¢ƒ",
    "problem_domain_quote": "JavaScript vulnerability patterns vary substantially across these environments: backend prototype pollution... while frontend prototype pollution predominantly causes DOM-based XSS or client-side denial of service.",
    "problem_difficulty": "ç°å®ä¸–ç•Œå·¥ä¸šçº§ï¼Œåæ˜ çœŸå®ä»“åº“çš„å¤æ‚æ€§",
    "problem_difficulty_quote": "fails to reflect the complexity of real-world repositories",
    "language": "JavaScript",
    "language_quote": "benchmark for JavaScript vulnerability detection",
    "data_size": "è¦†ç›–æ•°åƒä¸ªçœŸå®çš„JavaScripté¡¹ç›®",
    "data_size_quote": "A benchmark that spans thousands of real JavaScript projects",
    "source_type": "èšåˆå¼‚æ„æ¥æºï¼Œç»“åˆçœŸå®ä¸–ç•Œå’Œåˆæˆæ•°æ®",
    "source_type_quote": "aggregates heterogeneous sources... combining real-world and synthetic data",
    "last_updated": "2025-12-01 (æ ¹æ®arXivç‰ˆæœ¬æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2512.01255v1  [cs.CR]  1 Dec 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œé€šè¿‡FORGEJSè‡ªåŠ¨ç”Ÿæˆæ¡†æ¶æ„å»º",
    "build_type_quote": "we propose FORGEJS, the first automatic benchmark generation framework... Then, we use FORGEJS to construct ARENAJS",
    "contamination_status": "é€šè¿‡ä½¿ç”¨å®Œæ•´é¡¹ç›®è€Œéå•æ–‡ä»¶ç‰‡æ®µã€æ„å»ºä¿®å¤å‰åé¡¹ç›®å¯¹æ¥ç›´æ¥é‡åŒ–å‡é˜³æ€§ã€ä»¥åŠç³»ç»Ÿå¼•å…¥å››ç§å¢å¼ºç­–ç•¥æ¥ç ´åå¯¹æ–‡ä»¶åã€å¯¼å…¥è·¯å¾„å’Œæ³¨é‡Šçš„ä¾èµ–ï¼Œæ—¨åœ¨é¿å…é«˜ä¼°",
    "contamination_status_quote": "To avoid overestimation, FORGEJS uses complete projects rather than single-file snippets, constructs pre- and post-fix project pairs to directly quantify false positives and localize error sources, and systematically introduces four augmentation strategies to disrupt reliance on filenames, import paths, and comments.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ¼æ´æ£€æµ‹ï¼ˆåŒ…æ‹¬å®šä½æ¼æ´æ–‡ä»¶ã€å‡½æ•°ã€CWEç±»å‹å’Œå…·ä½“ä»£ç è¡Œï¼‰",
    "task_granularity_quote": "evaluation protocols commonly assess only whether vulnerability is detected and ignore reasoning quality such as whether the model can correctly localize the vulnerable file, function, and CWE type.",
    "evaluation_metrics": "F1åˆ†æ•°ã€å‡é˜³æ€§ç‡ï¼ˆFPRï¼‰ã€VD-SæŒ‡æ ‡",
    "evaluation_metrics_quote": "including F1, false positive rate (FPR), and detection efficacy under engineering constraints... from the perspective of the VD-S metric [6]",
    "input_modality": "JavaScriptä»£ç ï¼ˆå‡½æ•°æˆ–å®Œæ•´é¡¹ç›®ï¼‰",
    "input_modality_quote": "supports both function-level and project-level evaluation",
    "output_modality": "æ¼æ´æ£€æµ‹ç»“æœï¼ˆåŒ…æ‹¬æ˜¯å¦å­˜åœ¨æ¼æ´ã€CWEç±»å‹ã€å®šä½ä¿¡æ¯ã€æ¨ç†ç­‰ï¼‰",
    "output_modality_quote": "evaluation protocols commonly assess only whether vulnerability is detected and ignore reasoning quality such as whether the model can correctly localize the vulnerable file, function, and CWE type.",
    "task_io_type": "ä»£ç åˆ°å®‰å…¨åˆ†æ",
    "task_io_type_quote": "evaluating LLMsâ€™ capability in JavaScript vulnerability detection",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªåŸºäºLLMçš„JavaScriptæ¼æ´æ£€æµ‹ç³»ç»ŸåŸºå‡†ï¼›éµå¾ªä¸‰ä¸ªæ„å»ºåŸåˆ™ï¼šå…¨é¢æ€§ã€ä¸ä½ä¼°ã€ä¸é«˜ä¼°ï¼›è¦†ç›–218ç§CWEç±»å‹ï¼›æ”¯æŒå‡½æ•°çº§å’Œé¡¹ç›®çº§è¯„ä¼°ï¼›é€šè¿‡FORGEJSè‡ªåŠ¨ç”Ÿæˆï¼›åŒ…å«JUDGEJSè‡ªåŠ¨è¯„ä¼°æ¡†æ¶ï¼›æ—¨åœ¨å®¢è§‚å®šä¹‰LLMåœ¨å¯é‡å¤ã€å¯æ‰©å±•ã€éƒ¨ç½²å—é™æ¡ä»¶ä¸‹çš„ç°å®æ€§èƒ½ä¸Šä¸‹é™ã€‚",
    "unique_features_quote": "we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection that directly address these limitations: (1) comprehensiveness, (2) no underestimation, and (3) no overestimation... FORGEJS aggregates heterogeneous sources, covers 218 CWE types... supports both function-level and project-level evaluation... Taken together, these principles aim to objectively define the realistic upper and lower bounds of LLM performance under reproducible, scalable, deployment-constrained conditions.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": 1,
    "language_normalized": "['JavaScript']",
    "dimension_normalized": "['æ¼æ´æ£€æµ‹èƒ½åŠ›', 'æ¨ç†å……åˆ†æ€§', 'é²æ£’æ€§', 'ç°å®ä¸–ç•Œå¯ç”¨æ€§']",
    "evaluation_method_normalized": "['F1åˆ†æ•°', 'å‡é˜³æ€§ç‡ï¼ˆFPRï¼‰', 'VD-SæŒ‡æ ‡']",
    "problem_domain_normalized": "['JavaScriptå®‰å…¨æ¼æ´æ£€æµ‹', 'å‰ç«¯ï¼ˆå¦‚DOM-based XSSï¼‰', 'åç«¯ï¼ˆå¦‚SQLæ³¨å…¥ã€å‘½ä»¤æ³¨å…¥ã€åŸå‹æ±¡æŸ“ï¼‰', 'å…¨æ ˆç¯å¢ƒ']",
    "source_type_normalized": "['èšåˆå¼‚æ„æ¥æº', 'çœŸå®ä¸–ç•Œ', 'åˆæˆæ•°æ®']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2512.03421_output/content.md",
    "benchmark_name": "BugT",
    "benchmark_name_quote": "This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. ... We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMsâ€™ capabilities.",
    "dataset_url": "https://github.com/Xucranger/PLofLBFL",
    "dataset_url_quote": "To facilitate future study, we share our source code and experimental data in a GitHub repository 1. 1https://github.com/Xucranger/PLofLBFL",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨åˆå­¦è€…ç¨‹åºä¸­è¿›è¡Œæ•…éšœå®šä½ï¼ˆFault Localizationï¼‰çš„æ€§èƒ½ã€‚",
    "task_description_quote": "This study evaluates the fault localization performance of six closed-source LLMs (e.g., OpenAI o3, GPT-3.5-Turbo, etc.) and seven open-source LLMs (e.g., DeepSeekR1, Llama3, etc.) across Codeflaws, Condefects, and BugT datasets.",
    "dimension": "æ•…éšœå®šä½çš„å‡†ç¡®æ€§ã€æ•ˆç‡ã€å¯ç”¨æ€§ï¼Œä»¥åŠå¯¹é—®é¢˜éš¾åº¦çš„é²æ£’æ€§ã€‚",
    "dimension_quote": "To investigate these issues, we aim to empirically assess the performance of different LLMs across various datasets, evaluating their accuracy, efficiency, and usability in fault localization for novice programs. ... We examine how problem difficulty levels across three datasets affect LLMsâ€™ fault localization performance, providing insights into model behavior at varying complexity levels.",
    "evaluation_method": "åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¯„ä¼°LLMsçš„æ•…éšœå®šä½æ€§èƒ½ï¼Œå¹¶ä¸ä¼ ç»Ÿæ–¹æ³•ï¼ˆå¦‚SBFLã€MBFLï¼‰è¿›è¡Œæ¯”è¾ƒã€‚",
    "evaluation_method_quote": "This study evaluates the fault localization performance of six closed-source LLMs (e.g., OpenAI o3, GPT-3.5-Turbo, etc.) and seven open-source LLMs (e.g., DeepSeekR1, Llama3, etc.) across Codeflaws, Condefects, and BugT datasets. All closed-source LLMs outperform traditional SBFL and MBFL methods...",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "ç¼–ç¨‹æ•™è‚²ï¼Œåˆå­¦è€…ç¼–ç¨‹ã€‚",
    "problem_domain_quote": "Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. ... This study evaluates the fault localization performance ... for novice programs.",
    "problem_difficulty": "åŒ…å«ä¸åŒéš¾åº¦çº§åˆ«çš„é—®é¢˜ï¼Œä»ç®€å•åˆ°å¤æ‚ã€‚",
    "problem_difficulty_quote": "LLM accuracy decreases as problem difficulty increases in Codeflaws and Condefects, but top models maintain high accuracy even at peak difficulty in BugT, suggesting its lower complexity. ... We examine how problem difficulty levels across three datasets affect LLMsâ€™ fault localization performance...",
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "è‡ªå»ºçš„åŒ…å«çœŸå®ç¼–ç¨‹æ•…éšœçš„æ•°æ®é›†ã€‚",
    "source_type_quote": "We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMsâ€™ capabilities.",
    "last_updated": "2025-12-04",
    "last_updated_quote": "Preprint submitted to Journal of LATEX Templates December 4, 2025",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "We introduce a new self-created dataset with real programming faults...",
    "contamination_status": "ä¸“é—¨è®¾è®¡ä»¥å‡è½»æ•°æ®æ³„éœ²é—®é¢˜ã€‚",
    "contamination_status_quote": "BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "æ•…éšœå®šä½",
    "task_granularity_quote": "This study evaluates the fault localization performance...",
    "evaluation_metrics": "å‡†ç¡®æ€§ï¼ˆaccuracyï¼‰",
    "evaluation_metrics_quote": "LLM accuracy decreases as problem difficulty increases...",
    "input_modality": "åŒ…å«æ•…éšœçš„ä»£ç ",
    "input_modality_quote": "Novice Program refers to code written by novice programmers, usually in the early stages of learning programming. These programs usually contain multiple errors...",
    "output_modality": "æ•…éšœä½ç½®åŠè§£é‡Š",
    "output_modality_quote": "LLM generated fault explanations demonstrate significant value for novice programmer assistance...",
    "task_io_type": "ä»£ç åˆ°æ•…éšœä½ç½®/è§£é‡Š",
    "task_io_type_quote": "LLM-based fault localization methods leverage the modelsâ€™ understanding of program syntax and semantics to automatically localize faults in code, offering more precise and contextually relevant suggestions...",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“é—¨é’ˆå¯¹åˆå­¦è€…ç¨‹åºï¼ˆNovice Programsï¼‰æ„å»ºï¼Œæ—¨åœ¨å‡è½»LLMè¯„ä¼°ä¸­çš„æ•°æ®æ³„éœ²ï¼ˆdata leakageï¼‰é—®é¢˜ï¼ŒåŒ…å«çœŸå®çš„ç¼–ç¨‹æ•…éšœã€‚",
    "unique_features_quote": "BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. ... We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMsâ€™ capabilities.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": 12,
    "last_updated_day": 4,
    "language_normalized": "[]",
    "dimension_normalized": "['æ•…éšœå®šä½çš„å‡†ç¡®æ€§', 'æ•ˆç‡', 'å¯ç”¨æ€§', 'å¯¹é—®é¢˜éš¾åº¦çš„é²æ£’æ€§']",
    "evaluation_method_normalized": "['å‡†ç¡®æ€§ï¼ˆaccuracyï¼‰']",
    "problem_domain_normalized": "['ç¼–ç¨‹æ•™è‚²', 'åˆå­¦è€…ç¼–ç¨‹']",
    "source_type_normalized": "['è‡ªå»ºçš„åŒ…å«çœŸå®ç¼–ç¨‹æ•…éšœçš„æ•°æ®é›†']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2105.09938_output/content.md",
    "benchmark_name": "APPS",
    "benchmark_name_quote": "To meet this challenge, we introduce APPS, a benchmark for code generation.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To meet this challenge, we introduce APPS, a benchmark for code generation.",
    "dataset_url": "https://github.com/hendrycks/apps",
    "dataset_url_quote": "The dataset is available at https://github.com/hendrycks/apps.",
    "task_description": "ä»ä»»æ„è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆæ»¡è¶³è¦æ±‚çš„Pythonä»£ç ã€‚",
    "task_description_quote": "our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code.",
    "dimension": "ä»£ç ç”Ÿæˆèƒ½åŠ›ï¼ŒåŒ…æ‹¬ç†è§£ä»»åŠ¡æè¿°ã€è®¾è®¡ç®—æ³•ã€ç¼–å†™è¯­æ³•æ­£ç¡®ä¸”åŠŸèƒ½æ­£ç¡®çš„ç¨‹åºã€‚",
    "dimension_quote": "APPS evaluates models not only on their ability to code syntactically correct programs, but also on their ability to understand task descriptions and devise algorithms to solve these tasks.",
    "evaluation_method": "ä½¿ç”¨æµ‹è¯•ç”¨ä¾‹æ£€æŸ¥ç”Ÿæˆçš„ä»£ç ã€‚",
    "evaluation_method_quote": "Similar to how companies assess candidate software developers, we evaluate models by checking their generated code on test cases.",
    "context_dependency": "å•å‡½æ•°ï¼ˆCall-Based Formatï¼‰æˆ–å®Œæ•´è„šæœ¬ï¼ˆStandard Input/Output Formatï¼‰ã€‚",
    "context_dependency_quote": "â€¢ Call-Based Format problems generally provide initial starter code, usually in the form of a function header, and ask for the solution to be provided as the functionâ€™s return value.",
    "problem_domain": "ç¼–ç¨‹ä¸ç®—æ³•ï¼Œæ¶µç›–ä»ç®€å•å­—ç¬¦ä¸²æ“ä½œåˆ°å¤æ‚çš„å›¾è®ºã€æ•°æ®ç»“æ„ç­‰ç®—æ³•æŒ‘æˆ˜ã€‚",
    "problem_domain_quote": "Our benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges.",
    "problem_difficulty": "åˆ†ä¸ºå…¥é—¨çº§ã€é¢è¯•çº§å’Œç«èµ›çº§ä¸‰ä¸ªéš¾åº¦ã€‚",
    "problem_difficulty_quote": "It contains 10,000 programming problems at various levels of difficulty, covering simple introductory problems, interview-level problems, and coding competition challenges.",
    "language": "Python",
    "language_quote": "our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code.",
    "data_size": "åŒ…å«10,000ä¸ªç¼–ç¨‹é—®é¢˜ï¼Œ131,777ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œ232,421ä¸ªäººå·¥ç¼–å†™çš„å‚è€ƒç­”æ¡ˆã€‚",
    "data_size_quote": "The Automated Programming Progress Standard, abbreviated APPS, consists of 10,000 coding problems in total, with 131,777 test cases for checking solutions and 232,421 ground-truth solutions written by humans.",
    "source_type": "ä»å¼€æ”¾çš„ç¼–ç¨‹ç½‘ç«™ï¼ˆå¦‚Codeforces, Kattis, Codewars, AtCoderï¼‰æ‰‹åŠ¨æ”¶é›†å’Œæ•´ç†ã€‚",
    "source_type_quote": "The APPS dataset consists of problems collected from different open-access coding websites such as Codeforces, Kattis, and more.",
    "last_updated": "2021",
    "last_updated_quote": "35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks.",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±å¤šåç ”ç©¶ç”Ÿå’Œæœ¬ç§‘ç”Ÿåœ¨å…­ä¸ªæœˆå†…ç²¾å¿ƒæ•´ç†å’Œä¼˜åŒ–ã€‚",
    "build_type_quote": "Several graduate and undergraduate student authors polished and refined this dataset over the course of six months, ensuring a high-quality set of problems.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "APPS, a benchmark for code generation from natural language specifications.",
    "evaluation_metrics": "é€šè¿‡æµ‹è¯•ç”¨ä¾‹çš„å‡†ç¡®ç‡ï¼ˆä¾‹å¦‚ pass@kï¼‰ã€‚",
    "evaluation_metrics_quote": "we evaluate models by checking their generated code on test cases.",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "take an arbitrary natural language specification",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "generate satisfactory Python code",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "code generation from natural language specifications.",
    "execution_environment": "å…è®¸æ‰§è¡Œä»»æ„Pythonä»£ç ï¼ˆåŒ…æ‹¬å¯¼å…¥å¸¸è§æ¨¡å—å’Œåº“ï¼‰ï¼Œä½¿ç”¨è‡ªå®šä¹‰çš„æµ‹è¯•æ¡†æ¶ã€‚",
    "execution_environment_quote": "solutions are allowed to execute arbitrary Python code, and the results are compared against test cases for a given problem.",
    "unique_features": "1. é—®é¢˜æè¿°å¹³å‡é•¿åº¦è¾¾293.2ä¸ªå•è¯ï¼Œæ˜¯è‡ªåŒ…å«çš„å®Œæ•´è§„èŒƒã€‚2. æ‹¥æœ‰å¤§é‡æµ‹è¯•ç”¨ä¾‹ï¼ˆè¶…è¿‡13ä¸‡ä¸ªï¼‰ç”¨äºä¸¥æ ¼è¯„ä¼°åŠŸèƒ½æ­£ç¡®æ€§ã€‚3. éš¾åº¦åˆ†çº§æ˜ç¡®ï¼Œè¦†ç›–ä»å…¥é—¨åˆ°ç«èµ›çš„å¹¿æ³›èŒƒå›´ã€‚4. æ¨¡æ‹Ÿäº†äººç±»ç¨‹åºå‘˜ï¼ˆå¦‚æ±‚èŒé¢è¯•ï¼‰çš„è¯„ä¼°æ–¹å¼ã€‚",
    "unique_features_quote": "By comparison, problem specifications in our new APPS benchmark are self-contained and have a much larger average length of 293.2 words. Unlike Iyer et al. (2018), APPS contains test cases for every exercise, enabling a high-quality evaluation of code correctness. The APPS benchmark attempts to mirror how humans programmers are evaluated by posing coding problems in unrestricted natural language and using test cases to evaluate solution correctness.",
    "data_size_quantity": 10000,
    "data_size_unit": "ä¸ªç¼–ç¨‹é—®é¢˜",
    "last_updated_year": 2021,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆèƒ½åŠ›', 'ç†è§£ä»»åŠ¡æè¿°', 'è®¾è®¡ç®—æ³•', 'ç¼–å†™è¯­æ³•æ­£ç¡®ä¸”åŠŸèƒ½æ­£ç¡®çš„ç¨‹åº']",
    "evaluation_method_normalized": "['é€šè¿‡æµ‹è¯•ç”¨ä¾‹çš„å‡†ç¡®ç‡', 'pass@k']",
    "problem_domain_normalized": "['ç¼–ç¨‹ä¸ç®—æ³•', 'ç®€å•å­—ç¬¦ä¸²æ“ä½œ', 'å¤æ‚çš„å›¾è®º', 'æ•°æ®ç»“æ„', 'ç®—æ³•æŒ‘æˆ˜']",
    "source_type_normalized": "['å¼€æ”¾çš„ç¼–ç¨‹ç½‘ç«™', 'Codeforces', 'Kattis', 'Codewars', 'AtCoder', 'æ‰‹åŠ¨æ”¶é›†å’Œæ•´ç†']",
    "problem_difficulty_normalized": "å…¥é—¨çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2107.03374_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We evaluate functional correctness on a set of 164 hand-written programming problems, which we call the HumanEval dataset. We release the HumanEval dataset so that others can evaluate functional correctness and measure the problem-solving capabilities of their models.",
    "dataset_url": "https://www.github.com/openai/human-eval",
    "dataset_url_quote": "We release this data along with an evaluation framework at https://www.github.com/openai/human-eval.",
    "task_description": "ä»æ–‡æ¡£å­—ç¬¦ä¸²ï¼ˆdocstringsï¼‰åˆæˆç‹¬ç«‹çš„Pythonå‡½æ•°ï¼Œå¹¶è¯„ä¼°ç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚",
    "task_description_quote": "In this work, we focus on the task of generating standalone Python functions from docstrings, and evaluate the correctness of code samples automatically through unit tests.",
    "dimension": "ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "measure functional correctness for synthesizing programs from docstrings",
    "evaluation_method": "é€šè¿‡å•å…ƒæµ‹è¯•è¯„ä¼°åŠŸèƒ½æ­£ç¡®æ€§ï¼Œå¹¶ä½¿ç”¨pass@kæŒ‡æ ‡ã€‚",
    "evaluation_method_quote": "evaluate the correctness of code samples automatically through unit tests. Kulal et al. (2019) evaluate functional correctness using the pass@k metric",
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "generating standalone Python functions",
    "problem_domain": "è¯­è¨€ç†è§£ã€æ¨ç†ã€ç®—æ³•å’Œç®€å•æ•°å­¦",
    "problem_domain_quote": "Programming tasks in the HumanEval dataset assess language comprehension, reasoning, algorithms, and simple mathematics.",
    "problem_difficulty": "è¯„ä¼°è¯­è¨€ç†è§£ã€ç®—æ³•å’Œç®€å•æ•°å­¦ï¼Œéƒ¨åˆ†é—®é¢˜ç±»ä¼¼äºç®€å•çš„è½¯ä»¶é¢è¯•é¢˜ã€‚",
    "problem_difficulty_quote": "These problems assess language comprehension, algorithms, and simple mathematics, with some comparable to simple software interview questions.",
    "language": "Python",
    "language_quote": "study its Python code-writing capabilities. ... generating standalone Python functions from docstrings",
    "data_size": "åŒ…å«164ä¸ªæ‰‹å†™çš„ç¼–ç¨‹é—®é¢˜ï¼Œå¹³å‡æ¯ä¸ªé—®é¢˜æœ‰7.7ä¸ªæµ‹è¯•ç”¨ä¾‹ã€‚",
    "data_size_quote": "a dataset of 164 original programming problems with unit tests. ... an average of 7.7 tests per problem.",
    "source_type": "æ‰‹å†™æ„å»ºï¼Œæœªä»ç°æœ‰æ¥æºç¨‹åºåŒ–å¤åˆ¶ã€‚",
    "source_type_quote": "all problems were hand-written and not programmatically copied from existing sources.",
    "last_updated": "2021",
    "last_updated_quote": "arXiv:2107.03374v2 [cs.LG] 14 Jul 2021",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆOpenAIå›¢é˜Ÿæ‰‹å†™ï¼‰",
    "build_type_quote": "we create a dataset of 164 original programming problems. ... all problems were hand-written",
    "contamination_status": "ä¸ºæŠ—æ±¡æŸ“è€Œè®¾è®¡ï¼Œæ‰‹å†™é—®é¢˜ä»¥é¿å…è®­ç»ƒæ•°æ®ï¼ˆGitHubï¼‰ä¸­å·²å­˜åœ¨çš„è§£å†³æ–¹æ¡ˆã€‚",
    "contamination_status_quote": "It is important for these tasks to be hand-written, since our models are trained on a large fraction of GitHub, which already contains solutions to problems from a variety of sources.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»æ–‡æ¡£å­—ç¬¦ä¸²ç”Ÿæˆå®Œæ•´å‡½æ•°ä½“ï¼‰",
    "task_granularity_quote": "generating standalone Python functions from docstrings",
    "evaluation_metrics": "pass@k",
    "evaluation_metrics_quote": "Kulal et al. (2019) evaluate functional correctness using the pass@k metric",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆæ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ä¸ä»£ç ï¼ˆå‡½æ•°ç­¾åï¼‰",
    "input_modality_quote": "synthesizing programs from docstrings. ... prompt consisting of a header, a signature, and a docstring",
    "output_modality": "ä»£ç ï¼ˆPythonå‡½æ•°ä½“ï¼‰",
    "output_modality_quote": "generating standalone Python functions",
    "task_io_type": "æ–‡æœ¬ï¼ˆæ–‡æ¡£å­—ç¬¦ä¸²ï¼‰åˆ°ä»£ç ",
    "task_io_type_quote": "synthesizing programs from docstrings",
    "execution_environment": "ä½¿ç”¨gVisorå®¹å™¨è¿è¡Œæ—¶çš„æ²™ç›’ç¯å¢ƒï¼Œç”¨äºå®‰å…¨æ‰§è¡Œä¸å—ä¿¡ä»»çš„ç¨‹åºã€‚",
    "execution_environment_quote": "we developed a sandbox environment to safely run untrusted programs against unit tests. We selected the gVisor container runtime (Lacasse, 2018) as the main host protection component.",
    "unique_features": "ä¸“é—¨ä¸ºè¯„ä¼°ä»£ç ç”Ÿæˆæ¨¡å‹çš„åŠŸèƒ½æ­£ç¡®æ€§è€Œè®¾è®¡ï¼Œå¼ºè°ƒæ‰‹å†™é—®é¢˜ä»¥é¿å…æ•°æ®æ±¡æŸ“ï¼Œå¹¶æä¾›äº†å®‰å…¨çš„æ²™ç›’æ‰§è¡Œç¯å¢ƒã€‚",
    "unique_features_quote": "It is important for these tasks to be hand-written, since our models are trained on a large fraction of GitHub, which already contains solutions to problems from a variety of sources. ... we developed a sandbox environment to safely run untrusted programs against unit tests.",
    "data_size_quantity": 164,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": 2021,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@k']",
    "problem_domain_normalized": "['è¯­è¨€ç†è§£', 'æ¨ç†', 'ç®—æ³•', 'ç®€å•æ•°å­¦']",
    "source_type_normalized": "['æ‰‹å†™æ„å»º']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2108.07732_output/content.md",
    "benchmark_name": "Mostly Basic Programming Problems (MBPP)",
    "benchmark_name_quote": "We introduce two datasets to test Python code synthesis. The first is a new dataset called Mostly Basic Programming Problems (MBPP).",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce two datasets to test Python code synthesis. The first is a new dataset called Mostly Basic Programming Problems (MBPP).",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»è‡ªç„¶è¯­è¨€æè¿°ä¸­åˆæˆç®€çŸ­çš„Pythonç¨‹åºã€‚",
    "task_description_quote": "Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions.",
    "dimension": "ç¨‹åºåˆæˆèƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯ä»è‡ªç„¶è¯­è¨€æè¿°ç”ŸæˆåŠŸèƒ½æ­£ç¡®çš„Pythonä»£ç ã€‚",
    "dimension_quote": "This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages.",
    "evaluation_method": "é€šè¿‡æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹æ£€æŸ¥åŠŸèƒ½æ­£ç¡®æ€§ã€‚",
    "evaluation_method_quote": "Participants also provided a ground-truth solution that passes all three test cases.",
    "context_dependency": "å•å‡½æ•°ï¼Œè‡ªåŒ…å«ã€‚",
    "context_dependency_quote": "Participants were instructed to write code that is self-contained (that is, it runs by itself)...",
    "problem_domain": "æ¶µç›–æ•°å­¦ã€åˆ—è¡¨å¤„ç†ã€å­—ç¬¦ä¸²å¤„ç†ã€æ•´æ•°åºåˆ—ç­‰ã€‚",
    "problem_domain_quote": "Of these questions, 58% were mathematical in nature (e.g., calculating the volume of a sphere), 43% involve list processing, 19% require string processing, 9% involve integer seque",
    "problem_difficulty": "å…¥é—¨çº§ï¼Œè®¾è®¡ä¸ºå…¥é—¨çº§ç¨‹åºå‘˜å¯è§£å†³ã€‚",
    "problem_difficulty_quote": "The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers.",
    "language": "Python",
    "language_quote": "The Mostly Basic Programming Problems dataset contains 974 short Python programs...",
    "data_size": "åŒ…å«974ä¸ªç¼–ç¨‹ä»»åŠ¡ã€‚",
    "data_size_quote": "The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks...",
    "source_type": "é€šè¿‡ä¼—åŒ…æ„å»ºï¼Œéƒ¨åˆ†ç”±ä½œè€…ç¼–è¾‘å’Œæ‰‹åŠ¨éªŒè¯ã€‚",
    "source_type_quote": "This dataset consists of a large set of crowd-sourced questions and a smaller set of questions edited and hand-verified by the authors.",
    "last_updated": "2021",
    "last_updated_quote": "arXiv:2108.07732v1  [cs.PL]  16 Aug 2021",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆGoogle Researchï¼‰",
    "build_type_quote": "We construct two new datasets: one entirely new and the other modified from an existing benchmark. The first, Mostly Basic Programming Problems (MBPP), is an entirely new crowd-sourced programming dataset.",
    "contamination_status": "æ–‡ä¸­æåŠä¸é¢„è®­ç»ƒé›†çš„é‡å å¾ˆå°ï¼Œé™ä½äº†è®°å¿†é£é™©ã€‚",
    "contamination_status_quote": "Second, we find that the overlap between the solutions in MBPP and the pre-training set is small, reducing the chance that our synthesis results are due to memorization (Section 4.8).",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆå®Œæ•´çš„å‡½æ•°ä½“ï¼‰ã€‚",
    "task_granularity_quote": "Participants were instructed to write a short problem statement, a single self-contained Python function solving the problem specified...",
    "evaluation_metrics": "åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆé€šè¿‡æµ‹è¯•ç”¨ä¾‹ï¼‰ã€‚",
    "evaluation_metrics_quote": "Participants also provided a ground-truth solution that passes all three test cases.",
    "input_modality": "è‡ªç„¶è¯­è¨€æè¿°ï¼Œé€šå¸¸ç»“åˆå°‘é‡è¾“å…¥è¾“å‡ºç¤ºä¾‹ã€‚",
    "input_modality_quote": "a program can be specified by a short natural language description, possibly combined with a few (e.g., 2 or 3) input-output examples.",
    "output_modality": "ä»£ç ï¼ˆPythonå‡½æ•°ï¼‰ã€‚",
    "output_modality_quote": "Participants were instructed to write a short problem statement, a single self-contained Python function solving the problem specified...",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions.",
    "execution_environment": "è‡ªåŒ…å«ï¼Œå¯ä½¿ç”¨æ ‡å‡†åº“ï¼Œå…è®¸ä½¿ç”¨äº’è”ç½‘å‚è€ƒã€‚",
    "execution_environment_quote": "Participants were instructed to write code that is self-contained (that is, it runs by itself)... Use of internet references was allowed.",
    "unique_features": "åŒ…å«ä¸‰ä¸ªä¸€è‡´çš„ã€ä»¥assertè¯­å¥ç¼–å†™çš„è¾“å…¥è¾“å‡ºç¤ºä¾‹ï¼›é—®é¢˜æè¿°è®¾è®¡ä¸ºæ›´åŸºç¡€ã€å­—é¢åŒ–ï¼Œè€Œéç«èµ›é£æ ¼ï¼›åŒ…å«ä¼—åŒ…å’Œæ‰‹åŠ¨éªŒè¯ä¸¤éƒ¨åˆ†ã€‚",
    "unique_features_quote": "In contrast, our dataset consistently contains three I/O examples, written as assert statements. ... By contrast, our Mostly Basic Programming Problems dataset is designed to contain a more basic, literal description of the problems. ... This dataset consists of a large set of crowd-sourced questions and a smaller set of questions edited and hand-verified by the authors.",
    "data_size_quantity": 974,
    "data_size_unit": "ä¸ªç¼–ç¨‹ä»»åŠ¡",
    "last_updated_year": 2021,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ç¨‹åºåˆæˆèƒ½åŠ›', 'ç‰¹åˆ«æ˜¯ä»è‡ªç„¶è¯­è¨€æè¿°ç”ŸæˆåŠŸèƒ½æ­£ç¡®çš„Pythonä»£ç ']",
    "evaluation_method_normalized": "['åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆé€šè¿‡æµ‹è¯•ç”¨ä¾‹ï¼‰']",
    "problem_domain_normalized": "['æ•°å­¦', 'åˆ—è¡¨å¤„ç†', 'å­—ç¬¦ä¸²å¤„ç†', 'æ•´æ•°åºåˆ—']",
    "source_type_normalized": "['é€šè¿‡ä¼—åŒ…æ„å»º', 'éƒ¨åˆ†ç”±ä½œè€…ç¼–è¾‘å’Œæ‰‹åŠ¨éªŒè¯']",
    "problem_difficulty_normalized": "å…¥é—¨çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æ ‡å‡†åº“",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2203.13474_output/content.md",
    "benchmark_name": "Multi-Turn Programming Benchmark (MTPB)",
    "benchmark_name_quote": "To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "In this work, we develop a multi-turn programming benchmark to measure the modelsâ€™ capacity for multi-turn program synthesis. To the best of our knowledge, this is the first multi-turn program synthesis benchmark, which allows quantitative analysis of multi-turn program synthesis.",
    "dataset_url": "https://github.com/salesforce/CodeGen/tree/main/benchmark",
    "dataset_url_quote": "1Benchmark: https://github.com/salesforce/CodeGen/tree/main/benchmark",
    "task_description": "è¯„æµ‹æ¨¡å‹çš„å¤šè½®ç¨‹åºåˆæˆèƒ½åŠ›ã€‚ç”¨æˆ·ä»¥è‡ªç„¶è¯­è¨€åœ¨å¤šè½®å¯¹è¯ä¸­é€æ­¥æŒ‡å®šæ„å›¾ï¼Œæ¨¡å‹åœ¨æ¯ä¸€è½®ä¸­åˆæˆå­ç¨‹åºï¼Œæœ€ç»ˆå…±åŒå®Œæˆä¸€ä¸ªå®Œæ•´çš„ç¨‹åºã€‚",
    "task_description_quote": "To solve a problem in the benchmark, a model needs to synthesize a program in multiple steps with a user who specifies the intent in each turn in natural language.",
    "dimension": "å¤šè½®ç¨‹åºåˆæˆèƒ½åŠ›",
    "dimension_quote": "In this work, we develop a multi-turn programming benchmark to measure the modelsâ€™ capacity for multi-turn program synthesis.",
    "evaluation_method": "é€šè¿‡ä¸“å®¶ç¼–å†™çš„æµ‹è¯•ç”¨ä¾‹çš„é€šè¿‡ç‡ï¼ˆpass rateï¼‰æ¥è¡¡é‡æ€§èƒ½ã€‚",
    "evaluation_method_quote": "Performance on the benchmark is measured by pass rate on expert-written test cases.",
    "context_dependency": "å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ã€‚ä¸€ä¸ªå®Œæ•´çš„ç¨‹åºè¢«åˆ†è§£ä¸ºå¤šä¸ªå­é—®é¢˜ï¼Œæ¯ä¸€è½®å¯¹è¯æŒ‡å®šä¸€ä¸ªå­é—®é¢˜çš„æ„å›¾ã€‚",
    "context_dependency_quote": "consisting of 115 diverse problem sets that are factorized into multi-turn prompts.",
    "problem_domain": "é€šç”¨ç¼–ç¨‹é—®é¢˜ã€‚æ–‡ä¸­ç¤ºä¾‹ä¸ºä»ç”µå­é‚®ä»¶åœ°å€ä¸­æå–ç”¨æˆ·åã€‚",
    "problem_domain_quote": "Please refer to Figure 1 for an example where the model synthesizes a program to extract the user name of an email address.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æœªæ˜ç¡®æŒ‡å®šï¼Œä½†æ ¹æ®æ¨¡å‹è®­ç»ƒå’Œä¸Šä¸‹æ–‡ï¼ˆå¦‚HumanEvaléƒ¨åˆ†ï¼‰ï¼Œå¯èƒ½ä¸»è¦æ¶‰åŠPythonã€‚",
    "language_quote": "æ–‡ä¸­æœªæ˜ç¡®æè¿°MTPBçš„ç¼–ç¨‹è¯­è¨€ã€‚",
    "data_size": "åŒ…å«115ä¸ªå¤šæ ·åŒ–çš„é—®é¢˜é›†ã€‚",
    "data_size_quote": "consisting of 115 diverse problem sets that are factorized into multi-turn prompts.",
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2023ï¼ˆæ ¹æ®è®ºæ–‡ç‰ˆæœ¬æ—¥æœŸæ¨æ–­ï¼‰",
    "last_updated_quote": "arXiv:2203.13474v5 [cs.LG] 27 Feb 2023",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB)",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "å¤šè½®ç¨‹åºåˆæˆ",
    "task_granularity_quote": "To solve a problem in the benchmark, a model needs to synthesize a program in multiple steps",
    "evaluation_metrics": "é€šè¿‡ç‡ï¼ˆpass rateï¼‰",
    "evaluation_metrics_quote": "Performance on the benchmark is measured by pass rate on expert-written test cases.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆå¤šè½®å¯¹è¯ï¼‰",
    "input_modality_quote": "a user who specifies the intent in each turn in natural language.",
    "output_modality": "ä»£ç ï¼ˆå­ç¨‹åºæˆ–å®Œæ•´ç¨‹åºï¼‰",
    "output_modality_quote": "synthesize a program",
    "task_io_type": "è‡ªç„¶è¯­è¨€åˆ°ä»£ç ",
    "task_io_type_quote": "synthesize a program... with a user who specifies the intent in each turn in natural language.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªä¸“æ³¨äºå¤šè½®ç¨‹åºåˆæˆçš„è¯„æµ‹åŸºå‡†ã€‚é—®é¢˜è¢«åˆ†è§£ä¸ºå¤šè½®æç¤ºï¼Œæ¨¡æ‹Ÿç”¨æˆ·ä¸ç³»ç»Ÿé€æ­¥åä½œå®Œæˆç¼–ç¨‹ä»»åŠ¡çš„è¿‡ç¨‹ã€‚",
    "unique_features_quote": "To the best of our knowledge, this is the first multi-turn program synthesis benchmark, which allows quantitative analysis of multi-turn program synthesis.",
    "data_size_quantity": 115,
    "data_size_unit": "ä¸ªé—®é¢˜é›†",
    "last_updated_year": 2023,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['å¤šè½®ç¨‹åºåˆæˆèƒ½åŠ›']",
    "evaluation_method_normalized": "['é€šè¿‡ç‡ï¼ˆpass rateï¼‰']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹é—®é¢˜']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2207.01780_output/content.md",
    "benchmark_name": "APPS",
    "benchmark_name_quote": "Our comprehensive experiments (Section 4) show that our models can achieve SOTA performance on the challenging APPS benchmark [Hendrycks et al., 2021].",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Our comprehensive experiments (Section 4) show that our models can achieve SOTA performance on the challenging APPS benchmark [Hendrycks et al., 2021].",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ç¨‹åºåˆæˆæˆ–ä»£ç ç”Ÿæˆï¼Œæ—¨åœ¨æ ¹æ®è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°ç”Ÿæˆæ»¡è¶³åŠŸèƒ½æ­£ç¡®æ€§çš„ç¨‹åºã€‚",
    "task_description_quote": "Program synthesis or code generation aims to generate a program that satisfies a problem specification.",
    "dimension": "ç¨‹åºçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "The expected output is a program to be checked for functional correctness against some unit tests.",
    "evaluation_method": "ä½¿ç”¨å•å…ƒæµ‹è¯•æ£€æŸ¥ç¨‹åºçš„åŠŸèƒ½æ­£ç¡®æ€§ï¼Œå¹¶é‡‡ç”¨ pass@k æŒ‡æ ‡è¿›è¡Œè¯„ä¼°ã€‚",
    "evaluation_method_quote": "Specifically, our models reach more than 2% pass@1, 6% pass@5, and 20% pass@1000.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "é€šç”¨ç¼–ç¨‹ï¼ŒèŒƒå›´ä»åŸºç¡€ç¼–ç¨‹é—®é¢˜åˆ°ç«èµ›çº§ç¼–ç¨‹ä»»åŠ¡ã€‚",
    "problem_domain_quote": "This type of programming task can range from basic programming problems to competition-level programming tasks that require a high level of problem-solving skills.",
    "problem_difficulty": "å…·æœ‰æŒ‘æˆ˜æ€§ï¼ŒåŒ…å«ç«èµ›çº§å¤æ‚é—®é¢˜ã€‚",
    "problem_difficulty_quote": "Our comprehensive experiments (Section 4) show that our models can achieve SOTA performance on the challenging APPS benchmark [Hendrycks et al., 2021].",
    "language": "Python",
    "language_quote": "In our work, we focus on program synthesis from natural language problem specifications and the output programs are in general-purpose languages such as Python.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "å®Œæ•´ç¨‹åºç”Ÿæˆ",
    "task_granularity_quote": "Training only with NTP objective is hence, not ideal to tackle full program generation to solve programming problems.",
    "evaluation_metrics": "pass@1, pass@5, pass@1000",
    "evaluation_metrics_quote": "Specifically, our models reach more than 2% pass@1, 6% pass@5, and 20% pass@1000.",
    "input_modality": "è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°ï¼Œé€šå¸¸åŒ…å«ç¤ºä¾‹è¾“å…¥è¾“å‡ºå¯¹ã€‚",
    "input_modality_quote": "Each task is defined by a problem specification in natural language, often containing example input and output pairs.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "The expected output is a program to be checked for functional correctness against some unit tests.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "Program synthesis or code generation is the task of designing and building an executable computer program that satisfies a problem specification.",
    "execution_environment": "ç¼–è¯‘å™¨",
    "execution_environment_quote": "These program candidates are concatenated with the error information received from a compiler and passed to a program repair module.",
    "unique_features": "è¯¥åŸºå‡†ï¼ˆAPPSï¼‰åŒ…å«ä»åŸºç¡€åˆ°ç«èµ›çº§åˆ«çš„ç¼–ç¨‹é—®é¢˜ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹ç”ŸæˆåŠŸèƒ½æ­£ç¡®ç¨‹åºçš„èƒ½åŠ›ã€‚",
    "unique_features_quote": "This type of programming task can range from basic programming problems to competition-level programming tasks that require a high level of problem-solving skills.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ç¨‹åºçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@1', 'pass@5', 'pass@1000']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹', 'èŒƒå›´ä»åŸºç¡€ç¼–ç¨‹é—®é¢˜åˆ°ç«èµ›çº§ç¼–ç¨‹ä»»åŠ¡']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "ç«èµ›çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2207.10397_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS, and CodeContests",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS, and CodeContests",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»è‡ªç„¶è¯­è¨€æè¿°ï¼ˆä»£ç æ³¨é‡Šï¼‰å’Œå‡½æ•°ç­¾åç­‰ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸­ç”Ÿæˆä»£ç è§£å†³æ–¹æ¡ˆã€‚",
    "task_description_quote": "The task of code generation is to solve a programming problem: generate code solution x based on context c. As shown in Figure 2, context c contains natural language problem description in the form of code comment, and a code snippet that includes statements such as imports and the function header.",
    "dimension": "ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "A natural way to evaluate the quality and correctness of a code solution is to run it against a set of test cases",
    "evaluation_method": "æ‰§è¡Œå•å…ƒæµ‹è¯•ï¼Œä½¿ç”¨ pass@k æŒ‡æ ‡",
    "evaluation_method_quote": "We evaluate functional correctness using pass@1. ... For instance, Codex ... can achieve a pass@100 (pass if one or more among 100 generated solutions for a given problem can pass the corresponding test cases) of 77.4%, but a pass@1 (correct rate of a single solution) of only 33.5% on the HumanEval benchmark",
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "Figure 2: Code generation and test case generation: an example from the HumanEval benchmark. ... Context c contains natural language problem description in the form of code comment, and a code snippet that includes statements such as imports and the function header.",
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Python",
    "language_quote": "Figure 2: Code generation and test case generation: an example from the HumanEval benchmark. ... from typing import List def has_close_elements(numbers: List[float], threshold: float) -> bool:",
    "data_size": "164ä¸ªé—®é¢˜",
    "data_size_quote": "Benchmark Problems ... HumanEval 164",
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "The task of code generation is to solve a programming problem: generate code solution x based on context c.",
    "evaluation_metrics": "pass@1, pass@100",
    "evaluation_metrics_quote": "For instance, Codex ... can achieve a pass@100 (pass if one or more among 100 generated solutions for a given problem can pass the corresponding test cases) of 77.4%, but a pass@1 (correct rate of a single solution) of only 33.5% on the HumanEval benchmark",
    "input_modality": "è‡ªç„¶è¯­è¨€ä¸ä»£ç ï¼ˆå‡½æ•°ç­¾åï¼‰",
    "input_modality_quote": "context c contains natural language problem description in the form of code comment, and a code snippet that includes statements such as imports and the function header.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "generate code solution x",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "The task of code generation is to solve a programming problem: generate code solution x based on context c.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "æ¯ä¸ªé—®é¢˜å¹³å‡æœ‰7.77ä¸ªåœ°é¢çœŸå€¼æµ‹è¯•ç”¨ä¾‹",
    "unique_features_quote": "Benchmark ... GT Tests ... HumanEval ... 7.77",
    "data_size_quantity": 164,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@1', 'pass@100']",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€ä¸ä»£ç ",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2208.08227_output/content.md",
    "benchmark_name": "MultiPL-E",
    "benchmark_name_quote": "We propose MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We propose MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the ï¬rst massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages.",
    "dataset_url": "github.com/nuprl/MultiPL-E",
    "dataset_url_quote": "The MultiPL-E system, dataset, and tutorial are available at github.com/nuprl/MultiPL-E.",
    "task_description": "å°†åŸºäºå•å…ƒæµ‹è¯•çš„ä»£ç ç”ŸæˆåŸºå‡†ï¼ˆä»Pythonï¼‰ç¿»è¯‘åˆ°æ–°çš„ç¼–ç¨‹è¯­è¨€ï¼Œä»¥åˆ›å»ºå¤§è§„æ¨¡ã€å¤šè¯­è¨€çš„å¹¶è¡Œä»£ç ç”Ÿæˆè¯„æµ‹åŸºå‡†ã€‚",
    "task_description_quote": "MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the ï¬rst massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages.",
    "dimension": "å¤šè¯­è¨€ä»£ç ç”Ÿæˆèƒ½åŠ›ï¼Œæ¨¡å‹åœ¨ä¸åŒç¼–ç¨‹è¯­è¨€ä¸Šçš„æ³›åŒ–æ€§èƒ½",
    "dimension_quote": "We use these new parallel benchmarks to evaluate the multi-language performance of three state-of-the-art code generation models... The range of programming languages represented in MultiPL-E allow us to explore the impact of language frequency and language features on model performance.",
    "evaluation_method": "é€šè¿‡å•å…ƒæµ‹è¯•è¯„ä¼°ç”Ÿæˆä»£ç çš„æ­£ç¡®æ€§ï¼Œä½¿ç”¨pass@kç­‰æŒ‡æ ‡",
    "evaluation_method_quote": "We judge a generated function correct if it passes all tests... it is common to sample multiple completions per problem and report an estimated pass rate (Â§4.2).",
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "All of the problems are functions that receive and return ï¬rst-order values, which facilitates unit testing and test translation.",
    "problem_domain": "é€šç”¨ç¼–ç¨‹é—®é¢˜ï¼Œæ¶µç›–ç®—æ³•å’Œæ•°æ®å¤„ç†",
    "problem_domain_quote": "It is a diverse collection of 164 problems... All of the problems are functions that receive and return ï¬rst-order values.",
    "problem_difficulty": "å…·æœ‰æŒ‘æˆ˜æ€§",
    "problem_difficulty_quote": "Moreover, it is a challenging benchmark: the best model evaluated by Fried et al. [4] achieves only a 36% pass rate on Python.",
    "language": "Pythonï¼ˆæºè¯­è¨€ï¼‰åŠé€šè¿‡MultiPL-Eç¿»è¯‘çš„18ç§å…¶ä»–ç¼–ç¨‹è¯­è¨€ï¼ˆåŒ…æ‹¬JavaScript, C++, Scala, TypeScriptç­‰ï¼‰",
    "language_quote": "We use MultiPL-E to extend the HumanEval benchmark [1] and MBPP benchmark [2] to 18 languages that encompass a range of programming paradigms and popularity... MultiPL-E supports 18 languages and is straightforward to extend with more.",
    "data_size": "é€šè¿‡ç¿»è¯‘HumanEvalï¼ˆ164ä¸ªé—®é¢˜ï¼‰å’ŒMBPPï¼ˆæœªæ˜ç¡®æ•°é‡ï¼‰ä¸¤ä¸ªåŸºå‡†ï¼Œä¸º19ç§è¯­è¨€ï¼ˆPython + 18ç§ï¼‰åˆ›å»ºå¹¶è¡Œé—®é¢˜é›†",
    "data_size_quote": "We create the ï¬rst massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages... HumanEval is a diverse collection of 164 problems.",
    "source_type": "åŸºäºç°æœ‰PythonåŸºå‡†ï¼ˆHumanEvalå’ŒMBPPï¼‰é€šè¿‡è‡ªåŠ¨åŒ–ç¼–è¯‘å™¨ç¿»è¯‘ç”Ÿæˆ",
    "source_type_quote": "We use MultiPL-E to translate two widely-used code generation benchmarks, HumanEval [1] and MBPP [2], into 18 languages.",
    "last_updated": "2022",
    "last_updated_quote": "arXiv:2208.08227v4  [cs.LG]  19 Dec 2022",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆè®ºæ–‡ä½œè€…æ„å»ºï¼‰",
    "build_type_quote": "We propose MultiPL-E... We create the ï¬rst massively multilingual code generation benchmark...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆå‡½æ•°ä½“ï¼‰",
    "task_granularity_quote": "We focus on the natural-language-to-code task (NL2Code): given the description of a function in natural language, complete the function body.",
    "evaluation_metrics": "passç‡ï¼ˆåŸºäºå•å…ƒæµ‹è¯•ï¼‰",
    "evaluation_metrics_quote": "We judge a generated function correct if it passes all tests... it is common to sample multiple completions per problem and report an estimated pass rate (Â§4.2).",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆå‡½æ•°æè¿°ï¼‰ä¸ä»£ç ï¼ˆå‡½æ•°ç­¾åã€ç±»å‹æ³¨è§£ã€ç¤ºä¾‹ï¼‰",
    "input_modality_quote": "The prompt has several sources of information for the model: the function signature (its name and parameters); a brief comment describing the function; and, optionally, examples in the form of Python doctests.",
    "output_modality": "ä»£ç ï¼ˆå‡½æ•°ä½“ï¼‰",
    "output_modality_quote": "Given the prompt as input, the code generation model generates a completion that is likely to follow the given prompt.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "We focus on the natural-language-to-code task (NL2Code): given the description of a function in natural language, complete the function body.",
    "execution_environment": "å®¹å™¨åŒ–æ²™ç®±ï¼Œç”¨äºç¼–è¯‘ï¼ˆå¦‚éœ€è¦ï¼‰å’Œè¿è¡Œç¨‹åº",
    "execution_environment_quote": "MultiPL-E also includes a containerized sandbox that (1) compiles programs if necessary, (2) runs them with appropriate timeouts, (3) validates their results on unit tests, and (4) classiï¬es each output as successful, syntax error, etc.",
    "unique_features": "1. å¯æ‰©å±•å’Œå¯æ‰©å±•çš„ç³»ç»Ÿï¼Œç”¨äºå°†ä»£ç ç”ŸæˆåŸºå‡†ç¼–è¯‘åˆ°æ–°çš„ç¼–ç¨‹è¯­è¨€ã€‚2. åˆ›å»ºäº†ç¬¬ä¸€ä¸ªå¤§è§„æ¨¡ã€å¤šè¯­è¨€ã€å¹¶è¡Œçš„ä»£ç ç”ŸæˆåŸºå‡†ã€‚3. é€šè¿‡ä¸€å¥—å°å‹ç¼–è¯‘å™¨ï¼ˆæ¯ä¸ªçº¦200è¡Œä»£ç ï¼‰å®ç°ç¿»è¯‘ï¼Œä»…éœ€ç¿»è¯‘å‡½æ•°ç­¾åã€å•å…ƒæµ‹è¯•ã€æ³¨é‡Šå’Œç±»å‹æ³¨è§£ï¼Œè€Œæ— éœ€ç¿»è¯‘å‡½æ•°ä½“ã€‚4. åŒ…å«ä¸€ä¸ªè§„åˆ™å·¥å…·ï¼Œç”¨äºå°†æ³¨é‡Šä¸­çš„æŠ€æœ¯æœ¯è¯­ç¿»è¯‘å¾—æ›´ç¬¦åˆç›®æ ‡è¯­è¨€ä¹ æƒ¯ã€‚",
    "unique_features_quote": "MultiPL-E uses a suite of 18 little compilers from Python benchmarks to each target language... Each compiler must be able to translate four components from Python: (1) a function signature (name and arguments), (2) simple unit tests, (3) a comment describing the expected function behavior, and (4) type annotations if the target language is statically typed. Notably, the compiler does not have to translate the body of a function, since it is the job of the code generation model to synthesize it. Thus each MultiPL-E compiler is approximately 200 LOC and easy to build. MultiPL-E also includes a simple, rule-based tool to translate technical terms in comments to be more language appropriate, e.g. a Python list is approximately a C++ vector.",
    "data_size_quantity": 164,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": 2022,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python', 'JavaScript', 'C++', 'Scala', 'TypeScript']",
    "dimension_normalized": "['å¤šè¯­è¨€ä»£ç ç”Ÿæˆèƒ½åŠ›', 'æ¨¡å‹åœ¨ä¸åŒç¼–ç¨‹è¯­è¨€ä¸Šçš„æ³›åŒ–æ€§èƒ½']",
    "evaluation_method_normalized": "['passç‡']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹é—®é¢˜', 'ç®—æ³•', 'æ•°æ®å¤„ç†']",
    "source_type_normalized": "['åŸºäºç°æœ‰PythonåŸºå‡†', 'HumanEval', 'MBPP', 'è‡ªåŠ¨åŒ–ç¼–è¯‘å™¨ç¿»è¯‘ç”Ÿæˆ']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2211.11501_output/content.md",
    "benchmark_name": "DS-1000",
    "benchmark_name_quote": "We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas.",
    "dataset_url": "https://ds1000-code-gen.github.io",
    "dataset_url_quote": "We release our benchmark at https://ds1000-code-gen.github.io.",
    "task_description": "æ•°æ®ç§‘å­¦ä»£ç ç”Ÿæˆã€‚æ—¨åœ¨è¯„ä¼°æ¨¡å‹æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°å’Œä»£ç ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆç”¨äºè§£å†³å®é™…æ•°æ®ç§‘å­¦é—®é¢˜çš„ä»£ç çš„èƒ½åŠ›ã€‚",
    "task_description_quote": "We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas.",
    "dimension": "æ•°æ®ç§‘å­¦ä»£ç ç”Ÿæˆèƒ½åŠ›ã€å¯¹ç‰¹å®šåº“APIçš„ç†è§£å’Œä½¿ç”¨ã€è§£å†³ç°å®ä¸–ç•Œé—®é¢˜çš„å®ç”¨æ€§ã€æŠ—è®°å¿†åŒ–èƒ½åŠ›",
    "dimension_quote": "DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases... Finally, we proactively defend against memorization...",
    "evaluation_method": "åŸºäºæ‰§è¡Œçš„å¤šæ ‡å‡†è‡ªåŠ¨è¯„ä¼°ã€‚åŒ…æ‹¬ï¼š1) è¿è¡Œæµ‹è¯•ç”¨ä¾‹æ£€æŸ¥åŠŸèƒ½æ­£ç¡®æ€§ï¼›2) é€šè¿‡é™åˆ¶APIä½¿ç”¨æˆ–å…³é”®è¯æ¥æ£€æŸ¥è¡¨é¢å½¢å¼çº¦æŸã€‚",
    "evaluation_method_quote": "we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords.",
    "context_dependency": "åŒ…å«ä»£ç ä¸Šä¸‹æ–‡çš„å•æ–‡ä»¶ä»»åŠ¡ã€‚ç”¨æˆ·é—®é¢˜é€šå¸¸åŒ…å«é”™è¯¯çš„ä»£ç ã€é”™è¯¯æ¶ˆæ¯å’Œè¾“å…¥è¾“å‡ºç¤ºä¾‹ç­‰å¤šæ ·åŒ–ä¸Šä¸‹æ–‡ã€‚",
    "context_dependency_quote": "usersâ€™ data science coding problems usually have diverse contexts including their incorrect code, error messages, and input-output examples",
    "problem_domain": "æ•°æ®ç§‘å­¦ã€‚æ¶µç›–ä¸ƒä¸ªå¹¿æ³›ä½¿ç”¨çš„Pythonæ•°æ®ç§‘å­¦åº“ï¼šNumPy, Pandas, TensorFlow, PyTorch, SciPy, Scikit-learn, å’Œ Matplotlibã€‚",
    "problem_domain_quote": "a thousand problems covering seven widely-used Python data science libraries: NumPy, Pandas, TensorFlow, PyTorch, SciPy, Scikit-learn, and Matplotlib.",
    "problem_difficulty": "ç°å®ä¸–ç•Œåº”ç”¨çº§ã€‚é—®é¢˜æ¥æºäºStackOverflowï¼Œåæ˜ äº†æ—¥å¸¸æ•°æ®ç§‘å­¦åº”ç”¨ä¸­çš„å®é™…ç”¨ä¾‹ï¼Œè€Œéç«èµ›æˆ–é¢è¯•é£æ ¼ã€‚",
    "problem_difficulty_quote": "focuses on everyday data science applications... includes naturalistic intents and contexts... our problems reflect diverse, realistic, and practical use cases",
    "language": "Python",
    "language_quote": "a code generation benchmark with a thousand data science problems spanning seven Python libraries",
    "data_size": "åŒ…å«1000ä¸ªé—®é¢˜ã€‚",
    "data_size_quote": "a code generation benchmark with a thousand data science problems",
    "source_type": "ä»StackOverflowæ”¶é›†çš„è‡ªç„¶å‘ç”Ÿçš„é—®é¢˜ï¼Œç»è¿‡äººå·¥ç­›é€‰ã€ä¿®æ”¹å’Œæ‰°åŠ¨ã€‚",
    "source_type_quote": "we collected naturally occurring problems from Stack-Overflow, manually scored their representativeness and usefulness, and curated a subset of them to create our benchmark.",
    "last_updated": "2022-11-18 (æ ¹æ®arXivç‰ˆæœ¬v1æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2211.11501v1 [cs.SE] 18 Nov 2022",
    "build_type": "å®˜æ–¹è‡ªå»ºã€‚ç”±äº”ä½ç²¾é€šæ•°æ®ç§‘å­¦å’ŒPythonçš„è®¡ç®—æœºç§‘å­¦å­¦ç”Ÿä½œè€…èŠ±è´¹çº¦1200å°æ—¶æ„å»ºã€‚",
    "build_type_quote": "five authors who are computer science students and familiar with data science spent a total of about 1200 hours constructing DS-1000",
    "contamination_status": "æŠ—æ±¡æŸ“è®¾è®¡ã€‚é€šè¿‡ä¸»åŠ¨æ‰°åŠ¨åŸå§‹StackOverflowé—®é¢˜æ¥é˜²å¾¡æ¨¡å‹å¯¹é¢„è®­ç»ƒè¯­æ–™çš„è®°å¿†ã€‚",
    "contamination_status_quote": "we proactively defend against memorization by slightly modifying our problems to be different from the original Stack-Overflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆå¡«ç©ºï¼‰ã€‚æ¨¡å‹éœ€è¦å°†ä»£ç å¡«å…¥æç¤ºä¸­çš„â€œ[insert]â€ä½ç½®ã€‚",
    "task_granularity_quote": "The model needs to fill in the code into â€œ[insert]â€ in the prompt",
    "evaluation_metrics": "é€šè¿‡/ä¸é€šè¿‡ï¼ˆåŸºäºæµ‹è¯•ç”¨ä¾‹å’Œè¡¨é¢å½¢å¼çº¦æŸï¼‰ã€‚æ–‡ä¸­æœªæ˜ç¡®å‘½åå¦‚pass@kçš„æŒ‡æ ‡ï¼Œä½†æŠ¥å‘Šäº†å‡†ç¡®ç‡ï¼ˆå¦‚43.3%ï¼‰ã€‚",
    "evaluation_metrics_quote": "The current best public system (Codex-002) achieves 43.3% accuracy",
    "input_modality": "è‡ªç„¶è¯­è¨€ä¸ä»£ç ä¸Šä¸‹æ–‡æ··åˆã€‚è¾“å…¥åŒ…æ‹¬è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°å’Œéƒ¨åˆ†å¯æ‰§è¡Œçš„ä»£ç ä¸Šä¸‹æ–‡ã€‚",
    "input_modality_quote": "The model needs to fill in the code into â€œ[insert]â€ in the prompt on the left; the prompt includes natural language description and code context.",
    "output_modality": "ä»£ç ã€‚æœŸæœ›è¾“å‡ºæ˜¯ä¸€æ®µå¡«è¡¥ç¼ºå¤±éƒ¨åˆ†çš„Pythonä»£ç ã€‚",
    "output_modality_quote": "The model needs to fill in the code into â€œ[insert]â€",
    "task_io_type": "æ–‡æœ¬ä¸ä»£ç åˆ°ä»£ç ã€‚æ ¹æ®è‡ªç„¶è¯­è¨€æè¿°å’Œç»™å®šçš„ä»£ç ä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆç¼ºå¤±çš„ä»£ç ç‰‡æ®µã€‚",
    "task_io_type_quote": "synthesizing programs from docstrings. (ç»“åˆä¸Šä¸‹æ–‡æ¨æ–­ï¼Œä»»åŠ¡æ˜¯ä»è‡ªç„¶è¯­è¨€æè¿°å’Œä»£ç ä¸Šä¸‹æ–‡ä¸­åˆæˆç¨‹åº)",
    "execution_environment": "éœ€è¦ç‰¹å®šæ•°æ®ç§‘å­¦åº“ä¾èµ–çš„æ²™ç›’ç¯å¢ƒã€‚å›ºå®šäº†Python 3.7.10åŠç›¸åº”åº“çš„æœ€æ–°ç‰ˆæœ¬ã€‚",
    "execution_environment_quote": "We fixed the evaluation environment to include the latest versions of libraries that can be installed with Python 3.7.10",
    "unique_features": "1) åŒ…å«ç°å®é—®é¢˜ä¸å¤šæ ·åŒ–ä¸Šä¸‹æ–‡ï¼›2) å¯é çš„å¤šæ ‡å‡†æ‰§è¡Œè¯„ä¼°ï¼›3) ä¸»åŠ¨é˜²å¾¡è®°å¿†åŒ–ï¼›4) ä¸“æ³¨äºä¸ƒä¸ªæ ¸å¿ƒæ•°æ®ç§‘å­¦åº“ã€‚",
    "unique_features_quote": "We highlight three core features of DS-1000: 1) it contains realistic problems with diverse contexts, 2) it implements reliable multi-criteria execution-based evaluation metrics, and 3) it proactively defends against memorization.",
    "data_size_quantity": 1000,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": 2022,
    "last_updated_month": 11,
    "last_updated_day": 18,
    "language_normalized": "['Python']",
    "dimension_normalized": "['æ•°æ®ç§‘å­¦ä»£ç ç”Ÿæˆèƒ½åŠ›', 'å¯¹ç‰¹å®šåº“APIçš„ç†è§£å’Œä½¿ç”¨', 'è§£å†³ç°å®ä¸–ç•Œé—®é¢˜çš„å®ç”¨æ€§', 'æŠ—è®°å¿†åŒ–èƒ½åŠ›']",
    "evaluation_method_normalized": "['é€šè¿‡/ä¸é€šè¿‡', 'å‡†ç¡®ç‡']",
    "problem_domain_normalized": "['æ•°æ®ç§‘å­¦', 'NumPy', 'Pandas', 'TensorFlow', 'PyTorch', 'SciPy', 'Scikit-learn', 'Matplotlib']",
    "source_type_normalized": "['StackOverflow']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•æ–‡ä»¶",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2301.03988_output/content.md",
    "benchmark_name": "MultiPL-E",
    "benchmark_name_quote": "evaluate them on the MultiPL-E text-to-code benchmark (Cassano et al., 2022).",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "evaluate them on the MultiPL-E text-to-code benchmark (Cassano et al., 2022).",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„æµ‹ä»è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆä»£ç çš„èƒ½åŠ›ï¼ˆæ–‡æœ¬åˆ°ä»£ç ï¼‰",
    "task_description_quote": "MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages.",
    "dimension": "å¤šè¯­è¨€ä»£ç ç”Ÿæˆèƒ½åŠ›",
    "dimension_quote": "MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages.",
    "evaluation_method": "ä½¿ç”¨å•å…ƒæµ‹è¯•è¯„ä¼°ç”Ÿæˆä»£ç çš„æ­£ç¡®æ€§",
    "evaluation_method_quote": "The correctness of generated code can be tested using unit tests, a method for approximating semantic equivalence.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æ–‡ä¸­ä»…æåŠå®éªŒå­é›†ï¼ˆJava, JavaScript, Pythonï¼‰ï¼Œæœªæè¿°å®Œæ•´æ•°æ®é›†ã€‚MultiPL-Eæ‰©å±•äº†18ç§è¯­è¨€ã€‚",
    "language_quote": "evaluate them on the MultiPL-E text-to-code benchmark (Cassano et al., 2022). ... MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "åŸºäºHumanEvalå’ŒMBPPåŸºå‡†è‡ªåŠ¨ç¼–è¯‘æ‰©å±•",
    "source_type_quote": "MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. The doctests, function signatures, and unit tests for each benchmark suite are automatically compiled to new languages.",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆå®Œæ•´å‡½æ•°ï¼‰",
    "task_granularity_quote": "MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages.",
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "text-to-code benchmark",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "text-to-code benchmark",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "text-to-code benchmark",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. é€šè¿‡è‡ªåŠ¨ç¼–è¯‘å°†å•è¯­è¨€åŸºå‡†ï¼ˆHumanEval, MBPPï¼‰æ‰©å±•åˆ°å¤šç§ç¼–ç¨‹è¯­è¨€ã€‚2. è¯„ä¼°æ—¶éšè—æµ‹è¯•ç”¨ä¾‹ï¼Œä»…ç”¨äºéªŒè¯æ­£ç¡®æ€§ï¼ˆä¸MBXPä¸åŒï¼‰ã€‚",
    "unique_features_quote": "MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. ... In contrast, MultiPL-E keeps the tests hidden from the model and only uses them to test correctness.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java', 'JavaScript', 'Python', 'å¤šè¯­è¨€']",
    "dimension_normalized": "['å¤šè¯­è¨€ä»£ç ç”Ÿæˆèƒ½åŠ›']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "['åŸºäºHumanEvalå’ŒMBPPåŸºå‡†è‡ªåŠ¨ç¼–è¯‘æ‰©å±•']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2302.08468_output/content.md",
    "benchmark_name": "Spider",
    "benchmark_name_quote": "â–·Spider (Yu et al., 2018) is a semantic parsing dataset on generating SQL queries from natural language questions.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We conduct experiments on four language-to-code datasets across domains of semantic parsing, table QA, math reasoning and basic python programming.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»è‡ªç„¶è¯­è¨€é—®é¢˜ç”ŸæˆSQLæŸ¥è¯¢ï¼ˆè¯­ä¹‰è§£æï¼‰",
    "task_description_quote": "â–·Spider (Yu et al., 2018) is a semantic parsing dataset on generating SQL queries from natural language questions.",
    "dimension": "è¯­è¨€åˆ°ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "The ability of mapping natural language to executable code is the cornerstone of a variety AI applications such as database interfaces (Pasupat & Liang, 2015; Yu et al., 2018; Shi et al., 2020)...",
    "evaluation_method": "æ‰§è¡Œç”Ÿæˆçš„ç¨‹åºå¹¶æ¯”è¾ƒç»“æœ",
    "evaluation_method_quote": "Given x, a generation model P(y|x) generates a program y which is later executed via an executor E(Â·) to obtain the result3 E(y).",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "æ•°æ®åº“æŸ¥è¯¢ï¼ˆTable QAï¼‰",
    "problem_domain_quote": "Spider (Yu et al., 2018) is a semantic parsing dataset on generating SQL queries from natural language questions.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "SQL",
    "language_quote": "Target: SQL",
    "data_size": "è®­ç»ƒé›†7000æ¡ï¼Œå¼€å‘é›†1032æ¡",
    "data_size_quote": "# Train: 7,000, # Dev: 1,032",
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2018",
    "last_updated_quote": "Spider (Yu et al., 2018)",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "language-to-code generation",
    "evaluation_metrics": "æ‰§è¡Œå‡†ç¡®ç‡",
    "evaluation_metrics_quote": "LEVER consistently improves the execution accuracy of the generated programs.",
    "input_modality": "è‡ªç„¶è¯­è¨€ä¸æ•°æ®åº“æ¨¡å¼",
    "input_modality_quote": "Input Format: Schema + NL",
    "output_modality": "ä»£ç ï¼ˆSQLï¼‰",
    "output_modality_quote": "generating SQL queries from natural language questions.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "language-to-code generation",
    "execution_environment": "SQLæ‰§è¡Œå™¨",
    "execution_environment_quote": "generating SQL queries from natural language questions.",
    "unique_features": "æ‹¥æœ‰å®Œæ•´çš„ç¨‹åºæ ‡æ³¨ï¼ˆHas program: âœ“ï¼‰",
    "unique_features_quote": "Has program: âœ“",
    "data_size_quantity": 7000,
    "data_size_unit": "æ¡",
    "last_updated_year": 2018,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['SQL']",
    "dimension_normalized": "['è¯­è¨€åˆ°ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['æ‰§è¡Œå‡†ç¡®ç‡']",
    "problem_domain_normalized": "['æ•°æ®åº“æŸ¥è¯¢ï¼ˆTable QAï¼‰']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2303.17568_output/content.md",
    "benchmark_name": "HumanEval-X",
    "benchmark_name_quote": "Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We hand-craft the HumanEval-X benchmark to evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness, facilitating the understanding and development of pre-trained (multilingual) code models.",
    "dataset_url": "https://github.com/THUDM/CodeGeeX",
    "dataset_url_quote": "Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX.",
    "task_description": "è¯„ä¼°å¤šè¯­è¨€ä»£ç æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆå’Œä»£ç ç¿»è¯‘ä»»åŠ¡ä¸Šçš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚",
    "task_description_quote": "We hand-craft the HumanEval-X benchmark to evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness...",
    "dimension": "å¤šè¯­è¨€ä»£ç ç”Ÿæˆä¸ç¿»è¯‘çš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "...evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness...",
    "evaluation_method": "é€šè¿‡æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹æ¥éªŒè¯ç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§",
    "evaluation_method_quote": "...rather than really verify the functional correctness of generated code. Specifically, for each problem... in HumanEval, we manually rewrite its prompt, canonical solution, and test cases in C++, Java, JavaScript, and Go.",
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "æ–‡ä¸­æœªæ˜ç¡®æè¿°ä¸Šä¸‹æ–‡ä¾èµ–èŒƒå›´ã€‚HumanEval-XåŸºäºHumanEvalæ„å»ºï¼Œè€ŒHumanEvalæ˜¯å•å‡½æ•°ç”Ÿæˆä»»åŠ¡ã€‚",
    "problem_domain": "é€šç”¨ç¼–ç¨‹é—®é¢˜",
    "problem_domain_quote": "æ–‡ä¸­æœªæ˜ç¡®æè¿°é—®é¢˜æ‰€å±ä¸“ä¸šé¢†åŸŸã€‚",
    "problem_difficulty": "å…¥é—¨çº§ç¼–ç¨‹é—®é¢˜",
    "problem_difficulty_quote": "æ–‡ä¸­æœªç›´æ¥æè¿°HumanEval-Xçš„éš¾åº¦ï¼Œä½†æåˆ°å…¶åŸºç¡€HumanEvalç”¨äºè¯„ä¼°Codexè§£å†³å…¥é—¨çº§Pythoné—®é¢˜ã€‚",
    "language": "Python, C++, Java, JavaScript, Go",
    "language_quote": "...we manually rewrite its prompt, canonical solution, and test cases in C++, Java, JavaScript, and Go. In total, HumanEval-X covers 820 hand-written problem-solution pairs (164 problems, each having solutions in 5 languages).",
    "data_size": "820ä¸ªæ‰‹å†™çš„é—®é¢˜-è§£å†³æ–¹æ¡ˆå¯¹ï¼ˆ164ä¸ªé—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜æœ‰5ç§è¯­è¨€çš„è§£å†³æ–¹æ¡ˆï¼‰",
    "data_size_quote": "In total, HumanEval-X covers 820 hand-written problem-solution pairs (164 problems, each having solutions in 5 languages).",
    "source_type": "åŸºäºHumanEvalï¼ˆPythonï¼‰æ‰‹åŠ¨é‡å†™å’Œæ‰©å±•",
    "source_type_quote": "Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go.",
    "last_updated": "2022å¹´9æœˆ",
    "last_updated_quote": "Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X...",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆæ‰‹å·¥æ„å»ºï¼‰",
    "build_type_quote": "We hand-craft the HumanEval-X benchmark...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼Œä»£ç ç¿»è¯‘",
    "task_granularity_quote": "...evaluate multilingual code models for the tasks of code generation and translation...",
    "evaluation_metrics": "åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆé€šè¿‡æµ‹è¯•ç”¨ä¾‹ï¼‰",
    "evaluation_metrics_quote": "...evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness...",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆæè¿°ï¼‰",
    "input_modality_quote": "æ–‡ä¸­æœªæ˜ç¡®æè¿°è¾“å…¥ç±»å‹ï¼Œä½†åŸºäºHumanEvalä»»åŠ¡ï¼ˆä»æ–‡æ¡£å­—ç¬¦ä¸²ç”Ÿæˆä»£ç ï¼‰æ¨æ–­ã€‚",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "æ–‡ä¸­æœªæ˜ç¡®æè¿°æœŸæœ›è¾“å‡ºï¼Œä½†ä»»åŠ¡ä¸ºä»£ç ç”Ÿæˆå’Œç¿»è¯‘ï¼Œæ¨æ–­è¾“å‡ºä¸ºä»£ç ã€‚",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ï¼Œä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "HumanEval-X support the evaluation of both code generation and code translation between different languages.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. å¤šè¯­è¨€æ‰©å±•ï¼šåœ¨HumanEvalï¼ˆä»…Pythonï¼‰åŸºç¡€ä¸Šï¼Œæ‰‹å·¥ç¼–å†™äº†C++ã€Javaã€JavaScriptå’ŒGoçš„è§£å†³æ–¹æ¡ˆå’Œæµ‹è¯•ç”¨ä¾‹ã€‚2. æ”¯æŒåŒé‡ä»»åŠ¡ï¼šåŒæ—¶è¯„ä¼°ä»£ç ç”Ÿæˆå’Œä»£ç ç¿»è¯‘ã€‚3. ä¸“æ³¨äºåŠŸèƒ½æ­£ç¡®æ€§è¯„ä¼°ï¼Œè€Œéå­—ç¬¦ä¸²ç›¸ä¼¼åº¦ã€‚",
    "unique_features_quote": "1) HumanEval (Chen et al., 2021)â€”developed by OpenAI for evaluating Codexâ€”and other benchmarks only consist of programming problems in a single language and 2) existing multilingual datasets use string similarity metrics like BLEU for evaluation rather than really verify the functional correctness of generated code. Importantly, HumanEval-X support the evaluation of both code generation and code translation between different languages.",
    "data_size_quantity": 820,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2022,
    "last_updated_month": 9,
    "last_updated_day": null,
    "language_normalized": "['Python', 'C++', 'Java', 'JavaScript', 'Go']",
    "dimension_normalized": "['å¤šè¯­è¨€ä»£ç ç”Ÿæˆä¸ç¿»è¯‘çš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆé€šè¿‡æµ‹è¯•ç”¨ä¾‹ï¼‰']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹é—®é¢˜']",
    "source_type_normalized": "['åŸºäºHumanEvalï¼ˆPythonï¼‰æ‰‹åŠ¨é‡å†™å’Œæ‰©å±•']",
    "problem_difficulty_normalized": "å…¥é—¨çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2304.05128_output/content.md",
    "benchmark_name": "Spider",
    "benchmark_name_quote": "SELF-DEBUGGING achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "we evaluate SELF-DEBUGGING on the development set of the Spider benchmark (Yu et al., 2018).",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ–‡æœ¬åˆ°SQLç”Ÿæˆï¼šç»™å®šä¸€ä¸ªè‡ªç„¶è¯­è¨€é—®é¢˜å’Œæ•°æ®åº“ä¿¡æ¯ï¼Œç”Ÿæˆå¯¹åº”çš„SQLæŸ¥è¯¢ã€‚",
    "task_description_quote": "The goal of text-to-SQL tasks is to generate the corresponding SQL query given a question and the database information",
    "dimension": "ä»£ç ç”Ÿæˆæ­£ç¡®æ€§",
    "dimension_quote": "SELF-DEBUGGING can teach the large language model to debug its predicted program",
    "evaluation_method": "æ‰§è¡Œå•å…ƒæµ‹è¯•ï¼ˆå½“å¯ç”¨æ—¶ï¼‰æˆ–åŸºäºæ‰§è¡Œç»“æœçš„å¤šæ•°æŠ•ç¥¨",
    "evaluation_method_quote": "When unit tests are presented in the problem description, we filter out programs that do not pass the unit tests before performing the execution-based majority vote.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "æ•°æ®åº“æŸ¥è¯¢",
    "problem_domain_quote": "text-to-SQL generation",
    "problem_difficulty": "åŒ…å«æœ€å¤æ‚çº§åˆ«çš„é—®é¢˜",
    "problem_difficulty_quote": "improves the prediction accuracy on problems of the hardest level by 9%.",
    "language": "SQL",
    "language_quote": "text-to-SQL generation",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "text-to-SQL generation",
    "evaluation_metrics": "é¢„æµ‹å‡†ç¡®ç‡",
    "evaluation_metrics_quote": "improves the prediction accuracy on problems of the hardest level by 9%.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆé—®é¢˜ï¼‰å’Œæ•°æ®åº“ä¿¡æ¯",
    "input_modality_quote": "given a question and the database information",
    "output_modality": "ä»£ç ï¼ˆSQLæŸ¥è¯¢ï¼‰",
    "output_modality_quote": "generate the corresponding SQL query",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "text-to-SQL generation",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "è¯¥åŸºå‡†ï¼ˆSpiderï¼‰åœ¨é—®é¢˜æè¿°ä¸­æ²¡æœ‰å•å…ƒæµ‹è¯•æ¥éªŒè¯é¢„æµ‹çš„æ­£ç¡®æ€§ã€‚",
    "unique_features_quote": "On the Spider benchmark where there are no unit tests to verify the correctness of predictions",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['SQL']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆæ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['é¢„æµ‹å‡†ç¡®ç‡']",
    "problem_domain_normalized": "['æ•°æ®åº“æŸ¥è¯¢']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2305.01210_output/content.md",
    "benchmark_name": "HumanEval+",
    "benchmark_name_quote": "we extend the test-cases of the popular HUMANEVAL benchmark by 80Ã— to build HUMANEVAL+.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we propose EvalPlus â€“ a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator... we extend the test-cases of the popular HUMANEVAL benchmark by 80Ã— to build HUMANEVAL+.",
    "dataset_url": "https://github.com/evalplus/evalplus",
    "dataset_url_quote": "We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚",
    "task_description_quote": "EvalPlus â€“ a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code.",
    "dimension": "ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "rigorously benchmark the functional correctness of LLM-synthesized code.",
    "evaluation_method": "é€šè¿‡å¤§é‡æ–°ç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹ï¼ˆç»“åˆLLMå’ŒåŸºäºçªå˜çš„ç­–ç•¥ï¼‰è¿›è¡Œå·®åˆ†æµ‹è¯•ï¼Œå¹¶ä¸åŸºå‡†å®ç°è¿›è¡Œäº¤å‰éªŒè¯ã€‚",
    "evaluation_method_quote": "EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies... These newly generated test inputs are then used to evaluate the LLM-generated code through differential testing against the ground-truth implementation.",
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "The generated code snippet is then combined with the context to form a complete function that aligns with the user intent.",
    "problem_domain": "é€šç”¨ç¼–ç¨‹é—®é¢˜",
    "problem_domain_quote": "Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æ–‡ä¸­ä»…æåŠå®éªŒåŸºäºHumanEvalï¼ˆPythonï¼‰ï¼Œæœªæè¿°å®Œæ•´æ•°æ®é›†",
    "language_quote": "we extend the test-cases of the popular HUMANEVAL benchmark by 80Ã— to build HUMANEVAL+.",
    "data_size": "å°†HumanEvalçš„æµ‹è¯•ç”¨ä¾‹è§„æ¨¡æ‰©å±•äº†80å€",
    "data_size_quote": "EvalPlus extends the popular HUMANEVAL benchmark to create HUMANEVAL+, improving the test-case scale by 80Ã—.",
    "source_type": "åŸºäºç°æœ‰åŸºå‡†ï¼ˆHumanEvalï¼‰é€šè¿‡è‡ªåŠ¨æµ‹è¯•è¾“å…¥ç”Ÿæˆå™¨ï¼ˆç»“åˆLLMå’ŒåŸºäºçªå˜çš„ç­–ç•¥ï¼‰è¿›è¡Œå¢å¼ºã€‚",
    "source_type_quote": "EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies.",
    "last_updated": "2023",
    "last_updated_quote": "arXiv:2305.01210v3 [cs.SE] 30 Oct 2023",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆåŸºäºç°æœ‰åŸºå‡†å¢å¼ºï¼‰",
    "build_type_quote": "we propose EvalPlus â€“ a code synthesis evaluation framework... we extend the test-cases of the popular HUMANEVAL benchmark by 80Ã— to build HUMANEVAL+.",
    "contamination_status": "é€šè¿‡ç”Ÿæˆå¤§é‡æ–°æµ‹è¯•ç”¨ä¾‹æ¥ç¼“è§£ç°æœ‰åŸºå‡†æµ‹è¯•ä¸è¶³å¯¼è‡´çš„â€œè™šå‡æ­£ç¡®â€é—®é¢˜ï¼Œæ—¨åœ¨æ›´ä¸¥æ ¼åœ°è¯„ä¼°æ¨¡å‹ã€‚",
    "contamination_status_quote": "HUMANEVAL+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9%.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "program synthesis... applying LLMs for direct code generation.",
    "evaluation_metrics": "pass@k",
    "evaluation_metrics_quote": "reducing the pass@k by up-to 19.3-28.9%.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆå‡½æ•°ç­¾åå’Œæ–‡æ¡£å­—ç¬¦ä¸²ï¼‰",
    "input_modality_quote": "in the form of function signature and docstring that denote the desired program functionality.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "The generated code snippet is then combined with the context to form a complete function that aligns with the user intent.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "synthesizing programs from docstrings.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. é€šè¿‡è‡ªåŠ¨æµ‹è¯•ç”Ÿæˆï¼ˆç»“åˆLLMå’Œçªå˜ç­–ç•¥ï¼‰æ˜¾è‘—æ‰©å±•ç°æœ‰åŸºå‡†çš„æµ‹è¯•å¥—ä»¶ï¼ˆ80å€ï¼‰ã€‚2. æ—¨åœ¨æ­ç¤ºå› æµ‹è¯•ä¸è¶³è€Œè¢«ç°æœ‰åŸºå‡†é—æ¼çš„é”™è¯¯ä»£ç ã€‚3. æä¾›ç²¾ç®€ç‰ˆæµ‹è¯•å¥—ä»¶ï¼ˆHUMANEVAL+-MINIï¼‰ä»¥åŠ é€Ÿè¯„ä¼°ã€‚4. å‘ç°æµ‹è¯•ä¸è¶³å¯èƒ½å¯¼è‡´æ¨¡å‹æ’åé”™è¯¯ã€‚",
    "unique_features_quote": "EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies... HUMANEVAL+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs... we also produce HUMANEVAL+-MINI which distills HUMANEVAL+ tests by 47Ã—... test insufficiency can lead to mis-ranking.",
    "data_size_quantity": 80,
    "data_size_unit": "å€",
    "last_updated_year": 2023,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@k']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹é—®é¢˜']",
    "source_type_normalized": "['åŸºäºç°æœ‰åŸºå‡†ï¼ˆHumanEvalï¼‰é€šè¿‡è‡ªåŠ¨æµ‹è¯•è¾“å…¥ç”Ÿæˆå™¨ï¼ˆç»“åˆLLMå’ŒåŸºäºçªå˜çš„ç­–ç•¥ï¼‰è¿›è¡Œå¢å¼ºã€‚']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2305.02309_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "The HumanEval (Chen et al., 2021b) is recruited to evaluate the quality of synthesized programs.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "The HumanEval (Chen et al., 2021b) is recruited to evaluate the quality of synthesized programs.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ ¹æ®å‡½æ•°ç­¾åå’Œæ–‡æ¡£å­—ç¬¦ä¸²ï¼ˆæ„å›¾è¯´æ˜ï¼‰ç”Ÿæˆç¨‹åºä»£ç ã€‚",
    "task_description_quote": "The prompt as an intent specification is in the form of a function signature and doc-string. A program is conditionally sampled (or completed) based on the prompt in left-to-right auto-regressive fashion.",
    "dimension": "ç¨‹åºåˆæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "The HumanEval (Chen et al., 2021b) is recruited to evaluate the quality of synthesized programs.",
    "evaluation_method": NaN,
    "evaluation_method_quote": NaN,
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "The prompt as an intent specification is in the form of a function signature and doc-string.",
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æ–‡ä¸­æœªæ˜ç¡®æåŠå®Œæ•´æ•°æ®é›†çš„è¯­è¨€ï¼Œä»…æåŠå®éªŒä»»åŠ¡ã€‚",
    "language_quote": "The prompt as an intent specification is in the form of a function signature and doc-string. A program is conditionally sampled (or completed) based on the prompt...",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "Program Synthesis with left-to-right sampling (zero-shot)... A program is conditionally sampled (or completed) based on the prompt...",
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆæ–‡æ¡£å­—ç¬¦ä¸²ï¼‰ä¸ä»£ç ï¼ˆå‡½æ•°ç­¾åï¼‰",
    "input_modality_quote": "The prompt as an intent specification is in the form of a function signature and doc-string.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "A program is conditionally sampled (or completed) based on the prompt...",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "The prompt as an intent specification is in the form of a function signature and doc-string. A program is conditionally sampled...",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": NaN,
    "unique_features_quote": NaN,
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ç¨‹åºåˆæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2305.07922_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task against other open code LLMs, even surpassing the OpenAI code-cushman-001 model.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. ... in the zero-shot text-to-code generation task on HumanEval benchmark [Chen et al., 2021]",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ–‡ä¸­ä»…æåŠä½¿ç”¨è¯¥åŸºå‡†è¿›è¡Œä»£ç ç”Ÿæˆè¯„æµ‹ï¼Œæœªæè¿°å…¶åŸå§‹ä»»åŠ¡è®¾è®¡ã€‚",
    "task_description_quote": "in the zero-shot text-to-code generation task on HumanEval benchmark",
    "dimension": "æ–‡ä¸­ä»…æåŠä½¿ç”¨è¯¥åŸºå‡†è¿›è¡Œä»£ç ç”Ÿæˆè¯„æµ‹ï¼Œæœªæè¿°å…¶åŸå§‹è¯„æµ‹ç»´åº¦ã€‚",
    "dimension_quote": NaN,
    "evaluation_method": "æ–‡ä¸­ä»…æåŠä½¿ç”¨pass@1å’Œpass@10æŒ‡æ ‡ï¼Œæœªæè¿°å…¶åŸå§‹è¯„ä¼°æ–¹æ³•ã€‚",
    "evaluation_method_quote": "achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æ–‡ä¸­ä»…æåŠå®éªŒä»»åŠ¡ï¼ˆæ–‡æœ¬åˆ°ä»£ç ç”Ÿæˆï¼‰ï¼Œæœªæè¿°æ•°æ®é›†æ¶‰åŠçš„å…·ä½“ç¼–ç¨‹è¯­è¨€ã€‚",
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "in the zero-shot text-to-code generation task on HumanEval benchmark",
    "evaluation_metrics": "pass@1, pass@10",
    "evaluation_metrics_quote": "achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "in the zero-shot text-to-code generation task",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "in the zero-shot text-to-code generation task",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "in the zero-shot text-to-code generation task",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": NaN,
    "unique_features_quote": NaN,
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "[]",
    "evaluation_method_normalized": "['pass@1', 'pass@10']",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2305.18584_output/content.md",
    "benchmark_name": "PYCOMMITS",
    "benchmark_name_quote": "We address this issue by collecting a new dataset, PYCOMMITS, from the commit histories of 1650 open-source Python projects on GitHub.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation. ... We introduce the repo-level multi-round code editing task, along with the corresponding PYCOMMITS dataset and evaluation framework.",
    "dataset_url": "https://github.com/mrvplusone/Coeditor",
    "dataset_url_quote": "Available at https://github.com/mrvplusone/Coeditor.",
    "task_description": "ä»£ç è‡ªåŠ¨ç¼–è¾‘ã€‚æ—¨åœ¨é¢„æµ‹å¯¹ä»£ç åŒºåŸŸï¼ˆåŸºäºåŒä¸€ä»£ç åº“ä¸­æœ€è¿‘çš„æ›´æ”¹ï¼‰çš„ç¼–è¾‘ã€‚è¿™æ˜¯ä¸€ä¸ªå¤šè½®ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯æ ¹æ®ç”¨æˆ·ä¹‹å‰çš„ç¼–è¾‘æ¥é¢„æµ‹ä»£ç çš„ç¼–è¾‘ã€‚",
    "task_description_quote": "In this work, we explore a multi-round code auto-editing setting, aiming to predict edits to a code region based on recent changes within the same codebase. ... In this paper, we introduce a task that we call (multi-round) auto-editing where the goal is to predict edits to code conditioned on the userâ€™s previous edits.",
    "dimension": "ä»£ç ç¼–è¾‘çš„å‡†ç¡®æ€§å’Œè‡ªåŠ¨åŒ–ç¨‹åº¦ï¼Œå…³æ³¨æ¨¡å‹åœ¨ç»™å®šç¼–è¾‘å†å²å’Œä»£ç åº“ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹é¢„æµ‹æ­£ç¡®ç¼–è¾‘çš„èƒ½åŠ›ã€‚",
    "dimension_quote": "In a simplified single-round, single-edit task, Coeditor significantly outperforms GPT-3.5 and SOTA open-source code completion models (bringing exact-match accuracy from 34.7 up to 60.4), demonstrating the benefits of incorporating editing history for code completion. In a multi-round, multi-edit setting, we observe substantial gains by iteratively conditioning on additional user edits.",
    "evaluation_method": "ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ï¼ˆexact-match accuracyï¼‰ï¼Œä»¥åŠè‡ªåŠ¨åŒ–ç¼–è¾‘è¡Œæ•°å’ŒèŠ‚çœå‡»é”®æ•°çš„ç¼–è¾‘è·ç¦»åº¦é‡ã€‚",
    "evaluation_method_quote": "our method achieves 60.4% exact match accuracy using a 220M parameter model... In the full multi-round setting, we found that Coeditor automates editing 46.7% of the changed lines, saving the user 28.6% of keystrokes measured by an edit distance metric that accounts for cursor movement.",
    "context_dependency": "ä»“åº“çº§åˆ«ï¼ˆrepo-levelï¼‰ï¼Œä¾èµ–åŒä¸€ä»£ç åº“ä¸­çš„å…¶ä»–éƒ¨åˆ†å’Œç”¨æˆ·çš„å†å²ç¼–è¾‘ã€‚",
    "context_dependency_quote": "We introduce the repo-level multi-round code editing task... aiming to predict edits to a code region based on recent changes within the same codebase.",
    "problem_domain": "é€šç”¨è½¯ä»¶å·¥ç¨‹ï¼Œä»£ç ç»´æŠ¤ä¸é‡æ„ã€‚",
    "problem_domain_quote": "Developers often dedicate significant time to maintaining and refactoring existing code.",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼Œæ¶‰åŠå®é™…å¼€æºé¡¹ç›®ä¸­çš„ä»£ç å˜æ›´ã€‚",
    "problem_difficulty_quote": "We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation.",
    "language": "Python",
    "language_quote": "We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation.",
    "data_size": "ä»1650ä¸ªå¼€æºPythoné¡¹ç›®çš„æäº¤å†å²ä¸­æ”¶é›†ã€‚",
    "data_size_quote": "We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation.",
    "source_type": "æ¥è‡ªGitHubä¸Š1650ä¸ªå¼€æºPythoné¡¹ç›®çš„æäº¤å†å²ã€‚",
    "source_type_quote": "We address this issue by collecting a new dataset, PYCOMMITS, from the commit histories of 1650 open-source Python projects on GitHub.",
    "last_updated": "2024",
    "last_updated_quote": "Published as a conference paper at ICLR 2024",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±è®ºæ–‡ä½œè€…ä¸ºç ”ç©¶ç›®çš„æ„å»ºã€‚",
    "build_type_quote": "We address this issue by collecting a new dataset, PYCOMMITS, from the commit histories of 1650 open-source Python projects on GitHub.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç¼–è¾‘ï¼ˆåŸºäºè¡Œçš„å·®å¼‚ï¼‰ï¼ŒåŒ…æ‹¬æ·»åŠ ã€åˆ é™¤è¡Œã€‚",
    "task_granularity_quote": "We encode all prior code edits âˆ†1, . . . , âˆ†k using a line-based diffing scheme and decodes âˆ†u using masked span infilling... we adopt a line-diff-based format, enabling us to convert auto-editing into a masked span infilling problem.",
    "evaluation_metrics": "ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ï¼ˆexact-match accuracyï¼‰ï¼Œè‡ªåŠ¨åŒ–ç¼–è¾‘è¡Œç™¾åˆ†æ¯”ï¼ŒèŠ‚çœå‡»é”®ç™¾åˆ†æ¯”ï¼ˆåŸºäºç¼–è¾‘è·ç¦»ï¼‰ã€‚",
    "evaluation_metrics_quote": "our method achieves 60.4% exact match accuracy... Coeditor automates editing 46.7% of the changed lines, saving the user 28.6% of keystrokes measured by an edit distance metric that accounts for cursor movement.",
    "input_modality": "ä»£ç ï¼ˆåŸå§‹ä»£ç åº“å’Œä»¥è¡Œå·®å¼‚æ ¼å¼ç¼–ç çš„ç¼–è¾‘å†å²ï¼‰ï¼Œä»¥åŠé€šè¿‡é™æ€åˆ†ææå–çš„ç›¸å…³ä»£ç ç­¾åã€‚",
    "input_modality_quote": "given an original codebase U and a set of code changes âˆ†1, . . . , âˆ†k... we employ lightweight static analysis to pull in relevant parts of the codebase U.",
    "output_modality": "ä»£ç ï¼ˆä»¥è¡Œå·®å¼‚æ ¼å¼ç¼–ç çš„ç›®æ ‡ç¼–è¾‘ï¼‰ã€‚",
    "output_modality_quote": "the auto-editing problem is to predict how to modify a specified region of code u âˆˆU... We encode the input code by a function EncInput... we can encode âˆ†u using the following expression, EncOutput(âˆ†u)...",
    "task_io_type": "ä»£ç åˆ°ä»£ç ï¼ˆåœ¨ç»™å®šä»£ç åº“å’Œç¼–è¾‘å†å²ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ï¼Œé¢„æµ‹å¯¹ç›®æ ‡ä»£ç åŒºåŸŸçš„ç¼–è¾‘ï¼‰ã€‚",
    "task_io_type_quote": "the auto-editing problem is to predict how to modify a specified region of code u âˆˆU by learning the following distribution: P(âˆ†u | âˆ†k . . . âˆ†1, U).",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºå¤šè½®ã€ä»“åº“çº§åˆ«çš„ä»£ç è‡ªåŠ¨ç¼–è¾‘ä»»åŠ¡ï¼Œä½¿ç”¨åŸºäºè¡Œå·®å¼‚çš„ç¼–ç æ ¼å¼å’Œé™æ€åˆ†ææ¥æ„å»ºä¸Šä¸‹æ–‡ã€‚æ•°æ®é›†æºè‡ªçœŸå®é¡¹ç›®çš„æäº¤å†å²ï¼Œæ¨¡æ‹Ÿäº†å®é™…çš„ä»£ç ç¼–è¾‘å·¥ä½œæµã€‚",
    "unique_features_quote": "We introduce the repo-level multi-round code editing task... We encode all prior code edits âˆ†1, . . . , âˆ†k using a line-based diffing scheme... we employ lightweight static analysis to pull in relevant parts of the codebase U. We collect a code editing dataset from the commit histories of 1650 open-source Python projects.",
    "data_size_quantity": 1650,
    "data_size_unit": "ä¸ªå¼€æºPythoné¡¹ç›®",
    "last_updated_year": 2024,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç¼–è¾‘çš„å‡†ç¡®æ€§å’Œè‡ªåŠ¨åŒ–ç¨‹åº¦', 'å…³æ³¨æ¨¡å‹åœ¨ç»™å®šç¼–è¾‘å†å²å’Œä»£ç åº“ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹é¢„æµ‹æ­£ç¡®ç¼–è¾‘çš„èƒ½åŠ›']",
    "evaluation_method_normalized": "['ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡', 'è‡ªåŠ¨åŒ–ç¼–è¾‘è¡Œç™¾åˆ†æ¯”', 'èŠ‚çœå‡»é”®ç™¾åˆ†æ¯”']",
    "problem_domain_normalized": "['é€šç”¨è½¯ä»¶å·¥ç¨‹', 'ä»£ç ç»´æŠ¤ä¸é‡æ„']",
    "source_type_normalized": "['æ¥è‡ªGitHubä¸Š1650ä¸ªå¼€æºPythoné¡¹ç›®çš„æäº¤å†å²']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2306.02907_output/content.md",
    "benchmark_name": "DS-1000",
    "benchmark_name_quote": "We evaluate SELFEVOLVE on three code generation datasets, including DS-1000 for data science code",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate SELFEVOLVE on three code generation datasets, including DS-1000 for data science code, HumanEval for software engineering code, and TransCoder for C++-to-Python translation.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ•°æ®ç§‘å­¦ä»£ç ç”Ÿæˆä»»åŠ¡",
    "task_description_quote": "the data science code generation task DS-1000",
    "dimension": "ä»£ç ç”ŸæˆåŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "Extensive experiments show that SELFEVOLVE achieves a significant improvement in execution-based measurements",
    "evaluation_method": "åŸºäºæ‰§è¡Œçš„æµ‹é‡",
    "evaluation_method_quote": "Extensive experiments show that SELFEVOLVE achieves a significant improvement in execution-based measurements",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "æ•°æ®ç§‘å­¦",
    "problem_domain_quote": "the data science code generation task DS-1000",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Python",
    "language_quote": "JuPyT5 [7] conditions on Jupyter notebooksâ€™ context cells to generate data science code.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "the data science code generation task DS-1000",
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": "è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°",
    "input_modality_quote": "Given a problem description written in natural language d",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "the autoregressive language model pÎ¸ predicts the solution",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "Given a problem description written in natural language d, and code context c, the autoregressive language model pÎ¸ predicts the solution",
    "execution_environment": "Pythonè§£é‡Šå™¨",
    "execution_environment_quote": "This refinement mechanism teaches language models to depend on an executor like a Python interpreter to correct the preliminary code.",
    "unique_features": "ä¸“æ³¨äºæ•°æ®ç§‘å­¦é¢†åŸŸçš„ä»£ç ç”Ÿæˆ",
    "unique_features_quote": "the data science code generation task DS-1000",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”ŸæˆåŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "['æ•°æ®ç§‘å­¦']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æ ‡å‡†åº“",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2306.03091_output/content.md",
    "benchmark_name": "RepoBench",
    "benchmark_name_quote": "we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°ä»“åº“çº§åˆ«çš„ä»£ç è‡ªåŠ¨è¡¥å…¨ç³»ç»Ÿã€‚",
    "task_description_quote": "RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.",
    "dimension": "è·¨æ–‡ä»¶ä»£ç æ£€ç´¢èƒ½åŠ›ã€ç»™å®šä¸Šä¸‹æ–‡çš„ä»£ç è¡¥å…¨èƒ½åŠ›ã€ç»“åˆæ£€ç´¢ä¸è¡¥å…¨çš„ç«¯åˆ°ç«¯æµç¨‹å¤„ç†èƒ½åŠ›ã€‚",
    "dimension_quote": "RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the systemâ€™s ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction.",
    "evaluation_method": "å¯¹äºæ£€ç´¢ä»»åŠ¡ï¼ˆRepoBench-Rï¼‰ä½¿ç”¨Accuracy@k (acc@k)æŒ‡æ ‡ï¼›å¯¹äºè¡¥å…¨ä»»åŠ¡ï¼ˆRepoBench-Cï¼‰å’Œç«¯åˆ°ç«¯ä»»åŠ¡ï¼ˆRepoBench-Pï¼‰ï¼Œè¯„ä¼°æ¨¡å‹é¢„æµ‹ä¸‹ä¸€è¡Œä»£ç çš„èƒ½åŠ›ã€‚",
    "evaluation_method_quote": "For evaluative purposes, the Accuracy@k (acc@k) metric is employed to assess retrieval performance. The easy subset is evaluated using acc@1 and acc@3, while the hard subset is examined through acc@1, acc@3, and acc@5 metrics.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®ï¼ˆä»“åº“çº§åˆ«ï¼‰ï¼ŒåŒ…å«è·¨æ–‡ä»¶ä¸Šä¸‹æ–‡å’Œæ–‡ä»¶å†…ä¸Šä¸‹æ–‡ã€‚",
    "context_dependency_quote": "However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. ... RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.",
    "problem_domain": "é€šç”¨ç¼–ç¨‹",
    "problem_domain_quote": "benchmark for auto-code completion",
    "problem_difficulty": "çœŸå®ä¸–ç•Œç¼–ç¨‹åœºæ™¯ï¼ŒåŒ…å«ä¸åŒéš¾åº¦å­é›†ï¼ˆå¦‚æ£€ç´¢ä»»åŠ¡ä¸­çš„Easyå’ŒHardå­é›†ï¼‰ã€‚",
    "problem_difficulty_quote": "This benchmark comprises three interconnected tasks: RepoBench-R for code retrieval, RepoBench-C for code completion, and RepoBench-P, which integrates both aspects to reflect the entire workflow of an auto-completion system, offering a balanced assessment. ... we categorize them into two subsets: those with 5-9 candidates as the easy subset, and those with 10 or more candidates as the hard subset.",
    "language": "Python å’Œ Java",
    "language_quote": "RepoBench supports both Python and Java",
    "data_size": "è®­ç»ƒæ•°æ®ï¼š10,345ä¸ªPythonä»“åº“å’Œ14,956ä¸ªJavaä»“åº“ã€‚æµ‹è¯•æ•°æ®ï¼š1,075ä¸ªPythonä»“åº“å’Œ594ä¸ªJavaä»“åº“ã€‚å…·ä½“ä»»åŠ¡æ ·æœ¬æ•°é‡è¯¦è§è®ºæ–‡è¡¨1ã€‚",
    "data_size_quote": "After processing the data, our dataset comprises 10,345 Python and 14,956 Java historical repositories, serving as training data and are available for optional fine-tuning. Additionally, we have 1,075 Python and 594 Java new repositories from GitHub designated as test data for evaluation.",
    "source_type": "1. Github-Codeæ•°æ®é›†ï¼ˆæˆªæ­¢2022å¹´3æœˆ16æ—¥ï¼‰ï¼Œç”¨äºæ„å»ºè®­ç»ƒæ•°æ®ã€‚2. æ–°çˆ¬å–çš„GitHubä»“åº“ï¼ˆåˆ›å»ºäº2023å¹´2æœˆ9æ—¥è‡³8æœˆ3æ—¥ä¹‹é—´ï¼‰ï¼Œä¸“é—¨ç”¨ä½œæµ‹è¯•é›†ã€‚",
    "source_type_quote": "Github-Code Dataset: The first source of RepoBench is the github-code dataset2, which consists of a vast collection of code files sourced from GitHub repositories under open-source licenses with a data cutoff date of March 16, 2022. ... Newly Crawled GitHub Data: To mitigate impacts regarding data leakage and memorization, we augment the dataset by incoporating the most recent, non-forked GitHub repositories that are permitted under their respective licenses. Specifically, we use GitHubâ€™s official API to crawl Python and Java repositories created after February 9, 2023, which aligns with the newest knowledge cutoff date of The Stack [22], and before August 3, 2023. This newly-crawled data serves exclusively as our test set for evaluation.",
    "last_updated": "2023-10-04 (arXivç‰ˆæœ¬v2)",
    "last_updated_quote": "arXiv:2306.03091v2  [cs.CL]  4 Oct 2023",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we introduce RepoBench, a new benchmark",
    "contamination_status": "é€šè¿‡ä½¿ç”¨æ–°çˆ¬å–çš„æ•°æ®ä½œä¸ºæµ‹è¯•é›†æ¥å‡è½»æ•°æ®æ³„éœ²å’Œè®°å¿†çš„å½±å“ã€‚",
    "contamination_status_quote": "To mitigate impacts regarding data leakage and memorization, we augment the dataset by incoporating the most recent, non-forked GitHub repositories ... This newly-crawled data serves exclusively as our test set for evaluation.",
    "dataset_license": "æ•°æ®æ¥æºäºå¼€æºè®¸å¯ä¸‹çš„GitHubä»“åº“ï¼Œä½†æœªæ˜ç¡®æŒ‡å®šæ•°æ®é›†æœ¬èº«çš„è®¸å¯è¯ã€‚",
    "dataset_license_quote": "code files sourced from GitHub repositories under open-source licenses",
    "task_granularity": "ä»£ç è¡¥å…¨ï¼ˆä¸‹ä¸€è¡Œé¢„æµ‹ï¼‰ã€ä»£ç æ£€ç´¢ã€ç«¯åˆ°ç«¯æµç¨‹ï¼ˆæ£€ç´¢+è¡¥å…¨ï¼‰ã€‚",
    "task_granularity_quote": "RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline).",
    "evaluation_metrics": "Accuracy@k (acc@1, acc@3, acc@5)",
    "evaluation_metrics_quote": "For evaluative purposes, the Accuracy@k (acc@k) metric is employed to assess retrieval performance. The easy subset is evaluated using acc@1 and acc@3, while the hard subset is examined through acc@1, acc@3, and acc@5 metrics.",
    "input_modality": "ä»£ç ï¼ˆæ–‡ä»¶å†…ä¸Šä¸‹æ–‡å’Œè·¨æ–‡ä»¶ä»£ç ç‰‡æ®µï¼‰",
    "input_modality_quote": "the prompt is created by combining all the parsed snippets as cross-file contexts and an in-file context. The in-file context includes import statements and several preceding lines of code",
    "output_modality": "ä»£ç ï¼ˆä¸‹ä¸€è¡Œï¼‰",
    "output_modality_quote": "predict the next line of code",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "predict the next line of code given a pre-defined context. The context can involve content from different files (cross-file context) and within the file (in-file context)",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºä»“åº“çº§åˆ«çš„ä»£ç è‡ªåŠ¨è¡¥å…¨è¯„ä¼°ï¼ŒåŒ…å«ä¸‰ä¸ªç›¸äº’å…³è”çš„ä»»åŠ¡ï¼ˆæ£€ç´¢ã€è¡¥å…¨ã€ç«¯åˆ°ç«¯æµç¨‹ï¼‰ï¼Œå¹¶è®¾è®¡äº†ä¸åŒçš„ä¸Šä¸‹æ–‡è®¾ç½®ï¼ˆCross-File-First, Cross-File-Random, In-Fileï¼‰å’Œå­é›†ï¼ˆå¦‚2kå’Œ8k tokené™åˆ¶ï¼‰ä»¥é€‚åº”ä¸åŒæ¨¡å‹ã€‚",
    "unique_features_quote": "RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). ... To effectively evaluate next-line prediction in auto-completion systems, we define three settings: Cross-File-First (XF-F)... Cross-File-Random (XF-R)... In-File (IF)... RepoBench-C is divided into two subsets: RepoBench-C-2k and RepoBench-C-8k.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2023,
    "last_updated_month": 10,
    "last_updated_day": 4,
    "language_normalized": "['Python', 'Java']",
    "dimension_normalized": "['è·¨æ–‡ä»¶ä»£ç æ£€ç´¢èƒ½åŠ›', 'ç»™å®šä¸Šä¸‹æ–‡çš„ä»£ç è¡¥å…¨èƒ½åŠ›', 'ç»“åˆæ£€ç´¢ä¸è¡¥å…¨çš„ç«¯åˆ°ç«¯æµç¨‹å¤„ç†èƒ½åŠ›']",
    "evaluation_method_normalized": "['Accuracy@k', 'acc@1', 'acc@3', 'acc@5']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹']",
    "source_type_normalized": "['Github-Codeæ•°æ®é›†', 'æ–°çˆ¬å–çš„GitHubä»“åº“']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2306.08568_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "Through comprehensive experiments on five prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, DS-1000, and MultiPL-E, our models showcase outstanding performance.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Through comprehensive experiments on five prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, DS-1000, and MultiPL-E, our models showcase outstanding performance.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»£ç ç”Ÿæˆï¼ˆä»è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆä»£ç ï¼‰",
    "task_description_quote": "These models, pre-trained on substantial code data, excel in various code-related tasks, consistently delivering impressive performance. (ç»“åˆä¸Šä¸‹æ–‡ï¼Œè¯„æµ‹åŸºå‡†ç”¨äºè¯„ä¼°ä»£ç ç”Ÿæˆä»»åŠ¡)",
    "dimension": "ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "Remarkably, WizardCoder 15B even surpasses the well-known closed-source LLMs, including Anthropicâ€™s Claude and Googleâ€™s Bard, on the HumanEval and HumanEval+ benchmarks. (é€šè¿‡passç‡è¯„ä¼°ï¼Œéšå«äº†åŠŸèƒ½æ­£ç¡®æ€§ç»´åº¦)",
    "evaluation_method": "pass@1 (é€šè¿‡ç‡)",
    "evaluation_method_quote": "Figure 1: An illustration of our novel Code Evol-Instruct and the superior pass@1 performance of our WizardCoder 34B... The Python score is the mean between HumanEval and MBPP.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "é€šç”¨ç¼–ç¨‹é—®é¢˜",
    "problem_domain_quote": "Develop a Python program that creates a random password of length 8 characters. (ç¤ºä¾‹ä»»åŠ¡å±äºé€šç”¨ç¼–ç¨‹)",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "å¤šç§ç¼–ç¨‹è¯­è¨€ï¼ˆåŒ…æ‹¬Pythonï¼‰",
    "language_quote": "outperforming the open-source SOTA ... by a large margin in 9 different programming languages. The Python score is the mean between HumanEval and MBPP.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "Through comprehensive experiments on five prominent code generation benchmarks",
    "evaluation_metrics": "pass@1",
    "evaluation_metrics_quote": "Figure 1: An illustration of our novel Code Evol-Instruct and the superior pass@1 performance of our WizardCoder 34B",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆæŒ‡ä»¤/æè¿°ï¼‰",
    "input_modality_quote": "Develop a Python program that creates a random password of length 8 characters. (ç¤ºä¾‹ä¸­çš„è¾“å…¥æ˜¯è‡ªç„¶è¯­è¨€æŒ‡ä»¤)",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "Here's an example program that generates a random password... (ç¤ºä¾‹ä¸­çš„è¾“å‡ºæ˜¯ä»£ç )",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "Develop a Python program that creates a random password of length 8 characters. (ä»è‡ªç„¶è¯­è¨€æ–‡æœ¬æè¿°ç”Ÿæˆä»£ç )",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "æœ¬æ–‡æåŠäº†äº”ä¸ªåŸºå‡†ï¼šHumanEval, HumanEval+, MBPP, DS-1000, å’Œ MultiPL-Eã€‚å…¶ä¸­MultiPL-Eæ”¯æŒå¤šè¯­è¨€è¯„ä¼°ã€‚",
    "unique_features_quote": "Through comprehensive experiments on five prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, DS-1000, and MultiPL-E",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['å¤šç§ç¼–ç¨‹è¯­è¨€', 'Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@1']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹é—®é¢˜']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2306.09896_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "In this paper, we analyze Code Llama, GPT-3.5 and GPT-4â€™s ability to perform self-repair on problems taken from HumanEval and APPS.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "In this paper, we analyze Code Llama, GPT-3.5 and GPT-4â€™s ability to perform self-repair on problems taken from HumanEval and APPS.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»è‡ªç„¶è¯­è¨€è§„èŒƒç”Ÿæˆä»£ç ç‰‡æ®µ",
    "task_description_quote": "Large language models (LLMs) have proven capable of generating code snippets from natural language specifications",
    "dimension": "ä»£ç ç”Ÿæˆä¸è‡ªæˆ‘ä¿®å¤èƒ½åŠ›",
    "dimension_quote": "we focus on evaluating the modelsâ€™ capacity to reflect upon, provide feedback on and debug the code.",
    "evaluation_method": "pass@k æŒ‡æ ‡ï¼Œæ‰§è¡Œå•å…ƒæµ‹è¯•",
    "evaluation_method_quote": "performance is typically measured by pass@k (Chen et al., 2021; Kulal et al., 2019)â€”the probability that at least one of k i.i.d. program samples from the model satisfies a given specification.",
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "for self-contained Python programming tasks.",
    "problem_domain": "é€šç”¨ç¼–ç¨‹",
    "problem_domain_quote": "self-contained Python programming tasks.",
    "problem_difficulty": "ç«èµ›çº§å’Œé¢è¯•çº§",
    "problem_difficulty_quote": "complex coding challenges such as those found in competitions and professional software engineering interviews.",
    "language": "Python",
    "language_quote": "for self-contained Python programming tasks.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "generating code snippets from natural language specifications",
    "evaluation_metrics": "pass@k",
    "evaluation_metrics_quote": "performance is typically measured by pass@k (Chen et al., 2021; Kulal et al., 2019)",
    "input_modality": "è‡ªç„¶è¯­è¨€ä¸å•å…ƒæµ‹è¯•",
    "input_modality_quote": "Given a specification Ïˆ, a programming model MP first generates np samples i.i.d.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "generating code snippets",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "generating code snippets from natural language specifications",
    "execution_environment": "å•å…ƒæµ‹è¯•æ‰§è¡Œç¯å¢ƒ",
    "execution_environment_quote": "These np code samples are then executed against a test bed.",
    "unique_features": "æœ¬æ–‡é‡ç‚¹ç ”ç©¶è‡ªæˆ‘ä¿®å¤ï¼ˆself-repairï¼‰ç­–ç•¥åœ¨ä»£ç ç”Ÿæˆä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§ï¼Œè€Œéæå‡ºæ–°åŸºå‡†ã€‚",
    "unique_features_quote": "In this paper, we investigate the efficacy of self-repair techniques applied to CodeLlama-13b-instruct, GPT-3.5, and GPT-4 for self-contained Python programming tasks.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆä¸è‡ªæˆ‘ä¿®å¤èƒ½åŠ›']",
    "evaluation_method_normalized": "['pass@k']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "ç«èµ›çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2306.11644_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "The evaluation benchmark proposed in the latter work, HumanEval, has been widely adopted for comparing LLMsâ€™ performance on code.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We demonstrate the power of high quality data in breaking existing scaling laws by training a 1.3B-parameter model, which we call phi-1, ... we attain 50.6% pass@1 accuracy on HumanEval and 55.5% pass@1 accuracy on MBPP",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»è‡ªç„¶è¯­è¨€æ–‡æ¡£å­—ç¬¦ä¸²ï¼ˆdocstringsï¼‰ä¸­åˆæˆç®€å•çš„Pythonå‡½æ•°ã€‚",
    "task_description_quote": "We focus our attention on LLMs trained for code, and specifically writing simple Python functions from their docstrings as in [CTJ+21].",
    "dimension": "ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "The evaluation benchmark proposed in the latter work, HumanEval, has been widely adopted for comparing LLMsâ€™ performance on code.",
    "evaluation_method": "pass@1 å‡†ç¡®ç‡",
    "evaluation_method_quote": "we attain 50.6% pass@1 accuracy on HumanEval and 55.5% pass@1 accuracy on MBPP",
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "writing simple Python functions from their docstrings",
    "problem_domain": "åŸºç¡€ç¼–ç¨‹æ¦‚å¿µ",
    "problem_domain_quote": "determine its educational value for a student whose goal is to learn basic coding concepts",
    "problem_difficulty": "åŸºç¡€çº§åˆ«",
    "problem_difficulty_quote": "for a student whose goal is to learn basic coding concepts",
    "language": "Python",
    "language_quote": "writing simple Python functions from their docstrings",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2021",
    "last_updated_quote": "2021 Jul Codex-300M [CTJ+21]",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": "æ–‡ä¸­è®¨è®ºäº†è®­ç»ƒæ•°æ®å¯èƒ½å­˜åœ¨çš„æ±¡æŸ“é£é™©",
    "contamination_status_quote": "in Section 5 we study possible contamination of our training data with respect to HumanEval.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "writing simple Python functions from their docstrings",
    "evaluation_metrics": "pass@1",
    "evaluation_metrics_quote": "we attain 50.6% pass@1 accuracy on HumanEval",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "writing simple Python functions from their docstrings",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "writing simple Python functions",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "writing simple Python functions from their docstrings",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "è¢«å¹¿æ³›ç”¨äºæ¯”è¾ƒLLMåœ¨ä»£ç ç”Ÿæˆä¸Šçš„æ€§èƒ½",
    "unique_features_quote": "HumanEval, has been widely adopted for comparing LLMsâ€™ performance on code.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2021,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@1']",
    "problem_domain_normalized": "['åŸºç¡€ç¼–ç¨‹æ¦‚å¿µ']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "å…¥é—¨çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "é«˜æ±¡æŸ“é£é™©"
  },
  {
    "source_paper": "2306.14893_output/content.md",
    "benchmark_name": "LCC (Long Code Completion Benchmark)",
    "benchmark_name_quote": "In this paper, we introduce the Long Code Completion Benchmark (LCC), a new benchmark that focuses on code completion with long code context for three programming languages: Python, Java, and C#.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To evaluate the effectiveness of LongCoder and encourage future research on Long Code Completion, we construct a new dataset called LCC by filtering code from GitHub based on length, with the goal of focusing on longer code examples.",
    "dataset_url": "https://github.com/microsoft/CodeBERT",
    "dataset_url_quote": "1All the codes and data are available at https://github.com/microsoft/CodeBERT.",
    "task_description": "ä¸“æ³¨äºé•¿ä»£ç ä¸Šä¸‹æ–‡çš„ä»£ç è¡¥å…¨ä»»åŠ¡ï¼Œæ—¨åœ¨è¯„ä¼°æ¨¡å‹åœ¨æ›´å¤æ‚ã€æ›´ç°å®çš„ä»£ç æ–‡ä»¶çº§åˆ«åœºæ™¯ä¸‹çš„èƒ½åŠ›ã€‚",
    "task_description_quote": "In this paper, we introduce a new task for code completion that focuses on handling long code input... We introduce the Long Code Completion Benchmark (LCC), a new benchmark that focuses on code completion with long code context...",
    "dimension": "é•¿ä»£ç å»ºæ¨¡èƒ½åŠ›ã€ä»£ç è¡¥å…¨å‡†ç¡®æ€§ã€æ¨¡å‹æ•ˆç‡ï¼ˆè®¡ç®—èµ„æºæ¶ˆè€—ï¼‰",
    "dimension_quote": "...achieves superior performance on code completion tasks compared to previous models while maintaining comparable efficiency in terms of computational resources during inference.",
    "evaluation_method": "åœ¨é€è¡ŒåŸºç¡€ä¸Šï¼Œä½¿ç”¨ç²¾ç¡®åŒ¹é…ï¼ˆExact Match, EMï¼‰å’Œç¼–è¾‘ç›¸ä¼¼åº¦ï¼ˆEdit Similarity, Edit Simï¼‰è¿›è¡Œè¯„ä¼°ã€‚",
    "evaluation_method_quote": "We follow Lu et al. (2021) to evaluate the performance of the models in terms of Exact Match (EM) and Edit Similarity (Edit Sim) on a per-line basis (Svyatkovskiy et al., 2020).",
    "context_dependency": "æ–‡ä»¶çº§åˆ«ï¼ˆé•¿ä»£ç ä¸Šä¸‹æ–‡ï¼‰ï¼Œä¸Šä¸‹æ–‡é•¿åº¦é€šå¸¸è¶…è¿‡512ä¸ªä»£ç æ ‡è®°ï¼Œå¹³å‡é•¿åº¦åœ¨1800-2000ä¸ªæ ‡è®°å·¦å³ã€‚",
    "context_dependency_quote": "...focuses on code completion with long code context... The average length of the examples in LCC are 5Ã— longer than those in existing datasets... The average length of a Python source file on GitHub is 1,305 tokens.",
    "problem_domain": "é€šç”¨ç¼–ç¨‹ï¼Œæ•°æ®æ¥æºäºGitHubä¸Šçš„å¼€æºä»£ç æ–‡ä»¶ã€‚",
    "problem_domain_quote": "Specifically, we construct our datasets from the github-code dataset, which contains a vast number of code files sourced from GitHub with an open-source license that permits research use.",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼ŒåŒ…å«çœŸå®ä¸–ç•Œä¸­çš„é•¿ä»£ç æ–‡ä»¶ï¼Œç»“æ„å’Œä¾èµ–å…³ç³»æ›´å¤æ‚ã€‚",
    "problem_difficulty_quote": "Meanwhile, longer code sequences contain more complex structures and require models to consider more context and dependencies. This can be challenging for previously proposed code completion models that focus on short code...",
    "language": "Python, Java, C#",
    "language_quote": "...a new benchmark that focuses on code completion with long code context for three programming languages: Python, Java, and C#.",
    "data_size": "æ¯ç§è¯­è¨€åŒ…å«10ä¸‡ä¸ªè®­ç»ƒæ ·æœ¬ï¼Œ1ä¸‡ä¸ªå¼€å‘é›†æ ·æœ¬å’Œ1ä¸‡ä¸ªæµ‹è¯•é›†æ ·æœ¬ã€‚",
    "data_size_quote": "For each programming language, we sample 100k examples for training, and 10k examples for development and 10k for testing.",
    "source_type": "ä»GitHubä¸Šå…·æœ‰å¼€æºè®¸å¯è¯çš„ä»£ç æ–‡ä»¶ä¸­ç­›é€‰å’Œæ„å»ºï¼Œç»è¿‡å»é‡å’ŒASTè§£æè¿‡æ»¤ã€‚",
    "source_type_quote": "Specifically, we construct our datasets from the github-code dataset, which contains a vast number of code files sourced from GitHub with an open-source license that permits research use. The steps to construct the datasets are as follows: â€¢ We first follow Allamanis (2019) to deduplicate examples with high similarity (Jacobi similarity â‰¥0.9) in order to eliminate forked files, and then remove code files that canâ€™t be parsed into an abstract syntax tree using a standard compiler tool called tree-sitter.",
    "last_updated": "2023",
    "last_updated_quote": "Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆç”±è®ºæ–‡ä½œè€…æ„å»ºï¼‰",
    "build_type_quote": "We construct a new dataset (LCC) for code completion tasks that requires long code modeling to encourage more research in such scenarios.",
    "contamination_status": "æ–‡ä¸­æœªæ˜ç¡®è®¨è®º",
    "contamination_status_quote": NaN,
    "dataset_license": "å¼€æºè®¸å¯è¯ï¼ˆå…è®¸ç ”ç©¶ä½¿ç”¨ï¼‰ï¼Œå…·ä½“è®¸å¯è¯åç§°æœªæ˜ç¡®è¯´æ˜ã€‚",
    "dataset_license_quote": "...sourced from GitHub with an open-source license that permits research use.",
    "task_granularity": "ä»£ç è¡¥å…¨ï¼ˆå…·ä½“ä¸ºä¸‹ä¸€è¡Œé¢„æµ‹æˆ–è¡Œå†…è¡¥å…¨ï¼‰",
    "task_granularity_quote": "Code completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context.",
    "evaluation_metrics": "ç²¾ç¡®åŒ¹é…ï¼ˆExact Match, EMï¼‰ã€ç¼–è¾‘ç›¸ä¼¼åº¦ï¼ˆEdit Similarity, Edit Simï¼‰",
    "evaluation_metrics_quote": "We follow Lu et al. (2021) to evaluate the performance of the models in terms of Exact Match (EM) and Edit Similarity (Edit Sim) on a per-line basis (Svyatkovskiy et al., 2020).",
    "input_modality": "ä»£ç ",
    "input_modality_quote": "Code completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "Code completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context.",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "Code completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context.",
    "execution_environment": "æ–‡ä¸­æœªæ˜ç¡®æè¿°",
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“é—¨é’ˆå¯¹é•¿ä»£ç ä¸Šä¸‹æ–‡è®¾è®¡ï¼Œå¹³å‡ä»£ç é•¿åº¦æ˜¯ç°æœ‰æ•°æ®é›†çš„5å€ï¼›åŒ…å«Pythonã€Javaã€C#ä¸‰ç§è¯­è¨€ï¼›é€šè¿‡é•¿åº¦è¿‡æ»¤ï¼ˆ>512 tokensï¼‰ç¡®ä¿å…³æ³¨é•¿ä¸Šä¸‹æ–‡åœºæ™¯ã€‚",
    "unique_features_quote": "On average, the examples in LCC are 5Ã— longer than those in existing datasets (Lu et al., 2021)... Since the benchmark primarily focuses on the code completion task with long code context, we remove code files whose length of code tokens after tokenization is shorter than 512.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2023,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python', 'Java', 'C#']",
    "dimension_normalized": "['é•¿ä»£ç å»ºæ¨¡èƒ½åŠ›', 'ä»£ç è¡¥å…¨å‡†ç¡®æ€§', 'æ¨¡å‹æ•ˆç‡ï¼ˆè®¡ç®—èµ„æºæ¶ˆè€—ï¼‰']",
    "evaluation_method_normalized": "['ç²¾ç¡®åŒ¹é…ï¼ˆExact Match, EMï¼‰', 'ç¼–è¾‘ç›¸ä¼¼åº¦ï¼ˆEdit Similarity, Edit Simï¼‰']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹', 'æ•°æ®æ¥æºäºGitHubä¸Šçš„å¼€æºä»£ç æ–‡ä»¶']",
    "source_type_normalized": "['ä»GitHubä¸Šå…·æœ‰å¼€æºè®¸å¯è¯çš„ä»£ç æ–‡ä»¶ä¸­ç­›é€‰å’Œæ„å»º', 'ç»è¿‡å»é‡å’ŒASTè§£æè¿‡æ»¤']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç è¡¥å…¨",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2307.04349_output/content.md",
    "benchmark_name": "APPS",
    "benchmark_name_quote": "Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ç¨‹åºåˆæˆæˆ–ä»£ç ç”Ÿæˆï¼Œå³æ ¹æ®ç»™å®šçš„é«˜çº§è¡Œä¸ºæè¿°ç”Ÿæˆè®¡ç®—æœºä»£ç ã€‚",
    "task_description_quote": "Program synthesis, or code generation, involves creating an executable program that solves a given problem.",
    "dimension": "ä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆé€šè¿‡å•å…ƒæµ‹è¯•ï¼‰",
    "dimension_quote": "Unlike text generation, program synthesis requires both syntactic and functional accuracy since the generated code must pass compilation and unit tests.",
    "evaluation_method": "ä½¿ç”¨å•å…ƒæµ‹è¯•ç»“æœä½œä¸ºåé¦ˆä¿¡å·ï¼Œè¯„ä¼°ç”Ÿæˆçš„ä»£ç æ˜¯å¦èƒ½é€šè¿‡ç¼–è¯‘å’Œæµ‹è¯•ã€‚",
    "evaluation_method_quote": "RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "ç¼–ç¨‹æŒ‘æˆ˜ï¼ŒåŒ…æ‹¬ç«èµ›çº§ç®—æ³•é—®é¢˜ã€‚",
    "problem_domain_quote": "While LLMs have shown promising results in basic programming tasks, there is still progress to be made in tackling more challenging problems such as program competitions.",
    "problem_difficulty": "ä»åŸºç¡€ç¼–ç¨‹ä»»åŠ¡åˆ°ç«èµ›çº§æŒ‘æˆ˜ã€‚",
    "problem_difficulty_quote": "While LLMs have shown promising results in basic programming tasks, there is still progress to be made in tackling more challenging problems such as program competitions.",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æè¿°APPSæ•°æ®é›†çš„è¯­è¨€æ„æˆï¼Œä»…æåŠCodexè§£å†³PythonæŒ‘æˆ˜ã€‚",
    "language_quote": "Codex (Chen et al., 2021) is a noteworthy example of an LLM with 12 billion parameters that can successfully solve over 70% of complex Python programming challenges created by humans.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»æè¿°ç”Ÿæˆå®Œæ•´ä»£ç ï¼‰",
    "task_granularity_quote": "Program synthesis, or code generation, involves creating an executable program that solves a given problem.",
    "evaluation_metrics": "åŸºäºå•å…ƒæµ‹è¯•é€šè¿‡ç‡çš„åé¦ˆä¿¡å·ï¼ˆå¦‚ç²—ç²’åº¦ã€ç»†ç²’åº¦ã€è‡ªé€‚åº”åé¦ˆï¼‰",
    "evaluation_metrics_quote": "Built upon this framework, we develop multi-granularity feedback that is automatically extracted from unit test. To expand, we introduce coarse-grained and fine-grained feedbacks applicable to programs with errors, aimed at punishing the specific segments of code where the errors appear. For programs that do not pass all test cases, we propose an adaptive feedback mechanism that assigns varying penalties based on the ratio of passed test cases.",
    "input_modality": "è‡ªç„¶è¯­è¨€æè¿°",
    "input_modality_quote": "Program synthesis is an automated process that generates computer code W based on a high-level description of desired behavior D.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "Program synthesis is an automated process that generates computer code W based on a high-level description of desired behavior D.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "Program synthesis is an automated process that generates computer code W based on a high-level description of desired behavior D.",
    "execution_environment": "ç¼–è¯‘å™¨/å•å…ƒæµ‹è¯•ç¯å¢ƒ",
    "execution_environment_quote": "One generates the target program and interacts with the compiler to produce a training data pair...",
    "unique_features": "æœ¬æ–‡æ˜¯è¯„æµ‹è®ºæ–‡ï¼Œæœªæè¿°APPSåŸºå‡†çš„ç‹¬ç‰¹ä¹‹å¤„ã€‚æœ¬æ–‡æå‡ºçš„RLTFæ–¹æ³•çš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºå…¶åœ¨çº¿å¼ºåŒ–å­¦ä¹ æ¡†æ¶å’Œä»å•å…ƒæµ‹è¯•ä¸­æå–çš„å¤šç²’åº¦åé¦ˆï¼ˆç²—ç²’åº¦ã€ç»†ç²’åº¦ã€è‡ªé€‚åº”åé¦ˆï¼‰ã€‚",
    "unique_features_quote": "To address these issues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆé€šè¿‡å•å…ƒæµ‹è¯•ï¼‰']",
    "evaluation_method_normalized": "['åŸºäºå•å…ƒæµ‹è¯•é€šè¿‡ç‡çš„åé¦ˆä¿¡å·ï¼ˆå¦‚ç²—ç²’åº¦ã€ç»†ç²’åº¦ã€è‡ªé€‚åº”åé¦ˆï¼‰']",
    "problem_domain_normalized": "['ç¼–ç¨‹æŒ‘æˆ˜', 'åŒ…æ‹¬ç«èµ›çº§ç®—æ³•é—®é¢˜']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "ç«èµ›çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2307.14936_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Through extensive evaluation on three benchmarks, including HumanEval, CoderEval, and LeetCode, we conjecture that...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»è‡ªç„¶è¯­è¨€æè¿°ç”ŸæˆPythonä»£ç ï¼Œè¡¡é‡ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚",
    "task_description_quote": "demonstrating remarkable performance on the code generation task.",
    "dimension": "ä»£ç ç”Ÿæˆæ­£ç¡®æ€§",
    "dimension_quote": "measure functional correctness for synthesizing programs from docstrings.",
    "evaluation_method": "pass@k (æ–‡ä¸­ä¸»è¦æŠ¥å‘Špass@1)",
    "evaluation_method_quote": "PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "é€šç”¨ç¼–ç¨‹é—®é¢˜",
    "problem_domain_quote": "tackle up to 72% of Python programming problems.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Python",
    "language_quote": "tackle up to 72% of Python programming problems.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "demonstrating remarkable performance on the code generation task.",
    "evaluation_metrics": "pass@1",
    "evaluation_metrics_quote": "PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark.",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "generating code from natural language descriptions",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "generating code from natural language descriptions",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "generating code from natural language descriptions",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ç”±OpenAIå‘å¸ƒï¼Œæ˜¯ä»£ç ç”Ÿæˆé¢†åŸŸå¹¿æ³›ä½¿ç”¨çš„åŸºå‡†æµ‹è¯•ã€‚",
    "unique_features_quote": "OpenAI HumanEval benchmark",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆæ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@1']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹é—®é¢˜']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2308.07124_output/content.md",
    "benchmark_name": "HUMANEVALPACK",
    "benchmark_name_quote": "We further introduce HUMANEVALPACK, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust).",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We further introduce HUMANEVALPACK, expanding the HumanEval benchmark to a total of 3 coding tasks...",
    "dataset_url": "https://github.com/bigcode-project/octopack",
    "dataset_url_quote": "Code, models and data are freely available at https://github.com/bigcode-project/octopack.",
    "task_description": "è¯„æµ‹åŸºå‡†æ—¨åœ¨è¯„ä¼°ä»£ç å¤§è¯­è¨€æ¨¡å‹åœ¨å¤šç§ä»£ç ç›¸å…³ä»»åŠ¡ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œè¦†ç›–ä»£ç ä¿®å¤ã€ä»£ç è§£é‡Šå’Œä»£ç åˆæˆä¸‰ç§åœºæ™¯ã€‚",
    "task_description_quote": "HUMANEVALPACK: A benchmark for Code LLM generalization, spanning three scenarios (Code Repair, Code Explanation, Code Synthesis) and 6 programming languages (Python, JavaScript, Java, Go, C++, Rust)",
    "dimension": "ä»£ç å¤§è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€å¤šä»»åŠ¡å¤„ç†èƒ½åŠ›ã€å¤šè¯­è¨€ä»£ç èƒ½åŠ›",
    "dimension_quote": "A benchmark for Code LLM generalization, spanning three scenarios (Code Repair, Code Explanation, Code Synthesis) and 6 programming languages",
    "evaluation_method": "ä½¿ç”¨ pass@k æŒ‡æ ‡è¯„ä¼°åŠŸèƒ½æ­£ç¡®æ€§",
    "evaluation_method_quote": "Metric: Pass@k (Figure 3 caption)",
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "Given an incorrect code function with a subtle bug and accompanying unit tests, the model is tasked to fix the function. (æè¿° HUMANEVALFIX ä»»åŠ¡)",
    "problem_domain": "é€šç”¨ç¼–ç¨‹é—®é¢˜ï¼ŒåŒ…å«ç®—æ³•å’ŒåŸºç¡€åŠŸèƒ½å®ç°",
    "problem_domain_quote": "Write a Python function `has_close_elements(numbers: List[float], threshold: float) -> bool` to solve the following problem: Check if in given list of numbers, are any two numbers closer to each other than given threshold. (Figure 3 ç¤ºä¾‹)",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼ˆåŒ…å«éœ€è¦ä¿®å¤çš„å¾®å¦™bugï¼‰",
    "problem_difficulty_quote": "Given an incorrect code function with a subtle bug... Bugs are written such that the code still runs but produces an incorrect result leading to at least one unit test failing.",
    "language": "Python, JavaScript, Java, Go, C++, Rust",
    "language_quote": "spanning three scenarios (Code Repair, Code Explanation, Code Synthesis) and 6 programming languages (Python, JavaScript, Java, Go, C++, Rust)",
    "data_size": "åŸºäº164ä¸ªHumanEvalé—®é¢˜ï¼Œæ¯ä¸ªé—®é¢˜æ‰©å±•åˆ°6ç§è¯­è¨€å’Œ3ç§ä»»åŠ¡ï¼Œå…±984ä¸ªä¿®å¤ä»»åŠ¡ï¼Œä»¥åŠå¯¹åº”çš„è§£é‡Šå’Œåˆæˆä»»åŠ¡ã€‚",
    "data_size_quote": "We manually add a bug to each of the 164 HumanEval solutions across all 6 languages (984 total bugs).",
    "source_type": "åŸºäºHumanEvalåŸºå‡†äººå·¥æ‰©å±•æ„å»º",
    "source_type_quote": "we expand the code synthesis benchmark HumanEval (Chen et al., 2021; Zheng et al., 2023) to cover all three input-output combinations for six languages",
    "last_updated": "2024",
    "last_updated_quote": "arXiv:2308.07124v2 [cs.CL] 18 Feb 2024",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆæœ¬æ–‡ä½œè€…å›¢é˜Ÿï¼‰",
    "build_type_quote": "We further introduce HUMANEVALPACK",
    "contamination_status": "åŸºäºç°æœ‰åŸºå‡†æ‰©å±•ï¼Œå­˜åœ¨æ±¡æŸ“é£é™©",
    "contamination_status_quote": "expanding the HumanEval benchmark",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ä¿®å¤ã€ä»£ç è§£é‡Šã€ä»£ç åˆæˆ",
    "task_granularity_quote": "spanning three scenarios (Code Repair, Code Explanation, Code Synthesis)",
    "evaluation_metrics": "pass@k",
    "evaluation_metrics_quote": "Metric: Pass@k (Figure 3 caption)",
    "input_modality": "è‡ªç„¶è¯­è¨€ä¸ä»£ç çš„ç»„åˆï¼Œå–å†³äºå…·ä½“ä»»åŠ¡ï¼ˆNL, NL+C, Cï¼‰",
    "input_modality_quote": "When instruction tuning with code (C) data, code may either appear only in the input alongside the NL instruction (NL+Câ†’NL, e.g. code explanation), only in the output (NLâ†’C, e.g. code synthesis), or in both input and output (NL+Câ†’C, e.g. code modifications like bug fixing).",
    "output_modality": "è‡ªç„¶è¯­è¨€æˆ–ä»£ç ï¼Œå–å†³äºå…·ä½“ä»»åŠ¡ï¼ˆNL, Cï¼‰",
    "output_modality_quote": "code may either appear only in the input alongside the NL instruction (NL+Câ†’NL, e.g. code explanation), only in the output (NLâ†’C, e.g. code synthesis), or in both input and output (NL+Câ†’C, e.g. code modifications like bug fixing).",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ï¼ˆåˆæˆï¼‰ã€ä»£ç åˆ°æ–‡æœ¬ï¼ˆè§£é‡Šï¼‰ã€ä»£ç åˆ°ä»£ç ï¼ˆä¿®å¤ï¼‰",
    "task_io_type_quote": "cover all three input-output combinations: NL+Câ†’NL (e.g. code explanation), NLâ†’C (e.g. code synthesis), NL+Câ†’C (e.g. code modifications like bug fixing)",
    "execution_environment": "å•å…ƒæµ‹è¯•æ‰§è¡Œç¯å¢ƒ",
    "execution_environment_quote": "Given an incorrect code function with a subtle bug and accompanying unit tests",
    "unique_features": "æ‰©å±•äº†ç»å…¸çš„HumanEvalåŸºå‡†ï¼Œè¦†ç›–äº†ä»£ç ä¿®å¤å’Œè§£é‡Šç­‰æ›´å¹¿æ³›çš„ç°å®ä»»åŠ¡ï¼Œæ”¯æŒå…­ç§ç¼–ç¨‹è¯­è¨€ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰åŸºå‡†å› æ¨¡å‹æ€§èƒ½æ¥è¿‘é¥±å’Œè€Œå¯èƒ½å¤±æ•ˆçš„é—®é¢˜ã€‚",
    "unique_features_quote": "Our more challenging evaluation variants provide room for future LLMs to improve on the performance of the current state-of-the-art.",
    "data_size_quantity": 164,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": 2024,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python', 'JavaScript', 'Java', 'Go', 'C++', 'Rust']",
    "dimension_normalized": "['ä»£ç å¤§è¯­è¨€æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›', 'å¤šä»»åŠ¡å¤„ç†èƒ½åŠ›', 'å¤šè¯­è¨€ä»£ç èƒ½åŠ›']",
    "evaluation_method_normalized": "['pass@k']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹é—®é¢˜', 'åŒ…å«ç®—æ³•å’ŒåŸºç¡€åŠŸèƒ½å®ç°']",
    "source_type_normalized": "['åŸºäºHumanEvalåŸºå‡†äººå·¥æ‰©å±•æ„å»º']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "é«˜æ±¡æŸ“é£é™©"
  },
  {
    "source_paper": "2308.10335_output/content.md",
    "benchmark_name": "ROBUSTAPI",
    "benchmark_name_quote": "To fill the missing piece, we propose a dataset ROBUSTAPI for evaluating the reliability and robustness of code generated by LLMs.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To fill the missing piece, we propose a dataset ROBUSTAPI for evaluating the reliability and robustness of code generated by LLMs.",
    "dataset_url": "https://github.com/FloridSleeves/RobustAPI",
    "dataset_url_quote": "We open-source our dataset and evaluator on GitHub2. ... 2https://github.com/FloridSleeves/RobustAPI",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„ä»£ç çš„å¯é æ€§å’Œé²æ£’æ€§ï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹Java APIçš„è¯¯ç”¨æƒ…å†µã€‚",
    "task_description_quote": "We propose a dataset ROBUSTAPI for evaluating the reliability and robustness of code generated by LLMs.",
    "dimension": "ä»£ç ç”Ÿæˆçš„å¯é æ€§ä¸é²æ£’æ€§ï¼Œç‰¹åˆ«æ˜¯APIè¯¯ç”¨æ£€æµ‹ã€‚",
    "dimension_quote": "The main purpose of this benchmark is not to evaluate the functional correctness of the generated code, but instead, we focus on reliability and robustness.",
    "evaluation_method": "ä½¿ç”¨æŠ½è±¡è¯­æ³•æ ‘ï¼ˆASTï¼‰åˆ†æç”Ÿæˆçš„ä»£ç ï¼Œå¹¶ä¸é¢„æœŸçš„APIä½¿ç”¨æ¨¡å¼ï¼ˆç»“æ„åŒ–è°ƒç”¨åºåˆ—ï¼‰è¿›è¡Œæ¯”è¾ƒï¼Œæ£€æµ‹è¿è§„è¡Œä¸ºã€‚",
    "evaluation_method_quote": "We also provide an evaluator that analyzes the generated code snippets using the abstract syntax tree (AST) and compares them with the expected API usage patterns.",
    "context_dependency": "å•ä»£ç ç‰‡æ®µï¼ˆåŸºäºStack Overflowé—®ç­”ï¼‰ï¼Œæ¶‰åŠç‰¹å®šAPIçš„ä½¿ç”¨ã€‚",
    "context_dependency_quote": "We collect 1208 real questions from Stack Overflow which involves 18 representative Java APIs.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œæ¶µç›–å­—ç¬¦ä¸²å¤„ç†ã€æ•°æ®ç»“æ„ã€ç§»åŠ¨å¼€å‘ã€åŠ å¯†ã€I/Oå’Œæ•°æ®åº“æ“ä½œç­‰å¤šä¸ªé¢†åŸŸã€‚",
    "problem_domain_quote": "These 18 APIs cover 6 domains including string processing, data structure, mobile development, crypto, I/O and database operation.",
    "problem_difficulty": "å®é™…è½¯ä»¶å¼€å‘ä¸­å¸¸è§çš„ã€å¼€å‘è€…å®¹æ˜“çŠ¯é”™çš„APIä½¿ç”¨é—®é¢˜ã€‚",
    "problem_difficulty_quote": "In this way, we guarantee that the questions in ROBUSTAPI are answerable and non-trivial so we can use them to effectively evaluate the LLMsâ€™ ability in answering coding questions that humans are prone to make mistakes.",
    "language": "Java",
    "language_quote": "Thus we collect representative questions about Java from Stack Overflow.",
    "data_size": "åŒ…å«1208ä¸ªé—®é¢˜ï¼Œæ¶‰åŠ18ä¸ªä»£è¡¨æ€§çš„Java APIã€‚",
    "data_size_quote": "We collect 1208 real questions from Stack Overflow which involves 18 representative Java APIs.",
    "source_type": "ä»Stack Overflowçˆ¬å–çš„çœŸå®ç¼–ç¨‹é—®é¢˜ï¼Œå¹¶åŸºäºExampleCheckæ•°æ®é›†ç­›é€‰ã€‚",
    "source_type_quote": "We build ROBUSTAPI based on the dataset from ExampleCheck (Zhang et al. 2018) as our starting point. ... Then we crawl questions relevant to these APIs from Stack Overflow.",
    "last_updated": "2024-01-27 (æ ¹æ®arXivç‰ˆæœ¬v5æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2308.10335v5 [cs.CL] 27 Jan 2024",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆç”±æœ¬æ–‡ä½œè€…æ„å»ºå¹¶å‘å¸ƒï¼‰",
    "build_type_quote": "We propose a dataset ROBUSTAPI... We open-source our dataset and evaluator on GitHub.",
    "contamination_status": "åŸºäºStack OverflowçœŸå®é—®é¢˜æ„å»ºï¼Œå­˜åœ¨è¢«LLMè®­ç»ƒæ•°æ®åŒ…å«çš„é£é™©ï¼Œä½†æ–‡ä¸­æœªæ˜ç¡®è®¨è®ºæ±¡æŸ“çŠ¶æ€ã€‚",
    "contamination_status_quote": NaN,
    "dataset_license": "æ–‡ä¸­æœªæåŠå…·ä½“è®¸å¯è¯ã€‚",
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆæ ¹æ®è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°ç”Ÿæˆä½¿ç”¨ç‰¹å®šAPIçš„ä»£ç ç‰‡æ®µï¼‰",
    "task_granularity_quote": "The prompt simulates a user asking coding questions without providing any additional hints from the API documentation which is a typical scenario when novice developers seek help from large language models.",
    "evaluation_metrics": "APIè¯¯ç”¨æ£€æµ‹ç‡ï¼ˆé€šè¿‡ASTåˆ†æåˆ¤æ–­ç”Ÿæˆçš„ä»£ç æ˜¯å¦è¿åé¢„æœŸçš„APIä½¿ç”¨æ¨¡å¼ï¼‰",
    "evaluation_metrics_quote": "Any violations of such structured call sequences would be considered as API misuse from the perspective of software engineering.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆStack Overflowé—®é¢˜çš„æ ‡é¢˜å’Œæè¿°ï¼‰",
    "input_modality_quote": "question field contains the title and description of the Stack Overflow questions.",
    "output_modality": "ä»£ç ï¼ˆJavaä»£ç ç‰‡æ®µï¼‰",
    "output_modality_quote": "We design templates to trigger large language models to generate the code snippet and the corresponding explanation.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "The prompt simulates a user asking coding questions... instruct LLMs to generate answers (code) to the questions.",
    "execution_environment": "æ–‡ä¸­æœªæ˜ç¡®æè¿°æ‰§è¡Œç¯å¢ƒï¼Œè¯„ä¼°åŸºäºé™æ€åˆ†æï¼ˆASTï¼‰ï¼Œè€ŒéåŠ¨æ€æ‰§è¡Œã€‚",
    "execution_environment_quote": "We introduce the static analysis method in ROBUSTAPI for detecting the API usage violations which leverages the abstract syntax tree...",
    "unique_features": "ä¸“æ³¨äºè¯„ä¼°ä»£ç ç”Ÿæˆåœ¨çœŸå®è½¯ä»¶å¼€å‘åœºæ™¯ä¸‹çš„å¯é æ€§å’Œé²æ£’æ€§ï¼Œè€Œéä¼ ç»Ÿçš„åŠŸèƒ½æ­£ç¡®æ€§ï¼›é’ˆå¯¹APIè¯¯ç”¨è¿™ä¸€å…·ä½“é£é™©ï¼›åŒ…å«ä¸€ä¸ªåŸºäºASTçš„é™æ€åˆ†æè¯„ä¼°å™¨ã€‚",
    "unique_features_quote": "Unlike the online programming forums, the generated code snippets are not reviewed by the community peers and thus suffer from API misuse... The main purpose of this benchmark is not to evaluate the functional correctness of the generated code, but instead, we focus on reliability and robustness.",
    "data_size_quantity": 1208,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": 2024,
    "last_updated_month": 1,
    "last_updated_day": 27,
    "language_normalized": "['Java']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„å¯é æ€§ä¸é²æ£’æ€§', 'APIè¯¯ç”¨æ£€æµ‹']",
    "evaluation_method_normalized": "['APIè¯¯ç”¨æ£€æµ‹ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'å­—ç¬¦ä¸²å¤„ç†', 'æ•°æ®ç»“æ„', 'ç§»åŠ¨å¼€å‘', 'åŠ å¯†', 'I/O', 'æ•°æ®åº“æ“ä½œ']",
    "source_type_normalized": "['Stack Overflow', 'ExampleCheckæ•°æ®é›†']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "é«˜æ±¡æŸ“é£é™©"
  },
  {
    "source_paper": "2308.12950_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We perform exhaustive evaluations of our models on major code generation benchmarks: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and APPS (Hendrycks et al., 2021), as well as a multilingual version of HumanEval (MultiPL-E, Cassano et al., 2023)...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": NaN,
    "task_description_quote": NaN,
    "dimension": NaN,
    "dimension_quote": NaN,
    "evaluation_method": NaN,
    "evaluation_method_quote": NaN,
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": NaN,
    "task_granularity_quote": NaN,
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": NaN,
    "input_modality_quote": NaN,
    "output_modality": NaN,
    "output_modality_quote": NaN,
    "task_io_type": NaN,
    "task_io_type_quote": NaN,
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": NaN,
    "unique_features_quote": NaN,
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "[]",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "æœªçŸ¥",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2308.16458_output/content.md",
    "benchmark_name": "BioCoder",
    "benchmark_name_quote": "We present BioCoder, a benchmark developed to evaluate LLMs in generating bioinformatics-specific code.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce BioCoder (see Figure 1), a benchmark for code generation incorporating 2,269 bioinformatics-specific coding problems.",
    "dataset_url": "https://github.com/gersteinlab/biocoder and https://biocoder-benchmark.github.io/",
    "dataset_url_quote": "All datasets, benchmark, Docker images, and scripts required for testing are available at: https://github.com/gersteinlab/biocoder and https://biocoder-benchmark.github.io/.",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç”Ÿæˆç”Ÿç‰©ä¿¡æ¯å­¦ç‰¹å®šä»£ç æ–¹é¢çš„èƒ½åŠ›ã€‚",
    "task_description_quote": "BioCoder benchmark mainly targets bioinformatics data analysis, which tasks such as managing various biological data formats, understanding processing workflows, and utilizing APIs of various packages.",
    "dimension": "ä»£ç ç”ŸæˆåŠŸèƒ½æ­£ç¡®æ€§ã€å¤„ç†å¤æ‚ä¸Šä¸‹æ–‡ï¼ˆè·¨æ–‡ä»¶ä¾èµ–ã€ç±»å£°æ˜ã€å…¨å±€å˜é‡ï¼‰çš„èƒ½åŠ›ã€é¢†åŸŸç‰¹å®šçŸ¥è¯†ï¼ˆç”Ÿç‰©ä¿¡æ¯å­¦ï¼‰",
    "dimension_quote": "BioCoder spans much of the field, covering cross-file dependencies, class declarations, and global variables. ... They contain domain-specific knowledge of bioinformatics, beyond just general coding capability.",
    "evaluation_method": "æ¨¡ç³Šæµ‹è¯•æ¡†æ¶ï¼Œé€šè¿‡æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹æ¥éªŒè¯ç”Ÿæˆä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ï¼Œå¹¶è®¡ç®—Pass@Kç‡ã€‚",
    "evaluation_method_quote": "BioCoder incorporates a fuzz-testing framework for evaluation. ... Our benchmark results, derived from 1,000 iterations, indicate the Pass@K rate.",
    "context_dependency": "è·¨æ–‡ä»¶ä¾èµ–ã€åŒ…å«ç±»å£°æ˜å’Œå…¨å±€å˜é‡çš„å®Œæ•´é¡¹ç›®ä¸Šä¸‹æ–‡",
    "context_dependency_quote": "BioCoder spans much of the field, covering cross-file dependencies, class declarations, and global variables. ... we included all potentially required class declarations in the input.",
    "problem_domain": "ç”Ÿç‰©ä¿¡æ¯å­¦ï¼ŒåŒ…æ‹¬ç”Ÿç‰©æ•°æ®åˆ†æã€é—ä¼ æµ‹åºã€DNA/RNAåˆ†æã€ç”Ÿç‰©ä¿¡æ¯å­¦è½¯ä»¶å¼€å‘",
    "problem_domain_quote": "Here, we target bioinformatics due to the amount of domain knowledge, algorithms, and data operations this discipline requires. ... This project specializes in generating Python functions that address key bioinformatics topics such as genetic sequencing and DNA/RNA analysis.",
    "problem_difficulty": "å…·æœ‰æŒ‘æˆ˜æ€§ã€å®ç”¨çš„ç”Ÿç‰©ä¿¡æ¯å­¦åœºæ™¯ï¼ŒåŒ…å«æ—¥å¸¸æ•°æ®åˆ†æä»»åŠ¡å’Œéƒ¨åˆ†è½¯ä»¶å¼€å‘ä»»åŠ¡",
    "problem_difficulty_quote": "BIOCODER is a code generation benchmark designed for challenging, practical bioinformatics scenarios, ... This domain encapsulates the majority of daily tasks encountered by bioinformaticians in data analysis.",
    "language": "Python å’Œ Java",
    "language_quote": "It incorporates 1,026 Python functions and 1,243 Java methods extracted from GitHub, ... However, for the scope of this study, we focus on Python and Java, with the intention to expand to other languages in the future.",
    "data_size": "æ€»å…±2,269ä¸ªç”Ÿç‰©ä¿¡æ¯å­¦ç‰¹å®šçš„ç¼–ç é—®é¢˜ï¼ˆå…¬å¼€é›†460ä¸ªï¼Œéšè—é›†2,269ä¸ªï¼Œç›¸ä¼¼é›†460ä¸ªï¼‰ï¼ŒåŒ…å«1,026ä¸ªPythonå‡½æ•°å’Œ1,243ä¸ªJavaæ–¹æ³•ï¼Œä»¥åŠæ¥è‡ªRosalindé¡¹ç›®çš„253ä¸ªPythonç¤ºä¾‹ã€‚",
    "data_size_quote": "a benchmark for code generation incorporating 2,269 bioinformatics-specific coding problems. ... It incorporates 1,026 Python functions and 1,243 Java methods extracted from GitHub, along with 253 examples from the Rosalind Project, all pertaining to bioinformatics.",
    "source_type": "ä»1,720ä¸ªç»åŒè¡Œè¯„å®¡ç”Ÿç‰©ä¿¡æ¯å­¦æ–‡ç« å¼•ç”¨çš„GitHubä»“åº“ä¸­æå–ï¼Œä»¥åŠæ¥è‡ªRosalindé¡¹ç›®çš„ç¤ºä¾‹ã€‚",
    "source_type_quote": "curated from 1,720 bioinformatics repositories referenced in peer-reviewed bioinformatics articles. ... We included an additional 253 questions from the Rosalind project.",
    "last_updated": "2024å¹´5æœˆ20æ—¥ï¼ˆæ ¹æ®arXivç‰ˆæœ¬v5ï¼‰",
    "last_updated_quote": "arXiv:2308.16458v5 [cs.LG] 20 May 2024",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œé€šè¿‡è‡ªåŠ¨è¿‡æ»¤ã€GPTè¾…åŠ©è¿‡æ»¤å’Œäººå·¥æ£€æŸ¥ç›¸ç»“åˆçš„æ–¹å¼æ„å»ºã€‚",
    "build_type_quote": "We ensure that each function in our dataset requires a certain level of domain expertise in bioinformatics through a combination of automatic filtering, GPT-assisted filtering, and manual inspection.",
    "contamination_status": "ç»è¿‡ä¸¥æ ¼è¿‡æ»¤ã€æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†ï¼Œä»¥é˜²æ­¢æ¨¡å‹è®°å¿†ã€‚",
    "contamination_status_quote": "It has undergone rigorous filtering, extensive data cleaning, and preprocessing to prevent models from memorizing.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆç”Ÿæˆå®Œæ•´çš„å‡½æ•°æˆ–æ–¹æ³•ä½“ï¼‰",
    "task_granularity_quote": "a benchmark for code generation ... generating bioinformatics-specific code.",
    "evaluation_metrics": "Pass@Kç‡",
    "evaluation_metrics_quote": "Our benchmark results, derived from 1,000 iterations, indicate the Pass@K rate.",
    "input_modality": "è‡ªç„¶è¯­è¨€æè¿°ã€ä»£ç è§„èŒƒã€æ³¨é‡Šä»¥åŠå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆåŒ…æ‹¬ä¾èµ–é¡¹ã€ç±»å£°æ˜ã€å…¨å±€å˜é‡ï¼‰",
    "input_modality_quote": "We processed the data, rephrasing more detailed text descriptions, as well as associated comments and specifications, ... we included all potentially required class declarations in the input.",
    "output_modality": "ä»£ç ï¼ˆPythonæˆ–Javaï¼‰",
    "output_modality_quote": "generating bioinformatics-specific code.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "generating bioinformatics-specific code.",
    "execution_environment": "Dockerç¯å¢ƒï¼ŒåŒ…å«ä¸°å¯Œçš„æ‰€éœ€ä¾èµ–é¡¹ï¼Œç”¨äºåœ¨ç°å®é¡¹ç›®åœºæ™¯ä¸­è¿›è¡Œæµ‹è¯•ã€‚",
    "execution_environment_quote": "Testing incorporates a Docker environment, an abundance of required dependencies, ... This robust setup not only facilitates testing in realistic project scenarios",
    "unique_features": "ä¸“æ³¨äºç”Ÿç‰©ä¿¡æ¯å­¦é¢†åŸŸï¼›åŒ…å«è·¨æ–‡ä»¶ä¾èµ–å’Œå®Œæ•´é¡¹ç›®ä¸Šä¸‹æ–‡ï¼›è§„æ¨¡è¿œè¶…åŒç±»é¢†åŸŸç‰¹å®šåŸºå‡†ï¼ˆå¦‚CoderEvalï¼‰ï¼›æä¾›å¯æ‰©å±•çš„è§£æå·¥å…·å’Œæ¨¡ç³Šæµ‹è¯•å·¥å…·ï¼›é€šè¿‡ä¸»é¢˜å»ºæ¨¡éªŒè¯äº†ä»£ç è¦†ç›–èŒƒå›´å…·æœ‰ä»£è¡¨æ€§ã€‚",
    "unique_features_quote": "Here, we target bioinformatics ... BioCoder spans much of the field, covering cross-file dependencies, class declarations, and global variables. ... our dataset surpasses the scale of CoderEval ... Using topic modeling, we show that the overall coverage of the included code is representative of the full spectrum of bioinformatics calculations. ... We provide an extendable parsing tool ... We provide a fuzz testing tool capable of scaling to handle substantial datasets.",
    "data_size_quantity": 2269,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": 2024,
    "last_updated_month": 5,
    "last_updated_day": 20,
    "language_normalized": "['Python', 'Java']",
    "dimension_normalized": "['ä»£ç ç”ŸæˆåŠŸèƒ½æ­£ç¡®æ€§', 'å¤„ç†å¤æ‚ä¸Šä¸‹æ–‡ï¼ˆè·¨æ–‡ä»¶ä¾èµ–ã€ç±»å£°æ˜ã€å…¨å±€å˜é‡ï¼‰çš„èƒ½åŠ›', 'é¢†åŸŸç‰¹å®šçŸ¥è¯†ï¼ˆç”Ÿç‰©ä¿¡æ¯å­¦ï¼‰']",
    "evaluation_method_normalized": "['Pass@Kç‡']",
    "problem_domain_normalized": "['ç”Ÿç‰©ä¿¡æ¯å­¦', 'ç”Ÿç‰©æ•°æ®åˆ†æ', 'é—ä¼ æµ‹åº', 'DNA/RNAåˆ†æ', 'ç”Ÿç‰©ä¿¡æ¯å­¦è½¯ä»¶å¼€å‘']",
    "source_type_normalized": "['ä»1,720ä¸ªç»åŒè¡Œè¯„å®¡ç”Ÿç‰©ä¿¡æ¯å­¦æ–‡ç« å¼•ç”¨çš„GitHubä»“åº“ä¸­æå–', 'æ¥è‡ªRosalindé¡¹ç›®çš„ç¤ºä¾‹']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2310.06770_output/content.md",
    "benchmark_name": "SWE-bench",
    "benchmark_name_quote": "We introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories.",
    "dataset_url": "swebench.com",
    "dataset_url_quote": "Data, code, and leaderboard at swebench.com",
    "task_description": "è¯„ä¼°è¯­è¨€æ¨¡å‹åœ¨çœŸå®è½¯ä»¶å·¥ç¨‹åœºæ™¯ä¸‹çš„èƒ½åŠ›ã€‚ç»™å®šä¸€ä¸ªä»£ç åº“å’Œä¸€ä¸ªéœ€è¦è§£å†³çš„GitHubé—®é¢˜æè¿°ï¼Œæ¨¡å‹çš„ä»»åŠ¡æ˜¯ç¼–è¾‘ä»£ç åº“ä»¥è§£å†³è¯¥é—®é¢˜ã€‚",
    "task_description_quote": "Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue.",
    "dimension": "çœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹é—®é¢˜è§£å†³èƒ½åŠ›ï¼ŒåŒ…æ‹¬è·¨æ–‡ä»¶ã€è·¨å‡½æ•°ã€è·¨ç±»çš„å¤æ‚ä»£ç ç¼–è¾‘ä¸åè°ƒèƒ½åŠ›ï¼Œä»¥åŠé•¿ä¸Šä¸‹æ–‡å¤„ç†å’Œå¤æ‚æ¨ç†èƒ½åŠ›ã€‚",
    "dimension_quote": "Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks.",
    "evaluation_method": "æ‰§è¡Œå•å…ƒæµ‹è¯•å’Œç³»ç»Ÿæµ‹è¯•ã€‚å°†ç”Ÿæˆçš„è¡¥ä¸åº”ç”¨åˆ°ä»£ç åº“åï¼Œè¿è¡Œä¸ä»»åŠ¡å®ä¾‹ç›¸å…³çš„æµ‹è¯•ã€‚å¦‚æœè¡¥ä¸åº”ç”¨æˆåŠŸä¸”æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œåˆ™è®¤ä¸ºè§£å†³æ–¹æ¡ˆæˆåŠŸè§£å†³äº†é—®é¢˜ã€‚ä¸»è¦æŒ‡æ ‡æ˜¯è§£å†³é—®é¢˜çš„ä»»åŠ¡å®ä¾‹ç™¾åˆ†æ¯”ã€‚",
    "evaluation_method_quote": "To evaluate a proposed solution, we apply the generated patch, using unix's patch program, to the codebase and then execute the unit and system tests associated with the task instance. If the patch applies successfully and all of these tests pass we consider the proposed solution to have successfully resolved the issue. The metric for our benchmark is the percentage of task instances that are resolved.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®ã€‚ä»»åŠ¡æ¶‰åŠç†è§£å¤§å‹ã€å¤æ‚çš„ä»£ç åº“ï¼Œå¹¶è·¨å¤šä¸ªå‡½æ•°ã€ç±»ç”šè‡³æ–‡ä»¶è¿›è¡Œåè°ƒæ›´æ”¹ã€‚",
    "context_dependency_quote": "Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously... SWE-bench's reference solutions average editing 1.7 files, 3.0 functions, and 32.8 lines (added or removed).",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ï¼Œå…·ä½“ä¸ºä¿®å¤Bugæˆ–å®ç°æ–°åŠŸèƒ½ã€‚",
    "problem_domain_quote": "SWE-bench is a benchmark featuring GitHub issues from popular repositories that report bugs or request new features...",
    "problem_difficulty": "å·¥ç¨‹çº§ã€‚åŸºäºçœŸå®GitHubé—®é¢˜ï¼Œéœ€è¦å¤„ç†å¤§å‹ä»£ç åº“å’Œå¤æ‚çš„è·¨ä¸Šä¸‹æ–‡ç¼–è¾‘ï¼Œå¯¹ç°æœ‰æœ€å…ˆè¿›æ¨¡å‹æå…·æŒ‘æˆ˜æ€§ã€‚",
    "problem_difficulty_quote": "Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere 1.96% of the issues.",
    "language": "Pythonã€‚æ•°æ®é›†æºè‡ª12ä¸ªæµè¡Œçš„å¼€æºPythonä»“åº“ã€‚",
    "language_quote": "SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories.",
    "data_size": "åŒ…å«2,294ä¸ªä»»åŠ¡å®ä¾‹ã€‚",
    "data_size_quote": "Through these stages of filtering, the original 90,000 PRs are filtered down to the 2,294 task instances which comprise SWE-bench.",
    "source_type": "ä»12ä¸ªæµè¡Œçš„å¼€æºGitHub Pythonä»“åº“ä¸­æŠ“å–çš„çœŸå®GitHubé—®é¢˜å’Œå·²åˆå¹¶çš„æ‹‰å–è¯·æ±‚ã€‚",
    "source_type_quote": "SWE-bench sources task instances from real-world Python repositories by connecting GitHub issues to merged pull request solutions that resolve related tests.",
    "last_updated": "2024 (è®ºæ–‡ç‰ˆæœ¬ä¸º2024å¹´11æœˆ11æ—¥)",
    "last_updated_quote": "arXiv:2310.06770v3  [cs.CL]  11 Nov 2024",
    "build_type": "å®˜æ–¹è‡ªå»ºã€‚ä½œè€…é€šè¿‡ä¸€ä¸ªä¸‰é˜¶æ®µçš„ç­›é€‰æµç¨‹ä»GitHubæ•°æ®ä¸­æ„å»ºã€‚",
    "build_type_quote": "To find high-quality task instances at scale, we use a 3-stage pipeline as follows.",
    "contamination_status": "æŠ—æ±¡æŸ“è®¾è®¡ã€‚åŸºå‡†å¯ä»¥æŒç»­æ›´æ–°ï¼ŒåŒ…å«æ¨¡å‹è®­ç»ƒæ—¥æœŸä¹‹ååˆ›å»ºçš„é—®é¢˜ï¼Œç¡®ä¿è§£å†³æ–¹æ¡ˆæœªåŒ…å«åœ¨è®­ç»ƒè¯­æ–™ä¸­ã€‚",
    "contamination_status_quote": "Therefore, we can extend SWE-bench with a continual supply of new task instances and evaluate LMs on issues created after their training date, which ensures that the solution was not included in their training corpus.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç¼–è¾‘/ä¿®å¤ã€‚ç”Ÿæˆåº”ç”¨äºç°æœ‰ä»£ç åº“çš„è¡¥ä¸ä»¥è§£å†³é—®é¢˜ã€‚",
    "task_granularity_quote": "Each task requires generating a patch describing changes to apply to the existing codebase.",
    "evaluation_metrics": "é—®é¢˜è§£å†³ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰ã€‚",
    "evaluation_metrics_quote": "The metric for our benchmark is the percentage of task instances that are resolved.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆé—®é¢˜æè¿°ï¼‰ä¸ä»£ç ï¼ˆå®Œæ•´ä»£ç åº“ï¼‰ã€‚",
    "input_modality_quote": "A model is given an issue text description and a complete codebase.",
    "output_modality": "ä»£ç ï¼ˆè¡¥ä¸æ–‡ä»¶ï¼‰ã€‚",
    "output_modality_quote": "The model is then tasked to make an edit to the codebase to resolve the issue. In practice, we represent edits as patch files...",
    "task_io_type": "æ–‡æœ¬ä¸ä»£ç åˆ°ä»£ç ã€‚",
    "task_io_type_quote": "Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue.",
    "execution_environment": "ä»“åº“çš„æµ‹è¯•æ¡†æ¶ã€‚éœ€è¦å®‰è£…ä¾èµ–å¹¶è¿è¡Œå•å…ƒæµ‹è¯•å’Œç³»ç»Ÿæµ‹è¯•ã€‚",
    "execution_environment_quote": "The revised codebase is then evaluated using the repositoryâ€™s testing framework.",
    "unique_features": "1. çœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼šæºè‡ªçœŸå®GitHubé—®é¢˜å’Œè§£å†³æ–¹æ¡ˆã€‚2. å¯æŒç»­æ›´æ–°ï¼šå¯è½»æ¾æ‰©å±•åˆ°ä»»ä½•GitHub Pythonä»“åº“ï¼Œæœ€å°åŒ–äººå·¥å¹²é¢„ã€‚3. å¤šæ ·åŒ–çš„é•¿è¾“å…¥ï¼šé—®é¢˜æè¿°è¯¦ç»†ï¼Œä»£ç åº“åŒ…å«æ•°åƒä¸ªæ–‡ä»¶ã€‚4. é²æ£’çš„è¯„ä¼°ï¼šæ¯ä¸ªä»»åŠ¡å®ä¾‹è‡³å°‘æœ‰ä¸€ä¸ªâ€œå¤±è´¥è½¬é€šè¿‡â€çš„æµ‹è¯•ã€‚5. è·¨ä¸Šä¸‹æ–‡ä»£ç ç¼–è¾‘ï¼šä¸é™åˆ¶ç¼–è¾‘èŒƒå›´ï¼Œéœ€è¦åœ¨å¤§ä»£ç åº“çš„å¤šä¸ªä½ç½®ç”Ÿæˆä¿®è®¢ã€‚6. è§£å†³æ–¹æ¡ˆçš„å¹¿æ³›ç©ºé—´ï¼šå¯ä½œä¸ºæ¯”è¾ƒæ£€ç´¢ã€é•¿ä¸Šä¸‹æ–‡æ¨¡å‹å’Œå†³ç­–æ™ºèƒ½ä½“çš„å…¬å¹³å¹³å°ã€‚",
    "unique_features_quote": "SWE-bench offers several advantages over existing LM programming benchmarks. These include, a realistic setting that utilizes user-submitted issues and solutions, diverse inputs featuring unique code problems from 12 repositories, a robust framework for execution-based evaluation, and the ability to continuously update the benchmark with new instances, requiring minimal human intervention.",
    "data_size_quantity": 2294,
    "data_size_unit": "ä¸ªä»»åŠ¡å®ä¾‹",
    "last_updated_year": 2024,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['çœŸå®ä¸–ç•Œè½¯ä»¶å·¥ç¨‹é—®é¢˜è§£å†³èƒ½åŠ›', 'è·¨æ–‡ä»¶', 'è·¨å‡½æ•°', 'è·¨ç±»çš„å¤æ‚ä»£ç ç¼–è¾‘ä¸åè°ƒèƒ½åŠ›', 'é•¿ä¸Šä¸‹æ–‡å¤„ç†', 'å¤æ‚æ¨ç†èƒ½åŠ›']",
    "evaluation_method_normalized": "['é—®é¢˜è§£å†³ç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'ä¿®å¤Bug', 'å®ç°æ–°åŠŸèƒ½']",
    "source_type_normalized": "['ä»12ä¸ªæµè¡Œçš„å¼€æºGitHub Pythonä»“åº“ä¸­æŠ“å–çš„çœŸå®GitHubé—®é¢˜å’Œå·²åˆå¹¶çš„æ‹‰å–è¯·æ±‚']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ä¿®å¤",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2312.02120_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "Notably, MagicoderS-CL-7B based on CODELLAMA even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1).",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate Magicoder and MagicoderS on a wide range of coding tasks, including HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for Python text-to-code generation...",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "Pythonæ–‡æœ¬åˆ°ä»£ç ç”Ÿæˆï¼Œè¡¡é‡ä»è‡ªç„¶è¯­è¨€æè¿°ï¼ˆdocstringsï¼‰åˆæˆç¨‹åºçš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚",
    "task_description_quote": "...HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for Python text-to-code generation...",
    "dimension": "ä»£ç ç”ŸæˆåŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "...indicating that MagicoderS-CL can generate more robust code.",
    "evaluation_method": "pass@1",
    "evaluation_method_quote": "...surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1).",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Python",
    "language_quote": "...HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for Python text-to-code generation...",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": "æ–‡ä¸­æåŠäº†å»æ±¡æŸ“æªæ–½ï¼Œè¡¨æ˜å­˜åœ¨æ±¡æŸ“é£é™©å¹¶è¿›è¡Œäº†å¤„ç†",
    "contamination_status_quote": "Finally, we apply the same logic as StarCoder Li et al. (2023) to decontaminate our training data by removing coding problems that contain docstrings or solutions from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021)...",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "...for Python text-to-code generation...",
    "evaluation_metrics": "pass@1",
    "evaluation_metrics_quote": "...surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1).",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "...for Python text-to-code generation...",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "...for Python text-to-code generation...",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "...for Python text-to-code generation...",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": NaN,
    "unique_features_quote": NaN,
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”ŸæˆåŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@1']",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "é«˜æ±¡æŸ“é£é™©"
  },
  {
    "source_paper": "2402.16906_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "Experiments demonstrate that LDB consistently enhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and TransCoder benchmarks",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate LDB on three code generation benchmarks: HumanEval (Chen et al., 2021), TransCoder (Roziere et al., 2020), and MBPP (Austin et al., 2021).",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ–‡æœ¬åˆ°ä»£ç ç”Ÿæˆï¼Œä»»åŠ¡æè¿°æ˜¯ä¸€æ®µç®€è¦çš„è‡ªç„¶è¯­è¨€æ®µè½ï¼Œæ¦‚è¿°äº†è¦ç”Ÿæˆçš„ç¨‹åºçš„é¢„æœŸåŠŸèƒ½ã€‚",
    "task_description_quote": "HumanEval and MBPP are for text-to-code generation, where the task description is a brief passage outlines the intended functionality of the program to be generated.",
    "dimension": "ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "Experiments demonstrate that LDB consistently enhances the baseline performance... in code debugging for various LLM selections.",
    "evaluation_method": "ä½¿ç”¨éšè—æµ‹è¯•ç”¨ä¾‹è®¡ç®— Pass@1 å‡†ç¡®ç‡",
    "evaluation_method_quote": "We compute Pass@1 accuracy with hidden test cases for assesment.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": "åŸºç¡€ç¼–ç¨‹é—®é¢˜",
    "problem_difficulty_quote": "Despite these advanced approaches, they still fall short on basic programming questions from the HumanEval and MBPP datasets.",
    "language": "Python",
    "language_quote": "æ–‡ä¸­ä»…æåŠå®éªŒè®¾ç½®ï¼ˆè¯„æµ‹æ–‡æœ¬åˆ°ä»£ç ç”Ÿæˆï¼‰ï¼Œæœªæè¿°å®Œæ•´æ•°æ®é›†çš„è¯­è¨€æ„æˆã€‚",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2021",
    "last_updated_quote": "HumanEval (Chen et al., 2021)",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "HumanEval and MBPP are for text-to-code generation",
    "evaluation_metrics": "Pass@1",
    "evaluation_metrics_quote": "We compute Pass@1 accuracy with hidden test cases for assesment.",
    "input_modality": "è‡ªç„¶è¯­è¨€",
    "input_modality_quote": "the task description is a brief passage outlines the intended functionality of the program to be generated.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "text-to-code generation",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "text-to-code generation",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": NaN,
    "unique_features_quote": NaN,
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2021,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['Pass@1']",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "å…¥é—¨çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2403.07974_output/content.md",
    "benchmark_name": "LiveCodeBench",
    "benchmark_name_quote": "In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "In this work, we propose LiveCodeBench",
    "dataset_url": "https://livecodebench.github.io/",
    "dataset_url_quote": "Website: https://livecodebench.github.io/",
    "task_description": "å¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„ä»£ç ç›¸å…³èƒ½åŠ›è¿›è¡Œå…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬ä»£ç ç”Ÿæˆã€è‡ªæˆ‘ä¿®å¤ã€ä»£ç æ‰§è¡Œå’Œæµ‹è¯•è¾“å‡ºé¢„æµ‹ç­‰å¤šä¸ªæ–¹é¢ã€‚",
    "task_description_quote": "Notably, our benchmark also focuses on a broader range of code-related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation.",
    "dimension": "ä»£ç ç›¸å…³èƒ½åŠ›çš„å…¨é¢æ€§è¯„ä¼°ï¼ŒåŒ…æ‹¬ç”Ÿæˆã€ä¿®å¤ã€æ‰§è¡Œå’Œç†è§£ã€‚",
    "dimension_quote": "Holistic Evaluation. Current code evaluations primarily focus on natural language to code generation. However, programming is a multi-faceted task that requires a variety of capabilities beyond those measured by code generation.",
    "evaluation_method": "åŸºäºåŠŸèƒ½æ­£ç¡®æ€§ï¼Œä½¿ç”¨ä¸€ç»„æœªè§è¿‡çš„æµ‹è¯•ç”¨ä¾‹è¿›è¡Œè¯„ä¼°ã€‚å¯¹äºä»£ç ç”Ÿæˆåœºæ™¯ï¼Œä½¿ç”¨Pass@1æŒ‡æ ‡ã€‚",
    "evaluation_method_quote": "The evaluation is performed based on functional correctness, using a set of unseen test cases. We use the Pass@1 metric measured as the fraction of the problems for which the model was able to generate a program passing all tests.",
    "context_dependency": "å•é—®é¢˜è§£å†³ï¼Œä¸Šä¸‹æ–‡ä¸ºè‡ªç„¶è¯­è¨€é—®é¢˜æè¿°å’Œç¤ºä¾‹æµ‹è¯•ã€‚",
    "context_dependency_quote": "The model is given a problem statement, which includes a natural language description and example tests (input-output pairs), and is tasked with generating a correct solution.",
    "problem_domain": "ç«äº‰æ€§ç¼–ç¨‹ï¼ˆç®—æ³•é—®é¢˜ï¼‰ï¼Œæ¥æºäºLeetCodeã€AtCoderå’ŒCodeForcesç­‰ç«èµ›å¹³å°ã€‚",
    "problem_domain_quote": "which collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces.",
    "problem_difficulty": "å¹³è¡¡çš„éš¾åº¦åˆ†å¸ƒï¼ŒåŒ…å«ä»æ˜“åˆ°éš¾çš„é—®é¢˜ï¼Œå¹¶ä½¿ç”¨å¹³å°æä¾›çš„éš¾åº¦è¯„çº§è¿›è¡Œåˆ†ç±»å’Œè¿‡æ»¤ã€‚",
    "problem_difficulty_quote": "Therefore, we use problem difficulty ratings (sourced from the competition websites) for filtering the harder problems and classifying problem difficulties to ensure balanced problem difficulty distribution and allow granular model comparisons.",
    "language": "æ–‡ä¸­æœªæ˜ç¡®æåŠæ•°æ®é›†æ”¯æŒçš„å…·ä½“ç¼–ç¨‹è¯­è¨€ï¼Œä½†æ¥æºå¹³å°ï¼ˆLeetCode, AtCoder, CodeForcesï¼‰é€šå¸¸æ”¯æŒå¤šç§è¯­è¨€ã€‚",
    "language_quote": NaN,
    "data_size": "ç›®å‰åŒ…å«è¶…è¿‡500ä¸ªç¼–ç é—®é¢˜ï¼ˆ511ä¸ªï¼‰ï¼Œæ”¶é›†äº2023å¹´5æœˆè‡³2024å¹´5æœˆæœŸé—´ã€‚",
    "data_size_quote": "Currently, LiveCodeBench hosts over five hundred coding problems that were published between May 2023 and May 2024. / Particularly, we have collected 511 problems from contests across three competition platforms â€“ LeetCode, AtCoder, and CodeForces occurring from May 2023 to the present (May 2024)",
    "source_type": "ä»ä¸‰ä¸ªç«èµ›å¹³å°ï¼ˆLeetCodeã€AtCoderã€CodeForcesï¼‰çš„æ¯å‘¨æ¯”èµ›ä¸­æ”¶é›†çš„æ–°é—®é¢˜ã€‚",
    "source_type_quote": "Particularly, we collect problems from weekly contests on competition platforms and tag them with a release date.",
    "last_updated": "2024å¹´6æœˆ6æ—¥ï¼ˆè®ºæ–‡ç‰ˆæœ¬æ—¥æœŸï¼‰ï¼Œæ•°æ®é›†æŒç»­æ›´æ–°è‡³2024å¹´5æœˆã€‚",
    "last_updated_quote": "arXiv:2403.07974v2  [cs.SE]  6 Jun 2024",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±ç ”ç©¶å›¢é˜Ÿä»å…¬å¼€ç«èµ›å¹³å°æ”¶é›†å’Œæ„å»ºã€‚",
    "build_type_quote": "In this work, we propose LiveCodeBench",
    "contamination_status": "æŠ—æ±¡æŸ“è®¾è®¡ï¼Œé€šè¿‡æŒç»­æ›´æ–°ï¼ˆä½¿ç”¨æ–°å‘å¸ƒçš„é—®é¢˜ï¼‰å’Œæ—¶é—´åˆ†æ®µè¯„ä¼°æ¥é˜²æ­¢æ•°æ®æ±¡æŸ“ã€‚",
    "contamination_status_quote": "Live updates to prevent contamination. / to prevent the risk of problem contamination, we use live updates, that is evaluate models on new problems.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆã€ä»£ç ä¿®å¤ã€ä»£ç æ‰§è¡Œã€æµ‹è¯•è¾“å‡ºé¢„æµ‹ã€‚",
    "task_granularity_quote": "Specifically, we evaluate code LLMs in four scenarios, namely code generation, self-repair, code execution, and test output prediction.",
    "evaluation_metrics": "Pass@1ï¼ˆç”¨äºä»£ç ç”Ÿæˆåœºæ™¯ï¼‰ã€‚",
    "evaluation_metrics_quote": "We use the Pass@1 metric measured as the fraction of the problems for which the model was able to generate a program passing all tests.",
    "input_modality": "è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°ã€ç¤ºä¾‹æµ‹è¯•ã€é”™è¯¯ç¨‹åºï¼ˆç”¨äºä¿®å¤ï¼‰ã€ä»£ç å’Œè¾“å…¥ï¼ˆç”¨äºæ‰§è¡Œï¼‰ã€‚",
    "input_modality_quote": "The model is given a problem statement, which includes a natural language description and example tests (input-output pairs) / The model is given the natural language problem description, the incorrect program, the test case it fails on, and the execution feedback from that failure. / The model is given a program and an input",
    "output_modality": "ä»£ç ï¼ˆç”¨äºç”Ÿæˆå’Œä¿®å¤ï¼‰ã€æ‰§è¡Œç»“æœï¼ˆç”¨äºæ‰§è¡Œï¼‰ã€æµ‹è¯•è¾“å‡ºï¼ˆç”¨äºé¢„æµ‹ï¼‰ã€‚",
    "output_modality_quote": "The output should be a correct repaired program. / the output should be the result. / the output should be the output for the problem.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ï¼ˆç”Ÿæˆï¼‰ã€ä»£ç ä¸åé¦ˆåˆ°ä»£ç ï¼ˆä¿®å¤ï¼‰ã€ä»£ç ä¸è¾“å…¥åˆ°ç»“æœï¼ˆæ‰§è¡Œï¼‰ã€æ–‡æœ¬ä¸è¾“å…¥åˆ°ç»“æœï¼ˆé¢„æµ‹ï¼‰ã€‚",
    "task_io_type_quote": "generating code from natural language / Fix an incorrect program from execution information / â€œExecuteâ€ a program on an input / Solve the natural language task on a specified input",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. æŒç»­æ›´æ–°ä»¥é˜²æ­¢æ•°æ®æ±¡æŸ“ã€‚2. å…¨é¢çš„è¯„ä¼°åœºæ™¯ï¼Œè¶…è¶Šå•ä¸€çš„ä»£ç ç”Ÿæˆã€‚3. æ¥æºäºé«˜è´¨é‡ç«èµ›å¹³å°çš„é—®é¢˜å’Œæµ‹è¯•ã€‚4. å¹³è¡¡çš„é—®é¢˜éš¾åº¦åˆ†å¸ƒã€‚",
    "unique_features_quote": "Live updates to prevent contamination. / Holistic Evaluation. / High-quality problems and tests. / Balanced problem difficulty.",
    "data_size_quantity": 511,
    "data_size_unit": "ä¸ªç¼–ç é—®é¢˜",
    "last_updated_year": 2024,
    "last_updated_month": 6,
    "last_updated_day": 6,
    "language_normalized": "[]",
    "dimension_normalized": "['ä»£ç ç›¸å…³èƒ½åŠ›çš„å…¨é¢æ€§è¯„ä¼°ï¼ŒåŒ…æ‹¬ç”Ÿæˆã€ä¿®å¤ã€æ‰§è¡Œå’Œç†è§£']",
    "evaluation_method_normalized": "['Pass@1ï¼ˆç”¨äºä»£ç ç”Ÿæˆåœºæ™¯ï¼‰']",
    "problem_domain_normalized": "['ç«äº‰æ€§ç¼–ç¨‹ï¼ˆç®—æ³•é—®é¢˜ï¼‰']",
    "source_type_normalized": "['ä»ä¸‰ä¸ªç«èµ›å¹³å°ï¼ˆLeetCodeã€AtCoderã€CodeForcesï¼‰çš„æ¯å‘¨æ¯”èµ›ä¸­æ”¶é›†çš„æ–°é—®é¢˜']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2403.08604_output/content.md",
    "benchmark_name": "DevEval",
    "benchmark_name_quote": "In this case study, we explore the performance of LLMs across the entire software development lifecycle with DevEval",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To address these shortcomings and fill this gap, we present DevEval, a comprehensive case study that mirrors real-world software development.",
    "dataset_url": "https://github.com/open-compass/DevEval",
    "dataset_url_quote": "1Our data and code are available at https://github.com/open-compass/DevEval.",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•´ä¸ªè½¯ä»¶å¼€å‘ç”Ÿå‘½å‘¨æœŸä¸­çš„èƒ½åŠ›ï¼ŒåŒ…æ‹¬ä»è¯¦ç»†çš„äº§å“éœ€æ±‚æ–‡æ¡£ï¼ˆPRDï¼‰å¼€å§‹ï¼Œæ„å»ºä¸€ä¸ªå¤šæ–‡ä»¶çš„ä»£ç åº“ã€‚",
    "task_description_quote": "DevEval generally evaluates models on the task of constructing a multi-file codebase starting from a product requirement document (PRD) of detailed specifications.",
    "dimension": "è½¯ä»¶å¼€å‘ç”Ÿå‘½å‘¨æœŸçš„å…¨é¢èƒ½åŠ›ï¼ŒåŒ…æ‹¬è½¯ä»¶è®¾è®¡ã€ç¯å¢ƒé…ç½®ã€å®ç°ã€éªŒæ”¶æµ‹è¯•å’Œå•å…ƒæµ‹è¯•ã€‚",
    "dimension_quote": "DevEval encompasses stages including software design, environment setup, implementation, acceptance testing, and unit testing.",
    "evaluation_method": "æ ¹æ®ä»»åŠ¡ä¸åŒé‡‡ç”¨å¤šç§æ–¹æ³•ï¼šè½¯ä»¶è®¾è®¡ä»»åŠ¡ä½¿ç”¨LLMä½œä¸ºè¯„åˆ¤è€…è¿›è¡Œä¸»è§‚è¯„ä¼°ï¼›ç¯å¢ƒé…ç½®ä»»åŠ¡è¯„ä¼°ä¾èµ–æ–‡ä»¶æ‰§è¡Œå’Œç¤ºä¾‹ä»£ç è¿è¡Œçš„æˆåŠŸç‡ï¼›å®ç°ä»»åŠ¡ä½¿ç”¨è‡ªåŠ¨åŒ–æµ‹è¯•æ¡†æ¶ï¼ˆå¦‚PyTest, GTest, JUnit, Jestï¼‰æ‰§è¡Œå‚è€ƒæµ‹è¯•å¹¶è®¡ç®—é€šè¿‡ç‡ï¼›æµ‹è¯•ä»»åŠ¡è¯„ä¼°æµ‹è¯•ä»£ç çš„å¯æ‰§è¡Œæ€§ï¼ˆOracle Testï¼‰å’Œä»£ç è¦†ç›–ç‡ã€‚",
    "evaluation_method_quote": "Since the Software Design tasks are open-ended, we employ the LLM-as-a-judge approach... The evaluation is anchored by two principal metrics: general principles and faithfulness. The principal metric for evaluation in this task is the success rate of the executed example code. The evaluation procedure involves executing reference acceptance and unit tests within a predefined reference environment. Then the evaluation metric is determined by the pass rate of these tests. ...models generally fail to generate executable tests, with oracle test scores falling below 40%... the generated testing code demonstrates potential in code coverage, achieving as high as 79.4% when it is executable.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®çº§ï¼Œéœ€è¦ç†è§£äº§å“éœ€æ±‚æ–‡æ¡£ã€UMLå›¾ã€æ¶æ„è®¾è®¡ç­‰å®Œæ•´ä¸Šä¸‹æ–‡ã€‚",
    "context_dependency_quote": "DevEval generally evaluates models on the task of constructing a multi-file codebase starting from a product requirement document (PRD) of detailed specifications.",
    "problem_domain": "æ¶µç›–å¤šä¸ªå·¥ç¨‹å’ŒAIé¢†åŸŸï¼ŒåŒ…æ‹¬æ•°æ®åº“åº”ç”¨ã€WebæœåŠ¡ã€ç®—æ³•å®ç°ã€APIå¼€å‘ã€ç¥ç»ç½‘ç»œã€è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ã€‚",
    "problem_domain_quote": "DevEval covers a range of domains including NLP, computer vision, deep learning, algorithm implementation, API applications, Database applications, web service (both frontend and backend), and general tools and utilities.",
    "problem_difficulty": "å·¥ç¨‹çº§ï¼Œæ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„è½¯ä»¶å¼€å‘æŒ‘æˆ˜ï¼Œç°æœ‰é¡¶çº§æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰å¾—åˆ†å¾ˆä½ã€‚",
    "problem_difficulty_quote": "Empirical studies show that current LLMs, including GPT-4, fail to solve the challenges presented within DevEval. GPT-4-Turbo achieves the highest scores amongst all evaluated models, yet it obtains less than 10% on our repository-level implementation task.",
    "language": "Python, C/C++, Java, JavaScript (Vue.js)",
    "language_quote": "DevEval features four programming languages... we curated a collection of 22 repositories across four programming languages (Python, C/C++, Java, JavaScript)",
    "data_size": "åŒ…å«22ä¸ªä»£ç ä»“åº“ï¼Œè¦†ç›–4ç§ç¼–ç¨‹è¯­è¨€å’Œå¤šä¸ªé¢†åŸŸã€‚å¹³å‡æ¯ä¸ªä»“åº“åŒ…å«çº¦2-7ä¸ªä»£ç æ–‡ä»¶ï¼Œ276-617è¡Œä»£ç ï¼Œä»¥åŠè‹¥å¹²éªŒæ”¶æµ‹è¯•å’Œå•å…ƒæµ‹è¯•ã€‚",
    "data_size_quote": "we curated a collection of 22 repositories across four programming languages... Table 2: DevEval Statistics. Avg. #Code File 2.2-7.0, Avg. #Code Line 276-617, Avg. #Accep. Tests 2-5.4, Avg. #Unit Tests 8.2-12.4",
    "source_type": "ä»å…¬å¼€ä»“åº“ä¸­ç²¾å¿ƒç­–åˆ’æ”¶é›†ï¼Œå¹¶è¡¥å……äº†å®Œæ•´è½¯ä»¶å¼€å‘æ‰€éœ€çš„è®¾è®¡æ–‡æ¡£å’Œæµ‹è¯•ç¨‹åºã€‚",
    "source_type_quote": "One significant challenge in this study lies in the scarcity of publicly available repositories that include the full range of software development artifacts, particularly design documents and comprehensive testing programs. To overcome this, we curated a collection of 22 repositories...",
    "last_updated": "2024",
    "last_updated_quote": "arXiv:2403.08604v3 [cs.CL] 14 Dec 2024",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç”±ç ”ç©¶å›¢é˜Ÿä¸ºè¯„ä¼°ç›®çš„è€Œæ„å»ºã€‚",
    "build_type_quote": "we present DevEval, a comprehensive case study... To overcome this, we curated a collection...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç åº“ç”Ÿæˆï¼Œæ¶‰åŠä»è®¾è®¡åˆ°æµ‹è¯•çš„å®Œæ•´è½¯ä»¶å¼€å‘æµç¨‹ã€‚",
    "task_granularity_quote": "DevEval generally evaluates models on the task of constructing a multi-file codebase starting from a product requirement document (PRD) of detailed specifications.",
    "evaluation_metrics": "è½¯ä»¶è®¾è®¡ï¼šåŸºäºé€šç”¨åŸåˆ™å’Œå¿ å®åº¦çš„LLMä¸»è§‚è¯„åˆ†ï¼›ç¯å¢ƒé…ç½®ï¼šç¤ºä¾‹ä»£ç æ‰§è¡ŒæˆåŠŸç‡ï¼›å®ç°ï¼šå‚è€ƒéªŒæ”¶å’Œå•å…ƒæµ‹è¯•çš„é€šè¿‡ç‡ï¼›æµ‹è¯•ï¼šOracle Teståˆ†æ•°å’Œä»£ç è¦†ç›–ç‡ã€‚",
    "evaluation_metrics_quote": "The evaluation is anchored by two principal metrics: general principles and faithfulness... The principal metric for evaluation in this task is the success rate of the executed example code... the evaluation metric is determined by the pass rate of these tests... oracle test scores... code coverage, achieving as high as 79.4%",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆäº§å“éœ€æ±‚æ–‡æ¡£PRDï¼‰ã€ç»“æ„åŒ–å›¾è¡¨ï¼ˆUMLå›¾ã€æ¶æ„è®¾è®¡ï¼‰ã€ä»£ç ã€‚",
    "input_modality_quote": "models are provided with the PRD, UML diagrams and architecture design... Table 1: Task design in DevEval. Input: PRD, UML Diagrams, Architecture Design, Implementation Code",
    "output_modality": "ä»£ç ï¼ˆä¾èµ–æ–‡ä»¶ã€å®ç°ä»£ç ã€æµ‹è¯•ä»£ç ï¼‰ã€ç»“æ„åŒ–è®¾è®¡æ–‡æ¡£ï¼ˆUMLå›¾ã€æ¶æ„è®¾è®¡ï¼‰ã€‚",
    "output_modality_quote": "Table 1: Task design in DevEval. Output: UML Diagrams, Architecture Design, Dependency Files, Implementation Code, Acceptance Testing Code, Unit Testing Code",
    "task_io_type": "æ–‡æœ¬ä¸å›¾è¡¨åˆ°ä»£ç ã€æ–‡æœ¬ä¸å›¾è¡¨åˆ°è®¾è®¡æ–‡æ¡£ã€‚",
    "task_io_type_quote": "DevEval generally evaluates models on the task of constructing a multi-file codebase starting from a product requirement document (PRD)... The first subtask involves the generation of the UML class diagram...",
    "execution_environment": "ä½¿ç”¨Dockerå®šä¹‰çš„åŸºç¡€ç¯å¢ƒï¼Œé’ˆå¯¹ä¸åŒè¯­è¨€ä½¿ç”¨ç‰¹å®šå·¥å…·ï¼ˆConda, Gradle, NPMï¼‰å’Œæµ‹è¯•æ¡†æ¶ï¼ˆPyTest, GTest, JUnit, Jestï¼‰ã€‚",
    "execution_environment_quote": "The evaluation centers on the execution of dependency files across each programming language within a predetermined base environment delineated in a Docker file... For Python, the Conda environment manager is employed; for Java and JavaScript, Gradle and NPM are utilized respectively... integrates PyTest for Python, GTest for C++, JUnit for Java, and Jest for JavaScript.",
    "unique_features": "é¦–ä¸ªè¯„ä¼°æ¨¡å‹è½¯ä»¶è®¾è®¡å’Œç¯å¢ƒé…ç½®èƒ½åŠ›çš„åŸºå‡†ï¼›éµå¾ªç€‘å¸ƒæ¨¡å‹ï¼Œå°†è½¯ä»¶å¼€å‘åˆ†è§£ä¸ºå¤šä¸ªç›¸äº’å…³è”çš„é˜¶æ®µï¼›æä¾›æ–‡æ¡£çº§åˆ«çš„è¯¦ç»†éœ€æ±‚æè¿°ï¼›é‡‡ç”¨æ¨¡å—åŒ–è¯„ä¼°åè®®ï¼Œæ”¯æŒç«¯åˆ°ç«¯æˆ–åˆ†é˜¶æ®µè¯„ä¼°ã€‚",
    "unique_features_quote": "DevEval is the first to evaluate modelsâ€™ software design and environment setup capabilities. Subscribing to the traditional Waterfall software development model... DevEval breaks down this process into a diverse set of inter-related development stages... In contrast to similar systems... which generate outputs from brief requirement descriptions typically under 100 words, DevEval offers document-level detail to guide the models. ...our design utilizes reference inputs for each task. This strategy enables and concentrates on evaluating the efficacy of models in executing specific tasks.",
    "data_size_quantity": 22,
    "data_size_unit": "ä¸ªä»£ç ä»“åº“",
    "last_updated_year": 2024,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python', 'C/C++', 'Java', 'JavaScript (Vue.js)']",
    "dimension_normalized": "['è½¯ä»¶å¼€å‘ç”Ÿå‘½å‘¨æœŸçš„å…¨é¢èƒ½åŠ›', 'è½¯ä»¶è®¾è®¡', 'ç¯å¢ƒé…ç½®', 'å®ç°', 'éªŒæ”¶æµ‹è¯•', 'å•å…ƒæµ‹è¯•']",
    "evaluation_method_normalized": "['è½¯ä»¶è®¾è®¡ï¼šåŸºäºé€šç”¨åŸåˆ™å’Œå¿ å®åº¦çš„LLMä¸»è§‚è¯„åˆ†', 'ç¯å¢ƒé…ç½®ï¼šç¤ºä¾‹ä»£ç æ‰§è¡ŒæˆåŠŸç‡', 'å®ç°ï¼šå‚è€ƒéªŒæ”¶å’Œå•å…ƒæµ‹è¯•çš„é€šè¿‡ç‡', 'æµ‹è¯•ï¼šOracle Teståˆ†æ•°', 'ä»£ç è¦†ç›–ç‡']",
    "problem_domain_normalized": "['æ¶µç›–å¤šä¸ªå·¥ç¨‹å’ŒAIé¢†åŸŸ', 'æ•°æ®åº“åº”ç”¨', 'WebæœåŠ¡', 'ç®—æ³•å®ç°', 'APIå¼€å‘', 'ç¥ç»ç½‘ç»œ', 'è®¡ç®—æœºè§†è§‰', 'è‡ªç„¶è¯­è¨€å¤„ç†']",
    "source_type_normalized": "['ä»å…¬å¼€ä»“åº“ä¸­ç²¾å¿ƒç­–åˆ’æ”¶é›†', 'è¡¥å……äº†å®Œæ•´è½¯ä»¶å¼€å‘æ‰€éœ€çš„è®¾è®¡æ–‡æ¡£å’Œæµ‹è¯•ç¨‹åº']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "å›¾æ–‡åˆ°ä»£ç ",
    "execution_environment_normalized": "Dockerå®¹å™¨",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2406.06887_output/content.md",
    "benchmark_name": "HumanEval, MBPP, LiveCodeBench, LeetCode",
    "benchmark_name_quote": "PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench)... We evaluate PLUM on a diverse set of state-of-the-art code language models under different set-ups, on commonly used evaluation benchmarks: HumanEval(+) and MBPP(+) [7, 1, 28] as well as more challenging code generation datasets like LiveCodeBench and LeetCode [22, 19].",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench)... We evaluate PLUM on a diverse set of state-of-the-art code language models under different set-ups, on commonly used evaluation benchmarks: HumanEval(+) and MBPP(+) [7, 1, 28] as well as more challenging code generation datasets like LiveCodeBench and LeetCode [22, 19].",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ–‡ä¸­æœªæè¿°è¿™äº›åŸºå‡†çš„åŸå§‹ä»»åŠ¡å®šä¹‰ï¼Œä»…æåŠå®ƒä»¬æ˜¯ç”¨äºè¯„ä¼°ä»£ç ç”Ÿæˆæ¨¡å‹çš„å¸¸ç”¨åŸºå‡†ã€‚",
    "task_description_quote": "PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench)... We evaluate PLUM on a diverse set of state-of-the-art code language models under different set-ups, on commonly used evaluation benchmarks: HumanEval(+) and MBPP(+) [7, 1, 28] as well as more challenging code generation datasets like LiveCodeBench and LeetCode [22, 19].",
    "dimension": "ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "Works like AlphaCode [26] and LeTI [48] have introduced test outcomes as a means to define functional correctness in code generation.",
    "evaluation_method": "ä½¿ç”¨æµ‹è¯•ç”¨ä¾‹æ‰§è¡Œæ¥è¯„ä¼°åŠŸèƒ½æ­£ç¡®æ€§",
    "evaluation_method_quote": "Works like AlphaCode [26] and LeTI [48] have introduced test outcomes as a means to define functional correctness in code generation.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "é€šç”¨ç¼–ç¨‹é—®é¢˜",
    "problem_domain_quote": "These datasets provide a diverse range of programming tasks and instructions.",
    "problem_difficulty": "æ ‡å‡†åŸºå‡†å’Œæ›´å…·æŒ‘æˆ˜æ€§çš„åŸºå‡†",
    "problem_difficulty_quote": "PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench)... as well as more challenging code generation datasets like LiveCodeBench and LeetCode [22, 19].",
    "language": "æ–‡ä¸­ä»…æåŠå®éªŒèšç„¦äºPythonï¼Œæœªæè¿°å®Œæ•´æ•°æ®é›†çš„è¯­è¨€è¦†ç›–èŒƒå›´ã€‚",
    "language_quote": "We focus on Python due to its wide use and the availability of well-established training and evaluation resources.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "Language models pre-trained on code corpora have excelled at code generation [40, 25].",
    "evaluation_metrics": "é€šè¿‡ç‡ (pass rates)",
    "evaluation_metrics_quote": "PLUM increases pass rates by up to 4.8% on average on standard benchmarks and 11.8% on LiveCodeBench, demonstrating its effectiveness and generalizability.",
    "input_modality": "è‡ªç„¶è¯­è¨€æŒ‡ä»¤",
    "input_modality_quote": "PLUM utilizes natural language instructions from well-established datasets such as OSS-Instruct [49], Evol-Instruct-Code [31], and ShareGPT [8].",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "These solutions are evaluated using the generated test cases, with preference labels assigned based on the results: solutions passing the tests are preferred, while failures are dis-preferred.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "PLUM utilizes natural language instructions from well-established datasets... These solutions are evaluated using the generated test cases...",
    "execution_environment": "æ–‡ä¸­æœªæ˜ç¡®æè¿°åŸºå‡†çš„æ‰§è¡Œç¯å¢ƒï¼Œä½†æåŠäº†æ‰§è¡Œæ£€æŸ¥ã€‚",
    "execution_environment_quote": "With static and execution checks,3 we identify and filter out solutions that contain syntactic errors and fail to execute, as our focus is on functional correctness.",
    "unique_features": "æœ¬æ–‡æå‡ºçš„PLUMæ¡†æ¶æœ¬èº«æ˜¯ä¸€ä¸ªä½¿ç”¨æµ‹è¯•ç”¨ä¾‹è¿›è¡Œæ‰§è¡Œå¼•å¯¼ã€åŸºäºç­–ç•¥çš„åå¥½å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºæ”¹è¿›ä»£ç è¯­è¨€æ¨¡å‹ã€‚å®ƒå¹¶éä¸€ä¸ªæ•°æ®é›†ã€‚",
    "unique_features_quote": "we propose PLUM, an on-policy Preference Learning framework Augmented with test cases for code LMs.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['é€šè¿‡ç‡']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹é—®é¢˜']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2406.18294_output/content.md",
    "benchmark_name": "CrossCodeEval",
    "benchmark_name_quote": "To assess the code completion performance of Code LLMs in real development scenarios, we utilized CrossCodeEval (Ding et al., 2023) as the evaluation dataset.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "To assess the code completion performance of Code LLMs in real development scenarios, we utilized CrossCodeEval (Ding et al., 2023) as the evaluation dataset.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°ä»£ç å¤§è¯­è¨€æ¨¡å‹åœ¨çœŸå®å¼€å‘åœºæ™¯ä¸­çš„ä»£ç è¡¥å…¨æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯éœ€è¦åˆ©ç”¨è·¨æ–‡ä»¶ä»£ç ä¿¡æ¯è¿›è¡Œè¡¥å…¨çš„ä»»åŠ¡ã€‚",
    "task_description_quote": "To assess the code completion performance of Code LLMs in real development scenarios... The CrossCodeEval (Ding et al., 2023) benchmark provides test cases that require the use of cross-file code information for completion.",
    "dimension": "ä»“åº“çº§ä»£ç è¡¥å…¨èƒ½åŠ›ï¼Œåˆ©ç”¨è·¨æ–‡ä»¶ä¿¡æ¯çš„èƒ½åŠ›",
    "dimension_quote": "Some benchmarks for repository-level code completion have been proposed to evaluate the performance of code models in real-world completion tasks, such as CrossCodeEval (Ding et al., 2023)...",
    "evaluation_method": "ç²¾ç¡®åŒ¹é…ï¼ˆExact Match, EMï¼‰å’Œç¼–è¾‘ç›¸ä¼¼åº¦ï¼ˆEdit Similarity, ESï¼‰",
    "evaluation_method_quote": "Following the CrossCodeEval evaluation protocol, we evaluated the completion results using two metrics: Exact Match (EM) and Edit Similarity (ES).",
    "context_dependency": "è·¨æ–‡ä»¶ã€ä»“åº“çº§ä¸Šä¸‹æ–‡",
    "context_dependency_quote": "The CrossCodeEval (Ding et al., 2023) benchmark provides test cases that require the use of cross-file code information for completion.",
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": "çœŸå®ä¸–ç•Œå¼€å‘åœºæ™¯",
    "problem_difficulty_quote": "To assess the code completion performance of Code LLMs in real development scenarios...",
    "language": "æ–‡ä¸­ä»…æåŠå®éªŒå­é›†(Python)ï¼Œæœªæè¿°å®Œæ•´æ•°æ®é›†",
    "language_quote": "Without loss of generality, in this study, we have chosen Python language as the primary language for our research.",
    "data_size": "æœ¬æ–‡å®éªŒä½¿ç”¨äº†2,655ä¸ªçœŸå®ä¸–ç•Œè¡¥å…¨æµ‹è¯•ç”¨ä¾‹",
    "data_size_quote": "Ultimately, we obtained 2,655 real-world completion tests.",
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2023",
    "last_updated_quote": "CrossCodeEval (Ding et al., 2023)",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç è¡¥å…¨ï¼ˆåŒ…æ‹¬ä¸­æ®µå¡«å……ï¼‰",
    "task_granularity_quote": "Infilling scenarios constitute the majority of code completion tasks in the real world.",
    "evaluation_metrics": "Exact Match (EM), Edit Similarity (ES)",
    "evaluation_metrics_quote": "Following the CrossCodeEval evaluation protocol, we evaluated the completion results using two metrics: Exact Match (EM) and Edit Similarity (ES).",
    "input_modality": "ä»£ç ï¼ˆåŒ…å«éƒ¨åˆ†ç¼ºå¤±çš„ä»£ç ä¸Šä¸‹æ–‡ï¼‰",
    "input_modality_quote": "We then removed the code after the cursor in that line to form authentic completion test cases.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "The completion results are less than satisfactory.",
    "task_io_type": "ä»£ç åˆ°ä»£ç ï¼ˆè¡¥å…¨ï¼‰",
    "task_io_type_quote": "Infilling scenarios constitute the majority of code completion tasks in the real world.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºè¯„ä¼°æ¨¡å‹åˆ©ç”¨è·¨æ–‡ä»¶ä»£ç ä¿¡æ¯è¿›è¡Œä»“åº“çº§ä»£ç è¡¥å…¨çš„èƒ½åŠ›ï¼Œæµ‹è¯•ç”¨ä¾‹éœ€è¦è·¨æ–‡ä»¶ä¿¡æ¯ã€‚",
    "unique_features_quote": "The CrossCodeEval (Ding et al., 2023) benchmark provides test cases that require the use of cross-file code information for completion.",
    "data_size_quantity": 2655,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2023,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»“åº“çº§ä»£ç è¡¥å…¨èƒ½åŠ›', 'åˆ©ç”¨è·¨æ–‡ä»¶ä¿¡æ¯çš„èƒ½åŠ›']",
    "evaluation_method_normalized": "['Exact Match (EM)', 'Edit Similarity (ES)']",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç è¡¥å…¨",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2408.06450_output/content.md",
    "benchmark_name": "EVALPERF",
    "benchmark_name_quote": "As a proof of concept, we use DPE to create EVALPERF, a benchmark with 121 performance-challenging coding tasks.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "We introduce Differential Performance Evaluation (DPE), a framework designed to reliably evaluate Large Language Models (LLMs) for efficient code generation. ... As a proof of concept, we use DPE to create EVALPERF, a benchmark with 121 performance-challenging coding tasks.",
    "dataset_url": "github.com/evalplus/evalplus",
    "dataset_url_quote": "We also fully open-source and maintain the data curation pipeline and evaluator at github.com/evalplus/evalplus as part of EvalPlus.",
    "task_description": "è¯„æµ‹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ç”Ÿæˆçš„ä»£ç çš„æ•ˆç‡ã€‚è¯¥åŸºå‡†æ—¨åœ¨é€šè¿‡æä¾›å…·æœ‰æ€§èƒ½æŒ‘æˆ˜æ€§çš„ç¼–ç¨‹ä»»åŠ¡å’Œæœ‰æ•ˆçš„å¤åˆæŒ‡æ ‡ï¼Œå¯é åœ°è¯„ä¼°ä»£ç æ•ˆç‡ï¼Œå¼¥è¡¥ä¼ ç»Ÿä»£ç åŸºå‡†åœ¨æ•ˆç‡è¯„ä¼°æ–¹é¢çš„ä¸è¶³ã€‚",
    "task_description_quote": "We introduce Differential Performance Evaluation (DPE), a framework designed to reliably evaluate Large Language Models (LLMs) for efficient code generation. Traditional coding benchmarks often fail to provide reliable insights into code efficiency, due to their reliance on simplistic test inputs and the absence of effective compound metrics. DPE addresses these issues by focusing on efficiency-demanding programming tasks and establishing an insightful compound metric for performance evaluation.",
    "dimension": "ä»£ç ç”Ÿæˆæ•ˆç‡",
    "dimension_quote": "While the correctness evaluation of code generation has been well studied, we deliver a new and important aspect to the community by studying the data curation and assessment for the efficiency evaluation of LLM-generated code.",
    "evaluation_method": "å·®åˆ†æ€§èƒ½è¯„ä¼°ï¼ˆDPEï¼‰ã€‚è¯¥æ–¹æ³•åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š1ï¼‰æ•°æ®é›†æ„å»ºï¼šä»ç°æœ‰åŸºå‡†ä¸­é€‰æ‹©å¯¹æ•ˆç‡æœ‰è¦æ±‚çš„ä»»åŠ¡ï¼Œå¹¶ç”Ÿæˆè®¡ç®—å¯†é›†å‹çš„è¾“å…¥æ¥æµ‹è¯•LLMè§£å†³æ–¹æ¡ˆçš„æ•ˆç‡ã€‚2ï¼‰æ•ˆç‡è¯„ä¼°ï¼šåˆ†ææ–°è§£å†³æ–¹æ¡ˆçš„æ€§èƒ½ï¼Œå¹¶å°†å…¶ä¸ä¸€ç»„å…·æœ‰ä¸åŒæ•ˆç‡æ°´å¹³çš„å‚è€ƒè§£å†³æ–¹æ¡ˆè¿›è¡Œå…¨å±€æ¯”è¾ƒï¼ŒåŒ¹é…åˆ°çš„æ•ˆç‡æ°´å¹³å†³å®šäº†å…¶æ•ˆç‡å¾—åˆ†ã€‚",
    "evaluation_method_quote": "DPE operates in two phases: To curate efficiency datasets, it selects efficiency-demanding tasks from existing coding benchmarks and generates computationally expensive inputs to stress the efficiency of LLM solutions. To assess the code efficiency, DPE profiles the new solution and compares it globally against a set of reference solutions that exhibit distinct efficiency levels, where the matched level defines its efficiency score.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "é€šç”¨ç¼–ç¨‹ä»»åŠ¡ï¼Œæ¶µç›–ç®—æ³•ã€æ•°æ®ç»“æ„ç­‰",
    "problem_domain_quote": "At the high level, the input to the dataset creation phase is a set of programming tasks, e.g., from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021).",
    "problem_difficulty": "æ€§èƒ½æŒ‘æˆ˜å‹ä»»åŠ¡ï¼Œæ—¨åœ¨åŒºåˆ†ä¸åŒä»£ç è§£å†³æ–¹æ¡ˆçš„æ•ˆç‡",
    "problem_difficulty_quote": "DPE curates performance-demanding coding tasks by sampling synthesized test input generators and using filters to ensure evaluator quality.",
    "language": "Pythonï¼ˆä»HumanEvalå’ŒMBPPç­‰åŸºå‡†ä¸­é€‰å–ä»»åŠ¡ï¼Œè¿™äº›åŸºå‡†ä¸»è¦ä½¿ç”¨Pythonï¼‰",
    "language_quote": "At the high level, the input to the dataset creation phase is a set of programming tasks, e.g., from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021).",
    "data_size": "åŒ…å«121ä¸ªæ€§èƒ½æŒ‘æˆ˜å‹ç¼–ç¨‹ä»»åŠ¡",
    "data_size_quote": "As a proof of concept, we use DPE to create EVALPERF, a benchmark with 121 performance-challenging coding tasks.",
    "source_type": "ä»ç°æœ‰ä»£ç ç”ŸæˆåŸºå‡†ï¼ˆå¦‚HumanEvalã€MBPPï¼‰ä¸­ç­›é€‰å’Œæ”¹é€ ä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨LLMåˆæˆæ€§èƒ½æµ‹è¯•è¾“å…¥ç”Ÿæˆå™¨æ¥ç”Ÿæˆè®¡ç®—å¯†é›†å‹æµ‹è¯•è¾“å…¥ã€‚",
    "source_type_quote": "At the high level, the input to the dataset creation phase is a set of programming tasks, e.g., from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). ... To this end, we propose Synthesizing a Synthesizer (SAS) to automatically produce performance-exercising inputs of different programming tasks by prompting powerful code LLMs to generate test generators.",
    "last_updated": "2024.08",
    "last_updated_quote": "arXiv:2408.06450v1  [cs.SE]  12 Aug 2024",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ŒåŸºäºDPEæ¡†æ¶ä»ç°æœ‰åŸºå‡†ä¸­ç­›é€‰å’Œå¢å¼º",
    "build_type_quote": "As a proof of concept, we use DPE to create EVALPERF, a benchmark with 121 performance-challenging coding tasks.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆå®Œæ•´çš„å‡½æ•°/è§£å†³æ–¹æ¡ˆï¼‰",
    "task_granularity_quote": "Given a programming task, we collect a rich set of correct solutions by sampling various LLMs and test execution.",
    "evaluation_metrics": "æ•ˆç‡å¾—åˆ†ï¼ˆåŸºäºä¸å‚è€ƒè§£å†³æ–¹æ¡ˆæ€§èƒ½é›†ç¾¤çš„åŒ¹é…åº¦ï¼‰",
    "evaluation_metrics_quote": "During evaluation, if passing the correctness tests, the new solutions are profiled to compare against the reference solutions. Specifically, from slow to fast, the cumulative ratio of the cluster that includes the matched reference solution is the efficiency score of the evaluated solution.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆä»»åŠ¡æè¿°ï¼‰",
    "input_modality_quote": "Given a coding instruction in natural language, LLMs produce solutions...",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "LLMs produce solutions whose correctness is assessed through test execution.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "Given a coding instruction in natural language, LLMs produce solutions...",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "1. ä¸“æ³¨äºä»£ç æ•ˆç‡è¯„ä¼°ï¼Œè€Œéä¼ ç»Ÿçš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚2. ä½¿ç”¨DPEæ¡†æ¶ï¼ŒåŒ…å«â€œåˆæˆä¸€ä¸ªåˆæˆå™¨â€ï¼ˆSASï¼‰æ–¹æ³•æ¥è‡ªåŠ¨ç”Ÿæˆæ€§èƒ½æµ‹è¯•è¾“å…¥ã€‚3. é€šè¿‡æ€§èƒ½èšç±»å»ºç«‹å‚è€ƒè§£å†³æ–¹æ¡ˆé›†ï¼Œç”¨äºå…¨å±€æ¯”è¾ƒå’Œè¯„åˆ†ã€‚4. åŒ…å«ä»»åŠ¡ç­›é€‰æ ‡å‡†ï¼ˆè¶³å¤Ÿçš„è®¡ç®—é‡ã€ä½æ€§èƒ½å˜å¼‚ã€æ€§èƒ½å¤šæ ·æ€§ï¼‰ä»¥ç¡®ä¿è¯„ä¼°è´¨é‡ã€‚",
    "unique_features_quote": "DPE addresses these issues by focusing on efficiency-demanding programming tasks and establishing an insightful compound metric for performance evaluation. ... To this end, we propose Synthesizing a Synthesizer (SAS) to automatically produce performance-exercising inputs of different programming tasks by prompting powerful code LLMs to generate test generators. ... a selected programming task must meet the following criteria: 1. Sufficient computation ... 2. Low performance variation ... 3. Performance diversity...",
    "data_size_quantity": 121,
    "data_size_unit": "ä¸ª",
    "last_updated_year": 2024,
    "last_updated_month": 8,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆæ•ˆç‡']",
    "evaluation_method_normalized": "['æ•ˆç‡å¾—åˆ†']",
    "problem_domain_normalized": "['é€šç”¨ç¼–ç¨‹ä»»åŠ¡', 'ç®—æ³•', 'æ•°æ®ç»“æ„']",
    "source_type_normalized": "['ç°æœ‰ä»£ç ç”ŸæˆåŸºå‡†', 'HumanEval', 'MBPP', 'LLMåˆæˆæ€§èƒ½æµ‹è¯•è¾“å…¥ç”Ÿæˆå™¨']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2410.01215_output/content.md",
    "benchmark_name": "HumanEval",
    "benchmark_name_quote": "MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Extensive experiments across multiple models and benchmarks demonstrate that MGDebugger significantly outperforms existing debugging methods. Using open-source models like DeepSeek-Coder-V2-Lite, CodeQwen1.5, and Codestral, MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ–‡ä¸­ä»…æåŠå®éªŒä½¿ç”¨ï¼Œæœªæè¿°å®Œæ•´æ•°æ®é›†ã€‚æ ¹æ®å¼•ç”¨[7]ï¼ˆHumanEvalï¼‰æ¨æ–­ï¼Œè¯¥åŸºå‡†æ—¨åœ¨è¯„ä¼°ä»è‡ªç„¶è¯­è¨€æè¿°ï¼ˆdocstringï¼‰ç”ŸæˆPythonä»£ç çš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚",
    "task_description_quote": "Following the problem settings from Chen et al. [9] and Zhong et al. [75], we assume that the public test cases for the main function Tpub have been provided, which is common in most code generation benchmarks [7, 14, 44]2.",
    "dimension": "ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].",
    "evaluation_method": "æ‰§è¡Œå•å…ƒæµ‹è¯•ï¼ˆpass@kï¼‰",
    "evaluation_method_quote": "Following the problem settings from Chen et al. [9] and Zhong et al. [75], we assume that the public test cases for the main function Tpub have been provided, which is common in most code generation benchmarks [7, 14, 44]2.",
    "context_dependency": "å•å‡½æ•°",
    "context_dependency_quote": "Specifically, given an LLM-generated function ğ‘“, we decompose it into a hierarchical structure of subfunctions denoted as (ğ‘“1, ..., ğ‘“ğ‘›).",
    "problem_domain": NaN,
    "problem_domain_quote": NaN,
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "Python",
    "language_quote": "To effectively test each subfunction, we propose an LLM-simulated Python executor, which traces code execution and tracks important variable states to pinpoint errors accurately.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "Large language models (LLMs) such as GPT-4 [48], LLaMA [56], and DeepSeek-Coder [1] have made significant advances in AI-assisted coding tasks [7, 25, 34, 52]. Trained on vast corpora of text and code, LLMs can understand and generate code snippets for various programming tasks, ranging from simple data structures to complex algorithmic problems [34, 57].",
    "evaluation_metrics": "å‡†ç¡®ç‡ï¼ˆaccuracyï¼‰ï¼Œä¿®å¤æˆåŠŸç‡ï¼ˆrepair success rateï¼‰",
    "evaluation_metrics_quote": "MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆå‡½æ•°æè¿°/docstringï¼‰",
    "input_modality_quote": "Following the problem settings from Chen et al. [9] and Zhong et al. [75], we assume that the public test cases for the main function Tpub have been provided, which is common in most code generation benchmarks [7, 14, 44]2.",
    "output_modality": "ä»£ç ï¼ˆPythonå‡½æ•°ï¼‰",
    "output_modality_quote": "Specifically, given an LLM-generated function ğ‘“, we decompose it into a hierarchical structure of subfunctions denoted as (ğ‘“1, ..., ğ‘“ğ‘›).",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "Following the problem settings from Chen et al. [9] and Zhong et al. [75], we assume that the public test cases for the main function Tpub have been provided, which is common in most code generation benchmarks [7, 14, 44]2.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "æœ¬æ–‡æœªæè¿°HumanEvalçš„ç‹¬ç‰¹ä¹‹å¤„ï¼Œè€Œæ˜¯å°†å…¶ä½œä¸ºè¯„ä¼°æ¨¡å‹è°ƒè¯•èƒ½åŠ›çš„æ ‡å‡†åŸºå‡†ä¹‹ä¸€ã€‚",
    "unique_features_quote": "Extensive experiments across multiple models and benchmarks demonstrate that MGDebugger significantly outperforms existing debugging methods. Using open-source models like DeepSeek-Coder-V2-Lite, CodeQwen1.5, and Codestral, MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['å‡†ç¡®ç‡ï¼ˆaccuracyï¼‰', 'ä¿®å¤æˆåŠŸç‡ï¼ˆrepair success rateï¼‰']",
    "problem_domain_normalized": "[]",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2411.05830_output/content.md",
    "benchmark_name": "GitChameleon",
    "benchmark_name_quote": "To address this gap, we introduce GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests.",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To address this gap, we introduce GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests.",
    "dataset_url": "https://github.com/NizarIslah/GitChameleon",
    "dataset_url_quote": "For facilitation for further exploration of version-conditioned code generation, we make our code repository publicly accessible at https://github.com/NizarIslah/GitChameleon.",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆç‰¹å®šç‰ˆæœ¬åº“ä»£ç çš„èƒ½åŠ›ï¼Œè¦æ±‚ç”Ÿæˆçš„ä»£ç ä¸ä»…è¯­æ³•æ­£ç¡®ï¼Œè€Œä¸”åœ¨æ‰§è¡Œæ—¶åŠŸèƒ½å‡†ç¡®ã€‚",
    "task_description_quote": "GitChameleon is designed to rigorously assess the ability of modern large language models (LLMs) to generate version-specific code that is not only syntactically correct but also functionally accurate upon execution.",
    "dimension": "ç‰ˆæœ¬ç‰¹å®šä»£ç ç”Ÿæˆèƒ½åŠ›ã€ä»£ç åº“åŠ¨æ€é€‚åº”æ€§ã€åŠŸèƒ½æ­£ç¡®æ€§",
    "dimension_quote": "By providing an execution-based benchmark that emphasizes the dynamic nature of code libraries, GitChameleon serves as a critical tool to advance the development of more adaptable and reliable code generation models.",
    "evaluation_method": "åŸºäºæ‰§è¡Œçš„è¯„ä¼°ï¼Œä½¿ç”¨æ‰‹å†™çš„åŸºäºæ–­è¨€çš„å•å…ƒæµ‹è¯•æ¥éªŒè¯ç”Ÿæˆä»£ç çš„æ­£ç¡®æ€§ã€‚",
    "evaluation_method_quote": "To evaluate LLM performance on GitChameleon, each problem is accompanied by handwritten assertion-based unit tests, enabling a thorough execution-based assessment of the outputs generated by the code LLMs.",
    "context_dependency": "å•å‡½æ•°ä»£ç è¡¥å…¨",
    "context_dependency_quote": "GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems",
    "problem_domain": "Pythonç¼–ç¨‹ï¼Œæ¶‰åŠå¤šä¸ªæµè¡Œçš„æœºå™¨å­¦ä¹ å’Œæ•°æ®å¤„ç†åº“ï¼ˆå¦‚PyTorch, NumPy, Scikit-Learn, Pandasç­‰ï¼‰",
    "problem_domain_quote": "GitChameleon consists of 116 python-based version conditioned problems based on 11 libraries: PyTorch (Paszke et al., 2019), Geopandas (Jordahl et al., 2020), NLTK (Bird & Loper, 2004), NetworkX (Hagberg et al., 2008), GeoPy3, Gradio (Abid et al., 2019), Scikit-Learn (Buitinck et al., 2013), Matplotlib (Hunter, 2007), PyCaret4, Pandas (pandas development team, 2020; Wes McKinney, 2010) and NumPy (Harris et al., 2020).",
    "problem_difficulty": "åŸºäºçœŸå®ç‰ˆæœ¬å˜åŒ–çš„å®é™…é—®é¢˜ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿå¼€å‘è€…åœ¨æŠ€æœ¯å€ºåŠ¡çº¦æŸä¸‹çš„çœŸå®åœºæ™¯ã€‚",
    "problem_difficulty_quote": "our dataset offers a unique and complementary perspective by focusing on the real-world scenario where developers are often constrained to specific library versions due to technical debt.",
    "language": "Python",
    "language_quote": "GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems",
    "data_size": "åŒ…å«116ä¸ªPythonç‰ˆæœ¬æ¡ä»¶é—®é¢˜",
    "data_size_quote": "GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems",
    "source_type": "æ‰‹å·¥ç¼–å†™å’ŒLLMè¾…åŠ©ï¼ŒåŸºäºçœŸå®åº“çš„å˜æ›´æ—¥å¿—ï¼ˆchangelogsï¼‰",
    "source_type_quote": "The examples were manually crafted by the authors, who divided the task among themselves. We compiled a list of popular Python libraries, focusing on those with which at least one author was familiar and that had detailed changelogs documenting changes between versions.",
    "last_updated": "2024-11-05 (æ ¹æ®arXivç‰ˆæœ¬v1æ—¥æœŸ)",
    "last_updated_quote": "arXiv:2411.05830v1  [cs.SE]  5 Nov 2024",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ˆä½œè€…æ‰‹å·¥ç¼–å†™ï¼‰",
    "build_type_quote": "The examples were manually crafted by the authors, who divided the task among themselves.",
    "contamination_status": "å·²è€ƒè™‘è®­ç»ƒæ•°æ®æˆªæ­¢æ—¥æœŸï¼Œç¡®ä¿æ ·æœ¬åœ¨æ¨¡å‹è®­ç»ƒçª—å£æœŸå†…ï¼Œä»¥è¯„ä¼°æ¨¡å‹å¯¹å·²è®­ç»ƒç‰ˆæœ¬çš„çŸ¥è¯†ã€‚",
    "contamination_status_quote": "Since some of the models evaluated on GitChameleon have disclosed their training data cutoff dates, we have ensured that most, if not all, samples fall within the training window of these models.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç è¡¥å…¨",
    "task_granularity_quote": "GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems",
    "evaluation_metrics": "pass@k (ä¾‹å¦‚pass@10)",
    "evaluation_metrics_quote": "for instance, GPT-4o achieves a pass@10 of only 39.9% (43.7% when provided with error feedback)",
    "input_modality": "è‡ªç„¶è¯­è¨€é—®é¢˜æè¿°å’Œèµ·å§‹ä»£ç ",
    "input_modality_quote": "The problem statements average 20.4 tokens, and the starter code averages 47.4 tokens, leading to a combined average of 67.8 tokens per sample.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "generate version-specific code",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "synthesizing programs from docstrings.",
    "execution_environment": "éœ€è¦ç‰¹å®šåº“ç‰ˆæœ¬ä¾èµ–çš„è™šæ‹Ÿç¯å¢ƒ",
    "execution_environment_quote": "We used venv to create and manage virtual environments for testing. This process involved installing the appropriate library version and any additional dependencies.",
    "unique_features": "ä¸“æ³¨äºè¯„ä¼°æ¨¡å‹å¯¹çœŸå®åº“ç‰ˆæœ¬å˜åŒ–çš„é€‚åº”èƒ½åŠ›ï¼Œæ¯ä¸ªé—®é¢˜éƒ½ç»‘å®šåˆ°ç‰¹å®šçš„åº“ç‰ˆæœ¬ï¼Œå¹¶é…æœ‰å¯æ‰§è¡Œçš„å•å…ƒæµ‹è¯•ã€‚æ•°æ®åŸºäº2014å¹´è‡³2023å¹´çš„çœŸå®ç‰ˆæœ¬å‘å¸ƒï¼Œå¹¶æ ‡æ³¨äº†å˜æ›´ç±»å‹ï¼ˆå‚æ•°/å±æ€§å˜æ›´ã€å‡½æ•°åå˜æ›´ã€è¯­ä¹‰/è¡Œä¸ºå˜æ›´ã€æ–°åŠŸèƒ½ï¼‰ã€‚",
    "unique_features_quote": "GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. ... The samples were collected from version releases over a period from the year 2014 to 2023 ... we annotate each sample with the type of change that is classified into the following categories: Argument or Attribute change, Function Name change, Semantics or Function Behavior change, New feature or additional dependency-based change.",
    "data_size_quantity": 116,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": 2024,
    "last_updated_month": 11,
    "last_updated_day": 5,
    "language_normalized": "['Python']",
    "dimension_normalized": "['ç‰ˆæœ¬ç‰¹å®šä»£ç ç”Ÿæˆèƒ½åŠ›', 'ä»£ç åº“åŠ¨æ€é€‚åº”æ€§', 'åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['pass@k', 'pass@10']",
    "problem_domain_normalized": "['Pythonç¼–ç¨‹', 'PyTorch', 'NumPy', 'Scikit-Learn', 'Pandas']",
    "source_type_normalized": "['æ‰‹å·¥ç¼–å†™', 'LLMè¾…åŠ©', 'åŸºäºçœŸå®åº“çš„å˜æ›´æ—¥å¿—']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç è¡¥å…¨",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2411.12882_output/content.md",
    "benchmark_name": "PurpleLlama secure coding benchmark",
    "benchmark_name_quote": "We demonstrate the effectiveness of PROSEC on the PurpleLlama (Bhatt et al., 2023) secure coding benchmark.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We demonstrate the effectiveness of PROSEC on the PurpleLlama (Bhatt et al., 2023) secure coding benchmark.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°ä»£ç å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå®‰å…¨ä»£ç çš„èƒ½åŠ›ï¼Œæ£€æµ‹å…¶ç”Ÿæˆçš„ä»£ç æ˜¯å¦åŒ…å«å¸¸è§å¼±ç‚¹æšä¸¾ï¼ˆCWEï¼‰ä¸­å®šä¹‰çš„å®‰å…¨æ¼æ´ã€‚",
    "task_description_quote": "The goal of security alignment in code LLMs is to reduce the likelihood of generating insecure code... We demonstrate the effectiveness of PROSEC on the PurpleLlama (Bhatt et al., 2023) secure coding benchmark.",
    "dimension": "ä»£ç å®‰å…¨æ€§ï¼Œå³ç”Ÿæˆä»£ç æ˜¯å¦éµå¾ªå®‰å…¨ç¼–ç å®è·µï¼Œé¿å…å¼•å…¥å¸¸è§æ¼æ´ã€‚",
    "dimension_quote": "The safety of these models remains under-explored, posing potential risks as insecure code generated by these models may introduce vulnerabilities into real-world systems.",
    "evaluation_method": "ä½¿ç”¨é™æ€åˆ†æå™¨ä½œä¸ºé¢„è¨€æœºï¼Œæ£€æµ‹ç”Ÿæˆçš„ä»£ç ç‰‡æ®µä¸­æ˜¯å¦å­˜åœ¨CWEæ¼æ´ã€‚",
    "evaluation_method_quote": "Following previous work (Bhatt et al., 2023), we assume that there exists a static analyzer... as an oracle to detect whether a snippet of code follows secure code patterns. Specifically, the static analyzer takes as input a code snippet, and outputs a list of detected CWEs.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "å®‰å…¨ç¼–ç ï¼Œæ¶µç›–å¸¸è§å¼±ç‚¹æšä¸¾ï¼ˆCWEï¼‰ä¸­å®šä¹‰çš„å„ç§è½¯ä»¶å®‰å…¨æ¼æ´ç±»å‹ã€‚",
    "problem_domain_quote": "The Common Weakness Enumerations (CWEs) (MITRE, 2023), which abstract diverse program vulnerabilities, offer a generalizable foundation for simulating how vulnerabilities manifest across various coding tasks and programming languages.",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "æ–‡ä¸­æœªæ˜ç¡®æåŠPurpleLlamaåŸºå‡†æ”¯æŒçš„å…·ä½“ç¼–ç¨‹è¯­è¨€ï¼Œä½†æœ¬æ–‡å®éªŒæ¶‰åŠå¤šç§è¯­è¨€ã€‚",
    "language_quote": "PROSEC improves the ability of code LLMs to generate secure code... across multiple models, languages, and vulnerability types.",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": NaN,
    "source_type_quote": NaN,
    "last_updated": "2023",
    "last_updated_quote": "PurpleLlama (Bhatt et al., 2023)",
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "Large language models (LLMs) capable of generating code based on human instructions have revolutionized programming by significantly facilitating tasks such as code generation...",
    "evaluation_metrics": "å®‰å…¨ä»£ç ç”Ÿæˆèƒ½åŠ›çš„æå‡ç™¾åˆ†æ¯”ï¼ˆä¾‹å¦‚ï¼Œæ¯”åŸºçº¿æ¨¡å‹å®‰å…¨25.2%â€“35.4%ï¼‰",
    "evaluation_metrics_quote": "The models trained with the dataset synthesized by PROSEC are 25.2%â€“35.4% more secure than those trained with the SafeCoder dataset.",
    "input_modality": "è‡ªç„¶è¯­è¨€æŒ‡ä»¤",
    "input_modality_quote": "Consider an instruction following code LLM Ï€Î¸(y|x) = Î iÏ€Î¸(yi|y<i, x) that takes user instruction x and generates the code response y.",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "Consider an instruction following code LLM Ï€Î¸(y|x) = Î iÏ€Î¸(yi|y<i, x) that takes user instruction x and generates the code response y.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "Consider an instruction following code LLM Ï€Î¸(y|x) = Î iÏ€Î¸(yi|y<i, x) that takes user instruction x and generates the code response y.",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºä»£ç å®‰å…¨æ€§çš„è¯„æµ‹åŸºå‡†ï¼Œä½¿ç”¨CWEï¼ˆå¸¸è§å¼±ç‚¹æšä¸¾ï¼‰ä½œä¸ºæ¼æ´åˆ†ç±»å’Œæ£€æµ‹çš„åŸºç¡€æ¡†æ¶ã€‚",
    "unique_features_quote": "A widely recognized framework for categorizing such issues is the Common Weakness Enumerations (CWE) (MITRE, 2023)... Following previous work (Bhatt et al., 2023), we assume that there exists a static analyzer... to detect whether a snippet of code follows secure code patterns... and outputs a list of detected CWEs.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2023,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ä»£ç å®‰å…¨æ€§']",
    "evaluation_method_normalized": "['å®‰å…¨ä»£ç ç”Ÿæˆèƒ½åŠ›çš„æå‡ç™¾åˆ†æ¯”']",
    "problem_domain_normalized": "['å®‰å…¨ç¼–ç ']",
    "source_type_normalized": "[]",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2412.00535_output/content.md",
    "benchmark_name": "FullStack Bench",
    "benchmark_name_quote": "FullStack Bench: Evaluating LLMs as Full Stack Coders",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "To address this gap, we have developed a comprehensive code evaluation dataset FullStack Bench1,2 focusing on full-stack programming",
    "dataset_url": "https://huggingface.co/datasets/ByteDance/FullStackBench, https://github.com/bytedance/FullStackBench",
    "dataset_url_quote": "1https://huggingface.co/datasets/ByteDance/FullStackBench\n2https://github.com/bytedance/FullStackBench",
    "task_description": "è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ä½œä¸ºå…¨æ ˆç¨‹åºå‘˜çš„èƒ½åŠ›ï¼Œæ¶µç›–å¹¿æ³›çš„åº”ç”¨ç¨‹åºé¢†åŸŸã€‚",
    "task_description_quote": "focusing on full-stack programming, which encompasses a wide range of application domains (e.g., basic programming, data analysis, software engineering, mathematics, and machine learning).",
    "dimension": "å…¨æ ˆç¼–ç¨‹èƒ½åŠ›ã€å¤šè¯­è¨€ç¼–ç¨‹èƒ½åŠ›ã€è·¨åº”ç”¨é¢†åŸŸèƒ½åŠ›",
    "dimension_quote": "focusing on full-stack programming... to assess multi-lingual programming capabilities... a wide range of application domains",
    "evaluation_method": "ä½¿ç”¨å•å…ƒæµ‹è¯•ç”¨ä¾‹è¿›è¡Œä»£ç æ²™ç®±æ‰§è¡Œè¯„ä¼°",
    "evaluation_method_quote": "we design real-world instructions and corresponding unit test cases... we also release an effective code sandbox execution tool (i.e., SandboxFusion3)... to evaluate the performance of our FullStack Bench efficiently.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "åŸºç¡€ç¼–ç¨‹ã€é«˜çº§ç¼–ç¨‹ã€è½¯ä»¶å·¥ç¨‹ã€æ•°æ®åˆ†æã€æ•°å­¦ã€æ¡Œé¢ä¸Webå¼€å‘ã€æœºå™¨å­¦ä¹ ã€ç§‘å­¦è®¡ç®—ã€æ•°æ®åº“ã€å¤šåª’ä½“ã€æ“ä½œç³»ç»Ÿç­‰",
    "problem_domain_quote": "Basic Programming (BP)\nAdvanced Programming (AP)\nSoftware Engineering (SE)\nData Analysis (DA)\nMathematics (MA)\nDesktop and Web Development (DW)\nMachine Learning (ML)\nScientific Computing (SC)\nDataBase (DB)\nMultimedia (MM)\nOperating System (OS)\nOthers",
    "problem_difficulty": NaN,
    "problem_difficulty_quote": NaN,
    "language": "16ç§å¹¿æ³›ä½¿ç”¨çš„ç¼–ç¨‹è¯­è¨€",
    "language_quote": "in FullStack Bench, we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages",
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "è®¾è®¡çœŸå®ä¸–ç•Œçš„æŒ‡ä»¤å’Œå¯¹åº”çš„å•å…ƒæµ‹è¯•ç”¨ä¾‹ï¼Œè€Œéç®€å•ç¿»è¯‘",
    "source_type_quote": "we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations.",
    "last_updated": "2025-05-12",
    "last_updated_quote": "arXiv:2412.00535v6  [cs.AI]  12 May 2025",
    "build_type": "å®˜æ–¹è‡ªå»º",
    "build_type_quote": "we have developed a comprehensive code evaluation dataset FullStack Bench",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆ",
    "task_granularity_quote": "evaluating LLMs as Full Stack Coders",
    "evaluation_metrics": NaN,
    "evaluation_metrics_quote": NaN,
    "input_modality": "è‡ªç„¶è¯­è¨€æŒ‡ä»¤",
    "input_modality_quote": "we design real-world instructions",
    "output_modality": "ä»£ç ",
    "output_modality_quote": "evaluating LLMs as Full Stack Coders",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ",
    "task_io_type_quote": "we design real-world instructions and corresponding unit test cases",
    "execution_environment": "æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€å’ŒåŒ…çš„ä»£ç æ²™ç®±æ‰§è¡Œå·¥å…·(SandboxFusion)",
    "execution_environment_quote": "we also release an effective code sandbox execution tool (i.e., SandboxFusion3) supporting various programming languages and packages",
    "unique_features": "ä¸“æ³¨äºå…¨æ ˆç¼–ç¨‹è¯„ä¼°ï¼Œè¦†ç›–å¹¿æ³›çš„åº”ç”¨ç¨‹åºé¢†åŸŸï¼›åŒ…å«16ç§ç¼–ç¨‹è¯­è¨€çš„çœŸå®ä¸–ç•ŒæŒ‡ä»¤å’Œå•å…ƒæµ‹è¯•ï¼Œè€Œéç®€å•ç¿»è¯‘ï¼›é…å¥—å‘å¸ƒäº†æ”¯æŒå¤šè¯­è¨€å’Œå¤šåŒ…çš„ä»£ç æ²™ç®±æ‰§è¡Œå·¥å…·SandboxFusionã€‚",
    "unique_features_quote": "focusing on full-stack programming, which encompasses a wide range of application domains... we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations. Moreover, we also release an effective code sandbox execution tool (i.e., SandboxFusion3) supporting various programming languages and packages",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": 5,
    "last_updated_day": 12,
    "language_normalized": "['16ç§å¹¿æ³›ä½¿ç”¨çš„ç¼–ç¨‹è¯­è¨€']",
    "dimension_normalized": "['å…¨æ ˆç¼–ç¨‹èƒ½åŠ›', 'å¤šè¯­è¨€ç¼–ç¨‹èƒ½åŠ›', 'è·¨åº”ç”¨é¢†åŸŸèƒ½åŠ›']",
    "evaluation_method_normalized": "[]",
    "problem_domain_normalized": "['åŸºç¡€ç¼–ç¨‹', 'é«˜çº§ç¼–ç¨‹', 'è½¯ä»¶å·¥ç¨‹', 'æ•°æ®åˆ†æ', 'æ•°å­¦', 'æ¡Œé¢ä¸Webå¼€å‘', 'æœºå™¨å­¦ä¹ ', 'ç§‘å­¦è®¡ç®—', 'æ•°æ®åº“', 'å¤šåª’ä½“', 'æ“ä½œç³»ç»Ÿ']",
    "source_type_normalized": "['è®¾è®¡çœŸå®ä¸–ç•Œçš„æŒ‡ä»¤å’Œå¯¹åº”çš„å•å…ƒæµ‹è¯•ç”¨ä¾‹ï¼Œè€Œéç®€å•ç¿»è¯‘']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2412.05210_output/content.md",
    "benchmark_name": "CodeArena",
    "benchmark_name_quote": "we present a rigorous human-curated benchmark CodeArena to emulate the complexity and diversity of real-world coding tasks",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we first introduce a comprehensive human-curated benchmark, CodeArena",
    "dataset_url": "https://codearenaeval.github.io/",
    "dataset_url_quote": "1https://codearenaeval.github.io/",
    "task_description": "è¯„ä¼°æ¨¡å‹ç”Ÿæˆçš„ä»£ç å“åº”ä¸äººç±»åå¥½çš„å¯¹é½ç¨‹åº¦ï¼Œæ¨¡æ‹Ÿç°å®ä¸–ç•Œç¼–ç ä»»åŠ¡çš„å¤æ‚æ€§å’Œå¤šæ ·æ€§ã€‚",
    "task_description_quote": "to evaluate the alignment between the model-generated response and human preference, enabling the community to evaluate and track the alignment between human preferences and model-generated responses in real-world scenarios.",
    "dimension": "äººç±»åå¥½å¯¹é½ï¼ˆåŒ…æ‹¬ä»£ç è´¨é‡ã€è§£é‡Šã€æ ¼å¼ã€æ³¨é‡Šç­‰ï¼‰ï¼Œè€Œéå•çº¯çš„ä»£ç æ­£ç¡®æ€§ã€‚",
    "dimension_quote": "the current code LLMs focus on synthesizing the correct code snippet, ignoring the alignment with human preferences... underscoring the importance of the human preference alignment.",
    "evaluation_method": "ä½¿ç”¨GPT-4oä½œä¸ºè¯„åˆ¤å‘˜ï¼Œé€šè¿‡â€œæ¯”è¾ƒAå’ŒBâ€å’Œâ€œæ¯”è¾ƒBå’ŒAâ€ä¸¤ç§æ¸¸æˆè®¡ç®—æ¨¡å‹ç›¸å¯¹äºåŸºçº¿çš„èƒœç‡ã€‚",
    "evaluation_method_quote": "we apply GPT-4o-2024-08-06 as the judger to evaluate the model performance. Specifically, we use two games â€œcompare A and B â€ and â€œcompare B and Aâ€ (avoid the relative position of A and B affecting the results) to calculate the win rate of A compared to the baseline B.",
    "context_dependency": "åŸºäºç°å®ä¸–ç•Œç”¨æˆ·æŸ¥è¯¢çš„å®Œæ•´é—®é¢˜ï¼Œä¸å±€é™äºè‡ªåŒ…å«çš„å‡½æ•°ç‰‡æ®µã€‚",
    "context_dependency_quote": "Popular code-related benchmarks typically focus on self-contained function snippets... CodeArena provides many problems for evaluation under realistic scenarios, which are not suitable for verification through unit testing.",
    "problem_domain": "æ¶µç›–è½¯ä»¶å¼€å‘ã€ç”¨æˆ·ç•Œé¢/ä½“éªŒã€ä¸“ç”¨è®¡ç®—ã€å·¥å…·ä¸ç¯å¢ƒã€æ•°æ®åº“ä¸æ•°æ®å¤„ç†ã€æ–°å…´æŠ€æœ¯ã€é€šç”¨æŸ¥è¯¢ç­‰7å¤§ç±»40ä¸ªå­ç±»ã€‚",
    "problem_domain_quote": "comprising 397 high-quality samples across 40 categories derived from real-world user queries... encompassing 7 major categories and 40 subcategories.",
    "problem_difficulty": "åˆ†ä¸ºç®€å•ã€ä¸­ç­‰ã€å›°éš¾ä¸‰ä¸ªçº§åˆ«ï¼Œå¤§éƒ¨åˆ†æ ·æœ¬ä¸ºä¸­ç­‰æˆ–å›°éš¾ã€‚",
    "problem_difficulty_quote": "all samples are classified into easy, medium, and hard. The majority of the samples are recognized as medium or hard, presenting a significant challenge to LLMs.",
    "language": "æ¶µç›–44ç§ç¼–ç¨‹è¯­è¨€ï¼ŒåŒ…æ‹¬Pythonã€C++ã€Javaã€JavaScriptã€HTML/CSSã€SQLã€Bashã€Goã€Rustã€PowerShellã€Google Apps Scriptç­‰ã€‚",
    "language_quote": "spanning 40 categories and 44 programming languages... CodeArena provides a valuable comprehensive benchmark for 40 subtasks and 44 programming languages",
    "data_size": "åŒ…å«397ä¸ªé«˜è´¨é‡æ ·æœ¬ã€‚",
    "data_size_quote": "comprising 397 high-quality samples... CodeArena consists of nearly 400 problems.",
    "source_type": "ä»åœ¨çº¿é—®ç­”ç½‘ç«™çš„ç”¨æˆ·æŸ¥è¯¢ä¸­ç²¾å¿ƒç­›é€‰å’Œäººå·¥æ ‡æ³¨ã€‚",
    "source_type_quote": "carefully curated from user queries... derived from real-world user queries... Online Q&A",
    "last_updated": "2024å¹´12æœˆï¼ˆè®ºæ–‡ç‰ˆæœ¬æ—¥æœŸï¼‰",
    "last_updated_quote": "arXiv:2412.05210v1  [cs.CL]  6 Dec 2024",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œç»è¿‡ä¸¥æ ¼çš„äººå·¥æ ‡æ³¨å’Œè´¨é‡æ§åˆ¶æµç¨‹ã€‚",
    "build_type_quote": "We propose CodeArena comprised of 397 manually annotated samples... we implement a rigorous human annotation process involving 4 full-time employees proficient in various programming languages for human annotation and 4 other senior programming developers for quality check.",
    "contamination_status": "è¿›è¡Œäº†å»æ±¡æŸ“å¤„ç†ï¼Œé€šè¿‡ç§»é™¤ä¸ç°æœ‰åŸºå‡†ï¼ˆMultiPL-E, MBPP, McEval, NaturalCodeBenchï¼‰çš„ç²¾ç¡®åŒ¹é…ï¼ˆ10-gramé‡å ï¼‰æ¥ç¡®ä¿æç¤ºçš„å”¯ä¸€æ€§ã€‚",
    "contamination_status_quote": "To avoid data leakage, we apply decontamination to ensure the uniqueness of prompts in CodeArena, by removing exact matches (10-gram word overlap) from MultiPL-E, MBPP, McEval, and NaturalCodeBench.",
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆæ ¹æ®è‡ªç„¶è¯­è¨€æè¿°ç”Ÿæˆä»£ç ç‰‡æ®µæˆ–å®Œæ•´è§£å†³æ–¹æ¡ˆï¼‰ã€‚",
    "task_granularity_quote": "The query should be sampled from the practical application scenarios and the model-generated responses should satisfy the human preference.",
    "evaluation_metrics": "èƒœç‡ï¼ˆwin rateï¼‰ï¼ŒåŸºäºGPT-4oçš„åå¥½åˆ¤æ–­ã€‚",
    "evaluation_metrics_quote": "calculate the win rate of A compared to the baseline B.",
    "input_modality": "è‡ªç„¶è¯­è¨€ï¼ˆç”¨æˆ·æŸ¥è¯¢/é—®é¢˜æè¿°ï¼‰ã€‚",
    "input_modality_quote": "Each sample in CodeArena includes (question...)",
    "output_modality": "ä»£ç ä¸è‡ªç„¶è¯­è¨€æ··åˆï¼ˆæœŸæœ›çš„å“åº”åŒ…æ‹¬ä»£ç ç‰‡æ®µä»¥åŠè¯¦ç»†çš„è§£é‡Šã€æ ¼å¼åŒ–å’Œæ³¨é‡Šï¼‰ã€‚",
    "output_modality_quote": "Claude3.5 produces responses that include detailed explanations, well-structured formatting, and code comments, making it more favorable in terms of human preference.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ä¸æ–‡æœ¬ï¼ˆè‡ªç„¶è¯­è¨€é—®é¢˜åˆ°åŒ…å«ä»£ç å’Œè§£é‡Šçš„å“åº”ï¼‰ã€‚",
    "task_io_type_quote": "synthesizing the correct code snippet... alignment with human preferences (including explanations, formatting, comments)",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "ä¸“æ³¨äºè¯„ä¼°ä»£ç LLMä¸äººç±»åå¥½çš„å¯¹é½ï¼Œè€Œéå•çº¯çš„ä»£ç æ‰§è¡Œæ­£ç¡®æ€§ï¼›æ ·æœ¬æ¥æºäºçœŸå®ä¸–ç•Œç”¨æˆ·æŸ¥è¯¢ï¼Œè¦†ç›–å¹¿æ³›çš„ç¼–ç¨‹è¯­è¨€å’Œä»»åŠ¡ç±»åˆ«ï¼›åŒ…å«äººå·¥æ ‡æ³¨çš„éš¾åº¦ç­‰çº§å’ŒåŸºçº¿ç­”æ¡ˆã€‚",
    "unique_features_quote": "Unlike previous benchmarks, which consist of various programming exercises along with the corresponding test cases... our benchmarks emphasize a diverse range of programming languages that are commonly used in everyday programming tasks... provides a valuable comprehensive benchmark for 40 subtasks and 44 programming languages, which satisfies the evaluation in realistic scenarios.",
    "data_size_quantity": 397,
    "data_size_unit": "ä¸ªæ ·æœ¬",
    "last_updated_year": 2024,
    "last_updated_month": 12,
    "last_updated_day": null,
    "language_normalized": "['Python', 'C++', 'Java', 'JavaScript', 'HTML/CSS', 'SQL', 'Bash', 'Go', 'Rust', 'PowerShell', 'Google Apps Script']",
    "dimension_normalized": "['äººç±»åå¥½å¯¹é½', 'ä»£ç è´¨é‡', 'è§£é‡Š', 'æ ¼å¼', 'æ³¨é‡Š']",
    "evaluation_method_normalized": "['èƒœç‡', 'GPT-4oçš„åå¥½åˆ¤æ–­']",
    "problem_domain_normalized": "['è½¯ä»¶å¼€å‘', 'ç”¨æˆ·ç•Œé¢/ä½“éªŒ', 'ä¸“ç”¨è®¡ç®—', 'å·¥å…·ä¸ç¯å¢ƒ', 'æ•°æ®åº“ä¸æ•°æ®å¤„ç†', 'æ–°å…´æŠ€æœ¯', 'é€šç”¨æŸ¥è¯¢']",
    "source_type_normalized": "['åœ¨çº¿é—®ç­”ç½‘ç«™çš„ç”¨æˆ·æŸ¥è¯¢', 'äººå·¥æ ‡æ³¨']",
    "problem_difficulty_normalized": "æœªçŸ¥",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "æœªçŸ¥",
    "task_io_type_normalized": "æœªçŸ¥",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2505.08503_output/content.md",
    "benchmark_name": "ICVul",
    "benchmark_name_quote": "ICVul: A Well-labeled C/C++ Vulnerability Dataset with Comprehensive Metadata and VCCs",
    "is_original_proposal": "Yes",
    "is_original_proposal_quote": "we constructed ICVul, a dataset emphasizing data quality and enriched with comprehensive metadata, including Vulnerability-Contributing Commits (VCCs).",
    "dataset_url": "https://github.com/Chaomeng-Lu/ICVul.git",
    "dataset_url_quote": "The project, along with supporting scripts, is publicly available on GitHub1. 1https://github.com/Chaomeng-Lu/ICVul.git",
    "task_description": "ç”¨äºè®­ç»ƒå’Œè¯„ä¼°åŸºäºæœºå™¨å­¦ä¹ çš„è½¯ä»¶æ¼æ´æ£€æµ‹æ¨¡å‹ã€‚",
    "task_description_quote": "Machine learning-based software vulnerability detection requires high-quality datasets, which is essential for training effective models.",
    "dimension": "æ¼æ´æ£€æµ‹çš„æ•°æ®è´¨é‡ã€æ ‡ç­¾å¯é æ€§ã€å…ƒæ•°æ®ä¸°å¯Œåº¦ã€æ•°æ®å¹³è¡¡æ€§ã€‚",
    "dimension_quote": "To address challenges related to data label quality, diversity, and comprehensiveness, we constructed ICVul, a dataset emphasizing data quality and enriched with comprehensive metadata...",
    "evaluation_method": "æ–‡ä¸­æœªæ˜ç¡®æè¿°é’ˆå¯¹è¯¥æ•°æ®é›†çš„æ ‡å‡†è¯„ä¼°æ–¹æ³•ã€‚",
    "evaluation_method_quote": NaN,
    "context_dependency": "ä»£ç æäº¤ï¼ˆCommitï¼‰ã€æ–‡ä»¶ï¼ˆFileï¼‰ã€å‡½æ•°ï¼ˆFunctionï¼‰çº§åˆ«ã€‚",
    "context_dependency_quote": "ICVul provides a high-quality dataset tailored for training ML models to detect software vulnerabilities at the commit, function and file levels.",
    "problem_domain": "è½¯ä»¶å®‰å…¨ã€æ¼æ´æ£€æµ‹ã€‚",
    "problem_domain_quote": "Detecting and mitigating software vulnerabilities remains one of the most critical challenges in ensuring the security and integrity of modern software systems.",
    "problem_difficulty": "çœŸå®ä¸–ç•Œè½¯ä»¶é¡¹ç›®ä¸­çš„æ¼æ´ï¼Œå¤æ‚åº¦é«˜ã€‚",
    "problem_difficulty_quote": "With the growing complexity of applications and their codebases, traditional methods of vulnerability detection... struggle to keep up with the scale and intricacy of vulnerabilities present in large software projects.",
    "language": "C/C++",
    "language_quote": "ICVul, an Integrated and Comprehensive C/C++ Vulnerability dataset.",
    "data_size": "åŒ…å«807ä¸ªä»“åº“ï¼Œ146ä¸ªCWEç±»å‹ï¼Œ4327ä¸ªä¿®å¤æäº¤ï¼Œ6862ä¸ªæ–‡ä»¶ï¼Œ15396ä¸ªå‡½æ•°ï¼Œå…¶ä¸­6276ä¸ªä¸ºæ¼æ´å‡½æ•°ï¼ˆå æ¯”41%ï¼‰ã€‚",
    "data_size_quote": "ICVul 807 146 4,327 6,862 15,396 6,276 41%",
    "source_type": "ä»ç¾å›½å›½å®¶æ¼æ´æ•°æ®åº“ï¼ˆNVDï¼‰ä¸­ç­›é€‰ä¸GitHubä¿®å¤æäº¤ç›¸å…³è”çš„CVEæ¡ç›®ï¼Œå¹¶ä»ä¸­æå–ä»£ç å’Œå…ƒæ•°æ®ã€‚",
    "source_type_quote": "We began by filtering Common Vulnerabilities and Exposures from the NVD, retaining only those linked to GitHub fix commits. Then we extracted functions and files along with relevant metadata from these commits...",
    "last_updated": "2024å¹´11æœˆ13æ—¥ï¼ˆæ•°æ®æ”¶é›†æ—¥æœŸï¼‰",
    "last_updated_quote": "The dataset construction process began with the collection of 269,509 CVEs, gathered on November 13, 2024.",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œå¹¶æä¾›äº†å¯å¤ç°çš„æ„å»ºæ¡†æ¶ã€‚",
    "build_type_quote": "We introduce a vulnerability collection framework that can be re-run at any time to ensure the dataset remains up-to-date.",
    "contamination_status": "é€šè¿‡ESCæŠ€æœ¯æ’é™¤å¯ç–‘æäº¤ï¼Œæ—¨åœ¨æé«˜æ ‡ç­¾å¯é æ€§ï¼Œé™ä½å™ªå£°ã€‚",
    "contamination_status_quote": "To further enhance label reliability, we developed the ESC (Eliminate Suspicious Commit) technique, ensuring credible data labels.",
    "dataset_license": "æ–‡ä¸­æœªæåŠã€‚",
    "dataset_license_quote": NaN,
    "task_granularity": "æ¼æ´æ£€æµ‹ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰ã€‚",
    "task_granularity_quote": "ICVul provides a high-quality dataset tailored for training ML models to detect software vulnerabilities...",
    "evaluation_metrics": "æ–‡ä¸­æœªæåŠé’ˆå¯¹è¯¥æ•°æ®é›†çš„æ ‡å‡†è¯„ä¼°æŒ‡æ ‡ã€‚",
    "evaluation_metrics_quote": NaN,
    "input_modality": "æºä»£ç ï¼ˆC/C++ï¼‰åŠä¸°å¯Œçš„å…ƒæ•°æ®ï¼ˆå¦‚æäº¤ä¿¡æ¯ã€ä»£ç å˜æ›´ã€ä»“åº“ä¿¡æ¯ç­‰ï¼‰ã€‚",
    "input_modality_quote": "The dataset is stored in a relational-like database for improved usability and data integrity.",
    "output_modality": "æ¼æ´æ ‡ç­¾ï¼ˆæ˜¯å¦åŒ…å«æ¼æ´ï¼Œä»¥åŠCWEç±»å‹ï¼‰ã€‚",
    "output_modality_quote": "the dataset provides clear vulnerability labels at the CWE type level, enabling the development and research of multi-class classification models.",
    "task_io_type": "ä»£ç åˆ°æ ‡ç­¾ï¼ˆæ¼æ´åˆ†ç±»ï¼‰ã€‚",
    "task_io_type_quote": "training ML models to detect software vulnerabilities",
    "execution_environment": "ä¸æ¶‰åŠä»£ç æ‰§è¡Œï¼Œæ˜¯é™æ€åˆ†ææ•°æ®é›†ã€‚",
    "execution_environment_quote": NaN,
    "unique_features": "1. åŒ…å«æ¼æ´å¼•å…¥æäº¤ï¼ˆVCCï¼‰ä¿¡æ¯ï¼Œä½¿ç”¨SZZç®—æ³•è¿½æº¯ã€‚2. åº”ç”¨ESCæŠ€æœ¯è¿‡æ»¤å™ªå£°æ•°æ®ï¼Œæé«˜æ ‡ç­¾è´¨é‡ã€‚3. æ•°æ®å¹³è¡¡æ€§å¥½ï¼ˆæ¼æ´å‡½æ•°å æ¯”41%ï¼‰ã€‚4. ä»¥å…³ç³»å‹æ•°æ®åº“ç»“æ„å­˜å‚¨ï¼ŒåŒ…å«ä»“åº“ã€æäº¤ã€æ–‡ä»¶ã€å‡½æ•°ã€CVE-VCCæ˜ å°„äº”ä¸ªäº’ç›¸å…³è”çš„è¡¨ã€‚5. ä¸“æ³¨äºC/C++è¯­è¨€ã€‚",
    "unique_features_quote": "Another key feature of ICVul is its inclusion of VCCs, which trace specific commits responsible for introducing vulnerabilities into the codebase using the state-of-the-art SZZ algorithm... ICVul incorporates the ESC (Eliminate Suspicious Commit) technique, further enhancing label reliability... the dataset achieves a much better balance ratio of 41% at the function level. ICVul comprises five interconnected tables: repository info, cve fc vcc mapping, commit info, file info, and function info.",
    "data_size_quantity": 807,
    "data_size_unit": "ä¸ªä»“åº“",
    "last_updated_year": 2024,
    "last_updated_month": 11,
    "last_updated_day": 13,
    "language_normalized": "['C/C++']",
    "dimension_normalized": "['æ¼æ´æ£€æµ‹çš„æ•°æ®è´¨é‡', 'æ ‡ç­¾å¯é æ€§', 'å…ƒæ•°æ®ä¸°å¯Œåº¦', 'æ•°æ®å¹³è¡¡æ€§']",
    "evaluation_method_normalized": "['æ–‡ä¸­æœªæåŠé’ˆå¯¹è¯¥æ•°æ®é›†çš„æ ‡å‡†è¯„ä¼°æŒ‡æ ‡']",
    "problem_domain_normalized": "['è½¯ä»¶å®‰å…¨', 'æ¼æ´æ£€æµ‹']",
    "source_type_normalized": "['ä»ç¾å›½å›½å®¶æ¼æ´æ•°æ®åº“ï¼ˆNVDï¼‰ä¸­ç­›é€‰ä¸GitHubä¿®å¤æäº¤ç›¸å…³è”çš„CVEæ¡ç›®ï¼Œå¹¶ä»ä¸­æå–ä»£ç å’Œå…ƒæ•°æ®']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "JSON",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æŠ—æ±¡æŸ“è®¾è®¡"
  },
  {
    "source_paper": "2511.17330_output/content.md",
    "benchmark_name": "SV-COMP",
    "benchmark_name_quote": "Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ç¨‹åºéªŒè¯ï¼Œå³è¯æ˜ç¨‹åºæ»¡è¶³å…¶å½¢å¼åŒ–è§„èŒƒã€‚",
    "task_description_quote": "Formal program verification uses mathematically rigorous methods to prove that a program satisfies its specifications.",
    "dimension": "ç¨‹åºéªŒè¯çš„è‡ªåŠ¨åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯è¯æ˜ç”Ÿæˆçš„æœ‰æ•ˆæ€§ã€‚",
    "dimension_quote": "We thoroughly evaluate our approach on existing mathematical lemmas [51], as well as lemmas systematically extracted from SV-COMP programs [7]. The SV-COMP programsâ€™ lemmas capture intricate code logic and properties, which are more representative of program verification.",
    "evaluation_method": "é€šè¿‡è¯æ˜æˆåŠŸç‡ï¼ˆå³æˆåŠŸè¯æ˜çš„å¼•ç†æ¯”ä¾‹ï¼‰è¿›è¡Œè¯„ä¼°ã€‚",
    "evaluation_method_quote": "Evaluation shows that AutoRocq significantly outperforms state-of-the-art approaches. Specifically, AutoRocq is capable of proving 51.1% mathematical lemmas and 30.9% program lemmas, exceeding baseline approaches by 20.8% to 343.0% on mathematical lemmas, and by 42.4% to 204.6% on program lemmas.",
    "context_dependency": NaN,
    "context_dependency_quote": NaN,
    "problem_domain": "è½¯ä»¶éªŒè¯ã€å½¢å¼åŒ–æ–¹æ³•ã€å®šç†è¯æ˜ã€‚",
    "problem_domain_quote": "We showcase the feasibility of automatic and end-to-end verification with LLM agents. AutoRocq is evaluated on the widely used SV-COMP programs in software verification, as well as Linux kernel modules.",
    "problem_difficulty": "ä»£è¡¨çœŸå®ä¸–ç•Œç¨‹åºéªŒè¯çš„å¤æ‚é€»è¾‘å’Œå±æ€§ï¼Œéš¾åº¦è¾ƒé«˜ã€‚",
    "problem_difficulty_quote": "The SV-COMP programsâ€™ lemmas capture intricate code logic and properties, which are more representative of program verification.",
    "language": NaN,
    "language_quote": NaN,
    "data_size": NaN,
    "data_size_quote": NaN,
    "source_type": "ä»SV-COMPç¨‹åºå’ŒLinuxå†…æ ¸æ¨¡å—ä¸­ç³»ç»Ÿæå–çš„å¼•ç†ã€‚",
    "source_type_quote": "We thoroughly evaluate our approach on existing mathematical lemmas [51], as well as lemmas systematically extracted from SV-COMP programs [7].",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": NaN,
    "build_type_quote": NaN,
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "è¯æ˜ç”Ÿæˆï¼ˆç”Ÿæˆè¯æ˜è„šæœ¬ä»¥å±¥è¡Œè¯æ˜ä¹‰åŠ¡ï¼‰ã€‚",
    "task_granularity_quote": "Proof generation aims to automatically synthesize proof scripts that discharge the given proof obligations, which closes the last mile of program verification.",
    "evaluation_metrics": "è¯æ˜æˆåŠŸç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰ã€‚",
    "evaluation_metrics_quote": "AutoRocq is capable of proving 51.1% mathematical lemmas and 30.9% program lemmas...",
    "input_modality": "å½¢å¼åŒ–çš„è¯æ˜ä¹‰åŠ¡ï¼ˆé€»è¾‘å…¬å¼ï¼‰ã€‚",
    "input_modality_quote": "Proof generation aims to automatically synthesize proof scripts that discharge the given proof obligations...",
    "output_modality": "è¯æ˜è„šæœ¬ï¼ˆRocq/Coqæˆ˜æœ¯åºåˆ—ï¼‰ã€‚",
    "output_modality_quote": "Proof generation aims to automatically synthesize proof scripts that discharge the given proof obligations...",
    "task_io_type": "é€»è¾‘å…¬å¼åˆ°è¯æ˜è„šæœ¬ã€‚",
    "task_io_type_quote": "Proof generation aims to automatically synthesize proof scripts that discharge the given proof obligations...",
    "execution_environment": "Rocqï¼ˆåŸCoqï¼‰å®šç†è¯æ˜å™¨ç¯å¢ƒã€‚",
    "execution_environment_quote": "The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback.",
    "unique_features": "SV-COMPæ˜¯è½¯ä»¶éªŒè¯ç«èµ›çš„åŸºå‡†ï¼Œå…¶ç¨‹åºå¼•ç†æ•è·äº†å¤æ‚çš„ä»£ç é€»è¾‘å’Œå±æ€§ï¼Œæ›´èƒ½ä»£è¡¨çœŸå®çš„ç¨‹åºéªŒè¯åœºæ™¯ã€‚",
    "unique_features_quote": "The SV-COMP programsâ€™ lemmas capture intricate code logic and properties, which are more representative of program verification.",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "[]",
    "dimension_normalized": "['ç¨‹åºéªŒè¯çš„è‡ªåŠ¨åŒ–èƒ½åŠ›', 'è¯æ˜ç”Ÿæˆçš„æœ‰æ•ˆæ€§']",
    "evaluation_method_normalized": "['è¯æ˜æˆåŠŸç‡ï¼ˆç™¾åˆ†æ¯”ï¼‰']",
    "problem_domain_normalized": "['è½¯ä»¶éªŒè¯', 'å½¢å¼åŒ–æ–¹æ³•', 'å®šç†è¯æ˜']",
    "source_type_normalized": "['ä»SV-COMPç¨‹åºå’ŒLinuxå†…æ ¸æ¨¡å—ä¸­ç³»ç»Ÿæå–çš„å¼•ç†']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "æœªçŸ¥",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2512.05073_output/content.md",
    "benchmark_name": "Comprehensive Verilog Design Problems (CVDP)",
    "benchmark_name_quote": "The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "Our work tests this by evaluating Small language models (SLMs) coupled with a curated agentic AI framework on NVIDIAâ€™s Comprehensive verilog design problem (CVDP) benchmark.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "ä»è‡ªç„¶è¯­è¨€è§„èŒƒç”Ÿæˆç»è¿‡éªŒè¯çš„å¯„å­˜å™¨ä¼ è¾“çº§ï¼ˆRTLï¼‰ç¡¬ä»¶è®¾è®¡å®ç°ã€‚",
    "task_description_quote": "Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites. Derived from production IP blocks, it represents realistic complexity.",
    "dimension": "ç¡¬ä»¶è®¾è®¡è‡ªåŠ¨åŒ–èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯Verilogä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§ã€‚",
    "dimension_quote": "provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",
    "evaluation_method": "ä½¿ç”¨CocoTBæµ‹è¯•å¥—ä»¶è¯„ä¼°åŠŸèƒ½æ­£ç¡®æ€§ï¼ˆpass rateï¼‰ã€‚",
    "evaluation_method_quote": "Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",
    "context_dependency": "æ¨¡å—çº§è®¾è®¡ï¼ŒåŒ…å«æ¥å£è§„èŒƒã€åŠŸèƒ½éœ€æ±‚å’Œæµ‹è¯•å¥—ä»¶ã€‚",
    "context_dependency_quote": "Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",
    "problem_domain": "ç¡¬ä»¶è®¾è®¡ï¼Œå…·ä½“åŒ…æ‹¬ç®—æœ¯è¿ç®—ã€æ§åˆ¶é€»è¾‘ã€å†…å­˜ç³»ç»Ÿå’Œå…¶ä»–è®¾è®¡ã€‚",
    "problem_domain_quote": "336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",
    "problem_difficulty": "ä»£è¡¨å®é™…ç”Ÿäº§IPå—çš„å¤æ‚æ€§ï¼Œå…·æœ‰æŒ‘æˆ˜æ€§ï¼ˆæœ€å…ˆè¿›æ¨¡å‹å•æ¬¡é€šè¿‡ç‡ä»…26.5%ï¼‰ã€‚",
    "problem_difficulty_quote": "Derived from production IP blocks, it represents realistic complexity. State-of-the-art achieves only 26.5% pass rate (GPT-4o-mini, single-shot), highlighting substantial improvement opportunity.",
    "language": "Verilogï¼ˆç¡¬ä»¶æè¿°è¯­è¨€ï¼‰ã€‚",
    "language_quote": "The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA...",
    "data_size": "åŒ…å«336ä¸ªé—®é¢˜ã€‚",
    "data_size_quote": "provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",
    "source_type": "æºè‡ªç”Ÿäº§IPå—ï¼Œä»£è¡¨å®é™…å¤æ‚æ€§ã€‚",
    "source_type_quote": "Derived from production IP blocks, it represents realistic complexity.",
    "last_updated": NaN,
    "last_updated_quote": NaN,
    "build_type": "ç”±NVIDIAå¼€å‘ã€‚",
    "build_type_quote": "The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA...",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç”Ÿæˆï¼ˆä»è§„èŒƒç”ŸæˆVerilog RTLä»£ç ï¼‰ã€‚",
    "task_granularity_quote": "transforms design intent from the CVDP dataset into verified register transfer level (RTL) implementations.",
    "evaluation_metrics": "é€šè¿‡ç‡ï¼ˆpass rateï¼‰ã€‚",
    "evaluation_metrics_quote": "State-of-the-art achieves only 26.5% pass rate (GPT-4o-mini, single-shot)...",
    "input_modality": "è‡ªç„¶è¯­è¨€è§„èŒƒã€æ¨¡å—æ¥å£ã€åŠŸèƒ½éœ€æ±‚ã€‚",
    "input_modality_quote": "Each includes natural language specification, module interface, functional requirements...",
    "output_modality": "Verilog RTLä»£ç ã€‚",
    "output_modality_quote": "transforms design intent from the CVDP dataset into verified register transfer level (RTL) implementations.",
    "task_io_type": "æ–‡æœ¬åˆ°ä»£ç ï¼ˆè‡ªç„¶è¯­è¨€è§„èŒƒåˆ°Verilogä»£ç ï¼‰ã€‚",
    "task_io_type_quote": "Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",
    "execution_environment": "ä½¿ç”¨CocoTBæµ‹è¯•å¥—ä»¶è¿›è¡ŒéªŒè¯ã€‚",
    "execution_environment_quote": "Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",
    "unique_features": "ä¸“æ³¨äºç¡¬ä»¶è®¾è®¡ï¼ˆVerilogï¼‰ï¼Œé—®é¢˜æºè‡ªå®é™…ç”Ÿäº§IPå—ï¼ŒåŒ…å«å®Œæ•´çš„æµ‹è¯•å¥—ä»¶ï¼ˆCocoTBï¼‰ç”¨äºåŠŸèƒ½éªŒè¯ã€‚",
    "unique_features_quote": "Derived from production IP blocks, it represents realistic complexity. Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",
    "data_size_quantity": 336,
    "data_size_unit": "ä¸ªé—®é¢˜",
    "last_updated_year": null,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Verilog']",
    "dimension_normalized": "['ç¡¬ä»¶è®¾è®¡è‡ªåŠ¨åŒ–èƒ½åŠ›', 'Verilogä»£ç ç”Ÿæˆçš„åŠŸèƒ½æ­£ç¡®æ€§']",
    "evaluation_method_normalized": "['é€šè¿‡ç‡']",
    "problem_domain_normalized": "['ç¡¬ä»¶è®¾è®¡', 'ç®—æœ¯è¿ç®—', 'æ§åˆ¶é€»è¾‘', 'å†…å­˜ç³»ç»Ÿ', 'å…¶ä»–è®¾è®¡']",
    "source_type_normalized": "['ç”Ÿäº§IPå—', 'å®é™…å¤æ‚æ€§']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "æ–‡æœ¬åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  }
]