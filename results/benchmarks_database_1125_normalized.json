[
  {
    "source_paper": "2511.20403_output/content.md",
    "benchmark_name": "CLASSES2TEST",
    "benchmark_name_quote": "We introduce the CLASSES2TEST dataset, which maps Java classes under test to their corresponding test classes",
    "is_original_proposal": "Yesï¼Œæœ¬æ–‡æ˜¯è¯¥æ•°æ®é›†çš„åŸå§‹å‘å¸ƒè®ºæ–‡",
    "is_original_proposal_quote": "We introduce the CLASSES2TEST dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics",
    "dataset_url": "https://anonymous.4open.science/r/classes2test",
    "dataset_url_quote": "1https://anonymous.4open.science/r/classes2test",
    "task_description": "ç”¨äºè¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„Javaå•å…ƒæµ‹è¯•çš„è´¨é‡ï¼Œæ”¯æŒç ”ç©¶äººå‘˜å’Œå¼€å‘è€…æ¯”è¾ƒä¸åŒLLMå’Œæç¤ºç­–ç•¥",
    "task_description_quote": "AGONETEST does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions.",
    "dimension": "å•å…ƒæµ‹è¯•è´¨é‡è¯„ä¼°ï¼ŒåŒ…æ‹¬ç¼–è¯‘æˆåŠŸç‡ã€ä»£ç è¦†ç›–ç‡ã€ç¼ºé™·æ£€æµ‹èƒ½åŠ›ã€æµ‹è¯•å¼‚å‘³ç­‰",
    "dimension_quote": "a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",
    "evaluation_method": "é›†æˆé«˜çº§è¯„ä¼°æŒ‡æ ‡ï¼Œå¦‚å˜å¼‚åˆ†æ•°å’Œæµ‹è¯•å¼‚å‘³ï¼Œè¿›è¡Œç»¼åˆè¯„ä¼°",
    "evaluation_method_quote": "a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",
    "context_dependency": "ç±»çº§åˆ«æµ‹è¯•ï¼Œæ¶µç›–æ–¹æ³•äº¤äº’å’Œå…±äº«çŠ¶æ€",
    "context_dependency_quote": "Our approach focuses on class-level test code evaluation, which is closer to real-world practices as it covers method interactions and shared state",
    "problem_domain": "è½¯ä»¶æµ‹è¯•ï¼ŒJavaå•å…ƒæµ‹è¯•ç”Ÿæˆ",
    "problem_domain_quote": "Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly.",
    "problem_difficulty": "ç°å®ä¸–ç•Œè½¯ä»¶é¡¹ç›®çº§åˆ«ï¼Œæ¯”å•æ–¹æ³•æµ‹è¯•æ›´å¤æ‚",
    "problem_difficulty_quote": "This extended dataset makes it possible to assess the test performance of an LLM on a more complex scope (the entire class) than the single method.",
    "language": "Java",
    "language_quote": "An annotated open source Java project dataset extending METHODS2TEST [9]",
    "data_size": "åŸºäº9,410ä¸ªGitHubä»“åº“çš„æ•°æ®é›†",
    "data_size_quote": "AGONETEST offers far broader applicability by using a dataset of 9,410 GitHub repositories",
    "source_type": "æ‰©å±•è‡ªMETHODS2TESTæ•°æ®é›†çš„Javaå¼€æºé¡¹ç›®",
    "source_type_quote": "An annotated open source Java project dataset extending METHODS2TEST [9]",
    "last_updated": 2025,
    "last_updated_quote": "arXiv:2511.20403v1  [cs.SE]  25 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼ŒåŸºäºç°æœ‰æ•°æ®é›†æ‰©å±•",
    "build_type_quote": "Leveraging the METHODS2TEST dataset [9], we developed a new dataset specifically aimed at comparing human-written tests with those produced by LLMs.",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ç±»çº§åˆ«å•å…ƒæµ‹è¯•ç”Ÿæˆ",
    "task_granularity_quote": "Our approach focuses on class-level test code evaluation, which is closer to real-world practices as it covers method interactions and shared state",
    "evaluation_metrics": "å˜å¼‚åˆ†æ•°ã€æµ‹è¯•å¼‚å‘³ã€ä»£ç è¦†ç›–ç‡",
    "evaluation_metrics_quote": "a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",
    "input_modality": "Javaç±»ä»£ç ",
    "input_modality_quote": "which maps classes under test to their related test classes",
    "output_modality": "å•å…ƒæµ‹è¯•ä»£ç ",
    "output_modality_quote": "for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "which maps Java classes under test to their corresponding test classes",
    "execution_environment": "æ”¯æŒæ‰€æœ‰Java LTSç‰ˆæœ¬çš„é¡¹ç›®ç¯å¢ƒ",
    "execution_environment_quote": "AGONETEST overcomes this barrier by supporting all Java LTS versions.",
    "unique_features": "ä¸“æ³¨äºç±»çº§åˆ«æµ‹è¯•è¯„ä¼°ï¼Œæ”¯æŒå¤šç§LLMå’Œæç¤ºç­–ç•¥çš„æ¯”è¾ƒï¼Œæä¾›ç«¯åˆ°ç«¯çš„è‡ªåŠ¨åŒ–è¯„ä¼°æµæ°´çº¿",
    "unique_features_quote": "AGONETEST shifts the focus to the generation of class-level tests. Our approach makes it possible to use up-to-date LLMs and not constrain prompt design (our prompts can be customized), thereby handling more complex, real-world scenarios.",
    "data_size_quantity": 9410,
    "data_size_unit": "ä¸ªGitHubä»“åº“",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Java']",
    "dimension_normalized": "['å•å…ƒæµ‹è¯•è´¨é‡è¯„ä¼°', 'ç¼–è¯‘æˆåŠŸç‡', 'ä»£ç è¦†ç›–ç‡', 'ç¼ºé™·æ£€æµ‹èƒ½åŠ›', 'æµ‹è¯•å¼‚å‘³']",
    "evaluation_method_normalized": "['å˜å¼‚åˆ†æ•°', 'æµ‹è¯•å¼‚å‘³', 'ä»£ç è¦†ç›–ç‡']",
    "problem_domain_normalized": "['è½¯ä»¶æµ‹è¯•', 'Javaå•å…ƒæµ‹è¯•ç”Ÿæˆ']",
    "source_type_normalized": "['æ‰©å±•è‡ªMETHODS2TESTæ•°æ®é›†çš„Javaå¼€æºé¡¹ç›®']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•æ–‡ä»¶",
    "task_granularity_normalized": "ä»£ç ç”Ÿæˆ",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.21380_output/content.md",
    "benchmark_name": "ROCODE, LogHub2.0",
    "benchmark_name_quote": "Two projects (i.e., ROCODE [15] and LogHub2.0 [16]) are selected for the following experiments.",
    "is_original_proposal": "No, æœ¬æ–‡æ˜¯ä½¿ç”¨è¯¥æ•°æ®é›†è¿›è¡Œè¯„æµ‹",
    "is_original_proposal_quote": "We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0.",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "æ•°æ®é›†é€‚åº”ä»»åŠ¡ - å°†è½¯ä»¶å·¥ç¨‹ç ”ç©¶å·¥å…·è‡ªåŠ¨é€‚é…åˆ°æ–°çš„æ•°æ®é›†ï¼Œæ„å»ºå¯è¿è¡Œçš„å®éªŒå¹¶è·å–æ‰§è¡Œç»“æœ",
    "task_description_quote": "Our objective is to automatically modify ğ‘…ğ·or ğ‘…ğ‘‡, construct a runnable experiment, and obtain its execution results.",
    "dimension": "å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ•°æ®é›†é€‚åº”ä»»åŠ¡ä¸­çš„èƒ½åŠ›è¯„ä¼°",
    "dimension_quote": "This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks.",
    "evaluation_method": "äº”é˜¶æ®µè¯„ä¼°æµç¨‹ï¼šæ–‡ä»¶ç†è§£ã€ä»£ç ç¼–è¾‘ã€å‘½ä»¤ç”Ÿæˆã€éªŒè¯å’Œæœ€ç»ˆæ‰§è¡Œï¼Œæµ‹é‡æˆåŠŸç‡å¹¶åˆ†æå¤±è´¥æ¨¡å¼",
    "evaluation_method_quote": "Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance.",
    "context_dependency": "å¤šæ–‡ä»¶é¡¹ç›®ç¯å¢ƒï¼Œéœ€è¦ç†è§£æ•´ä¸ªä»£ç ä»“åº“çš„æ¶æ„",
    "context_dependency_quote": "Before modifying code or generating commands for a repository, a multi-agent system must browse the essential files in a repository to understand its architecture.",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ç ”ç©¶ï¼ŒåŒ…æ‹¬ä»£ç ç”Ÿæˆã€bugä¿®å¤ã€æ€§èƒ½åˆ†æå’Œå®‰å…¨ç­‰é¢†åŸŸ",
    "problem_domain_quote": "In areas ranging from code generation and bug fixing to performance analysis and security, researchers are constantly proposing new techniques",
    "problem_difficulty": "å¤æ‚è½¯ä»¶å·¥ç¨‹ä»»åŠ¡ï¼Œéœ€è¦å¤šæ­¥éª¤åè°ƒå’Œè¿­ä»£ä¿®å¤",
    "problem_difficulty_quote": "Most of the evaluated multi-agent systems failed to complete the assigned tasks. Under such circumstances, nearly all results would be classified as failures",
    "language": "Python",
    "language_quote": "To ensure a consistent and manageable experimental environment, we retained only artifacts implemented in Python.",
    "data_size": "ä»é¡¶çº§è½¯ä»¶å·¥ç¨‹ä¼šè®®ï¼ˆFSE, ICSE, ASE, ISSTAï¼‰2024-2025å¹´æ¥å—çš„è®ºæ–‡ä¸­ç­›é€‰çš„å¯é‡ç”¨ç ”ç©¶æ„ä»¶",
    "data_size_quote": "We began by collecting all papers accepted by the top four SE (FSE, ICSE, ASE and ISSTA) in 2024-2025 that were either (i) awarded a Reusable Artifact badge or (ii) explicitly identified by the authors as providing reusable artifacts.",
    "source_type": "å­¦æœ¯ç ”ç©¶æ„ä»¶ï¼Œæ¥è‡ªé¡¶çº§è½¯ä»¶å·¥ç¨‹ä¼šè®®çš„å¯é‡ç”¨ç ”ç©¶é¡¹ç›®",
    "source_type_quote": "To construct a representative set of high-quality SE research artifacts, we followed a systematic multi-stage selection process",
    "last_updated": 2025,
    "last_updated_quote": "arXiv:2511.21380v1  [cs.SE]  26 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºçš„ç ”ç©¶æ„ä»¶é›†åˆ",
    "build_type_quote": "We began by collecting all papers accepted by the top four SE (FSE, ICSE, ASE and ISSTA) in 2024-2025",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "ä»£ç ç¼–è¾‘ã€æ–‡ä»¶åˆ›å»ºã€å‘½ä»¤ç”Ÿæˆã€éªŒè¯ä¿®å¤",
    "task_granularity_quote": "Edit and create necessary files... Generate and execute the commands... Validating and repairing",
    "evaluation_metrics": "æˆåŠŸç‡ã€ç»“æ„ç›¸ä¼¼åº¦ï¼ˆä»7.25%åˆ°67.14%ï¼‰ã€å®ŒæˆçŠ¶æ€è®°å½•",
    "evaluation_metrics_quote": "Results show that... substantially improve structural similarity to ground truth (from 7.25% to 67.14%)",
    "input_modality": "ä»£ç ä»“åº“ã€è‡ªç„¶è¯­è¨€æç¤º",
    "input_modality_quote": "Each multi-agent system is provided with a processed repository... along with a simple prompt that specifies the adaptation task",
    "output_modality": "ä¿®æ”¹åçš„ä»£ç ã€ç”Ÿæˆçš„è„šæœ¬å‘½ä»¤ã€æ‰§è¡Œç»“æœ",
    "output_modality_quote": "the code adaptations performed to ensure compatibility, the executable scripts or commands derived for running the experiment, and the results produced by executing those scripts or commands",
    "task_io_type": "ä»£ç åˆ°ä»£ç çš„é€‚åº”ä»»åŠ¡",
    "task_io_type_quote": "automatically modify ğ‘…ğ·or ğ‘…ğ‘‡, construct a runnable experiment, and obtain its execution results",
    "execution_environment": "Pythonç¯å¢ƒï¼Œéœ€è¦å¯é çš„éƒ¨ç½²ç¯å¢ƒ",
    "execution_environment_quote": "This choice allows for reliable environment deployment and isolates our investigation from environment setup challenges",
    "unique_features": "ä¸“æ³¨äºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ•°æ®é›†é€‚åº”ä»»åŠ¡ä¸­çš„è¡¨ç°è¯„ä¼°ï¼Œé‡‡ç”¨äº”é˜¶æ®µè¯„ä¼°æµç¨‹ï¼Œç ”ç©¶æç¤ºçº§å¹²é¢„å¯¹æ€§èƒ½çš„å½±å“",
    "unique_features_quote": "This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. Through a five-stage evaluation pipeline... we measure success rates, analyze failure patterns, and assess prompt-based interventions",
    "data_size_quantity": null,
    "data_size_unit": null,
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨æ•°æ®é›†é€‚åº”ä»»åŠ¡ä¸­çš„èƒ½åŠ›è¯„ä¼°']",
    "evaluation_method_normalized": "['æˆåŠŸç‡', 'ç»“æ„ç›¸ä¼¼åº¦', 'å®ŒæˆçŠ¶æ€è®°å½•']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹ç ”ç©¶', 'ä»£ç ç”Ÿæˆ', 'bugä¿®å¤', 'æ€§èƒ½åˆ†æ', 'å®‰å…¨']",
    "source_type_normalized": "['å­¦æœ¯ç ”ç©¶æ„ä»¶', 'é¡¶çº§è½¯ä»¶å·¥ç¨‹ä¼šè®®', 'å¯é‡ç”¨ç ”ç©¶é¡¹ç›®']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å¤šæ–‡ä»¶/é¡¹ç›®çº§",
    "task_granularity_normalized": "æœªçŸ¥",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "éœ€è¦ç‰¹å®šä¾èµ–",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  },
  {
    "source_paper": "2511.21022_output/content.md",
    "benchmark_name": "EDAPIBench",
    "benchmark_name_quote": "We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances.",
    "is_original_proposal": "Yesï¼Œæœ¬æ–‡æ˜¯è¯¥æ•°æ®é›†çš„åŸå§‹å‘å¸ƒè®ºæ–‡",
    "is_original_proposal_quote": "We introduce EDAPIBench, a dedicated benchmark... We construct EDAPIBenchâ€”the first dedicated benchmark for evaluating deprecated API knowledge editing in LLMs",
    "dataset_url": NaN,
    "dataset_url_quote": NaN,
    "task_description": "è¯„ä¼°å¤§è¯­è¨€æ¨¡å‹ä¸­åºŸå¼ƒAPIçŸ¥è¯†ç¼–è¾‘çš„æ€§èƒ½ï¼Œä¸“é—¨ç”¨äºæµ‹è¯•æ¨¡å‹ç¼–è¾‘æŠ€æœ¯èƒ½å¦æœ‰æ•ˆæ›´æ–°åºŸå¼ƒAPIçŸ¥è¯†å¹¶ç”Ÿæˆæœ€æ–°çš„API",
    "task_description_quote": "a dedicated benchmark for evaluating deprecated API knowledge editing in LLMs... whether existing model editing methods can effectively update deprecated API knowledge within LLMs, and enable the edited models to correctly replace deprecated APIs with up-to-date ones",
    "dimension": "æœ‰æ•ˆæ€§ã€æ³›åŒ–æ€§ã€å¯ç§»æ¤æ€§ã€ç‰¹å¼‚æ€§",
    "dimension_quote": "comprehensively assess model editing performance across four key dimensions: Effectiveness, Generalization, Portability, Specificity",
    "evaluation_method": "åŸºäºå››ä¸ªç»´åº¦çš„è¯„ä¼°ï¼šæœ‰æ•ˆæ€§ï¼ˆç¼–è¾‘åæ¨¡å‹æ˜¯å¦ç”Ÿæˆæœ€æ–°APIï¼‰ã€æ³›åŒ–æ€§ï¼ˆè¯­ä¹‰ç­‰ä»·ä½†è¯­æ³•å˜åŒ–çš„è¾“å…¥ï¼‰ã€å¯ç§»æ¤æ€§ï¼ˆä¸åŒè¾“å…¥ä½†æ¶‰åŠç›¸åŒåºŸå¼ƒAPIï¼‰ã€ç‰¹å¼‚æ€§ï¼ˆä¿æŒä¸ç¼–è¾‘ä»»åŠ¡æ— å…³è¾“å…¥çš„è¡Œä¸ºä¸€è‡´æ€§ï¼‰",
    "evaluation_method_quote": "Effectiveness: Measuring whether the edited model generates the up-to-date API for the original inputs; Generalization: Measuring whether it generates the up-to-date API on semantically equivalent but syntactically varied inputs; Portability: Measuring whether it generates the up-to-date API across different inputs... Specificity: Measuring whether it preserves consistent pre-editing behavior on inputs unrelated to the editing task",
    "context_dependency": "å•å‡½æ•°çº§åˆ«çš„ä»£ç è¡¥å…¨ä¸Šä¸‹æ–‡",
    "context_dependency_quote": "Our study frames the model editing scenario as a code completion task... we extract lines preceding the API invocation as candidate editing inputs (to prompt LLM API completion) and the invocation line as the target API (ground truth)",
    "problem_domain": "è½¯ä»¶å·¥ç¨‹ã€APIç»´æŠ¤ã€ç¬¬ä¸‰æ–¹åº“æ›´æ–°",
    "problem_domain_quote": "deprecated APIs from 8 popular Python libraries... covering eight popular Python libraries such as PyTorch and TensorFlow",
    "problem_difficulty": "å®é™…å·¥ç¨‹çº§éš¾åº¦ï¼Œæ¶‰åŠçœŸå®ä¸–ç•ŒåºŸå¼ƒAPIçš„è¯†åˆ«å’Œæ›´æ–°",
    "problem_difficulty_quote": "Using these mappings, we extract 65,596 real-world functions from GitHub that call the up-to-date APIs",
    "language": "Python",
    "language_quote": "deprecated APIs from 8 popular Python libraries... covering eight popular Python libraries such as PyTorch and TensorFlow",
    "data_size": "åŒ…å«70å¤šä¸ªåºŸå¼ƒAPIï¼Œè¶…è¿‡3000ä¸ªç¼–è¾‘å®ä¾‹ï¼Œä»GitHubæå–äº†65,596ä¸ªçœŸå®ä¸–ç•Œå‡½æ•°",
    "data_size_quote": "featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances... we extract 65,596 real-world functions from GitHub",
    "source_type": "ä»GitHubçœŸå®é¡¹ç›®ä»£ç ä¸­æå–ï¼ŒåŸºäºå·²éªŒè¯çš„APIæ˜ å°„å…³ç³»",
    "source_type_quote": "We begin with 145 verified API mappings (deprecated â†’up-to-date) from Wang et al. [49]... Using these mappings, we extract 65,596 real-world functions from GitHub that call the up-to-date APIs",
    "last_updated": "2025-2026",
    "last_updated_quote": "Publication date: November 2026. arXiv:2511.21022v1 [cs.SE] 26 Nov 2025",
    "build_type": "å®˜æ–¹è‡ªå»ºï¼Œå…¨è‡ªåŠ¨æ„å»º",
    "build_type_quote": "which can be fully automatically constructed... with fully automated construction",
    "contamination_status": NaN,
    "contamination_status_quote": NaN,
    "dataset_license": NaN,
    "dataset_license_quote": NaN,
    "task_granularity": "APIçº§åˆ«çš„ä»£ç è¡¥å…¨",
    "task_granularity_quote": "code completion tasks... during code completion, LLMs frequently suggest deprecated API invocations",
    "evaluation_metrics": "åŸºäºå››ä¸ªç»´åº¦çš„å®šæ€§è¯„ä¼°ï¼šæœ‰æ•ˆæ€§ã€æ³›åŒ–æ€§ã€å¯ç§»æ¤æ€§ã€ç‰¹å¼‚æ€§",
    "evaluation_metrics_quote": "Effectiveness, Generalization, Portability, Specificity",
    "input_modality": "ä»£ç ç‰‡æ®µ",
    "input_modality_quote": "the editing input (a code snippet to be completed)",
    "output_modality": "APIè°ƒç”¨ä»£ç ",
    "output_modality_quote": "the specific correct, up-to-date API that should replace the deprecated one",
    "task_io_type": "ä»£ç åˆ°ä»£ç ",
    "task_io_type_quote": "code completion task... generating code",
    "execution_environment": NaN,
    "execution_environment_quote": NaN,
    "unique_features": "é¦–ä¸ªä¸“é—¨é’ˆå¯¹åºŸå¼ƒAPIçŸ¥è¯†ç¼–è¾‘çš„åŸºå‡†æµ‹è¯•ï¼Œæ”¯æŒå…¨è‡ªåŠ¨æ„å»ºï¼ŒåŒ…å«å››ä¸ªç»´åº¦çš„ç»¼åˆè¯„ä¼°",
    "unique_features_quote": "the first dedicated benchmark for evaluating deprecated API knowledge editing in LLMs, with fully automated construction, serving as a standardized, rigorous platform",
    "data_size_quantity": 65596,
    "data_size_unit": "ä¸ªå‡½æ•°",
    "last_updated_year": 2025,
    "last_updated_month": null,
    "last_updated_day": null,
    "language_normalized": "['Python']",
    "dimension_normalized": "['æœ‰æ•ˆæ€§', 'æ³›åŒ–æ€§', 'å¯ç§»æ¤æ€§', 'ç‰¹å¼‚æ€§']",
    "evaluation_method_normalized": "['æœ‰æ•ˆæ€§', 'æ³›åŒ–æ€§', 'å¯ç§»æ¤æ€§', 'ç‰¹å¼‚æ€§']",
    "problem_domain_normalized": "['è½¯ä»¶å·¥ç¨‹', 'APIç»´æŠ¤', 'ç¬¬ä¸‰æ–¹åº“æ›´æ–°']",
    "source_type_normalized": "['ä»GitHubçœŸå®é¡¹ç›®ä»£ç ä¸­æå–', 'åŸºäºå·²éªŒè¯çš„APIæ˜ å°„å…³ç³»']",
    "problem_difficulty_normalized": "å·¥ç¨‹çº§",
    "context_dependency_normalized": "å•å‡½æ•°",
    "task_granularity_normalized": "ä»£ç è¡¥å…¨",
    "input_modality_normalized": "ä»£ç ä¸è‡ªç„¶è¯­è¨€",
    "output_modality_normalized": "ä»£ç ",
    "task_io_type_normalized": "ä»£ç åˆ°ä»£ç ",
    "execution_environment_normalized": "æœªçŸ¥",
    "dataset_license_normalized": "æœªçŸ¥",
    "contamination_status_normalized": "æœªçŸ¥"
  }
]