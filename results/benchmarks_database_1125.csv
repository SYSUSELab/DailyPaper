source_paper,benchmark_name,benchmark_name_quote,is_original_proposal,is_original_proposal_quote,dataset_url,dataset_url_quote,task_description,task_description_quote,dimension,dimension_quote,evaluation_method,evaluation_method_quote,context_dependency,context_dependency_quote,problem_domain,problem_domain_quote,problem_difficulty,problem_difficulty_quote,language,language_quote,data_size,data_size_quote,source_type,source_type_quote,last_updated,last_updated_quote,build_type,build_type_quote,contamination_status,contamination_status_quote,dataset_license,dataset_license_quote,task_granularity,task_granularity_quote,evaluation_metrics,evaluation_metrics_quote,input_modality,input_modality_quote,output_modality,output_modality_quote,task_io_type,task_io_type_quote,execution_environment,execution_environment_quote,unique_features,unique_features_quote
2511.20403_output/content.md,CLASSES2TEST,"We introduce the CLASSES2TEST dataset, which maps Java classes under test to their corresponding test classes",Yes，本文是该数据集的原始发布论文,"We introduce the CLASSES2TEST dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics",https://anonymous.4open.science/r/classes2test,1https://anonymous.4open.science/r/classes2test,用于评估大型语言模型生成的Java单元测试的质量，支持研究人员和开发者比较不同LLM和提示策略,"AGONETEST does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions.",单元测试质量评估，包括编译成功率、代码覆盖率、缺陷检测能力、测试异味等,"a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",集成高级评估指标，如变异分数和测试异味，进行综合评估,"a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",类级别测试，涵盖方法交互和共享状态,"Our approach focuses on class-level test code evaluation, which is closer to real-world practices as it covers method interactions and shared state",软件测试，Java单元测试生成,"Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly.",现实世界软件项目级别，比单方法测试更复杂,This extended dataset makes it possible to assess the test performance of an LLM on a more complex scope (the entire class) than the single method.,Java,An annotated open source Java project dataset extending METHODS2TEST [9],"基于9,410个GitHub仓库的数据集","AGONETEST offers far broader applicability by using a dataset of 9,410 GitHub repositories",扩展自METHODS2TEST数据集的Java开源项目,An annotated open source Java project dataset extending METHODS2TEST [9],2025,arXiv:2511.20403v1  [cs.SE]  25 Nov 2025,官方自建，基于现有数据集扩展,"Leveraging the METHODS2TEST dataset [9], we developed a new dataset specifically aimed at comparing human-written tests with those produced by LLMs.",,,,,类级别单元测试生成,"Our approach focuses on class-level test code evaluation, which is closer to real-world practices as it covers method interactions and shared state",变异分数、测试异味、代码覆盖率,"a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",Java类代码,which maps classes under test to their related test classes,单元测试代码,"for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection",代码到代码,which maps Java classes under test to their corresponding test classes,支持所有Java LTS版本的项目环境,AGONETEST overcomes this barrier by supporting all Java LTS versions.,专注于类级别测试评估，支持多种LLM和提示策略的比较，提供端到端的自动化评估流水线,"AGONETEST shifts the focus to the generation of class-level tests. Our approach makes it possible to use up-to-date LLMs and not constrain prompt design (our prompts can be customized), thereby handling more complex, real-world scenarios."
2511.21380_output/content.md,"ROCODE, LogHub2.0","Two projects (i.e., ROCODE [15] and LogHub2.0 [16]) are selected for the following experiments.","No, 本文是使用该数据集进行评测","We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0.",,,数据集适应任务 - 将软件工程研究工具自动适配到新的数据集，构建可运行的实验并获取执行结果,"Our objective is to automatically modify 𝑅𝐷or 𝑅𝑇, construct a runnable experiment, and obtain its execution results.",多智能体系统在数据集适应任务中的能力评估,This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks.,五阶段评估流程：文件理解、代码编辑、命令生成、验证和最终执行，测量成功率并分析失败模式,"Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance.",多文件项目环境，需要理解整个代码仓库的架构,"Before modifying code or generating commands for a repository, a multi-agent system must browse the essential files in a repository to understand its architecture.",软件工程研究，包括代码生成、bug修复、性能分析和安全等领域,"In areas ranging from code generation and bug fixing to performance analysis and security, researchers are constantly proposing new techniques",复杂软件工程任务，需要多步骤协调和迭代修复,"Most of the evaluated multi-agent systems failed to complete the assigned tasks. Under such circumstances, nearly all results would be classified as failures",Python,"To ensure a consistent and manageable experimental environment, we retained only artifacts implemented in Python.","从顶级软件工程会议（FSE, ICSE, ASE, ISSTA）2024-2025年接受的论文中筛选的可重用研究构件","We began by collecting all papers accepted by the top four SE (FSE, ICSE, ASE and ISSTA) in 2024-2025 that were either (i) awarded a Reusable Artifact badge or (ii) explicitly identified by the authors as providing reusable artifacts.",学术研究构件，来自顶级软件工程会议的可重用研究项目,"To construct a representative set of high-quality SE research artifacts, we followed a systematic multi-stage selection process",2025,arXiv:2511.21380v1  [cs.SE]  26 Nov 2025,官方自建的研究构件集合,"We began by collecting all papers accepted by the top four SE (FSE, ICSE, ASE and ISSTA) in 2024-2025",,,,,代码编辑、文件创建、命令生成、验证修复,Edit and create necessary files... Generate and execute the commands... Validating and repairing,成功率、结构相似度（从7.25%到67.14%）、完成状态记录,Results show that... substantially improve structural similarity to ground truth (from 7.25% to 67.14%),代码仓库、自然语言提示,Each multi-agent system is provided with a processed repository... along with a simple prompt that specifies the adaptation task,修改后的代码、生成的脚本命令、执行结果,"the code adaptations performed to ensure compatibility, the executable scripts or commands derived for running the experiment, and the results produced by executing those scripts or commands",代码到代码的适应任务,"automatically modify 𝑅𝐷or 𝑅𝑇, construct a runnable experiment, and obtain its execution results",Python环境，需要可靠的部署环境,This choice allows for reliable environment deployment and isolates our investigation from environment setup challenges,专注于多智能体系统在数据集适应任务中的表现评估，采用五阶段评估流程，研究提示级干预对性能的影响,"This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. Through a five-stage evaluation pipeline... we measure success rates, analyze failure patterns, and assess prompt-based interventions"
2511.21022_output/content.md,EDAPIBench,"We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances.",Yes，本文是该数据集的原始发布论文,"We introduce EDAPIBench, a dedicated benchmark... We construct EDAPIBench—the first dedicated benchmark for evaluating deprecated API knowledge editing in LLMs",,,评估大语言模型中废弃API知识编辑的性能，专门用于测试模型编辑技术能否有效更新废弃API知识并生成最新的API,"a dedicated benchmark for evaluating deprecated API knowledge editing in LLMs... whether existing model editing methods can effectively update deprecated API knowledge within LLMs, and enable the edited models to correctly replace deprecated APIs with up-to-date ones",有效性、泛化性、可移植性、特异性,"comprehensively assess model editing performance across four key dimensions: Effectiveness, Generalization, Portability, Specificity",基于四个维度的评估：有效性（编辑后模型是否生成最新API）、泛化性（语义等价但语法变化的输入）、可移植性（不同输入但涉及相同废弃API）、特异性（保持与编辑任务无关输入的行为一致性）,Effectiveness: Measuring whether the edited model generates the up-to-date API for the original inputs; Generalization: Measuring whether it generates the up-to-date API on semantically equivalent but syntactically varied inputs; Portability: Measuring whether it generates the up-to-date API across different inputs... Specificity: Measuring whether it preserves consistent pre-editing behavior on inputs unrelated to the editing task,单函数级别的代码补全上下文,Our study frames the model editing scenario as a code completion task... we extract lines preceding the API invocation as candidate editing inputs (to prompt LLM API completion) and the invocation line as the target API (ground truth),软件工程、API维护、第三方库更新,deprecated APIs from 8 popular Python libraries... covering eight popular Python libraries such as PyTorch and TensorFlow,实际工程级难度，涉及真实世界废弃API的识别和更新,"Using these mappings, we extract 65,596 real-world functions from GitHub that call the up-to-date APIs",Python,deprecated APIs from 8 popular Python libraries... covering eight popular Python libraries such as PyTorch and TensorFlow,"包含70多个废弃API，超过3000个编辑实例，从GitHub提取了65,596个真实世界函数","featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances... we extract 65,596 real-world functions from GitHub",从GitHub真实项目代码中提取，基于已验证的API映射关系,"We begin with 145 verified API mappings (deprecated →up-to-date) from Wang et al. [49]... Using these mappings, we extract 65,596 real-world functions from GitHub that call the up-to-date APIs",2025-2026,Publication date: November 2026. arXiv:2511.21022v1 [cs.SE] 26 Nov 2025,官方自建，全自动构建,which can be fully automatically constructed... with fully automated construction,,,,,API级别的代码补全,"code completion tasks... during code completion, LLMs frequently suggest deprecated API invocations",基于四个维度的定性评估：有效性、泛化性、可移植性、特异性,"Effectiveness, Generalization, Portability, Specificity",代码片段,the editing input (a code snippet to be completed),API调用代码,"the specific correct, up-to-date API that should replace the deprecated one",代码到代码,code completion task... generating code,,,首个专门针对废弃API知识编辑的基准测试，支持全自动构建，包含四个维度的综合评估,"the first dedicated benchmark for evaluating deprecated API knowledge editing in LLMs, with fully automated construction, serving as a standardized, rigorous platform"
2511.19875_output/content.md,CodeFuse-CommitEval,"We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models.",Yes，本文是该数据集的原始发布论文,"We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models.",https://figshare.com/s/21fe4ec9cb960b52bffe,The dataset is publicly available at https://figshare.com/s/21fe4ec9cb960b52bffe.,"检测提交信息与代码变更之间的不一致性（Message-Code Inconsistency, MCI）",A Message-Code Inconsistency (MCI) occurs when the natural language description in a commit message does not accurately reflect the actual modifications in the associated code diff.,提交信息与代码变更的一致性检测能力,evaluate models for MCI detection,使用Recall、Precision、Specificity等指标评估模型检测结果,"Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%)",提交信息与对应的代码差异（diff）,"Each item of the verified dataset contains a commit message, the corresponding code diff, and the ground truth label",软件工程、版本控制、代码审查,Version control relies on commit messages to convey the rationale for code changes,需要语义理解和上下文推理的复杂任务,purpose inconsistencies require deeper semantic understanding and contextual reasoning,多种编程语言（基于ApacheCM数据集的多样性）,because of its diversity in programming languages and the high-quality commits,包含正负样本的平衡数据集,we generate a balanced dataset with both positive and negative samples,基于ApacheCM数据集，通过规则引导的突变生成不一致提交信息,"Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits",2025,arXiv:2511.19875v1 [cs.SE] 25 Nov 2025,官方自建，结合LLM数据合成能力与现有提交语料库,We present a comprehensive pipeline for constructing MCI datasets. By combining the data synthesis capabilities of LLMs with existing commit corpora,通过双重验证确保数据质量,apply two-fold validation to verify both positive and negative samples,,,一致性检测,detecting inconsistencies between commit messages and code,"Recall, Precision, Specificity","average Recall 85.95%, Precision 80.28%, Specificity 63.8%",自然语言（提交信息）与代码差异,"Each item of the verified dataset contains a commit message, the corresponding code diff",二元分类（一致或不一致）,allowing the models to detect whether the commit is consistent or not,文本与代码到分类,detect whether the commit is consistent or not,,,首个专门用于MCI检测的基准，包含七种不一致类型，支持多种增强策略评估,CODEFUSE-COMMITEVAL is the first benchmarking work tailored for comprehensively evaluating LLM's MCI detection ability. We define seven mutation rules to guide powerful LLMs in generating various types of inconsistent commit messages
2511.20709_output/content.md,DUALGAUGE-BENCH,"we further curated DUALGAUGE-BENCH, a comprehensive benchmark suite of 154 programming tasks",Yes，本文是该数据集的原始发布论文,"We present DUALGAUGE, the first fully automated benchmarking framework... we also present DUALGAUGE-BENCH, a curated benchmark suite",https://anonymous.4open.science/r/DualBench-6D1D,"Our system source code, benchmark, and evaluation/study data can all be found at https://anonymous.4open.science/r/DualBench-6D1D",联合评估代码生成的安全性和功能性，确保生成的代码既满足功能规范又不会引入安全漏洞,rigorously evaluate the security and correctness of LLM-generated code in unison... ensuring that generated programs fulfill their specifications without introducing vulnerabilities,代码生成的安全性和功能性联合评估,designed to rigorously evaluate the security and correctness of LLM-generated code in unison,在沙盒环境中执行程序，针对功能和安全性测试套件运行，基于执行结果评估测试通过率和联合正确性-安全性指标,"executes it in a sandboxed environment against functional and security test suites, and evaluates the execution results against both test suites to report test pass rates and joint correctness-security metrics",单任务代码生成，基于自然语言提示生成完整程序,Given an pure-natural-language prompt as functional specification... generates the model's code output for the prompt,跨领域编程任务，涵盖多样化功能领域,spanning diverse functionality domains,,,语言无关，支持多种编程语言,This dataset design and curation process allow it to be agnostic to programming languages,包含154个编程任务，每个任务都配有功能和安全性测试套件,a comprehensive benchmark suite of 154 programming tasks... Each task/prompt is paired with both a functional test suite... and a security test suite,人工与LLM协同创建，通过多评分者的人类专业知识进行精炼和修正,"constructed these test suites through a human-and-LLM co-creation process—leveraging multiple LLMs to generate candidate tests, then refining and amending these with human expertise of multiple raters",2025,arXiv:2511.20709v1 [cs.SE] 24 Nov 2025,官方自建，基于规范测试范式,following a specification-based testing paradigm,,,,,代码生成,code generation... translating natural language prompts—typically serving as functional specifications—into programs,测试通过率、联合正确性-安全性指标,report test pass rates and joint correctness-security metrics,自然语言,Given an pure-natural-language prompt as functional specification,代码,generates the model's code output for the prompt,文本到代码,translating natural language prompts... into programs,沙盒隔离容器环境，支持依赖解析和环境配置,executes it in a sandboxed environment... The execution engine compiles and runs the model-generated code within isolated containers,首个支持安全性和功能性联合评估的基准套件，每个任务都配有覆盖驱动的功能和安全性测试套件，采用人工与LLM协同创建过程,"the first benchmark suite that pairs each code-generation prompt with dual (functional and security), coverage-enforced test suites... constructed through a human-and-LLM co-creation process"
2511.23408_output/content.md,Vul4J,"Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects and covering distinct Common Weakness Enumeration (CWE) classes.","No, 本文是使用该数据集进行评测","Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects...",,,评估大型语言模型（LLM）在自动化漏洞修复方面的有效性，具体针对真实漏洞和人工生成的漏洞。,"In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs... using both real and artificial vulnerabilities.",漏洞修复的有效性、模型间的互补性与重叠性,Our paper empirically investigates the effectiveness and complementarity of several prominent LLMs in automated vulnerability patching...,"使用漏洞证明（Proof-of-Vulnerability, PoV）测试执行来验证生成的补丁是否成功修复漏洞。",Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities.,单函数/代码片段,"Given a vulnerable code snippet, these models generate a patched version that aims to eliminate security flaws while preserving functionality.",软件安全、漏洞修复,Automated vulnerability patching is crucial for software security...,,,Java,"Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities...",15个真实漏洞及其41个人工生成的对应漏洞,"To perform this evaluation, we employed 15 real vulnerabilities and their 41 artificial counterparts (vulnerabilities).",真实漏洞来自开源项目和公共漏洞数据库（如CVE），人工漏洞由CodeBERT生成并经过验证。,"Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects... Secondly, we augmented our evaluation with artificially generated vulnerabilities... we incorporated artificial vulnerabilities derived from the work of Garg et al.[15]. Garg et al. used CodeBERT[11] to generate thousands of candidate artificial vulnerabilities...",,,官方自建（Vul4J数据集）,"Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects...",,,,,代码修复（漏洞补丁生成）,"Given a vulnerable code snippet, these models generate a patched version that aims to eliminate security flaws while preserving functionality.",PoV测试通过率（基于执行的验证）,Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities.,代码（包含漏洞的代码片段）,"Given a vulnerable code snippet, these models generate a patched version...",代码（修复后的代码片段）,"Given a vulnerable code snippet, these models generate a patched version...",代码到代码,"Given a vulnerable code snippet, these models generate a patched version...",Maven/Gradle构建环境，包含PoV JUnit测试用例,"When no suitable test existed in the original project, Bui et al. [2] manually wrote one by following the exploit steps in the CVE report, thereby guaranteeing that every vulnerability in the dataset can be triggered, reproduced, and validated in a Maven/Gradle build.",该研究不仅评估LLM对真实漏洞的修复能力，还评估其对人工生成漏洞的修复能力，以测试模型的泛化性和鲁棒性。数据集（Vul4J）为每个漏洞提供了可执行的漏洞证明（PoV）测试用例，用于自动化验证补丁的有效性。,"Our paper empirically investigates the effectiveness and complementarity of several prominent LLMs in automated vulnerability patching, specifically focusing on both real vulnerabilities and their corresponding artificial vulnerabilities... For every entry Vul4J provides... one or more Proof-of-Vulnerability (PoV) JUnit test cases. A PoV test is an executable exploit oracle..."
2401.01062_output/content.md,CAASD (Capability Assessment of Automatic Software Development),we have developed a novel benchmark named CAASD (Capability Assessment of Automatic Software Development).,Yes,we have developed a novel benchmark named CAASD (Capability Assessment of Automatic Software Development).,,,评估AI辅助软件开发系统的能力，特别是针对系统级实现任务的能力,all of them either are limited to simple function-level implementation tasks or lack detailed requirements specifications for system-level implementation tasks,系统级软件开发能力评估,for assessing how well a software development task is completed,通过参考用例评估系统实现的质量和完整性,Each task of CAASD is equipped with a list of reference use cases depicting the system requirements. The reference use cases are used to evaluate the quality and completeness of a system implementation.,系统级开发（多文件项目）,system-level implementation tasks,软件开发,software development,非平凡软件项目,non-trivial software projects,,,,,,,2024,arXiv:2401.01062v1 [cs.SE] 2 Jan 2024,官方自建,we have developed a novel benchmark named CAASD,,,,,系统级实现任务,system-level implementation tasks,通过率,AISD achieves an impressive pass rate of 75.2%,自然语言需求描述,high-level (potentially vague) user requirements as inputs,系统实现,system implementation,需求到系统实现,"taking high-level (potentially vague) user requirements as inputs, generates detailed use cases, prototype system designs, and subsequently system implementation",,,首个评估软件开发任务完成质量的基准，包含详细的系统需求规范,this is the first benchmark that offers criteria for assessing how well a software development task is completed
2401.12554_output/content.md,ParEval,we propose the Parallel Code Generation Evaluation (ParEval) benchmark: a set of benchmarks (prompts) for evaluating how well LLMs generate parallel code.,Yes,we propose the Parallel Code Generation Evaluation (ParEval) benchmark,github.com/parallelcodefoundry/ParEval,ParEval is available online at: github.com/parallelcodefoundry/ParEval.,评估大型语言模型生成并行代码的能力,we study the capabilities of state-of-the-art language models to generate parallel code.,并行代码生成正确性和性能,"We evaluate several state-of-the-art open- and closed-source LLMs using these benchmarks, and report metrics that represent the correctness and performance of the generated code.",新颖的代码生成评估指标，包括speedup𝑛@k和efficiency𝑛@k，用于评估生成代码的性能和扩展性,We introduce novel code generation evaluation metrics that assess performance and parallel scaling.,,,科学和并行计算,consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.,,,C/C++,"Traditional Python code generation benchmarks are tested by running eval on the generated code for a small number of small unit tests. On the other hand, in the case of parallel code — we must compile C/C++ code, link against one or more parallel libraries, and run the code in the proper parallel environment.",420个不同的编码任务,consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.,手动设计,These benchmarks are challenging to test. Traditional Python code generation benchmarks are tested by running eval on the generated code for a small number of small unit tests.,2024,"HPDC ’24, June 3–7, 2024, Pisa, Italy",官方自建,we propose the Parallel Code Generation Evaluation (ParEval) benchmark: a set of benchmarks (prompts) for evaluating how well LLMs generate parallel code.,,,,,代码生成,we study the capabilities of state-of-the-art language models to generate parallel code.,speedup𝑛@k和efficiency𝑛@k,"We introduce two novel metrics, speedup𝑛@k and efficiency𝑛@k, for evaluating the performance and scaling of LLM generated code.",自然语言,consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.,代码,we study the capabilities of state-of-the-art language models to generate parallel code.,文本到代码,consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.,需要特定依赖（并行库）,"we must compile C/C++ code, link against one or more parallel libraries, and run the code in the proper parallel environment.",专注于并行代码生成评估，覆盖12种计算问题类型和7种执行模型,"These benchmarks cover twelve different computational problem types, and seven different execution models: serial, OpenMP, Kokkos, MPI, MPI+OpenMP, CUDA, and HIP."
2512.01010_output/content.md,Chain of Unit-Physics,"To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation.",Yes,"To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation.",,,该评测基准旨在解决科学计算中的代码生成任务，特别是针对具有严格物理约束的高风险科学问题，如燃烧科学中的计算流体动力学（CFD）求解器开发。,Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community.,物理一致性、数值稳定性、算法正确性,"The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",基于单元物理测试的验证方法，包括物理约束检查、数值一致性检查和诊断分析,The Verification Agent applies formalized unit-physics tests to assess physical and numerical consistency. These primitives yield physics-grounded verification even without reference datasets.,多步骤科学计算任务，涉及复杂的物理约束和数值计算,"The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",计算科学，特别是燃烧科学和计算流体动力学,"To overcome these limitations, this work systematically applies an inverse code design methodology (see Fig. 1), formally known as test-driven development (TDD) [26], to scientific software in combustion science, one such domain where TDD approaches have not been thoroughly evaluated.",高难度，涉及12自由度的复杂燃烧任务,"The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",,,,,基于人类专家知识构建的单元物理测试,"This work proposes Chain of Unit-Physics, an approach that embeds human expert knowledge directly into distinct reasoning chains of the agentic system via 'unit-physics'—formalized, testable constraints that encode fundamental physics (e.g., energy conservation laws) and practitioner experience (e.g., dimensional / bound checks, and floating-point diagnostics).",2025,arXiv:2512.01010v1 [cs.MA] 30 Nov 2025,官方自建，基于人类专家知识,"This work proposes Chain of Unit-Physics, an approach that embeds human expert knowledge directly into distinct reasoning chains of the agentic system via 'unit-physics'—formalized, testable constraints that encode fundamental physics (e.g., energy conservation laws) and practitioner experience (e.g., dimensional / bound checks, and floating-point diagnostics).",,,,,端到端科学代码生成,"Closed-weight systems and code-focused agentic variants fail to produce correct end-to-end solvers, despite tool and web access, exhibiting four recurrent error classes: interface (syntax/API) hallucinations, overconfident assumptions, numerical/physical incoherence, and configuration fragility.",物理一致性、数值误差、运行时间和内存使用效率,"On the benchmark task, the proposed framework converges within 5–6 iterations, matches the human-expert implementation (mean error of 3.1ˆ10´3%), with a „33.4% faster runtime and a „30% efficient memory usage at a cost comparable to mid-sized commercial APIs, yielding a practical template for physics-grounded scientific code generation.",自然语言查询和单元物理测试,"In the input, the user’s scientific question is paired with 'basis prompts' that establish execution permissions, tool availability, coding language settings, and transfer protocols. Additional input scopes the unit-physics tests that concatenated with the scientific query to form the complete input to the framework.",科学计算代码和可视化结果,"Finally, the agent consolidates the output into domain-relevant visualizations (for example, line graphs and contour graphs of key quantities of interest), closing the loop between natural-language inquiry and quantitative scientific insight.",自然语言到科学代码,Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community.,Python虚拟沙箱环境，具有隔离的执行权限,"The framework runs inside a dedicated Python virtual sandbox environment that is not pre-configured with metadata on all available libraries. The agent is granted isolated code execution privileges within this sandbox, ensuring that any dependency installs or script executions cannot affect the system root directory or global files.",基于第一性原理的单元物理测试驱动代码生成，强调物理一致性和数值稳定性,"This inverse-design method provides two key advantages in scientific software. First, human-authored tests embed deep domain expertise (first principles or primitives) into targeted validation checks, ensuring that each algorithmic component faithfully represents the underlying physics."
2512.01396_output/content.md,BackportBench,"we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",Yes,"we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",https://github.com/BackportBench/BackportBench,The BackportBench is available on https://github.com/BackportBench/BackportBench.,自动化补丁回移植。旨在为未打补丁的软件版本生成补丁，基于已有的补丁和两个版本之间的代码差异。,BackportBench defines the backporting problem as generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.,自动化补丁回移植的有效性、多语言能力、处理跨文件不兼容性的能力,"To facilitate the development and evaluation of automated backporting techniques... BackportBench is a multilingual benchmark... This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges.",在可执行的Docker环境中运行相关测试用例进行验证,"each task instance provides an executable Docker environment with scripts for running relevant test cases, thereby aligns with the criteria for a successful benchmark [5].",仓库级别，涉及跨文件的不兼容性处理,"This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges.",软件安全、软件维护、漏洞修复,BackportBench contains 202 real-world vulnerability patch backporting task instances curated from three popular OSS ecosystems... To remove such vulnerabilities... patch backporting is a helpful practice that adapts patches for other versions and makes it easier to deliver patches to users on older branches.,现实世界工程级，涉及逻辑和结构性变更,"the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes.","Python, Java, JavaScript","BackportBench contains 202 patch backporting problems from PyPI, Maven, and npm, covering Python, Java, and JavaScript, respectively.",包含202个补丁回移植任务实例,BackportBench contains 202 real-world vulnerability patch backporting task instances,"从三个流行的开源软件生态系统（PyPI, Maven, npm）中收集的真实世界漏洞补丁回移植任务","BackportBench contains 202 real-world vulnerability patch backporting task instances curated from three popular OSS ecosystems, PyPI , Maven, and npm",2018 (根据论文版权日期)，但arXiv版本为2025年12月,© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ... arXiv:2512.01396v1  [cs.SE]  1 Dec 2025,官方自建,"we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",,,ACM版权，允许个人或课堂使用，禁止商业用途,Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.,代码修复/补丁生成,generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.,通过测试用例验证补丁有效性，而非基于等价性或相似性的指标,each task instance provides an executable Docker environment with scripts for running relevant test cases... We argue that whether backported patches achieve equivalence is not the only way to prove the patch is effective.,代码（两个版本的代码库及原始补丁）,generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.,代码（回移植后的补丁）,generating a patch for the unpatched software version,代码到代码,generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.,可执行的Docker环境，包含运行相关测试用例的脚本,each task instance provides an executable Docker environment with scripts for running relevant test cases,首个针对补丁回移植问题的综合性、多语言基准测试套件，包含可执行环境和测试用例，将问题从代码块/函数级别提升到仓库级别，更贴近现实软件维护挑战。,"the first comprehensive benchmark suite for patch backporting problem... a multilingual benchmark... contains executable Docker environments and test cases for validation... This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges."
2512.01255_output/content.md,ARENAJS,we construct ARENAJS—the first systematic benchmark for LLM-based JavaScript vulnerability detection,Yes,"In this paper, we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection... we propose FORGEJS... Then, we use FORGEJS to construct ARENAJS—the first systematic benchmark for LLM-based JavaScript vulnerability detection",,,评估大型语言模型在JavaScript漏洞检测方面的能力,evaluating LLMs’ capability in JavaScript vulnerability detection,漏洞检测能力、推理充分性、鲁棒性、现实世界可用性,"reveals key limitations in reasoning sufficiency, robustness, and real-world usability",使用JUDGEJS自动评估框架，包含提示模板、响应解析、标签对齐和稳健评分，利用语义等价和模糊匹配，在函数级和项目级生成统一的、严格的、可追溯的指标套件，包括F1、假阳性率（FPR）以及工程约束下的检测效能,"we propose JUDGEJS, an automated evaluation framework that spans prompt templates, response parsing, label alignment, and robust scoring; leveraging semantic equivalence and fuzzy matching, it harmonizes heterogeneous outputs and yields a unified, rigorous, and traceable metric suite across function- and project-levels, including F1, false positive rate (FPR), and detection efficacy under engineering constraints.",函数级和项目级（完整项目）,supports both function-level and project-level evaluation,JavaScript安全漏洞检测，涵盖前端（如DOM-based XSS）、后端（如SQL注入、命令注入、原型污染）和全栈环境,JavaScript vulnerability patterns vary substantially across these environments: backend prototype pollution... while frontend prototype pollution predominantly causes DOM-based XSS or client-side denial of service.,现实世界工业级，反映真实仓库的复杂性,fails to reflect the complexity of real-world repositories,JavaScript,benchmark for JavaScript vulnerability detection,覆盖数千个真实的JavaScript项目,A benchmark that spans thousands of real JavaScript projects,聚合异构来源，结合真实世界和合成数据,aggregates heterogeneous sources... combining real-world and synthetic data,2025-12-01 (根据arXiv版本日期),arXiv:2512.01255v1  [cs.CR]  1 Dec 2025,官方自建，通过FORGEJS自动生成框架构建,"we propose FORGEJS, the first automatic benchmark generation framework... Then, we use FORGEJS to construct ARENAJS",通过使用完整项目而非单文件片段、构建修复前后项目对来直接量化假阳性、以及系统引入四种增强策略来破坏对文件名、导入路径和注释的依赖，旨在避免高估,"To avoid overestimation, FORGEJS uses complete projects rather than single-file snippets, constructs pre- and post-fix project pairs to directly quantify false positives and localize error sources, and systematically introduces four augmentation strategies to disrupt reliance on filenames, import paths, and comments.",,,漏洞检测（包括定位漏洞文件、函数、CWE类型和具体代码行）,"evaluation protocols commonly assess only whether vulnerability is detected and ignore reasoning quality such as whether the model can correctly localize the vulnerable file, function, and CWE type.",F1分数、假阳性率（FPR）、VD-S指标,"including F1, false positive rate (FPR), and detection efficacy under engineering constraints... from the perspective of the VD-S metric [6]",JavaScript代码（函数或完整项目）,supports both function-level and project-level evaluation,漏洞检测结果（包括是否存在漏洞、CWE类型、定位信息、推理等）,"evaluation protocols commonly assess only whether vulnerability is detected and ignore reasoning quality such as whether the model can correctly localize the vulnerable file, function, and CWE type.",代码到安全分析,evaluating LLMs’ capability in JavaScript vulnerability detection,,,首个基于LLM的JavaScript漏洞检测系统基准；遵循三个构建原则：全面性、不低估、不高估；覆盖218种CWE类型；支持函数级和项目级评估；通过FORGEJS自动生成；包含JUDGEJS自动评估框架；旨在客观定义LLM在可重复、可扩展、部署受限条件下的现实性能上下限。,"we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection that directly address these limitations: (1) comprehensiveness, (2) no underestimation, and (3) no overestimation... FORGEJS aggregates heterogeneous sources, covers 218 CWE types... supports both function-level and project-level evaluation... Taken together, these principles aim to objectively define the realistic upper and lower bounds of LLM performance under reproducible, scalable, deployment-constrained conditions."
2105.09938_output/content.md,APPS,"To meet this challenge, we introduce APPS, a benchmark for code generation.",Yes,"To meet this challenge, we introduce APPS, a benchmark for code generation.",https://github.com/hendrycks/apps,The dataset is available at https://github.com/hendrycks/apps.,从任意自然语言描述生成满足要求的Python代码。,our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code.,代码生成能力，包括理解任务描述、设计算法、编写语法正确且功能正确的程序。,"APPS evaluates models not only on their ability to code syntactically correct programs, but also on their ability to understand task descriptions and devise algorithms to solve these tasks.",使用测试用例检查生成的代码。,"Similar to how companies assess candidate software developers, we evaluate models by checking their generated code on test cases.",单函数（Call-Based Format）或完整脚本（Standard Input/Output Format）。,"• Call-Based Format problems generally provide initial starter code, usually in the form of a function header, and ask for the solution to be provided as the function’s return value.",编程与算法，涵盖从简单字符串操作到复杂的图论、数据结构等算法挑战。,"Our benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges.",分为入门级、面试级和竞赛级三个难度。,"It contains 10,000 programming problems at various levels of difficulty, covering simple introductory problems, interview-level problems, and coding competition challenges.",Python,our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code.,"包含10,000个编程问题，131,777个测试用例，232,421个人工编写的参考答案。","The Automated Programming Progress Standard, abbreviated APPS, consists of 10,000 coding problems in total, with 131,777 test cases for checking solutions and 232,421 ground-truth solutions written by humans.","从开放的编程网站（如Codeforces, Kattis, Codewars, AtCoder）手动收集和整理。","The APPS dataset consists of problems collected from different open-access coding websites such as Codeforces, Kattis, and more.",2021,35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks.,官方自建，由多名研究生和本科生在六个月内精心整理和优化。,"Several graduate and undergraduate student authors polished and refined this dataset over the course of six months, ensuring a high-quality set of problems.",,,,,代码生成,"APPS, a benchmark for code generation from natural language specifications.",通过测试用例的准确率（例如 pass@k）。,we evaluate models by checking their generated code on test cases.,自然语言,take an arbitrary natural language specification,代码,generate satisfactory Python code,文本到代码,code generation from natural language specifications.,允许执行任意Python代码（包括导入常见模块和库），使用自定义的测试框架。,"solutions are allowed to execute arbitrary Python code, and the results are compared against test cases for a given problem.",1. 问题描述平均长度达293.2个单词，是自包含的完整规范。2. 拥有大量测试用例（超过13万个）用于严格评估功能正确性。3. 难度分级明确，覆盖从入门到竞赛的广泛范围。4. 模拟了人类程序员（如求职面试）的评估方式。,"By comparison, problem specifications in our new APPS benchmark are self-contained and have a much larger average length of 293.2 words. Unlike Iyer et al. (2018), APPS contains test cases for every exercise, enabling a high-quality evaluation of code correctness. The APPS benchmark attempts to mirror how humans programmers are evaluated by posing coding problems in unrestricted natural language and using test cases to evaluate solution correctness."
2107.03374_output/content.md,HumanEval,"On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings",Yes,"We evaluate functional correctness on a set of 164 hand-written programming problems, which we call the HumanEval dataset. We release the HumanEval dataset so that others can evaluate functional correctness and measure the problem-solving capabilities of their models.",https://www.github.com/openai/human-eval,We release this data along with an evaluation framework at https://www.github.com/openai/human-eval.,从文档字符串（docstrings）合成独立的Python函数，并评估生成代码的功能正确性。,"In this work, we focus on the task of generating standalone Python functions from docstrings, and evaluate the correctness of code samples automatically through unit tests.",代码生成的功能正确性,measure functional correctness for synthesizing programs from docstrings,通过单元测试评估功能正确性，并使用pass@k指标。,evaluate the correctness of code samples automatically through unit tests. Kulal et al. (2019) evaluate functional correctness using the pass@k metric,单函数,generating standalone Python functions,语言理解、推理、算法和简单数学,"Programming tasks in the HumanEval dataset assess language comprehension, reasoning, algorithms, and simple mathematics.",评估语言理解、算法和简单数学，部分问题类似于简单的软件面试题。,"These problems assess language comprehension, algorithms, and simple mathematics, with some comparable to simple software interview questions.",Python,study its Python code-writing capabilities. ... generating standalone Python functions from docstrings,包含164个手写的编程问题，平均每个问题有7.7个测试用例。,a dataset of 164 original programming problems with unit tests. ... an average of 7.7 tests per problem.,手写构建，未从现有来源程序化复制。,all problems were hand-written and not programmatically copied from existing sources.,2021,arXiv:2107.03374v2 [cs.LG] 14 Jul 2021,官方自建（OpenAI团队手写）,we create a dataset of 164 original programming problems. ... all problems were hand-written,为抗污染而设计，手写问题以避免训练数据（GitHub）中已存在的解决方案。,"It is important for these tasks to be hand-written, since our models are trained on a large fraction of GitHub, which already contains solutions to problems from a variety of sources.",,,代码生成（从文档字符串生成完整函数体）,generating standalone Python functions from docstrings,pass@k,Kulal et al. (2019) evaluate functional correctness using the pass@k metric,自然语言（文档字符串）与代码（函数签名）,"synthesizing programs from docstrings. ... prompt consisting of a header, a signature, and a docstring",代码（Python函数体）,generating standalone Python functions,文本（文档字符串）到代码,synthesizing programs from docstrings,使用gVisor容器运行时的沙盒环境，用于安全执行不受信任的程序。,"we developed a sandbox environment to safely run untrusted programs against unit tests. We selected the gVisor container runtime (Lacasse, 2018) as the main host protection component.",专门为评估代码生成模型的功能正确性而设计，强调手写问题以避免数据污染，并提供了安全的沙盒执行环境。,"It is important for these tasks to be hand-written, since our models are trained on a large fraction of GitHub, which already contains solutions to problems from a variety of sources. ... we developed a sandbox environment to safely run untrusted programs against unit tests."
2108.07732_output/content.md,Mostly Basic Programming Problems (MBPP),We introduce two datasets to test Python code synthesis. The first is a new dataset called Mostly Basic Programming Problems (MBPP).,Yes,We introduce two datasets to test Python code synthesis. The first is a new dataset called Mostly Basic Programming Problems (MBPP).,,,从自然语言描述中合成简短的Python程序。,Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions.,程序合成能力，特别是从自然语言描述生成功能正确的Python代码。,This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages.,通过执行测试用例检查功能正确性。,Participants also provided a ground-truth solution that passes all three test cases.,单函数，自包含。,"Participants were instructed to write code that is self-contained (that is, it runs by itself)...",涵盖数学、列表处理、字符串处理、整数序列等。,"Of these questions, 58% were mathematical in nature (e.g., calculating the volume of a sphere), 43% involve list processing, 19% require string processing, 9% involve integer seque",入门级，设计为入门级程序员可解决。,"The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers.",Python,The Mostly Basic Programming Problems dataset contains 974 short Python programs...,包含974个编程任务。,The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks...,通过众包构建，部分由作者编辑和手动验证。,This dataset consists of a large set of crowd-sourced questions and a smaller set of questions edited and hand-verified by the authors.,2021,arXiv:2108.07732v1  [cs.PL]  16 Aug 2021,官方自建（Google Research）,"We construct two new datasets: one entirely new and the other modified from an existing benchmark. The first, Mostly Basic Programming Problems (MBPP), is an entirely new crowd-sourced programming dataset.",文中提及与预训练集的重叠很小，降低了记忆风险。,"Second, we find that the overlap between the solutions in MBPP and the pre-training set is small, reducing the chance that our synthesis results are due to memorization (Section 4.8).",,,代码生成（从自然语言描述生成完整的函数体）。,"Participants were instructed to write a short problem statement, a single self-contained Python function solving the problem specified...",功能正确性（通过测试用例）。,Participants also provided a ground-truth solution that passes all three test cases.,自然语言描述，通常结合少量输入输出示例。,"a program can be specified by a short natural language description, possibly combined with a few (e.g., 2 or 3) input-output examples.",代码（Python函数）。,"Participants were instructed to write a short problem statement, a single self-contained Python function solving the problem specified...",文本到代码,Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions.,自包含，可使用标准库，允许使用互联网参考。,"Participants were instructed to write code that is self-contained (that is, it runs by itself)... Use of internet references was allowed.",包含三个一致的、以assert语句编写的输入输出示例；问题描述设计为更基础、字面化，而非竞赛风格；包含众包和手动验证两部分。,"In contrast, our dataset consistently contains three I/O examples, written as assert statements. ... By contrast, our Mostly Basic Programming Problems dataset is designed to contain a more basic, literal description of the problems. ... This dataset consists of a large set of crowd-sourced questions and a smaller set of questions edited and hand-verified by the authors."
2203.13474_output/content.md,Multi-Turn Programming Benchmark (MTPB),"To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts.",Yes,"In this work, we develop a multi-turn programming benchmark to measure the models’ capacity for multi-turn program synthesis. To the best of our knowledge, this is the first multi-turn program synthesis benchmark, which allows quantitative analysis of multi-turn program synthesis.",https://github.com/salesforce/CodeGen/tree/main/benchmark,1Benchmark: https://github.com/salesforce/CodeGen/tree/main/benchmark,评测模型的多轮程序合成能力。用户以自然语言在多轮对话中逐步指定意图，模型在每一轮中合成子程序，最终共同完成一个完整的程序。,"To solve a problem in the benchmark, a model needs to synthesize a program in multiple steps with a user who specifies the intent in each turn in natural language.",多轮程序合成能力,"In this work, we develop a multi-turn programming benchmark to measure the models’ capacity for multi-turn program synthesis.",通过专家编写的测试用例的通过率（pass rate）来衡量性能。,Performance on the benchmark is measured by pass rate on expert-written test cases.,多轮对话上下文。一个完整的程序被分解为多个子问题，每一轮对话指定一个子问题的意图。,consisting of 115 diverse problem sets that are factorized into multi-turn prompts.,通用编程问题。文中示例为从电子邮件地址中提取用户名。,Please refer to Figure 1 for an example where the model synthesizes a program to extract the user name of an email address.,,,未明确指定，但根据模型训练和上下文（如HumanEval部分），可能主要涉及Python。,文中未明确描述MTPB的编程语言。,包含115个多样化的问题集。,consisting of 115 diverse problem sets that are factorized into multi-turn prompts.,,,2023（根据论文版本日期推断）,arXiv:2203.13474v5 [cs.LG] 27 Feb 2023,官方自建,"we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB)",,,,,多轮程序合成,"To solve a problem in the benchmark, a model needs to synthesize a program in multiple steps",通过率（pass rate）,Performance on the benchmark is measured by pass rate on expert-written test cases.,自然语言（多轮对话）,a user who specifies the intent in each turn in natural language.,代码（子程序或完整程序）,synthesize a program,自然语言到代码,synthesize a program... with a user who specifies the intent in each turn in natural language.,,,首个专注于多轮程序合成的评测基准。问题被分解为多轮提示，模拟用户与系统逐步协作完成编程任务的过程。,"To the best of our knowledge, this is the first multi-turn program synthesis benchmark, which allows quantitative analysis of multi-turn program synthesis."
2207.01780_output/content.md,APPS,"Our comprehensive experiments (Section 4) show that our models can achieve SOTA performance on the challenging APPS benchmark [Hendrycks et al., 2021].","No, 本文是使用该数据集进行评测","Our comprehensive experiments (Section 4) show that our models can achieve SOTA performance on the challenging APPS benchmark [Hendrycks et al., 2021].",,,程序合成或代码生成，旨在根据自然语言问题描述生成满足功能正确性的程序。,Program synthesis or code generation aims to generate a program that satisfies a problem specification.,程序的功能正确性,The expected output is a program to be checked for functional correctness against some unit tests.,使用单元测试检查程序的功能正确性，并采用 pass@k 指标进行评估。,"Specifically, our models reach more than 2% pass@1, 6% pass@5, and 20% pass@1000.",,,通用编程，范围从基础编程问题到竞赛级编程任务。,This type of programming task can range from basic programming problems to competition-level programming tasks that require a high level of problem-solving skills.,具有挑战性，包含竞赛级复杂问题。,"Our comprehensive experiments (Section 4) show that our models can achieve SOTA performance on the challenging APPS benchmark [Hendrycks et al., 2021].",Python,"In our work, we focus on program synthesis from natural language problem specifications and the output programs are in general-purpose languages such as Python.",,,,,,,,,,,,,完整程序生成,"Training only with NTP objective is hence, not ideal to tackle full program generation to solve programming problems.","pass@1, pass@5, pass@1000","Specifically, our models reach more than 2% pass@1, 6% pass@5, and 20% pass@1000.",自然语言问题描述，通常包含示例输入输出对。,"Each task is defined by a problem specification in natural language, often containing example input and output pairs.",代码,The expected output is a program to be checked for functional correctness against some unit tests.,文本到代码,Program synthesis or code generation is the task of designing and building an executable computer program that satisfies a problem specification.,编译器,These program candidates are concatenated with the error information received from a compiler and passed to a program repair module.,该基准（APPS）包含从基础到竞赛级别的编程问题，用于评估模型生成功能正确程序的能力。,This type of programming task can range from basic programming problems to competition-level programming tasks that require a high level of problem-solving skills.
2207.10397_output/content.md,HumanEval,"We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS, and CodeContests","No, 本文是使用该数据集进行评测","We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS, and CodeContests",,,从自然语言描述（代码注释）和函数签名等上下文信息中生成代码解决方案。,"The task of code generation is to solve a programming problem: generate code solution x based on context c. As shown in Figure 2, context c contains natural language problem description in the form of code comment, and a code snippet that includes statements such as imports and the function header.",代码生成的功能正确性,A natural way to evaluate the quality and correctness of a code solution is to run it against a set of test cases,执行单元测试，使用 pass@k 指标,"We evaluate functional correctness using pass@1. ... For instance, Codex ... can achieve a pass@100 (pass if one or more among 100 generated solutions for a given problem can pass the corresponding test cases) of 77.4%, but a pass@1 (correct rate of a single solution) of only 33.5% on the HumanEval benchmark",单函数,"Figure 2: Code generation and test case generation: an example from the HumanEval benchmark. ... Context c contains natural language problem description in the form of code comment, and a code snippet that includes statements such as imports and the function header.",,,,,Python,"Figure 2: Code generation and test case generation: an example from the HumanEval benchmark. ... from typing import List def has_close_elements(numbers: List[float], threshold: float) -> bool:",164个问题,Benchmark Problems ... HumanEval 164,,,,,,,,,,,代码生成,The task of code generation is to solve a programming problem: generate code solution x based on context c.,"pass@1, pass@100","For instance, Codex ... can achieve a pass@100 (pass if one or more among 100 generated solutions for a given problem can pass the corresponding test cases) of 77.4%, but a pass@1 (correct rate of a single solution) of only 33.5% on the HumanEval benchmark",自然语言与代码（函数签名）,"context c contains natural language problem description in the form of code comment, and a code snippet that includes statements such as imports and the function header.",代码,generate code solution x,文本到代码,The task of code generation is to solve a programming problem: generate code solution x based on context c.,,,每个问题平均有7.77个地面真值测试用例,Benchmark ... GT Tests ... HumanEval ... 7.77
2208.08227_output/content.md,MultiPL-E,"We propose MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages.",Yes,"We propose MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the ﬁrst massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages.",github.com/nuprl/MultiPL-E,"The MultiPL-E system, dataset, and tutorial are available at github.com/nuprl/MultiPL-E.",将基于单元测试的代码生成基准（从Python）翻译到新的编程语言，以创建大规模、多语言的并行代码生成评测基准。,"MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the ﬁrst massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages.",多语言代码生成能力，模型在不同编程语言上的泛化性能,We use these new parallel benchmarks to evaluate the multi-language performance of three state-of-the-art code generation models... The range of programming languages represented in MultiPL-E allow us to explore the impact of language frequency and language features on model performance.,通过单元测试评估生成代码的正确性，使用pass@k等指标,We judge a generated function correct if it passes all tests... it is common to sample multiple completions per problem and report an estimated pass rate (§4.2).,单函数,"All of the problems are functions that receive and return ﬁrst-order values, which facilitates unit testing and test translation.",通用编程问题，涵盖算法和数据处理,It is a diverse collection of 164 problems... All of the problems are functions that receive and return ﬁrst-order values.,具有挑战性,"Moreover, it is a challenging benchmark: the best model evaluated by Fried et al. [4] achieves only a 36% pass rate on Python.","Python（源语言）及通过MultiPL-E翻译的18种其他编程语言（包括JavaScript, C++, Scala, TypeScript等）",We use MultiPL-E to extend the HumanEval benchmark [1] and MBPP benchmark [2] to 18 languages that encompass a range of programming paradigms and popularity... MultiPL-E supports 18 languages and is straightforward to extend with more.,通过翻译HumanEval（164个问题）和MBPP（未明确数量）两个基准，为19种语言（Python + 18种）创建并行问题集,We create the ﬁrst massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages... HumanEval is a diverse collection of 164 problems.,基于现有Python基准（HumanEval和MBPP）通过自动化编译器翻译生成,"We use MultiPL-E to translate two widely-used code generation benchmarks, HumanEval [1] and MBPP [2], into 18 languages.",2022,arXiv:2208.08227v4  [cs.LG]  19 Dec 2022,官方自建（论文作者构建）,We propose MultiPL-E... We create the ﬁrst massively multilingual code generation benchmark...,,,,,代码生成（从自然语言描述生成函数体）,"We focus on the natural-language-to-code task (NL2Code): given the description of a function in natural language, complete the function body.",pass率（基于单元测试）,We judge a generated function correct if it passes all tests... it is common to sample multiple completions per problem and report an estimated pass rate (§4.2).,自然语言（函数描述）与代码（函数签名、类型注解、示例）,"The prompt has several sources of information for the model: the function signature (its name and parameters); a brief comment describing the function; and, optionally, examples in the form of Python doctests.",代码（函数体）,"Given the prompt as input, the code generation model generates a completion that is likely to follow the given prompt.",文本到代码,"We focus on the natural-language-to-code task (NL2Code): given the description of a function in natural language, complete the function body.",容器化沙箱，用于编译（如需要）和运行程序,"MultiPL-E also includes a containerized sandbox that (1) compiles programs if necessary, (2) runs them with appropriate timeouts, (3) validates their results on unit tests, and (4) classiﬁes each output as successful, syntax error, etc.",1. 可扩展和可扩展的系统，用于将代码生成基准编译到新的编程语言。2. 创建了第一个大规模、多语言、并行的代码生成基准。3. 通过一套小型编译器（每个约200行代码）实现翻译，仅需翻译函数签名、单元测试、注释和类型注解，而无需翻译函数体。4. 包含一个规则工具，用于将注释中的技术术语翻译得更符合目标语言习惯。,"MultiPL-E uses a suite of 18 little compilers from Python benchmarks to each target language... Each compiler must be able to translate four components from Python: (1) a function signature (name and arguments), (2) simple unit tests, (3) a comment describing the expected function behavior, and (4) type annotations if the target language is statically typed. Notably, the compiler does not have to translate the body of a function, since it is the job of the code generation model to synthesize it. Thus each MultiPL-E compiler is approximately 200 LOC and easy to build. MultiPL-E also includes a simple, rule-based tool to translate technical terms in comments to be more language appropriate, e.g. a Python list is approximately a C++ vector."
2211.11501_output/content.md,DS-1000,"We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas.",Yes,"We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas.",https://ds1000-code-gen.github.io,We release our benchmark at https://ds1000-code-gen.github.io.,数据科学代码生成。旨在评估模型根据自然语言描述和代码上下文，生成用于解决实际数据科学问题的代码的能力。,"We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas.",数据科学代码生成能力、对特定库API的理解和使用、解决现实世界问题的实用性、抗记忆化能力,"DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases... Finally, we proactively defend against memorization...",基于执行的多标准自动评估。包括：1) 运行测试用例检查功能正确性；2) 通过限制API使用或关键词来检查表面形式约束。,"we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords.",包含代码上下文的单文件任务。用户问题通常包含错误的代码、错误消息和输入输出示例等多样化上下文。,"users’ data science coding problems usually have diverse contexts including their incorrect code, error messages, and input-output examples","数据科学。涵盖七个广泛使用的Python数据科学库：NumPy, Pandas, TensorFlow, PyTorch, SciPy, Scikit-learn, 和 Matplotlib。","a thousand problems covering seven widely-used Python data science libraries: NumPy, Pandas, TensorFlow, PyTorch, SciPy, Scikit-learn, and Matplotlib.",现实世界应用级。问题来源于StackOverflow，反映了日常数据科学应用中的实际用例，而非竞赛或面试风格。,"focuses on everyday data science applications... includes naturalistic intents and contexts... our problems reflect diverse, realistic, and practical use cases",Python,a code generation benchmark with a thousand data science problems spanning seven Python libraries,包含1000个问题。,a code generation benchmark with a thousand data science problems,从StackOverflow收集的自然发生的问题，经过人工筛选、修改和扰动。,"we collected naturally occurring problems from Stack-Overflow, manually scored their representativeness and usefulness, and curated a subset of them to create our benchmark.",2022-11-18 (根据arXiv版本v1日期),arXiv:2211.11501v1 [cs.SE] 18 Nov 2022,官方自建。由五位精通数据科学和Python的计算机科学学生作者花费约1200小时构建。,five authors who are computer science students and familiar with data science spent a total of about 1200 hours constructing DS-1000,抗污染设计。通过主动扰动原始StackOverflow问题来防御模型对预训练语料的记忆。,"we proactively defend against memorization by slightly modifying our problems to be different from the original Stack-Overflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training.",,,代码生成（填空）。模型需要将代码填入提示中的“[insert]”位置。,The model needs to fill in the code into “[insert]” in the prompt,通过/不通过（基于测试用例和表面形式约束）。文中未明确命名如pass@k的指标，但报告了准确率（如43.3%）。,The current best public system (Codex-002) achieves 43.3% accuracy,自然语言与代码上下文混合。输入包括自然语言问题描述和部分可执行的代码上下文。,The model needs to fill in the code into “[insert]” in the prompt on the left; the prompt includes natural language description and code context.,代码。期望输出是一段填补缺失部分的Python代码。,The model needs to fill in the code into “[insert]”,文本与代码到代码。根据自然语言描述和给定的代码上下文，生成缺失的代码片段。,synthesizing programs from docstrings. (结合上下文推断，任务是从自然语言描述和代码上下文中合成程序),需要特定数据科学库依赖的沙盒环境。固定了Python 3.7.10及相应库的最新版本。,We fixed the evaluation environment to include the latest versions of libraries that can be installed with Python 3.7.10,1) 包含现实问题与多样化上下文；2) 可靠的多标准执行评估；3) 主动防御记忆化；4) 专注于七个核心数据科学库。,"We highlight three core features of DS-1000: 1) it contains realistic problems with diverse contexts, 2) it implements reliable multi-criteria execution-based evaluation metrics, and 3) it proactively defends against memorization."
2301.03988_output/content.md,MultiPL-E,"evaluate them on the MultiPL-E text-to-code benchmark (Cassano et al., 2022).","No, 本文是使用该数据集进行评测","evaluate them on the MultiPL-E text-to-code benchmark (Cassano et al., 2022).",,,评测从自然语言描述生成代码的能力（文本到代码）,"MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages.",多语言代码生成能力,"MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages.",使用单元测试评估生成代码的正确性,"The correctness of generated code can be tested using unit tests, a method for approximating semantic equivalence.",,,,,,,"文中仅提及实验子集（Java, JavaScript, Python），未描述完整数据集。MultiPL-E扩展了18种语言。","evaluate them on the MultiPL-E text-to-code benchmark (Cassano et al., 2022). ... MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages.",,,基于HumanEval和MBPP基准自动编译扩展,"MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. The doctests, function signatures, and unit tests for each benchmark suite are automatically compiled to new languages.",,,,,,,,,代码生成（从自然语言描述生成完整函数）,"MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages.",,,自然语言,text-to-code benchmark,代码,text-to-code benchmark,文本到代码,text-to-code benchmark,,,"1. 通过自动编译将单语言基准（HumanEval, MBPP）扩展到多种编程语言。2. 评估时隐藏测试用例，仅用于验证正确性（与MBXP不同）。","MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. ... In contrast, MultiPL-E keeps the tests hidden from the model and only uses them to test correctness."
2302.08468_output/content.md,Spider,"▷Spider (Yu et al., 2018) is a semantic parsing dataset on generating SQL queries from natural language questions.","No, 本文是使用该数据集进行评测","We conduct experiments on four language-to-code datasets across domains of semantic parsing, table QA, math reasoning and basic python programming.",,,从自然语言问题生成SQL查询（语义解析）,"▷Spider (Yu et al., 2018) is a semantic parsing dataset on generating SQL queries from natural language questions.",语言到代码生成的功能正确性,"The ability of mapping natural language to executable code is the cornerstone of a variety AI applications such as database interfaces (Pasupat & Liang, 2015; Yu et al., 2018; Shi et al., 2020)...",执行生成的程序并比较结果,"Given x, a generation model P(y|x) generates a program y which is later executed via an executor E(·) to obtain the result3 E(y).",,,数据库查询（Table QA）,"Spider (Yu et al., 2018) is a semantic parsing dataset on generating SQL queries from natural language questions.",,,SQL,Target: SQL,训练集7000条，开发集1032条,"# Train: 7,000, # Dev: 1,032",,,2018,"Spider (Yu et al., 2018)",,,,,,,代码生成,language-to-code generation,执行准确率,LEVER consistently improves the execution accuracy of the generated programs.,自然语言与数据库模式,Input Format: Schema + NL,代码（SQL）,generating SQL queries from natural language questions.,文本到代码,language-to-code generation,SQL执行器,generating SQL queries from natural language questions.,拥有完整的程序标注（Has program: ✓）,Has program: ✓
2303.17568_output/content.md,HumanEval-X,"Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go.",Yes,"We hand-craft the HumanEval-X benchmark to evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness, facilitating the understanding and development of pre-trained (multilingual) code models.",https://github.com/THUDM/CodeGeeX,"Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX.",评估多语言代码模型在代码生成和代码翻译任务上的功能正确性。,We hand-craft the HumanEval-X benchmark to evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness...,多语言代码生成与翻译的功能正确性,...evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness...,通过执行测试用例来验证生成代码的功能正确性,"...rather than really verify the functional correctness of generated code. Specifically, for each problem... in HumanEval, we manually rewrite its prompt, canonical solution, and test cases in C++, Java, JavaScript, and Go.",单函数,文中未明确描述上下文依赖范围。HumanEval-X基于HumanEval构建，而HumanEval是单函数生成任务。,通用编程问题,文中未明确描述问题所属专业领域。,入门级编程问题,文中未直接描述HumanEval-X的难度，但提到其基础HumanEval用于评估Codex解决入门级Python问题。,"Python, C++, Java, JavaScript, Go","...we manually rewrite its prompt, canonical solution, and test cases in C++, Java, JavaScript, and Go. In total, HumanEval-X covers 820 hand-written problem-solution pairs (164 problems, each having solutions in 5 languages).",820个手写的问题-解决方案对（164个问题，每个问题有5种语言的解决方案）,"In total, HumanEval-X covers 820 hand-written problem-solution pairs (164 problems, each having solutions in 5 languages).",基于HumanEval（Python）手动重写和扩展,"Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go.",2022年9月,"Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X...",官方自建（手工构建）,We hand-craft the HumanEval-X benchmark...,,,,,代码生成，代码翻译,...evaluate multilingual code models for the tasks of code generation and translation...,功能正确性（通过测试用例）,...evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness...,自然语言（描述）,文中未明确描述输入类型，但基于HumanEval任务（从文档字符串生成代码）推断。,代码,文中未明确描述期望输出，但任务为代码生成和翻译，推断输出为代码。,文本到代码，代码到代码,HumanEval-X support the evaluation of both code generation and code translation between different languages.,,,1. 多语言扩展：在HumanEval（仅Python）基础上，手工编写了C++、Java、JavaScript和Go的解决方案和测试用例。2. 支持双重任务：同时评估代码生成和代码翻译。3. 专注于功能正确性评估，而非字符串相似度。,"1) HumanEval (Chen et al., 2021)—developed by OpenAI for evaluating Codex—and other benchmarks only consist of programming problems in a single language and 2) existing multilingual datasets use string similarity metrics like BLEU for evaluation rather than really verify the functional correctness of generated code. Importantly, HumanEval-X support the evaluation of both code generation and code translation between different languages."
2304.05128_output/content.md,Spider,"SELF-DEBUGGING achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation","No, 本文是使用该数据集进行评测","we evaluate SELF-DEBUGGING on the development set of the Spider benchmark (Yu et al., 2018).",,,文本到SQL生成：给定一个自然语言问题和数据库信息，生成对应的SQL查询。,The goal of text-to-SQL tasks is to generate the corresponding SQL query given a question and the database information,代码生成正确性,SELF-DEBUGGING can teach the large language model to debug its predicted program,执行单元测试（当可用时）或基于执行结果的多数投票,"When unit tests are presented in the problem description, we filter out programs that do not pass the unit tests before performing the execution-based majority vote.",,,数据库查询,text-to-SQL generation,包含最复杂级别的问题,improves the prediction accuracy on problems of the hardest level by 9%.,SQL,text-to-SQL generation,,,,,,,,,,,,,代码生成,text-to-SQL generation,预测准确率,improves the prediction accuracy on problems of the hardest level by 9%.,自然语言（问题）和数据库信息,given a question and the database information,代码（SQL查询）,generate the corresponding SQL query,文本到代码,text-to-SQL generation,,,该基准（Spider）在问题描述中没有单元测试来验证预测的正确性。,On the Spider benchmark where there are no unit tests to verify the correctness of predictions
2305.01210_output/content.md,HumanEval+,we extend the test-cases of the popular HUMANEVAL benchmark by 80× to build HUMANEVAL+.,Yes,we propose EvalPlus – a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator... we extend the test-cases of the popular HUMANEVAL benchmark by 80× to build HUMANEVAL+.,https://github.com/evalplus/evalplus,"We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.",评估大型语言模型生成的代码的功能正确性。,EvalPlus – a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code.,代码生成的功能正确性,rigorously benchmark the functional correctness of LLM-synthesized code.,通过大量新生成的测试用例（结合LLM和基于突变的策略）进行差分测试，并与基准实现进行交叉验证。,"EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies... These newly generated test inputs are then used to evaluate the LLM-generated code through differential testing against the ground-truth implementation.",单函数,The generated code snippet is then combined with the context to form a complete function that aligns with the user intent.,通用编程问题,"Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis.",,,文中仅提及实验基于HumanEval（Python），未描述完整数据集,we extend the test-cases of the popular HUMANEVAL benchmark by 80× to build HUMANEVAL+.,将HumanEval的测试用例规模扩展了80倍,"EvalPlus extends the popular HUMANEVAL benchmark to create HUMANEVAL+, improving the test-case scale by 80×.",基于现有基准（HumanEval）通过自动测试输入生成器（结合LLM和基于突变的策略）进行增强。,"EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies.",2023,arXiv:2305.01210v3 [cs.SE] 30 Oct 2023,官方自建（基于现有基准增强）,we propose EvalPlus – a code synthesis evaluation framework... we extend the test-cases of the popular HUMANEVAL benchmark by 80× to build HUMANEVAL+.,通过生成大量新测试用例来缓解现有基准测试不足导致的“虚假正确”问题，旨在更严格地评估模型。,"HUMANEVAL+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9%.",,,代码生成,program synthesis... applying LLMs for direct code generation.,pass@k,reducing the pass@k by up-to 19.3-28.9%.,自然语言（函数签名和文档字符串）,in the form of function signature and docstring that denote the desired program functionality.,代码,The generated code snippet is then combined with the context to form a complete function that aligns with the user intent.,文本到代码,synthesizing programs from docstrings.,,,1. 通过自动测试生成（结合LLM和突变策略）显著扩展现有基准的测试套件（80倍）。2. 旨在揭示因测试不足而被现有基准遗漏的错误代码。3. 提供精简版测试套件（HUMANEVAL+-MINI）以加速评估。4. 发现测试不足可能导致模型排名错误。,"EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies... HUMANEVAL+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs... we also produce HUMANEVAL+-MINI which distills HUMANEVAL+ tests by 47×... test insufficiency can lead to mis-ranking."
2305.02309_output/content.md,HumanEval,"The HumanEval (Chen et al., 2021b) is recruited to evaluate the quality of synthesized programs.","No, 本文是使用该数据集进行评测","The HumanEval (Chen et al., 2021b) is recruited to evaluate the quality of synthesized programs.",,,根据函数签名和文档字符串（意图说明）生成程序代码。,The prompt as an intent specification is in the form of a function signature and doc-string. A program is conditionally sampled (or completed) based on the prompt in left-to-right auto-regressive fashion.,程序合成的功能正确性,"The HumanEval (Chen et al., 2021b) is recruited to evaluate the quality of synthesized programs.",,,单函数,The prompt as an intent specification is in the form of a function signature and doc-string.,,,,,文中未明确提及完整数据集的语言，仅提及实验任务。,The prompt as an intent specification is in the form of a function signature and doc-string. A program is conditionally sampled (or completed) based on the prompt...,,,,,,,,,,,,,代码生成,Program Synthesis with left-to-right sampling (zero-shot)... A program is conditionally sampled (or completed) based on the prompt...,,,自然语言（文档字符串）与代码（函数签名）,The prompt as an intent specification is in the form of a function signature and doc-string.,代码,A program is conditionally sampled (or completed) based on the prompt...,文本到代码,The prompt as an intent specification is in the form of a function signature and doc-string. A program is conditionally sampled...,,,,
2305.07922_output/content.md,HumanEval,"Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task against other open code LLMs, even surpassing the OpenAI code-cushman-001 model.","No, 本文是使用该数据集进行评测","We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. ... in the zero-shot text-to-code generation task on HumanEval benchmark [Chen et al., 2021]",,,文中仅提及使用该基准进行代码生成评测，未描述其原始任务设计。,in the zero-shot text-to-code generation task on HumanEval benchmark,文中仅提及使用该基准进行代码生成评测，未描述其原始评测维度。,,文中仅提及使用pass@1和pass@10指标，未描述其原始评估方法。,achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task,,,,,,,文中仅提及实验任务（文本到代码生成），未描述数据集涉及的具体编程语言。,,,,,,,,,,,,,,代码生成,in the zero-shot text-to-code generation task on HumanEval benchmark,"pass@1, pass@10",achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task,自然语言,in the zero-shot text-to-code generation task,代码,in the zero-shot text-to-code generation task,文本到代码,in the zero-shot text-to-code generation task,,,,
2305.18584_output/content.md,PYCOMMITS,"We address this issue by collecting a new dataset, PYCOMMITS, from the commit histories of 1650 open-source Python projects on GitHub.",Yes,"We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation. ... We introduce the repo-level multi-round code editing task, along with the corresponding PYCOMMITS dataset and evaluation framework.",https://github.com/mrvplusone/Coeditor,Available at https://github.com/mrvplusone/Coeditor.,代码自动编辑。旨在预测对代码区域（基于同一代码库中最近的更改）的编辑。这是一个多轮任务，目标是根据用户之前的编辑来预测代码的编辑。,"In this work, we explore a multi-round code auto-editing setting, aiming to predict edits to a code region based on recent changes within the same codebase. ... In this paper, we introduce a task that we call (multi-round) auto-editing where the goal is to predict edits to code conditioned on the user’s previous edits.",代码编辑的准确性和自动化程度，关注模型在给定编辑历史和代码库上下文的情况下预测正确编辑的能力。,"In a simplified single-round, single-edit task, Coeditor significantly outperforms GPT-3.5 and SOTA open-source code completion models (bringing exact-match accuracy from 34.7 up to 60.4), demonstrating the benefits of incorporating editing history for code completion. In a multi-round, multi-edit setting, we observe substantial gains by iteratively conditioning on additional user edits.",精确匹配准确率（exact-match accuracy），以及自动化编辑行数和节省击键数的编辑距离度量。,"our method achieves 60.4% exact match accuracy using a 220M parameter model... In the full multi-round setting, we found that Coeditor automates editing 46.7% of the changed lines, saving the user 28.6% of keystrokes measured by an edit distance metric that accounts for cursor movement.",仓库级别（repo-level），依赖同一代码库中的其他部分和用户的历史编辑。,We introduce the repo-level multi-round code editing task... aiming to predict edits to a code region based on recent changes within the same codebase.,通用软件工程，代码维护与重构。,Developers often dedicate significant time to maintaining and refactoring existing code.,工程级，涉及实际开源项目中的代码变更。,We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation.,Python,We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation.,从1650个开源Python项目的提交历史中收集。,We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation.,来自GitHub上1650个开源Python项目的提交历史。,"We address this issue by collecting a new dataset, PYCOMMITS, from the commit histories of 1650 open-source Python projects on GitHub.",2024,Published as a conference paper at ICLR 2024,官方自建，由论文作者为研究目的构建。,"We address this issue by collecting a new dataset, PYCOMMITS, from the commit histories of 1650 open-source Python projects on GitHub.",,,,,代码编辑（基于行的差异），包括添加、删除行。,"We encode all prior code edits ∆1, . . . , ∆k using a line-based diffing scheme and decodes ∆u using masked span infilling... we adopt a line-diff-based format, enabling us to convert auto-editing into a masked span infilling problem.",精确匹配准确率（exact-match accuracy），自动化编辑行百分比，节省击键百分比（基于编辑距离）。,"our method achieves 60.4% exact match accuracy... Coeditor automates editing 46.7% of the changed lines, saving the user 28.6% of keystrokes measured by an edit distance metric that accounts for cursor movement.",代码（原始代码库和以行差异格式编码的编辑历史），以及通过静态分析提取的相关代码签名。,"given an original codebase U and a set of code changes ∆1, . . . , ∆k... we employ lightweight static analysis to pull in relevant parts of the codebase U.",代码（以行差异格式编码的目标编辑）。,"the auto-editing problem is to predict how to modify a specified region of code u ∈U... We encode the input code by a function EncInput... we can encode ∆u using the following expression, EncOutput(∆u)...",代码到代码（在给定代码库和编辑历史上下文的情况下，预测对目标代码区域的编辑）。,"the auto-editing problem is to predict how to modify a specified region of code u ∈U by learning the following distribution: P(∆u | ∆k . . . ∆1, U).",,,专注于多轮、仓库级别的代码自动编辑任务，使用基于行差异的编码格式和静态分析来构建上下文。数据集源自真实项目的提交历史，模拟了实际的代码编辑工作流。,"We introduce the repo-level multi-round code editing task... We encode all prior code edits ∆1, . . . , ∆k using a line-based diffing scheme... we employ lightweight static analysis to pull in relevant parts of the codebase U. We collect a code editing dataset from the commit histories of 1650 open-source Python projects."
2306.02907_output/content.md,DS-1000,"We evaluate SELFEVOLVE on three code generation datasets, including DS-1000 for data science code","No, 本文是使用该数据集进行评测","We evaluate SELFEVOLVE on three code generation datasets, including DS-1000 for data science code, HumanEval for software engineering code, and TransCoder for C++-to-Python translation.",,,数据科学代码生成任务,the data science code generation task DS-1000,代码生成功能正确性,Extensive experiments show that SELFEVOLVE achieves a significant improvement in execution-based measurements,基于执行的测量,Extensive experiments show that SELFEVOLVE achieves a significant improvement in execution-based measurements,,,数据科学,the data science code generation task DS-1000,,,Python,JuPyT5 [7] conditions on Jupyter notebooks’ context cells to generate data science code.,,,,,,,,,,,,,代码生成,the data science code generation task DS-1000,,,自然语言问题描述,Given a problem description written in natural language d,代码,the autoregressive language model pθ predicts the solution,文本到代码,"Given a problem description written in natural language d, and code context c, the autoregressive language model pθ predicts the solution",Python解释器,This refinement mechanism teaches language models to depend on an executor like a Python interpreter to correct the preliminary code.,专注于数据科学领域的代码生成,the data science code generation task DS-1000
2306.03091_output/content.md,RepoBench,"we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.",Yes,"we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.",,,评估仓库级别的代码自动补全系统。,"RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.",跨文件代码检索能力、给定上下文的代码补全能力、结合检索与补全的端到端流程处理能力。,"RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system’s ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction.",对于检索任务（RepoBench-R）使用Accuracy@k (acc@k)指标；对于补全任务（RepoBench-C）和端到端任务（RepoBench-P），评估模型预测下一行代码的能力。,"For evaluative purposes, the Accuracy@k (acc@k) metric is employed to assess retrieval performance. The easy subset is evaluated using acc@1 and acc@3, while the hard subset is examined through acc@1, acc@3, and acc@5 metrics.",多文件项目（仓库级别），包含跨文件上下文和文件内上下文。,"However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. ... RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.",通用编程,benchmark for auto-code completion,真实世界编程场景，包含不同难度子集（如检索任务中的Easy和Hard子集）。,"This benchmark comprises three interconnected tasks: RepoBench-R for code retrieval, RepoBench-C for code completion, and RepoBench-P, which integrates both aspects to reflect the entire workflow of an auto-completion system, offering a balanced assessment. ... we categorize them into two subsets: those with 5-9 candidates as the easy subset, and those with 10 or more candidates as the hard subset.",Python 和 Java,RepoBench supports both Python and Java,"训练数据：10,345个Python仓库和14,956个Java仓库。测试数据：1,075个Python仓库和594个Java仓库。具体任务样本数量详见论文表1。","After processing the data, our dataset comprises 10,345 Python and 14,956 Java historical repositories, serving as training data and are available for optional fine-tuning. Additionally, we have 1,075 Python and 594 Java new repositories from GitHub designated as test data for evaluation.",1. Github-Code数据集（截止2022年3月16日），用于构建训练数据。2. 新爬取的GitHub仓库（创建于2023年2月9日至8月3日之间），专门用作测试集。,"Github-Code Dataset: The first source of RepoBench is the github-code dataset2, which consists of a vast collection of code files sourced from GitHub repositories under open-source licenses with a data cutoff date of March 16, 2022. ... Newly Crawled GitHub Data: To mitigate impacts regarding data leakage and memorization, we augment the dataset by incoporating the most recent, non-forked GitHub repositories that are permitted under their respective licenses. Specifically, we use GitHub’s official API to crawl Python and Java repositories created after February 9, 2023, which aligns with the newest knowledge cutoff date of The Stack [22], and before August 3, 2023. This newly-crawled data serves exclusively as our test set for evaluation.",2023-10-04 (arXiv版本v2),arXiv:2306.03091v2  [cs.CL]  4 Oct 2023,官方自建,"we introduce RepoBench, a new benchmark",通过使用新爬取的数据作为测试集来减轻数据泄露和记忆的影响。,"To mitigate impacts regarding data leakage and memorization, we augment the dataset by incoporating the most recent, non-forked GitHub repositories ... This newly-crawled data serves exclusively as our test set for evaluation.",数据来源于开源许可下的GitHub仓库，但未明确指定数据集本身的许可证。,code files sourced from GitHub repositories under open-source licenses,代码补全（下一行预测）、代码检索、端到端流程（检索+补全）。,"RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline).","Accuracy@k (acc@1, acc@3, acc@5)","For evaluative purposes, the Accuracy@k (acc@k) metric is employed to assess retrieval performance. The easy subset is evaluated using acc@1 and acc@3, while the hard subset is examined through acc@1, acc@3, and acc@5 metrics.",代码（文件内上下文和跨文件代码片段）,the prompt is created by combining all the parsed snippets as cross-file contexts and an in-file context. The in-file context includes import statements and several preceding lines of code,代码（下一行）,predict the next line of code,代码到代码,predict the next line of code given a pre-defined context. The context can involve content from different files (cross-file context) and within the file (in-file context),,,"专注于仓库级别的代码自动补全评估，包含三个相互关联的任务（检索、补全、端到端流程），并设计了不同的上下文设置（Cross-File-First, Cross-File-Random, In-File）和子集（如2k和8k token限制）以适应不同模型。","RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). ... To effectively evaluate next-line prediction in auto-completion systems, we define three settings: Cross-File-First (XF-F)... Cross-File-Random (XF-R)... In-File (IF)... RepoBench-C is divided into two subsets: RepoBench-C-2k and RepoBench-C-8k."
2306.08568_output/content.md,HumanEval,"Through comprehensive experiments on five prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, DS-1000, and MultiPL-E, our models showcase outstanding performance.","No, 本文是使用该数据集进行评测","Through comprehensive experiments on five prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, DS-1000, and MultiPL-E, our models showcase outstanding performance.",,,代码生成（从自然语言描述生成代码）,"These models, pre-trained on substantial code data, excel in various code-related tasks, consistently delivering impressive performance. (结合上下文，评测基准用于评估代码生成任务)",代码生成的功能正确性,"Remarkably, WizardCoder 15B even surpasses the well-known closed-source LLMs, including Anthropic’s Claude and Google’s Bard, on the HumanEval and HumanEval+ benchmarks. (通过pass率评估，隐含了功能正确性维度)",pass@1 (通过率),Figure 1: An illustration of our novel Code Evol-Instruct and the superior pass@1 performance of our WizardCoder 34B... The Python score is the mean between HumanEval and MBPP.,,,通用编程问题,Develop a Python program that creates a random password of length 8 characters. (示例任务属于通用编程),,,多种编程语言（包括Python）,outperforming the open-source SOTA ... by a large margin in 9 different programming languages. The Python score is the mean between HumanEval and MBPP.,,,,,,,,,,,,,代码生成,Through comprehensive experiments on five prominent code generation benchmarks,pass@1,Figure 1: An illustration of our novel Code Evol-Instruct and the superior pass@1 performance of our WizardCoder 34B,自然语言（指令/描述）,Develop a Python program that creates a random password of length 8 characters. (示例中的输入是自然语言指令),代码,Here's an example program that generates a random password... (示例中的输出是代码),文本到代码,Develop a Python program that creates a random password of length 8 characters. (从自然语言文本描述生成代码),,,"本文提及了五个基准：HumanEval, HumanEval+, MBPP, DS-1000, 和 MultiPL-E。其中MultiPL-E支持多语言评估。","Through comprehensive experiments on five prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, DS-1000, and MultiPL-E"
2306.09896_output/content.md,HumanEval,"In this paper, we analyze Code Llama, GPT-3.5 and GPT-4’s ability to perform self-repair on problems taken from HumanEval and APPS.","No, 本文是使用该数据集进行评测","In this paper, we analyze Code Llama, GPT-3.5 and GPT-4’s ability to perform self-repair on problems taken from HumanEval and APPS.",,,从自然语言规范生成代码片段,Large language models (LLMs) have proven capable of generating code snippets from natural language specifications,代码生成与自我修复能力,"we focus on evaluating the models’ capacity to reflect upon, provide feedback on and debug the code.",pass@k 指标，执行单元测试,"performance is typically measured by pass@k (Chen et al., 2021; Kulal et al., 2019)—the probability that at least one of k i.i.d. program samples from the model satisfies a given specification.",单函数,for self-contained Python programming tasks.,通用编程,self-contained Python programming tasks.,竞赛级和面试级,complex coding challenges such as those found in competitions and professional software engineering interviews.,Python,for self-contained Python programming tasks.,,,,,,,,,,,,,代码生成,generating code snippets from natural language specifications,pass@k,"performance is typically measured by pass@k (Chen et al., 2021; Kulal et al., 2019)",自然语言与单元测试,"Given a specification ψ, a programming model MP first generates np samples i.i.d.",代码,generating code snippets,文本到代码,generating code snippets from natural language specifications,单元测试执行环境,These np code samples are then executed against a test bed.,本文重点研究自我修复（self-repair）策略在代码生成任务中的有效性，而非提出新基准。,"In this paper, we investigate the efficacy of self-repair techniques applied to CodeLlama-13b-instruct, GPT-3.5, and GPT-4 for self-contained Python programming tasks."
2306.11644_output/content.md,HumanEval,"The evaluation benchmark proposed in the latter work, HumanEval, has been widely adopted for comparing LLMs’ performance on code.","No, 本文是使用该数据集进行评测","We demonstrate the power of high quality data in breaking existing scaling laws by training a 1.3B-parameter model, which we call phi-1, ... we attain 50.6% pass@1 accuracy on HumanEval and 55.5% pass@1 accuracy on MBPP",,,从自然语言文档字符串（docstrings）中合成简单的Python函数。,"We focus our attention on LLMs trained for code, and specifically writing simple Python functions from their docstrings as in [CTJ+21].",代码生成的功能正确性,"The evaluation benchmark proposed in the latter work, HumanEval, has been widely adopted for comparing LLMs’ performance on code.",pass@1 准确率,we attain 50.6% pass@1 accuracy on HumanEval and 55.5% pass@1 accuracy on MBPP,单函数,writing simple Python functions from their docstrings,基础编程概念,determine its educational value for a student whose goal is to learn basic coding concepts,基础级别,for a student whose goal is to learn basic coding concepts,Python,writing simple Python functions from their docstrings,,,,,2021,2021 Jul Codex-300M [CTJ+21],,,文中讨论了训练数据可能存在的污染风险,in Section 5 we study possible contamination of our training data with respect to HumanEval.,,,代码生成,writing simple Python functions from their docstrings,pass@1,we attain 50.6% pass@1 accuracy on HumanEval,自然语言,writing simple Python functions from their docstrings,代码,writing simple Python functions,文本到代码,writing simple Python functions from their docstrings,,,被广泛用于比较LLM在代码生成上的性能,"HumanEval, has been widely adopted for comparing LLMs’ performance on code."
2306.14893_output/content.md,LCC (Long Code Completion Benchmark),"In this paper, we introduce the Long Code Completion Benchmark (LCC), a new benchmark that focuses on code completion with long code context for three programming languages: Python, Java, and C#.",Yes,"To evaluate the effectiveness of LongCoder and encourage future research on Long Code Completion, we construct a new dataset called LCC by filtering code from GitHub based on length, with the goal of focusing on longer code examples.",https://github.com/microsoft/CodeBERT,1All the codes and data are available at https://github.com/microsoft/CodeBERT.,专注于长代码上下文的代码补全任务，旨在评估模型在更复杂、更现实的代码文件级别场景下的能力。,"In this paper, we introduce a new task for code completion that focuses on handling long code input... We introduce the Long Code Completion Benchmark (LCC), a new benchmark that focuses on code completion with long code context...",长代码建模能力、代码补全准确性、模型效率（计算资源消耗）,...achieves superior performance on code completion tasks compared to previous models while maintaining comparable efficiency in terms of computational resources during inference.,"在逐行基础上，使用精确匹配（Exact Match, EM）和编辑相似度（Edit Similarity, Edit Sim）进行评估。","We follow Lu et al. (2021) to evaluate the performance of the models in terms of Exact Match (EM) and Edit Similarity (Edit Sim) on a per-line basis (Svyatkovskiy et al., 2020).",文件级别（长代码上下文），上下文长度通常超过512个代码标记，平均长度在1800-2000个标记左右。,"...focuses on code completion with long code context... The average length of the examples in LCC are 5× longer than those in existing datasets... The average length of a Python source file on GitHub is 1,305 tokens.",通用编程，数据来源于GitHub上的开源代码文件。,"Specifically, we construct our datasets from the github-code dataset, which contains a vast number of code files sourced from GitHub with an open-source license that permits research use.",工程级，包含真实世界中的长代码文件，结构和依赖关系更复杂。,"Meanwhile, longer code sequences contain more complex structures and require models to consider more context and dependencies. This can be challenging for previously proposed code completion models that focus on short code...","Python, Java, C#","...a new benchmark that focuses on code completion with long code context for three programming languages: Python, Java, and C#.",每种语言包含10万个训练样本，1万个开发集样本和1万个测试集样本。,"For each programming language, we sample 100k examples for training, and 10k examples for development and 10k for testing.",从GitHub上具有开源许可证的代码文件中筛选和构建，经过去重和AST解析过滤。,"Specifically, we construct our datasets from the github-code dataset, which contains a vast number of code files sourced from GitHub with an open-source license that permits research use. The steps to construct the datasets are as follows: • We first follow Allamanis (2019) to deduplicate examples with high similarity (Jacobi similarity ≥0.9) in order to eliminate forked files, and then remove code files that can’t be parsed into an abstract syntax tree using a standard compiler tool called tree-sitter.",2023,"Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).",官方自建（由论文作者构建）,We construct a new dataset (LCC) for code completion tasks that requires long code modeling to encourage more research in such scenarios.,文中未明确讨论,,开源许可证（允许研究使用），具体许可证名称未明确说明。,...sourced from GitHub with an open-source license that permits research use.,代码补全（具体为下一行预测或行内补全）,Code completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context.,"精确匹配（Exact Match, EM）、编辑相似度（Edit Similarity, Edit Sim）","We follow Lu et al. (2021) to evaluate the performance of the models in terms of Exact Match (EM) and Edit Similarity (Edit Sim) on a per-line basis (Svyatkovskiy et al., 2020).",代码,Code completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context.,代码,Code completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context.,代码到代码,Code completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context.,文中未明确描述,,专门针对长代码上下文设计，平均代码长度是现有数据集的5倍；包含Python、Java、C#三种语言；通过长度过滤（>512 tokens）确保关注长上下文场景。,"On average, the examples in LCC are 5× longer than those in existing datasets (Lu et al., 2021)... Since the benchmark primarily focuses on the code completion task with long code context, we remove code files whose length of code tokens after tokenization is shorter than 512."
2307.04349_output/content.md,APPS,Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks.,"No, 本文是使用该数据集进行评测",Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks.,,,程序合成或代码生成，即根据给定的高级行为描述生成计算机代码。,"Program synthesis, or code generation, involves creating an executable program that solves a given problem.",代码的功能正确性（通过单元测试）,"Unlike text generation, program synthesis requires both syntactic and functional accuracy since the generated code must pass compilation and unit tests.",使用单元测试结果作为反馈信号，评估生成的代码是否能通过编译和测试。,"RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs.",,,编程挑战，包括竞赛级算法问题。,"While LLMs have shown promising results in basic programming tasks, there is still progress to be made in tackling more challenging problems such as program competitions.",从基础编程任务到竞赛级挑战。,"While LLMs have shown promising results in basic programming tasks, there is still progress to be made in tackling more challenging problems such as program competitions.",文中未明确描述APPS数据集的语言构成，仅提及Codex解决Python挑战。,"Codex (Chen et al., 2021) is a noteworthy example of an LLM with 12 billion parameters that can successfully solve over 70% of complex Python programming challenges created by humans.",,,,,,,,,,,,,代码生成（从描述生成完整代码）,"Program synthesis, or code generation, involves creating an executable program that solves a given problem.",基于单元测试通过率的反馈信号（如粗粒度、细粒度、自适应反馈）,"Built upon this framework, we develop multi-granularity feedback that is automatically extracted from unit test. To expand, we introduce coarse-grained and fine-grained feedbacks applicable to programs with errors, aimed at punishing the specific segments of code where the errors appear. For programs that do not pass all test cases, we propose an adaptive feedback mechanism that assigns varying penalties based on the ratio of passed test cases.",自然语言描述,Program synthesis is an automated process that generates computer code W based on a high-level description of desired behavior D.,代码,Program synthesis is an automated process that generates computer code W based on a high-level description of desired behavior D.,文本到代码,Program synthesis is an automated process that generates computer code W based on a high-level description of desired behavior D.,编译器/单元测试环境,One generates the target program and interacts with the compiler to produce a training data pair...,本文是评测论文，未描述APPS基准的独特之处。本文提出的RLTF方法的独特之处在于其在线强化学习框架和从单元测试中提取的多粒度反馈（粗粒度、细粒度、自适应反馈）。,"To address these issues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs."
2307.14936_output/content.md,HumanEval,"PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark.","No, 本文是使用该数据集进行评测","Through extensive evaluation on three benchmarks, including HumanEval, CoderEval, and LeetCode, we conjecture that...",,,从自然语言描述生成Python代码，衡量代码生成的功能正确性。,demonstrating remarkable performance on the code generation task.,代码生成正确性,measure functional correctness for synthesizing programs from docstrings.,pass@k (文中主要报告pass@1),"PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark.",,,通用编程问题,tackle up to 72% of Python programming problems.,,,Python,tackle up to 72% of Python programming problems.,,,,,,,,,,,,,代码生成,demonstrating remarkable performance on the code generation task.,pass@1,"PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark.",自然语言,generating code from natural language descriptions,代码,generating code from natural language descriptions,文本到代码,generating code from natural language descriptions,,,由OpenAI发布，是代码生成领域广泛使用的基准测试。,OpenAI HumanEval benchmark
2308.07124_output/content.md,HUMANEVALPACK,"We further introduce HUMANEVALPACK, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust).",Yes,"We further introduce HUMANEVALPACK, expanding the HumanEval benchmark to a total of 3 coding tasks...",https://github.com/bigcode-project/octopack,"Code, models and data are freely available at https://github.com/bigcode-project/octopack.",评测基准旨在评估代码大语言模型在多种代码相关任务上的泛化能力，覆盖代码修复、代码解释和代码合成三种场景。,"HUMANEVALPACK: A benchmark for Code LLM generalization, spanning three scenarios (Code Repair, Code Explanation, Code Synthesis) and 6 programming languages (Python, JavaScript, Java, Go, C++, Rust)",代码大语言模型的泛化能力、多任务处理能力、多语言代码能力,"A benchmark for Code LLM generalization, spanning three scenarios (Code Repair, Code Explanation, Code Synthesis) and 6 programming languages",使用 pass@k 指标评估功能正确性,Metric: Pass@k (Figure 3 caption),单函数,"Given an incorrect code function with a subtle bug and accompanying unit tests, the model is tasked to fix the function. (描述 HUMANEVALFIX 任务)",通用编程问题，包含算法和基础功能实现,"Write a Python function `has_close_elements(numbers: List[float], threshold: float) -> bool` to solve the following problem: Check if in given list of numbers, are any two numbers closer to each other than given threshold. (Figure 3 示例)",工程级（包含需要修复的微妙bug）,Given an incorrect code function with a subtle bug... Bugs are written such that the code still runs but produces an incorrect result leading to at least one unit test failing.,"Python, JavaScript, Java, Go, C++, Rust","spanning three scenarios (Code Repair, Code Explanation, Code Synthesis) and 6 programming languages (Python, JavaScript, Java, Go, C++, Rust)",基于164个HumanEval问题，每个问题扩展到6种语言和3种任务，共984个修复任务，以及对应的解释和合成任务。,We manually add a bug to each of the 164 HumanEval solutions across all 6 languages (984 total bugs).,基于HumanEval基准人工扩展构建,"we expand the code synthesis benchmark HumanEval (Chen et al., 2021; Zheng et al., 2023) to cover all three input-output combinations for six languages",2024,arXiv:2308.07124v2 [cs.CL] 18 Feb 2024,官方自建（本文作者团队）,We further introduce HUMANEVALPACK,基于现有基准扩展，存在污染风险,expanding the HumanEval benchmark,,,代码修复、代码解释、代码合成,"spanning three scenarios (Code Repair, Code Explanation, Code Synthesis)",pass@k,Metric: Pass@k (Figure 3 caption),"自然语言与代码的组合，取决于具体任务（NL, NL+C, C）","When instruction tuning with code (C) data, code may either appear only in the input alongside the NL instruction (NL+C→NL, e.g. code explanation), only in the output (NL→C, e.g. code synthesis), or in both input and output (NL+C→C, e.g. code modifications like bug fixing).","自然语言或代码，取决于具体任务（NL, C）","code may either appear only in the input alongside the NL instruction (NL+C→NL, e.g. code explanation), only in the output (NL→C, e.g. code synthesis), or in both input and output (NL+C→C, e.g. code modifications like bug fixing).",文本到代码（合成）、代码到文本（解释）、代码到代码（修复）,"cover all three input-output combinations: NL+C→NL (e.g. code explanation), NL→C (e.g. code synthesis), NL+C→C (e.g. code modifications like bug fixing)",单元测试执行环境,Given an incorrect code function with a subtle bug and accompanying unit tests,扩展了经典的HumanEval基准，覆盖了代码修复和解释等更广泛的现实任务，支持六种编程语言，旨在解决现有基准因模型性能接近饱和而可能失效的问题。,Our more challenging evaluation variants provide room for future LLMs to improve on the performance of the current state-of-the-art.
2308.10335_output/content.md,ROBUSTAPI,"To fill the missing piece, we propose a dataset ROBUSTAPI for evaluating the reliability and robustness of code generated by LLMs.",Yes,"To fill the missing piece, we propose a dataset ROBUSTAPI for evaluating the reliability and robustness of code generated by LLMs.",https://github.com/FloridSleeves/RobustAPI,We open-source our dataset and evaluator on GitHub2. ... 2https://github.com/FloridSleeves/RobustAPI,评估大型语言模型生成的代码的可靠性和鲁棒性，特别是针对Java API的误用情况。,We propose a dataset ROBUSTAPI for evaluating the reliability and robustness of code generated by LLMs.,代码生成的可靠性与鲁棒性，特别是API误用检测。,"The main purpose of this benchmark is not to evaluate the functional correctness of the generated code, but instead, we focus on reliability and robustness.",使用抽象语法树（AST）分析生成的代码，并与预期的API使用模式（结构化调用序列）进行比较，检测违规行为。,We also provide an evaluator that analyzes the generated code snippets using the abstract syntax tree (AST) and compares them with the expected API usage patterns.,单代码片段（基于Stack Overflow问答），涉及特定API的使用。,We collect 1208 real questions from Stack Overflow which involves 18 representative Java APIs.,软件工程，涵盖字符串处理、数据结构、移动开发、加密、I/O和数据库操作等多个领域。,"These 18 APIs cover 6 domains including string processing, data structure, mobile development, crypto, I/O and database operation.",实际软件开发中常见的、开发者容易犯错的API使用问题。,"In this way, we guarantee that the questions in ROBUSTAPI are answerable and non-trivial so we can use them to effectively evaluate the LLMs’ ability in answering coding questions that humans are prone to make mistakes.",Java,Thus we collect representative questions about Java from Stack Overflow.,包含1208个问题，涉及18个代表性的Java API。,We collect 1208 real questions from Stack Overflow which involves 18 representative Java APIs.,从Stack Overflow爬取的真实编程问题，并基于ExampleCheck数据集筛选。,We build ROBUSTAPI based on the dataset from ExampleCheck (Zhang et al. 2018) as our starting point. ... Then we crawl questions relevant to these APIs from Stack Overflow.,2024-01-27 (根据arXiv版本v5日期),arXiv:2308.10335v5 [cs.CL] 27 Jan 2024,官方自建（由本文作者构建并发布）,We propose a dataset ROBUSTAPI... We open-source our dataset and evaluator on GitHub.,基于Stack Overflow真实问题构建，存在被LLM训练数据包含的风险，但文中未明确讨论污染状态。,,文中未提及具体许可证。,,代码生成（根据自然语言问题描述生成使用特定API的代码片段）,The prompt simulates a user asking coding questions without providing any additional hints from the API documentation which is a typical scenario when novice developers seek help from large language models.,API误用检测率（通过AST分析判断生成的代码是否违反预期的API使用模式）,Any violations of such structured call sequences would be considered as API misuse from the perspective of software engineering.,自然语言（Stack Overflow问题的标题和描述）,question field contains the title and description of the Stack Overflow questions.,代码（Java代码片段）,We design templates to trigger large language models to generate the code snippet and the corresponding explanation.,文本到代码,The prompt simulates a user asking coding questions... instruct LLMs to generate answers (code) to the questions.,文中未明确描述执行环境，评估基于静态分析（AST），而非动态执行。,We introduce the static analysis method in ROBUSTAPI for detecting the API usage violations which leverages the abstract syntax tree...,专注于评估代码生成在真实软件开发场景下的可靠性和鲁棒性，而非传统的功能正确性；针对API误用这一具体风险；包含一个基于AST的静态分析评估器。,"Unlike the online programming forums, the generated code snippets are not reviewed by the community peers and thus suffer from API misuse... The main purpose of this benchmark is not to evaluate the functional correctness of the generated code, but instead, we focus on reliability and robustness."
2308.12950_output/content.md,HumanEval,"Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively.","No, 本文是使用该数据集进行评测","We perform exhaustive evaluations of our models on major code generation benchmarks: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and APPS (Hendrycks et al., 2021), as well as a multilingual version of HumanEval (MultiPL-E, Cassano et al., 2023)...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2308.16458_output/content.md,BioCoder,"We present BioCoder, a benchmark developed to evaluate LLMs in generating bioinformatics-specific code.",Yes,"We introduce BioCoder (see Figure 1), a benchmark for code generation incorporating 2,269 bioinformatics-specific coding problems.",https://github.com/gersteinlab/biocoder and https://biocoder-benchmark.github.io/,"All datasets, benchmark, Docker images, and scripts required for testing are available at: https://github.com/gersteinlab/biocoder and https://biocoder-benchmark.github.io/.",评估大型语言模型在生成生物信息学特定代码方面的能力。,"BioCoder benchmark mainly targets bioinformatics data analysis, which tasks such as managing various biological data formats, understanding processing workflows, and utilizing APIs of various packages.",代码生成功能正确性、处理复杂上下文（跨文件依赖、类声明、全局变量）的能力、领域特定知识（生物信息学）,"BioCoder spans much of the field, covering cross-file dependencies, class declarations, and global variables. ... They contain domain-specific knowledge of bioinformatics, beyond just general coding capability.",模糊测试框架，通过执行测试用例来验证生成代码的功能正确性，并计算Pass@K率。,"BioCoder incorporates a fuzz-testing framework for evaluation. ... Our benchmark results, derived from 1,000 iterations, indicate the Pass@K rate.",跨文件依赖、包含类声明和全局变量的完整项目上下文,"BioCoder spans much of the field, covering cross-file dependencies, class declarations, and global variables. ... we included all potentially required class declarations in the input.",生物信息学，包括生物数据分析、遗传测序、DNA/RNA分析、生物信息学软件开发,"Here, we target bioinformatics due to the amount of domain knowledge, algorithms, and data operations this discipline requires. ... This project specializes in generating Python functions that address key bioinformatics topics such as genetic sequencing and DNA/RNA analysis.",具有挑战性、实用的生物信息学场景，包含日常数据分析任务和部分软件开发任务,"BIOCODER is a code generation benchmark designed for challenging, practical bioinformatics scenarios, ... This domain encapsulates the majority of daily tasks encountered by bioinformaticians in data analysis.",Python 和 Java,"It incorporates 1,026 Python functions and 1,243 Java methods extracted from GitHub, ... However, for the scope of this study, we focus on Python and Java, with the intention to expand to other languages in the future.","总共2,269个生物信息学特定的编码问题（公开集460个，隐藏集2,269个，相似集460个），包含1,026个Python函数和1,243个Java方法，以及来自Rosalind项目的253个Python示例。","a benchmark for code generation incorporating 2,269 bioinformatics-specific coding problems. ... It incorporates 1,026 Python functions and 1,243 Java methods extracted from GitHub, along with 253 examples from the Rosalind Project, all pertaining to bioinformatics.","从1,720个经同行评审生物信息学文章引用的GitHub仓库中提取，以及来自Rosalind项目的示例。","curated from 1,720 bioinformatics repositories referenced in peer-reviewed bioinformatics articles. ... We included an additional 253 questions from the Rosalind project.",2024年5月20日（根据arXiv版本v5）,arXiv:2308.16458v5 [cs.LG] 20 May 2024,官方自建，通过自动过滤、GPT辅助过滤和人工检查相结合的方式构建。,"We ensure that each function in our dataset requires a certain level of domain expertise in bioinformatics through a combination of automatic filtering, GPT-assisted filtering, and manual inspection.",经过严格过滤、数据清洗和预处理，以防止模型记忆。,"It has undergone rigorous filtering, extensive data cleaning, and preprocessing to prevent models from memorizing.",,,代码生成（生成完整的函数或方法体）,a benchmark for code generation ... generating bioinformatics-specific code.,Pass@K率,"Our benchmark results, derived from 1,000 iterations, indicate the Pass@K rate.",自然语言描述、代码规范、注释以及完整的上下文（包括依赖项、类声明、全局变量）,"We processed the data, rephrasing more detailed text descriptions, as well as associated comments and specifications, ... we included all potentially required class declarations in the input.",代码（Python或Java）,generating bioinformatics-specific code.,文本到代码,generating bioinformatics-specific code.,Docker环境，包含丰富的所需依赖项，用于在现实项目场景中进行测试。,"Testing incorporates a Docker environment, an abundance of required dependencies, ... This robust setup not only facilitates testing in realistic project scenarios",专注于生物信息学领域；包含跨文件依赖和完整项目上下文；规模远超同类领域特定基准（如CoderEval）；提供可扩展的解析工具和模糊测试工具；通过主题建模验证了代码覆盖范围具有代表性。,"Here, we target bioinformatics ... BioCoder spans much of the field, covering cross-file dependencies, class declarations, and global variables. ... our dataset surpasses the scale of CoderEval ... Using topic modeling, we show that the overall coverage of the included code is representative of the full spectrum of bioinformatics calculations. ... We provide an extendable parsing tool ... We provide a fuzz testing tool capable of scaling to handle substantial datasets."
2310.06770_output/content.md,SWE-bench,"We introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories.",Yes,"We introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories.",swebench.com,"Data, code, and leaderboard at swebench.com",评估语言模型在真实软件工程场景下的能力。给定一个代码库和一个需要解决的GitHub问题描述，模型的任务是编辑代码库以解决该问题。,"Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue.",真实世界软件工程问题解决能力，包括跨文件、跨函数、跨类的复杂代码编辑与协调能力，以及长上下文处理和复杂推理能力。,"Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks.",执行单元测试和系统测试。将生成的补丁应用到代码库后，运行与任务实例相关的测试。如果补丁应用成功且所有测试通过，则认为解决方案成功解决了问题。主要指标是解决问题的任务实例百分比。,"To evaluate a proposed solution, we apply the generated patch, using unix's patch program, to the codebase and then execute the unit and system tests associated with the task instance. If the patch applies successfully and all of these tests pass we consider the proposed solution to have successfully resolved the issue. The metric for our benchmark is the percentage of task instances that are resolved.",多文件项目。任务涉及理解大型、复杂的代码库，并跨多个函数、类甚至文件进行协调更改。,"Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously... SWE-bench's reference solutions average editing 1.7 files, 3.0 functions, and 32.8 lines (added or removed).",软件工程，具体为修复Bug或实现新功能。,SWE-bench is a benchmark featuring GitHub issues from popular repositories that report bugs or request new features...,工程级。基于真实GitHub问题，需要处理大型代码库和复杂的跨上下文编辑，对现有最先进模型极具挑战性。,"Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere 1.96% of the issues.",Python。数据集源自12个流行的开源Python仓库。,"SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories.","包含2,294个任务实例。","Through these stages of filtering, the original 90,000 PRs are filtered down to the 2,294 task instances which comprise SWE-bench.",从12个流行的开源GitHub Python仓库中抓取的真实GitHub问题和已合并的拉取请求。,SWE-bench sources task instances from real-world Python repositories by connecting GitHub issues to merged pull request solutions that resolve related tests.,2024 (论文版本为2024年11月11日),arXiv:2310.06770v3  [cs.CL]  11 Nov 2024,官方自建。作者通过一个三阶段的筛选流程从GitHub数据中构建。,"To find high-quality task instances at scale, we use a 3-stage pipeline as follows.",抗污染设计。基准可以持续更新，包含模型训练日期之后创建的问题，确保解决方案未包含在训练语料中。,"Therefore, we can extend SWE-bench with a continual supply of new task instances and evaluate LMs on issues created after their training date, which ensures that the solution was not included in their training corpus.",,,代码编辑/修复。生成应用于现有代码库的补丁以解决问题。,Each task requires generating a patch describing changes to apply to the existing codebase.,问题解决率（百分比）。,The metric for our benchmark is the percentage of task instances that are resolved.,自然语言（问题描述）与代码（完整代码库）。,A model is given an issue text description and a complete codebase.,代码（补丁文件）。,"The model is then tasked to make an edit to the codebase to resolve the issue. In practice, we represent edits as patch files...",文本与代码到代码。,"Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue.",仓库的测试框架。需要安装依赖并运行单元测试和系统测试。,The revised codebase is then evaluated using the repository’s testing framework.,1. 真实世界软件工程任务：源自真实GitHub问题和解决方案。2. 可持续更新：可轻松扩展到任何GitHub Python仓库，最小化人工干预。3. 多样化的长输入：问题描述详细，代码库包含数千个文件。4. 鲁棒的评估：每个任务实例至少有一个“失败转通过”的测试。5. 跨上下文代码编辑：不限制编辑范围，需要在大代码库的多个位置生成修订。6. 解决方案的广泛空间：可作为比较检索、长上下文模型和决策智能体的公平平台。,"SWE-bench offers several advantages over existing LM programming benchmarks. These include, a realistic setting that utilizes user-submitted issues and solutions, diverse inputs featuring unique code problems from 12 repositories, a robust framework for execution-based evaluation, and the ability to continuously update the benchmark with new instances, requiring minimal human intervention."
2312.02120_output/content.md,HumanEval,"Notably, MagicoderS-CL-7B based on CODELLAMA even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1).","No, 本文是使用该数据集进行评测","We evaluate Magicoder and MagicoderS on a wide range of coding tasks, including HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for Python text-to-code generation...",,,Python文本到代码生成，衡量从自然语言描述（docstrings）合成程序的功能正确性。,"...HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for Python text-to-code generation...",代码生成功能正确性,...indicating that MagicoderS-CL can generate more robust code.,pass@1,...surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1).,,,,,,,Python,"...HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for Python text-to-code generation...",,,,,,,,,文中提及了去污染措施，表明存在污染风险并进行了处理,"Finally, we apply the same logic as StarCoder Li et al. (2023) to decontaminate our training data by removing coding problems that contain docstrings or solutions from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021)...",,,代码生成,...for Python text-to-code generation...,pass@1,...surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1).,自然语言,...for Python text-to-code generation...,代码,...for Python text-to-code generation...,文本到代码,...for Python text-to-code generation...,,,,
2402.16906_output/content.md,HumanEval,"Experiments demonstrate that LDB consistently enhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and TransCoder benchmarks","No, 本文是使用该数据集进行评测","We evaluate LDB on three code generation benchmarks: HumanEval (Chen et al., 2021), TransCoder (Roziere et al., 2020), and MBPP (Austin et al., 2021).",,,文本到代码生成，任务描述是一段简要的自然语言段落，概述了要生成的程序的预期功能。,"HumanEval and MBPP are for text-to-code generation, where the task description is a brief passage outlines the intended functionality of the program to be generated.",代码生成的功能正确性,Experiments demonstrate that LDB consistently enhances the baseline performance... in code debugging for various LLM selections.,使用隐藏测试用例计算 Pass@1 准确率,We compute Pass@1 accuracy with hidden test cases for assesment.,,,,,基础编程问题,"Despite these advanced approaches, they still fall short on basic programming questions from the HumanEval and MBPP datasets.",Python,文中仅提及实验设置（评测文本到代码生成），未描述完整数据集的语言构成。,,,,,2021,"HumanEval (Chen et al., 2021)",,,,,,,代码生成,HumanEval and MBPP are for text-to-code generation,Pass@1,We compute Pass@1 accuracy with hidden test cases for assesment.,自然语言,the task description is a brief passage outlines the intended functionality of the program to be generated.,代码,text-to-code generation,文本到代码,text-to-code generation,,,,
2403.07974_output/content.md,LiveCodeBench,"In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code",Yes,"In this work, we propose LiveCodeBench",https://livecodebench.github.io/,Website: https://livecodebench.github.io/,对大型语言模型的代码相关能力进行全面评估，包括代码生成、自我修复、代码执行和测试输出预测等多个方面。,"Notably, our benchmark also focuses on a broader range of code-related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation.",代码相关能力的全面性评估，包括生成、修复、执行和理解。,"Holistic Evaluation. Current code evaluations primarily focus on natural language to code generation. However, programming is a multi-faceted task that requires a variety of capabilities beyond those measured by code generation.",基于功能正确性，使用一组未见过的测试用例进行评估。对于代码生成场景，使用Pass@1指标。,"The evaluation is performed based on functional correctness, using a set of unseen test cases. We use the Pass@1 metric measured as the fraction of the problems for which the model was able to generate a program passing all tests.",单问题解决，上下文为自然语言问题描述和示例测试。,"The model is given a problem statement, which includes a natural language description and example tests (input-output pairs), and is tasked with generating a correct solution.",竞争性编程（算法问题），来源于LeetCode、AtCoder和CodeForces等竞赛平台。,"which collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces.",平衡的难度分布，包含从易到难的问题，并使用平台提供的难度评级进行分类和过滤。,"Therefore, we use problem difficulty ratings (sourced from the competition websites) for filtering the harder problems and classifying problem difficulties to ensure balanced problem difficulty distribution and allow granular model comparisons.","文中未明确提及数据集支持的具体编程语言，但来源平台（LeetCode, AtCoder, CodeForces）通常支持多种语言。",,目前包含超过500个编码问题（511个），收集于2023年5月至2024年5月期间。,"Currently, LiveCodeBench hosts over five hundred coding problems that were published between May 2023 and May 2024. / Particularly, we have collected 511 problems from contests across three competition platforms – LeetCode, AtCoder, and CodeForces occurring from May 2023 to the present (May 2024)",从三个竞赛平台（LeetCode、AtCoder、CodeForces）的每周比赛中收集的新问题。,"Particularly, we collect problems from weekly contests on competition platforms and tag them with a release date.",2024年6月6日（论文版本日期），数据集持续更新至2024年5月。,arXiv:2403.07974v2  [cs.SE]  6 Jun 2024,官方自建，由研究团队从公开竞赛平台收集和构建。,"In this work, we propose LiveCodeBench",抗污染设计，通过持续更新（使用新发布的问题）和时间分段评估来防止数据污染。,"Live updates to prevent contamination. / to prevent the risk of problem contamination, we use live updates, that is evaluate models on new problems.",,,代码生成、代码修复、代码执行、测试输出预测。,"Specifically, we evaluate code LLMs in four scenarios, namely code generation, self-repair, code execution, and test output prediction.",Pass@1（用于代码生成场景）。,We use the Pass@1 metric measured as the fraction of the problems for which the model was able to generate a program passing all tests.,自然语言问题描述、示例测试、错误程序（用于修复）、代码和输入（用于执行）。,"The model is given a problem statement, which includes a natural language description and example tests (input-output pairs) / The model is given the natural language problem description, the incorrect program, the test case it fails on, and the execution feedback from that failure. / The model is given a program and an input",代码（用于生成和修复）、执行结果（用于执行）、测试输出（用于预测）。,The output should be a correct repaired program. / the output should be the result. / the output should be the output for the problem.,文本到代码（生成）、代码与反馈到代码（修复）、代码与输入到结果（执行）、文本与输入到结果（预测）。,generating code from natural language / Fix an incorrect program from execution information / “Execute” a program on an input / Solve the natural language task on a specified input,,,1. 持续更新以防止数据污染。2. 全面的评估场景，超越单一的代码生成。3. 来源于高质量竞赛平台的问题和测试。4. 平衡的问题难度分布。,Live updates to prevent contamination. / Holistic Evaluation. / High-quality problems and tests. / Balanced problem difficulty.
2403.08604_output/content.md,DevEval,"In this case study, we explore the performance of LLMs across the entire software development lifecycle with DevEval",Yes,"To address these shortcomings and fill this gap, we present DevEval, a comprehensive case study that mirrors real-world software development.",https://github.com/open-compass/DevEval,1Our data and code are available at https://github.com/open-compass/DevEval.,评估大型语言模型在整个软件开发生命周期中的能力，包括从详细的产品需求文档（PRD）开始，构建一个多文件的代码库。,DevEval generally evaluates models on the task of constructing a multi-file codebase starting from a product requirement document (PRD) of detailed specifications.,软件开发生命周期的全面能力，包括软件设计、环境配置、实现、验收测试和单元测试。,"DevEval encompasses stages including software design, environment setup, implementation, acceptance testing, and unit testing.","根据任务不同采用多种方法：软件设计任务使用LLM作为评判者进行主观评估；环境配置任务评估依赖文件执行和示例代码运行的成功率；实现任务使用自动化测试框架（如PyTest, GTest, JUnit, Jest）执行参考测试并计算通过率；测试任务评估测试代码的可执行性（Oracle Test）和代码覆盖率。","Since the Software Design tasks are open-ended, we employ the LLM-as-a-judge approach... The evaluation is anchored by two principal metrics: general principles and faithfulness. The principal metric for evaluation in this task is the success rate of the executed example code. The evaluation procedure involves executing reference acceptance and unit tests within a predefined reference environment. Then the evaluation metric is determined by the pass rate of these tests. ...models generally fail to generate executable tests, with oracle test scores falling below 40%... the generated testing code demonstrates potential in code coverage, achieving as high as 79.4% when it is executable.",多文件项目级，需要理解产品需求文档、UML图、架构设计等完整上下文。,DevEval generally evaluates models on the task of constructing a multi-file codebase starting from a product requirement document (PRD) of detailed specifications.,涵盖多个工程和AI领域，包括数据库应用、Web服务、算法实现、API开发、神经网络、计算机视觉、自然语言处理等。,"DevEval covers a range of domains including NLP, computer vision, deep learning, algorithm implementation, API applications, Database applications, web service (both frontend and backend), and general tools and utilities.",工程级，模拟真实世界的软件开发挑战，现有顶级模型（如GPT-4）得分很低。,"Empirical studies show that current LLMs, including GPT-4, fail to solve the challenges presented within DevEval. GPT-4-Turbo achieves the highest scores amongst all evaluated models, yet it obtains less than 10% on our repository-level implementation task.","Python, C/C++, Java, JavaScript (Vue.js)","DevEval features four programming languages... we curated a collection of 22 repositories across four programming languages (Python, C/C++, Java, JavaScript)",包含22个代码仓库，覆盖4种编程语言和多个领域。平均每个仓库包含约2-7个代码文件，276-617行代码，以及若干验收测试和单元测试。,"we curated a collection of 22 repositories across four programming languages... Table 2: DevEval Statistics. Avg. #Code File 2.2-7.0, Avg. #Code Line 276-617, Avg. #Accep. Tests 2-5.4, Avg. #Unit Tests 8.2-12.4",从公开仓库中精心策划收集，并补充了完整软件开发所需的设计文档和测试程序。,"One significant challenge in this study lies in the scarcity of publicly available repositories that include the full range of software development artifacts, particularly design documents and comprehensive testing programs. To overcome this, we curated a collection of 22 repositories...",2024,arXiv:2403.08604v3 [cs.CL] 14 Dec 2024,官方自建，由研究团队为评估目的而构建。,"we present DevEval, a comprehensive case study... To overcome this, we curated a collection...",,,,,代码库生成，涉及从设计到测试的完整软件开发流程。,DevEval generally evaluates models on the task of constructing a multi-file codebase starting from a product requirement document (PRD) of detailed specifications.,软件设计：基于通用原则和忠实度的LLM主观评分；环境配置：示例代码执行成功率；实现：参考验收和单元测试的通过率；测试：Oracle Test分数和代码覆盖率。,"The evaluation is anchored by two principal metrics: general principles and faithfulness... The principal metric for evaluation in this task is the success rate of the executed example code... the evaluation metric is determined by the pass rate of these tests... oracle test scores... code coverage, achieving as high as 79.4%",自然语言（产品需求文档PRD）、结构化图表（UML图、架构设计）、代码。,"models are provided with the PRD, UML diagrams and architecture design... Table 1: Task design in DevEval. Input: PRD, UML Diagrams, Architecture Design, Implementation Code",代码（依赖文件、实现代码、测试代码）、结构化设计文档（UML图、架构设计）。,"Table 1: Task design in DevEval. Output: UML Diagrams, Architecture Design, Dependency Files, Implementation Code, Acceptance Testing Code, Unit Testing Code",文本与图表到代码、文本与图表到设计文档。,DevEval generally evaluates models on the task of constructing a multi-file codebase starting from a product requirement document (PRD)... The first subtask involves the generation of the UML class diagram...,"使用Docker定义的基础环境，针对不同语言使用特定工具（Conda, Gradle, NPM）和测试框架（PyTest, GTest, JUnit, Jest）。","The evaluation centers on the execution of dependency files across each programming language within a predetermined base environment delineated in a Docker file... For Python, the Conda environment manager is employed; for Java and JavaScript, Gradle and NPM are utilized respectively... integrates PyTest for Python, GTest for C++, JUnit for Java, and Jest for JavaScript.",首个评估模型软件设计和环境配置能力的基准；遵循瀑布模型，将软件开发分解为多个相互关联的阶段；提供文档级别的详细需求描述；采用模块化评估协议，支持端到端或分阶段评估。,"DevEval is the first to evaluate models’ software design and environment setup capabilities. Subscribing to the traditional Waterfall software development model... DevEval breaks down this process into a diverse set of inter-related development stages... In contrast to similar systems... which generate outputs from brief requirement descriptions typically under 100 words, DevEval offers document-level detail to guide the models. ...our design utilizes reference inputs for each task. This strategy enables and concentrates on evaluating the efficacy of models in executing specific tasks."
2406.06887_output/content.md,"HumanEval, MBPP, LiveCodeBench, LeetCode","PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench)... We evaluate PLUM on a diverse set of state-of-the-art code language models under different set-ups, on commonly used evaluation benchmarks: HumanEval(+) and MBPP(+) [7, 1, 28] as well as more challenging code generation datasets like LiveCodeBench and LeetCode [22, 19].","No, 本文是使用该数据集进行评测","PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench)... We evaluate PLUM on a diverse set of state-of-the-art code language models under different set-ups, on commonly used evaluation benchmarks: HumanEval(+) and MBPP(+) [7, 1, 28] as well as more challenging code generation datasets like LiveCodeBench and LeetCode [22, 19].",,,文中未描述这些基准的原始任务定义，仅提及它们是用于评估代码生成模型的常用基准。,"PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench)... We evaluate PLUM on a diverse set of state-of-the-art code language models under different set-ups, on commonly used evaluation benchmarks: HumanEval(+) and MBPP(+) [7, 1, 28] as well as more challenging code generation datasets like LiveCodeBench and LeetCode [22, 19].",代码生成的功能正确性,Works like AlphaCode [26] and LeTI [48] have introduced test outcomes as a means to define functional correctness in code generation.,使用测试用例执行来评估功能正确性,Works like AlphaCode [26] and LeTI [48] have introduced test outcomes as a means to define functional correctness in code generation.,,,通用编程问题,These datasets provide a diverse range of programming tasks and instructions.,标准基准和更具挑战性的基准,"PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench)... as well as more challenging code generation datasets like LiveCodeBench and LeetCode [22, 19].",文中仅提及实验聚焦于Python，未描述完整数据集的语言覆盖范围。,We focus on Python due to its wide use and the availability of well-established training and evaluation resources.,,,,,,,,,,,,,代码生成,"Language models pre-trained on code corpora have excelled at code generation [40, 25].",通过率 (pass rates),"PLUM increases pass rates by up to 4.8% on average on standard benchmarks and 11.8% on LiveCodeBench, demonstrating its effectiveness and generalizability.",自然语言指令,"PLUM utilizes natural language instructions from well-established datasets such as OSS-Instruct [49], Evol-Instruct-Code [31], and ShareGPT [8].",代码,"These solutions are evaluated using the generated test cases, with preference labels assigned based on the results: solutions passing the tests are preferred, while failures are dis-preferred.",文本到代码,PLUM utilizes natural language instructions from well-established datasets... These solutions are evaluated using the generated test cases...,文中未明确描述基准的执行环境，但提及了执行检查。,"With static and execution checks,3 we identify and filter out solutions that contain syntactic errors and fail to execute, as our focus is on functional correctness.",本文提出的PLUM框架本身是一个使用测试用例进行执行引导、基于策略的偏好学习方法，用于改进代码语言模型。它并非一个数据集。,"we propose PLUM, an on-policy Preference Learning framework Augmented with test cases for code LMs."
2406.18294_output/content.md,CrossCodeEval,"To assess the code completion performance of Code LLMs in real development scenarios, we utilized CrossCodeEval (Ding et al., 2023) as the evaluation dataset.","No, 本文是使用该数据集进行评测","To assess the code completion performance of Code LLMs in real development scenarios, we utilized CrossCodeEval (Ding et al., 2023) as the evaluation dataset.",,,评估代码大语言模型在真实开发场景中的代码补全性能，特别是需要利用跨文件代码信息进行补全的任务。,"To assess the code completion performance of Code LLMs in real development scenarios... The CrossCodeEval (Ding et al., 2023) benchmark provides test cases that require the use of cross-file code information for completion.",仓库级代码补全能力，利用跨文件信息的能力,"Some benchmarks for repository-level code completion have been proposed to evaluate the performance of code models in real-world completion tasks, such as CrossCodeEval (Ding et al., 2023)...","精确匹配（Exact Match, EM）和编辑相似度（Edit Similarity, ES）","Following the CrossCodeEval evaluation protocol, we evaluated the completion results using two metrics: Exact Match (EM) and Edit Similarity (ES).",跨文件、仓库级上下文,"The CrossCodeEval (Ding et al., 2023) benchmark provides test cases that require the use of cross-file code information for completion.",,,真实世界开发场景,To assess the code completion performance of Code LLMs in real development scenarios...,文中仅提及实验子集(Python)，未描述完整数据集,"Without loss of generality, in this study, we have chosen Python language as the primary language for our research.","本文实验使用了2,655个真实世界补全测试用例","Ultimately, we obtained 2,655 real-world completion tests.",,,2023,"CrossCodeEval (Ding et al., 2023)",,,,,,,代码补全（包括中段填充）,Infilling scenarios constitute the majority of code completion tasks in the real world.,"Exact Match (EM), Edit Similarity (ES)","Following the CrossCodeEval evaluation protocol, we evaluated the completion results using two metrics: Exact Match (EM) and Edit Similarity (ES).",代码（包含部分缺失的代码上下文）,We then removed the code after the cursor in that line to form authentic completion test cases.,代码,The completion results are less than satisfactory.,代码到代码（补全）,Infilling scenarios constitute the majority of code completion tasks in the real world.,,,专注于评估模型利用跨文件代码信息进行仓库级代码补全的能力，测试用例需要跨文件信息。,"The CrossCodeEval (Ding et al., 2023) benchmark provides test cases that require the use of cross-file code information for completion."
2408.06450_output/content.md,EVALPERF,"As a proof of concept, we use DPE to create EVALPERF, a benchmark with 121 performance-challenging coding tasks.",Yes,"We introduce Differential Performance Evaluation (DPE), a framework designed to reliably evaluate Large Language Models (LLMs) for efficient code generation. ... As a proof of concept, we use DPE to create EVALPERF, a benchmark with 121 performance-challenging coding tasks.",github.com/evalplus/evalplus,We also fully open-source and maintain the data curation pipeline and evaluator at github.com/evalplus/evalplus as part of EvalPlus.,评测大型语言模型（LLMs）生成的代码的效率。该基准旨在通过提供具有性能挑战性的编程任务和有效的复合指标，可靠地评估代码效率，弥补传统代码基准在效率评估方面的不足。,"We introduce Differential Performance Evaluation (DPE), a framework designed to reliably evaluate Large Language Models (LLMs) for efficient code generation. Traditional coding benchmarks often fail to provide reliable insights into code efficiency, due to their reliance on simplistic test inputs and the absence of effective compound metrics. DPE addresses these issues by focusing on efficiency-demanding programming tasks and establishing an insightful compound metric for performance evaluation.",代码生成效率,"While the correctness evaluation of code generation has been well studied, we deliver a new and important aspect to the community by studying the data curation and assessment for the efficiency evaluation of LLM-generated code.",差分性能评估（DPE）。该方法分为两个阶段：1）数据集构建：从现有基准中选择对效率有要求的任务，并生成计算密集型的输入来测试LLM解决方案的效率。2）效率评估：分析新解决方案的性能，并将其与一组具有不同效率水平的参考解决方案进行全局比较，匹配到的效率水平决定了其效率得分。,"DPE operates in two phases: To curate efficiency datasets, it selects efficiency-demanding tasks from existing coding benchmarks and generates computationally expensive inputs to stress the efficiency of LLM solutions. To assess the code efficiency, DPE profiles the new solution and compares it globally against a set of reference solutions that exhibit distinct efficiency levels, where the matched level defines its efficiency score.",,,通用编程任务，涵盖算法、数据结构等,"At the high level, the input to the dataset creation phase is a set of programming tasks, e.g., from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021).",性能挑战型任务，旨在区分不同代码解决方案的效率,DPE curates performance-demanding coding tasks by sampling synthesized test input generators and using filters to ensure evaluator quality.,Python（从HumanEval和MBPP等基准中选取任务，这些基准主要使用Python）,"At the high level, the input to the dataset creation phase is a set of programming tasks, e.g., from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021).",包含121个性能挑战型编程任务,"As a proof of concept, we use DPE to create EVALPERF, a benchmark with 121 performance-challenging coding tasks.",从现有代码生成基准（如HumanEval、MBPP）中筛选和改造任务，并利用LLM合成性能测试输入生成器来生成计算密集型测试输入。,"At the high level, the input to the dataset creation phase is a set of programming tasks, e.g., from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). ... To this end, we propose Synthesizing a Synthesizer (SAS) to automatically produce performance-exercising inputs of different programming tasks by prompting powerful code LLMs to generate test generators.",2024.08,arXiv:2408.06450v1  [cs.SE]  12 Aug 2024,官方自建，基于DPE框架从现有基准中筛选和增强,"As a proof of concept, we use DPE to create EVALPERF, a benchmark with 121 performance-challenging coding tasks.",,,,,代码生成（从自然语言描述生成完整的函数/解决方案）,"Given a programming task, we collect a rich set of correct solutions by sampling various LLMs and test execution.",效率得分（基于与参考解决方案性能集群的匹配度）,"During evaluation, if passing the correctness tests, the new solutions are profiled to compare against the reference solutions. Specifically, from slow to fast, the cumulative ratio of the cluster that includes the matched reference solution is the efficiency score of the evaluated solution.",自然语言（任务描述）,"Given a coding instruction in natural language, LLMs produce solutions...",代码,LLMs produce solutions whose correctness is assessed through test execution.,文本到代码,"Given a coding instruction in natural language, LLMs produce solutions...",,,1. 专注于代码效率评估，而非传统的功能正确性。2. 使用DPE框架，包含“合成一个合成器”（SAS）方法来自动生成性能测试输入。3. 通过性能聚类建立参考解决方案集，用于全局比较和评分。4. 包含任务筛选标准（足够的计算量、低性能变异、性能多样性）以确保评估质量。,"DPE addresses these issues by focusing on efficiency-demanding programming tasks and establishing an insightful compound metric for performance evaluation. ... To this end, we propose Synthesizing a Synthesizer (SAS) to automatically produce performance-exercising inputs of different programming tasks by prompting powerful code LLMs to generate test generators. ... a selected programming task must meet the following criteria: 1. Sufficient computation ... 2. Low performance variation ... 3. Performance diversity..."
2410.01215_output/content.md,HumanEval,MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].,"No, 本文是使用该数据集进行评测","Extensive experiments across multiple models and benchmarks demonstrate that MGDebugger significantly outperforms existing debugging methods. Using open-source models like DeepSeek-Coder-V2-Lite, CodeQwen1.5, and Codestral, MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].",,,文中仅提及实验使用，未描述完整数据集。根据引用[7]（HumanEval）推断，该基准旨在评估从自然语言描述（docstring）生成Python代码的功能正确性。,"Following the problem settings from Chen et al. [9] and Zhong et al. [75], we assume that the public test cases for the main function Tpub have been provided, which is common in most code generation benchmarks [7, 14, 44]2.",代码生成的功能正确性,MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].,执行单元测试（pass@k）,"Following the problem settings from Chen et al. [9] and Zhong et al. [75], we assume that the public test cases for the main function Tpub have been provided, which is common in most code generation benchmarks [7, 14, 44]2.",单函数,"Specifically, given an LLM-generated function 𝑓, we decompose it into a hierarchical structure of subfunctions denoted as (𝑓1, ..., 𝑓𝑛).",,,,,Python,"To effectively test each subfunction, we propose an LLM-simulated Python executor, which traces code execution and tracks important variable states to pinpoint errors accurately.",,,,,,,,,,,,,代码生成,"Large language models (LLMs) such as GPT-4 [48], LLaMA [56], and DeepSeek-Coder [1] have made significant advances in AI-assisted coding tasks [7, 25, 34, 52]. Trained on vast corpora of text and code, LLMs can understand and generate code snippets for various programming tasks, ranging from simple data structures to complex algorithmic problems [34, 57].",准确率（accuracy），修复成功率（repair success rate）,MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].,自然语言（函数描述/docstring）,"Following the problem settings from Chen et al. [9] and Zhong et al. [75], we assume that the public test cases for the main function Tpub have been provided, which is common in most code generation benchmarks [7, 14, 44]2.",代码（Python函数）,"Specifically, given an LLM-generated function 𝑓, we decompose it into a hierarchical structure of subfunctions denoted as (𝑓1, ..., 𝑓𝑛).",文本到代码,"Following the problem settings from Chen et al. [9] and Zhong et al. [75], we assume that the public test cases for the main function Tpub have been provided, which is common in most code generation benchmarks [7, 14, 44]2.",,,本文未描述HumanEval的独特之处，而是将其作为评估模型调试能力的标准基准之一。,"Extensive experiments across multiple models and benchmarks demonstrate that MGDebugger significantly outperforms existing debugging methods. Using open-source models like DeepSeek-Coder-V2-Lite, CodeQwen1.5, and Codestral, MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44]."
2411.05830_output/content.md,GitChameleon,"To address this gap, we introduce GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests.",Yes,"To address this gap, we introduce GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests.",https://github.com/NizarIslah/GitChameleon,"For facilitation for further exploration of version-conditioned code generation, we make our code repository publicly accessible at https://github.com/NizarIslah/GitChameleon.",评估大型语言模型生成特定版本库代码的能力，要求生成的代码不仅语法正确，而且在执行时功能准确。,GitChameleon is designed to rigorously assess the ability of modern large language models (LLMs) to generate version-specific code that is not only syntactically correct but also functionally accurate upon execution.,版本特定代码生成能力、代码库动态适应性、功能正确性,"By providing an execution-based benchmark that emphasizes the dynamic nature of code libraries, GitChameleon serves as a critical tool to advance the development of more adaptable and reliable code generation models.",基于执行的评估，使用手写的基于断言的单元测试来验证生成代码的正确性。,"To evaluate LLM performance on GitChameleon, each problem is accompanied by handwritten assertion-based unit tests, enabling a thorough execution-based assessment of the outputs generated by the code LLMs.",单函数代码补全,"GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems","Python编程，涉及多个流行的机器学习和数据处理库（如PyTorch, NumPy, Scikit-Learn, Pandas等）","GitChameleon consists of 116 python-based version conditioned problems based on 11 libraries: PyTorch (Paszke et al., 2019), Geopandas (Jordahl et al., 2020), NLTK (Bird & Loper, 2004), NetworkX (Hagberg et al., 2008), GeoPy3, Gradio (Abid et al., 2019), Scikit-Learn (Buitinck et al., 2013), Matplotlib (Hunter, 2007), PyCaret4, Pandas (pandas development team, 2020; Wes McKinney, 2010) and NumPy (Harris et al., 2020).",基于真实版本变化的实际问题，旨在模拟开发者在技术债务约束下的真实场景。,our dataset offers a unique and complementary perspective by focusing on the real-world scenario where developers are often constrained to specific library versions due to technical debt.,Python,"GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems",包含116个Python版本条件问题,"GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems",手工编写和LLM辅助，基于真实库的变更日志（changelogs）,"The examples were manually crafted by the authors, who divided the task among themselves. We compiled a list of popular Python libraries, focusing on those with which at least one author was familiar and that had detailed changelogs documenting changes between versions.",2024-11-05 (根据arXiv版本v1日期),arXiv:2411.05830v1  [cs.SE]  5 Nov 2024,官方自建（作者手工编写）,"The examples were manually crafted by the authors, who divided the task among themselves.",已考虑训练数据截止日期，确保样本在模型训练窗口期内，以评估模型对已训练版本的知识。,"Since some of the models evaluated on GitChameleon have disclosed their training data cutoff dates, we have ensured that most, if not all, samples fall within the training window of these models.",,,代码补全,"GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems",pass@k (例如pass@10),"for instance, GPT-4o achieves a pass@10 of only 39.9% (43.7% when provided with error feedback)",自然语言问题描述和起始代码,"The problem statements average 20.4 tokens, and the starter code averages 47.4 tokens, leading to a combined average of 67.8 tokens per sample.",代码,generate version-specific code,文本到代码,synthesizing programs from docstrings.,需要特定库版本依赖的虚拟环境,We used venv to create and manage virtual environments for testing. This process involved installing the appropriate library version and any additional dependencies.,专注于评估模型对真实库版本变化的适应能力，每个问题都绑定到特定的库版本，并配有可执行的单元测试。数据基于2014年至2023年的真实版本发布，并标注了变更类型（参数/属性变更、函数名变更、语义/行为变更、新功能）。,"GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. ... The samples were collected from version releases over a period from the year 2014 to 2023 ... we annotate each sample with the type of change that is classified into the following categories: Argument or Attribute change, Function Name change, Semantics or Function Behavior change, New feature or additional dependency-based change."
2411.12882_output/content.md,PurpleLlama secure coding benchmark,"We demonstrate the effectiveness of PROSEC on the PurpleLlama (Bhatt et al., 2023) secure coding benchmark.","No, 本文是使用该数据集进行评测","We demonstrate the effectiveness of PROSEC on the PurpleLlama (Bhatt et al., 2023) secure coding benchmark.",,,评估代码大语言模型生成安全代码的能力，检测其生成的代码是否包含常见弱点枚举（CWE）中定义的安全漏洞。,"The goal of security alignment in code LLMs is to reduce the likelihood of generating insecure code... We demonstrate the effectiveness of PROSEC on the PurpleLlama (Bhatt et al., 2023) secure coding benchmark.",代码安全性，即生成代码是否遵循安全编码实践，避免引入常见漏洞。,"The safety of these models remains under-explored, posing potential risks as insecure code generated by these models may introduce vulnerabilities into real-world systems.",使用静态分析器作为预言机，检测生成的代码片段中是否存在CWE漏洞。,"Following previous work (Bhatt et al., 2023), we assume that there exists a static analyzer... as an oracle to detect whether a snippet of code follows secure code patterns. Specifically, the static analyzer takes as input a code snippet, and outputs a list of detected CWEs.",,,安全编码，涵盖常见弱点枚举（CWE）中定义的各种软件安全漏洞类型。,"The Common Weakness Enumerations (CWEs) (MITRE, 2023), which abstract diverse program vulnerabilities, offer a generalizable foundation for simulating how vulnerabilities manifest across various coding tasks and programming languages.",,,文中未明确提及PurpleLlama基准支持的具体编程语言，但本文实验涉及多种语言。,"PROSEC improves the ability of code LLMs to generate secure code... across multiple models, languages, and vulnerability types.",,,,,2023,"PurpleLlama (Bhatt et al., 2023)",,,,,,,代码生成,Large language models (LLMs) capable of generating code based on human instructions have revolutionized programming by significantly facilitating tasks such as code generation...,安全代码生成能力的提升百分比（例如，比基线模型安全25.2%–35.4%）,The models trained with the dataset synthesized by PROSEC are 25.2%–35.4% more secure than those trained with the SafeCoder dataset.,自然语言指令,"Consider an instruction following code LLM πθ(y|x) = Πiπθ(yi|y<i, x) that takes user instruction x and generates the code response y.",代码,"Consider an instruction following code LLM πθ(y|x) = Πiπθ(yi|y<i, x) that takes user instruction x and generates the code response y.",文本到代码,"Consider an instruction following code LLM πθ(y|x) = Πiπθ(yi|y<i, x) that takes user instruction x and generates the code response y.",,,专注于代码安全性的评测基准，使用CWE（常见弱点枚举）作为漏洞分类和检测的基础框架。,"A widely recognized framework for categorizing such issues is the Common Weakness Enumerations (CWE) (MITRE, 2023)... Following previous work (Bhatt et al., 2023), we assume that there exists a static analyzer... to detect whether a snippet of code follows secure code patterns... and outputs a list of detected CWEs."
2412.00535_output/content.md,FullStack Bench,FullStack Bench: Evaluating LLMs as Full Stack Coders,Yes,"To address this gap, we have developed a comprehensive code evaluation dataset FullStack Bench1,2 focusing on full-stack programming","https://huggingface.co/datasets/ByteDance/FullStackBench, https://github.com/bytedance/FullStackBench","1https://huggingface.co/datasets/ByteDance/FullStackBench
2https://github.com/bytedance/FullStackBench",评估大型语言模型作为全栈程序员的能力，涵盖广泛的应用程序领域。,"focusing on full-stack programming, which encompasses a wide range of application domains (e.g., basic programming, data analysis, software engineering, mathematics, and machine learning).",全栈编程能力、多语言编程能力、跨应用领域能力,focusing on full-stack programming... to assess multi-lingual programming capabilities... a wide range of application domains,使用单元测试用例进行代码沙箱执行评估,"we design real-world instructions and corresponding unit test cases... we also release an effective code sandbox execution tool (i.e., SandboxFusion3)... to evaluate the performance of our FullStack Bench efficiently.",,,基础编程、高级编程、软件工程、数据分析、数学、桌面与Web开发、机器学习、科学计算、数据库、多媒体、操作系统等,"Basic Programming (BP)
Advanced Programming (AP)
Software Engineering (SE)
Data Analysis (DA)
Mathematics (MA)
Desktop and Web Development (DW)
Machine Learning (ML)
Scientific Computing (SC)
DataBase (DB)
Multimedia (MM)
Operating System (OS)
Others",,,16种广泛使用的编程语言,"in FullStack Bench, we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages",,,设计真实世界的指令和对应的单元测试用例，而非简单翻译,we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations.,2025-05-12,arXiv:2412.00535v6  [cs.AI]  12 May 2025,官方自建,we have developed a comprehensive code evaluation dataset FullStack Bench,,,,,代码生成,evaluating LLMs as Full Stack Coders,,,自然语言指令,we design real-world instructions,代码,evaluating LLMs as Full Stack Coders,文本到代码,we design real-world instructions and corresponding unit test cases,支持多种编程语言和包的代码沙箱执行工具(SandboxFusion),"we also release an effective code sandbox execution tool (i.e., SandboxFusion3) supporting various programming languages and packages",专注于全栈编程评估，覆盖广泛的应用程序领域；包含16种编程语言的真实世界指令和单元测试，而非简单翻译；配套发布了支持多语言和多包的代码沙箱执行工具SandboxFusion。,"focusing on full-stack programming, which encompasses a wide range of application domains... we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations. Moreover, we also release an effective code sandbox execution tool (i.e., SandboxFusion3) supporting various programming languages and packages"
2412.05210_output/content.md,CodeArena,we present a rigorous human-curated benchmark CodeArena to emulate the complexity and diversity of real-world coding tasks,Yes,"we first introduce a comprehensive human-curated benchmark, CodeArena",https://codearenaeval.github.io/,1https://codearenaeval.github.io/,评估模型生成的代码响应与人类偏好的对齐程度，模拟现实世界编码任务的复杂性和多样性。,"to evaluate the alignment between the model-generated response and human preference, enabling the community to evaluate and track the alignment between human preferences and model-generated responses in real-world scenarios.",人类偏好对齐（包括代码质量、解释、格式、注释等），而非单纯的代码正确性。,"the current code LLMs focus on synthesizing the correct code snippet, ignoring the alignment with human preferences... underscoring the importance of the human preference alignment.",使用GPT-4o作为评判员，通过“比较A和B”和“比较B和A”两种游戏计算模型相对于基线的胜率。,"we apply GPT-4o-2024-08-06 as the judger to evaluate the model performance. Specifically, we use two games “compare A and B ” and “compare B and A” (avoid the relative position of A and B affecting the results) to calculate the win rate of A compared to the baseline B.",基于现实世界用户查询的完整问题，不局限于自包含的函数片段。,"Popular code-related benchmarks typically focus on self-contained function snippets... CodeArena provides many problems for evaluation under realistic scenarios, which are not suitable for verification through unit testing.",涵盖软件开发、用户界面/体验、专用计算、工具与环境、数据库与数据处理、新兴技术、通用查询等7大类40个子类。,comprising 397 high-quality samples across 40 categories derived from real-world user queries... encompassing 7 major categories and 40 subcategories.,分为简单、中等、困难三个级别，大部分样本为中等或困难。,"all samples are classified into easy, medium, and hard. The majority of the samples are recognized as medium or hard, presenting a significant challenge to LLMs.",涵盖44种编程语言，包括Python、C++、Java、JavaScript、HTML/CSS、SQL、Bash、Go、Rust、PowerShell、Google Apps Script等。,spanning 40 categories and 44 programming languages... CodeArena provides a valuable comprehensive benchmark for 40 subtasks and 44 programming languages,包含397个高质量样本。,comprising 397 high-quality samples... CodeArena consists of nearly 400 problems.,从在线问答网站的用户查询中精心筛选和人工标注。,carefully curated from user queries... derived from real-world user queries... Online Q&A,2024年12月（论文版本日期）,arXiv:2412.05210v1  [cs.CL]  6 Dec 2024,官方自建，经过严格的人工标注和质量控制流程。,We propose CodeArena comprised of 397 manually annotated samples... we implement a rigorous human annotation process involving 4 full-time employees proficient in various programming languages for human annotation and 4 other senior programming developers for quality check.,"进行了去污染处理，通过移除与现有基准（MultiPL-E, MBPP, McEval, NaturalCodeBench）的精确匹配（10-gram重叠）来确保提示的唯一性。","To avoid data leakage, we apply decontamination to ensure the uniqueness of prompts in CodeArena, by removing exact matches (10-gram word overlap) from MultiPL-E, MBPP, McEval, and NaturalCodeBench.",,,代码生成（根据自然语言描述生成代码片段或完整解决方案）。,The query should be sampled from the practical application scenarios and the model-generated responses should satisfy the human preference.,胜率（win rate），基于GPT-4o的偏好判断。,calculate the win rate of A compared to the baseline B.,自然语言（用户查询/问题描述）。,Each sample in CodeArena includes (question...),代码与自然语言混合（期望的响应包括代码片段以及详细的解释、格式化和注释）。,"Claude3.5 produces responses that include detailed explanations, well-structured formatting, and code comments, making it more favorable in terms of human preference.",文本到代码与文本（自然语言问题到包含代码和解释的响应）。,"synthesizing the correct code snippet... alignment with human preferences (including explanations, formatting, comments)",,,专注于评估代码LLM与人类偏好的对齐，而非单纯的代码执行正确性；样本来源于真实世界用户查询，覆盖广泛的编程语言和任务类别；包含人工标注的难度等级和基线答案。,"Unlike previous benchmarks, which consist of various programming exercises along with the corresponding test cases... our benchmarks emphasize a diverse range of programming languages that are commonly used in everyday programming tasks... provides a valuable comprehensive benchmark for 40 subtasks and 44 programming languages, which satisfies the evaluation in realistic scenarios."
2505.08503_output/content.md,ICVul,ICVul: A Well-labeled C/C++ Vulnerability Dataset with Comprehensive Metadata and VCCs,Yes,"we constructed ICVul, a dataset emphasizing data quality and enriched with comprehensive metadata, including Vulnerability-Contributing Commits (VCCs).",https://github.com/Chaomeng-Lu/ICVul.git,"The project, along with supporting scripts, is publicly available on GitHub1. 1https://github.com/Chaomeng-Lu/ICVul.git",用于训练和评估基于机器学习的软件漏洞检测模型。,"Machine learning-based software vulnerability detection requires high-quality datasets, which is essential for training effective models.",漏洞检测的数据质量、标签可靠性、元数据丰富度、数据平衡性。,"To address challenges related to data label quality, diversity, and comprehensiveness, we constructed ICVul, a dataset emphasizing data quality and enriched with comprehensive metadata...",文中未明确描述针对该数据集的标准评估方法。,,代码提交（Commit）、文件（File）、函数（Function）级别。,"ICVul provides a high-quality dataset tailored for training ML models to detect software vulnerabilities at the commit, function and file levels.",软件安全、漏洞检测。,Detecting and mitigating software vulnerabilities remains one of the most critical challenges in ensuring the security and integrity of modern software systems.,真实世界软件项目中的漏洞，复杂度高。,"With the growing complexity of applications and their codebases, traditional methods of vulnerability detection... struggle to keep up with the scale and intricacy of vulnerabilities present in large software projects.",C/C++,"ICVul, an Integrated and Comprehensive C/C++ Vulnerability dataset.",包含807个仓库，146个CWE类型，4327个修复提交，6862个文件，15396个函数，其中6276个为漏洞函数（占比41%）。,"ICVul 807 146 4,327 6,862 15,396 6,276 41%",从美国国家漏洞数据库（NVD）中筛选与GitHub修复提交相关联的CVE条目，并从中提取代码和元数据。,"We began by filtering Common Vulnerabilities and Exposures from the NVD, retaining only those linked to GitHub fix commits. Then we extracted functions and files along with relevant metadata from these commits...",2024年11月13日（数据收集日期）,"The dataset construction process began with the collection of 269,509 CVEs, gathered on November 13, 2024.",官方自建，并提供了可复现的构建框架。,We introduce a vulnerability collection framework that can be re-run at any time to ensure the dataset remains up-to-date.,通过ESC技术排除可疑提交，旨在提高标签可靠性，降低噪声。,"To further enhance label reliability, we developed the ESC (Eliminate Suspicious Commit) technique, ensuring credible data labels.",文中未提及。,,漏洞检测（分类任务）。,ICVul provides a high-quality dataset tailored for training ML models to detect software vulnerabilities...,文中未提及针对该数据集的标准评估指标。,,源代码（C/C++）及丰富的元数据（如提交信息、代码变更、仓库信息等）。,The dataset is stored in a relational-like database for improved usability and data integrity.,漏洞标签（是否包含漏洞，以及CWE类型）。,"the dataset provides clear vulnerability labels at the CWE type level, enabling the development and research of multi-class classification models.",代码到标签（漏洞分类）。,training ML models to detect software vulnerabilities,不涉及代码执行，是静态分析数据集。,,1. 包含漏洞引入提交（VCC）信息，使用SZZ算法追溯。2. 应用ESC技术过滤噪声数据，提高标签质量。3. 数据平衡性好（漏洞函数占比41%）。4. 以关系型数据库结构存储，包含仓库、提交、文件、函数、CVE-VCC映射五个互相关联的表。5. 专注于C/C++语言。,"Another key feature of ICVul is its inclusion of VCCs, which trace specific commits responsible for introducing vulnerabilities into the codebase using the state-of-the-art SZZ algorithm... ICVul incorporates the ESC (Eliminate Suspicious Commit) technique, further enhancing label reliability... the dataset achieves a much better balance ratio of 41% at the function level. ICVul comprises five interconnected tables: repository info, cve fc vcc mapping, commit info, file info, and function info."
2511.17330_output/content.md,SV-COMP,Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification.,"No, 本文是使用该数据集进行评测",Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification.,,,程序验证，即证明程序满足其形式化规范。,Formal program verification uses mathematically rigorous methods to prove that a program satisfies its specifications.,程序验证的自动化能力，特别是证明生成的有效性。,"We thoroughly evaluate our approach on existing mathematical lemmas [51], as well as lemmas systematically extracted from SV-COMP programs [7]. The SV-COMP programs’ lemmas capture intricate code logic and properties, which are more representative of program verification.",通过证明成功率（即成功证明的引理比例）进行评估。,"Evaluation shows that AutoRocq significantly outperforms state-of-the-art approaches. Specifically, AutoRocq is capable of proving 51.1% mathematical lemmas and 30.9% program lemmas, exceeding baseline approaches by 20.8% to 343.0% on mathematical lemmas, and by 42.4% to 204.6% on program lemmas.",,,软件验证、形式化方法、定理证明。,"We showcase the feasibility of automatic and end-to-end verification with LLM agents. AutoRocq is evaluated on the widely used SV-COMP programs in software verification, as well as Linux kernel modules.",代表真实世界程序验证的复杂逻辑和属性，难度较高。,"The SV-COMP programs’ lemmas capture intricate code logic and properties, which are more representative of program verification.",,,,,从SV-COMP程序和Linux内核模块中系统提取的引理。,"We thoroughly evaluate our approach on existing mathematical lemmas [51], as well as lemmas systematically extracted from SV-COMP programs [7].",,,,,,,,,证明生成（生成证明脚本以履行证明义务）。,"Proof generation aims to automatically synthesize proof scripts that discharge the given proof obligations, which closes the last mile of program verification.",证明成功率（百分比）。,AutoRocq is capable of proving 51.1% mathematical lemmas and 30.9% program lemmas...,形式化的证明义务（逻辑公式）。,Proof generation aims to automatically synthesize proof scripts that discharge the given proof obligations...,证明脚本（Rocq/Coq战术序列）。,Proof generation aims to automatically synthesize proof scripts that discharge the given proof obligations...,逻辑公式到证明脚本。,Proof generation aims to automatically synthesize proof scripts that discharge the given proof obligations...,Rocq（原Coq）定理证明器环境。,The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback.,SV-COMP是软件验证竞赛的基准，其程序引理捕获了复杂的代码逻辑和属性，更能代表真实的程序验证场景。,"The SV-COMP programs’ lemmas capture intricate code logic and properties, which are more representative of program verification."
2512.03421_output/content.md,BugT,"This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns.",Yes,"BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. ... We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMs’ capabilities.",https://github.com/Xucranger/PLofLBFL,"To facilitate future study, we share our source code and experimental data in a GitHub repository 1. 1https://github.com/Xucranger/PLofLBFL",评估大型语言模型在初学者程序中进行故障定位（Fault Localization）的性能。,"This study evaluates the fault localization performance of six closed-source LLMs (e.g., OpenAI o3, GPT-3.5-Turbo, etc.) and seven open-source LLMs (e.g., DeepSeekR1, Llama3, etc.) across Codeflaws, Condefects, and BugT datasets.",故障定位的准确性、效率、可用性，以及对问题难度的鲁棒性。,"To investigate these issues, we aim to empirically assess the performance of different LLMs across various datasets, evaluating their accuracy, efficiency, and usability in fault localization for novice programs. ... We examine how problem difficulty levels across three datasets affect LLMs’ fault localization performance, providing insights into model behavior at varying complexity levels.",在多个数据集上评估LLMs的故障定位性能，并与传统方法（如SBFL、MBFL）进行比较。,"This study evaluates the fault localization performance of six closed-source LLMs (e.g., OpenAI o3, GPT-3.5-Turbo, etc.) and seven open-source LLMs (e.g., DeepSeekR1, Llama3, etc.) across Codeflaws, Condefects, and BugT datasets. All closed-source LLMs outperform traditional SBFL and MBFL methods...",,,编程教育，初学者编程。,Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. ... This study evaluates the fault localization performance ... for novice programs.,包含不同难度级别的问题，从简单到复杂。,"LLM accuracy decreases as problem difficulty increases in Codeflaws and Condefects, but top models maintain high accuracy even at peak difficulty in BugT, suggesting its lower complexity. ... We examine how problem difficulty levels across three datasets affect LLMs’ fault localization performance...",,,,,自建的包含真实编程故障的数据集。,"We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMs’ capabilities.",2025-12-04,"Preprint submitted to Journal of LATEX Templates December 4, 2025",官方自建,We introduce a new self-created dataset with real programming faults...,专门设计以减轻数据泄露问题。,BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns.,,,故障定位,This study evaluates the fault localization performance...,准确性（accuracy）,LLM accuracy decreases as problem difficulty increases...,包含故障的代码,"Novice Program refers to code written by novice programmers, usually in the early stages of learning programming. These programs usually contain multiple errors...",故障位置及解释,LLM generated fault explanations demonstrate significant value for novice programmer assistance...,代码到故障位置/解释,"LLM-based fault localization methods leverage the models’ understanding of program syntax and semantics to automatically localize faults in code, offering more precise and contextually relevant suggestions...",,,专门针对初学者程序（Novice Programs）构建，旨在减轻LLM评估中的数据泄露（data leakage）问题，包含真实的编程故障。,"BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. ... We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMs’ capabilities."
2512.05073_output/content.md,Comprehensive Verilog Design Problems (CVDP),"The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.","No, 本文是使用该数据集进行评测",Our work tests this by evaluating Small language models (SLMs) coupled with a curated agentic AI framework on NVIDIA’s Comprehensive verilog design problem (CVDP) benchmark.,,,从自然语言规范生成经过验证的寄存器传输级（RTL）硬件设计实现。,"Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites. Derived from production IP blocks, it represents realistic complexity.",硬件设计自动化能力，特别是Verilog代码生成的功能正确性。,"provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",使用CocoTB测试套件评估功能正确性（pass rate）。,"Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",模块级设计，包含接口规范、功能需求和测试套件。,"Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",硬件设计，具体包括算术运算、控制逻辑、内存系统和其他设计。,"336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",代表实际生产IP块的复杂性，具有挑战性（最先进模型单次通过率仅26.5%）。,"Derived from production IP blocks, it represents realistic complexity. State-of-the-art achieves only 26.5% pass rate (GPT-4o-mini, single-shot), highlighting substantial improvement opportunity.",Verilog（硬件描述语言）。,"The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA...",包含336个问题。,"provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",源自生产IP块，代表实际复杂性。,"Derived from production IP blocks, it represents realistic complexity.",,,由NVIDIA开发。,"The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA...",,,,,代码生成（从规范生成Verilog RTL代码）。,transforms design intent from the CVDP dataset into verified register transfer level (RTL) implementations.,通过率（pass rate）。,"State-of-the-art achieves only 26.5% pass rate (GPT-4o-mini, single-shot)...",自然语言规范、模块接口、功能需求。,"Each includes natural language specification, module interface, functional requirements...",Verilog RTL代码。,transforms design intent from the CVDP dataset into verified register transfer level (RTL) implementations.,文本到代码（自然语言规范到Verilog代码）。,"Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",使用CocoTB测试套件进行验证。,"Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",专注于硬件设计（Verilog），问题源自实际生产IP块，包含完整的测试套件（CocoTB）用于功能验证。,"Derived from production IP blocks, it represents realistic complexity. Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites."
2512.05100_output/content.md,SAP software-documentation benchmark,Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics...,"No, 本文是使用该数据集进行评测","Our experimental results demonstrate significant improvements on the software documentation dataset (Buschbeck and Exel, 2020) across four translation directions...",,,结构化文档翻译，即将带有XML或HTML标记的软件文档从源语言翻译为目标语言，同时保持标记结构的完整性。,This work addresses the task of translating a structured document Ds in the source language into its counterpart Dt in the target language.,翻译质量与结构保真度,...making structural fidelity as important as content translation quality.,使用多种指标进行评估，包括XML-Match、XML-BLEU、Content-BLEU、StrucAUC、TreeSim和Node-chrF。,"...FORMATRL achieving average gains of 3.69 XML-Match, 2.16 XML-BLEU, 0.22 Content-BLEU, and 0.93 StrucAUC scores... We propose two rewards: TreeSim and Node-chrF.",文档级别,"Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures.",软件文档本地化,Translating structured documents such as software manuals is essential for product localization.,,,文中未明确提及完整数据集的语言覆盖范围，仅提及了实验中的翻译方向（如英语到日语）。,"A structured document translation example (English→Japanese)... Our experimental results demonstrate significant improvements on the software documentation dataset (Buschbeck and Exel, 2020) across four translation directions...",,,SAP软件文档,Experiments on the SAP software-documentation benchmark...,2020,"...software documentation dataset (Buschbeck and Exel, 2020)...",,,,,,,文档翻译,...translating a structured document Ds in the source language into its counterpart Dt in the target language.,"XML-Match, XML-BLEU, Content-BLEU, StrucAUC, TreeSim, Node-chrF","...FORMATRL achieving average gains of 3.69 XML-Match, 2.16 XML-BLEU, 0.22 Content-BLEU, and 0.93 StrucAUC scores... We propose two rewards: TreeSim and Node-chrF.",带有标记的结构化文档（XML/HTML）,"A structured document D can be viewed as an XML tree D = (VD, ED)...",带有标记的结构化文档（XML/HTML）,...generating the target document Dt given the source document Ds...,结构化文档到结构化文档,This work addresses the task of translating a structured document Ds in the source language into its counterpart Dt in the target language.,,,专注于软件文档的本地化，要求同时保证翻译质量和XML/HTML标记的结构保真度。,"Translating structured documents such as software manuals is essential for product localization. As shown in Figure 1, they carry markup that defines layout and interactive elements, making structural fidelity as important as content translation quality."
2512.04673_output/content.md,CoNaLa (Code/Natural Language Challenge),"To evaluate an LLMs ability to understand and generate natural language descriptions of code snippets, we use the CoNaLa (Code/Natural Language Challenge) [23] dataset.","No, 本文是使用该数据集进行评测","In this paper, we present a comprehensive comparative analysis of five general-purpose and five code-specific LLMs across six diverse benchmarks... We further extend our evaluation to the CoNaLa dataset [22], focusing on the task of code explanation...",,,给定一个输入代码片段，生成该代码片段意图/解释。,"Given an input code snippet, the task is to generate intents/explanation that the code is achieving.",语义代码理解、意图建模和文本生成能力,"This focuses on semantic code understanding, intent modeling and text generation.","使用标准评估指标，包括基于词元的指标（BLEU, METEOR, ROUGE）和基于语义的指标（使用CodeBERT计算余弦相似度）。","We use the following standard metrics that capture various aspects such as lexical overlap and semantic similarity using the following standard evaluation metrics for the task [1, 5, 10]. (i) Token-based: These metrics collectively quantify surface-level similarity... (ii) Semantics-based: We use this measure to assess the semantic similarity between the model generated explanation (𝑚) and the ground truth explanation (𝑔)... We then take a cosine similarity between the embeddings...",,,,,,,Python,"The dataset consists of 1, 666 Python code and paired natural language annotations.","包含1,666个Python代码片段及其配对的自然语言注释。","The dataset consists of 1, 666 Python code and paired natural language annotations.",,,,,,,,,,,代码解释,"Given an input code snippet, the task is to generate intents/explanation that the code is achieving.","BLEU, METEOR, ROUGE-1, ROUGE-2, ROUGE-L, CodeBERTScore（基于CodeBERT的余弦相似度）",BLEU [19] evaluates n-gram precision... METEOR [3] extends this... ROUGE [15] metrics adopt a recall-oriented perspective: ROUGE-1... ROUGE-2... ROUGE-L... We refer to this metric as CodeBERT in the rest of the paper.,代码,Given an input code snippet...,自然语言,...the task is to generate intents/explanation...,代码到文本,"Given an input code snippet, the task is to generate intents/explanation that the code is achieving.",,,专注于代码到自然语言的解释任务，评估模型对代码语义的理解和意图建模能力。,"This focuses on semantic code understanding, intent modeling and text generation."
