source_paper,benchmark_name,benchmark_name_quote,is_original_proposal,is_original_proposal_quote,dataset_url,dataset_url_quote,task_description,task_description_quote,dimension,dimension_quote,evaluation_method,evaluation_method_quote,context_dependency,context_dependency_quote,problem_domain,problem_domain_quote,problem_difficulty,problem_difficulty_quote,language,language_quote,data_size,data_size_quote,source_type,source_type_quote,last_updated,last_updated_quote,build_type,build_type_quote,contamination_status,contamination_status_quote,dataset_license,dataset_license_quote,task_granularity,task_granularity_quote,evaluation_metrics,evaluation_metrics_quote,input_modality,input_modality_quote,output_modality,output_modality_quote,task_io_type,task_io_type_quote,execution_environment,execution_environment_quote,unique_features,unique_features_quote
2511.20403_output/content.md,CLASSES2TEST,"We introduce the CLASSES2TEST dataset, which maps Java classes under test to their corresponding test classes",Yes，本文是该数据集的原始发布论文,"We introduce the CLASSES2TEST dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics",https://anonymous.4open.science/r/classes2test,1https://anonymous.4open.science/r/classes2test,用于评估大型语言模型生成的Java单元测试的质量，支持研究人员和开发者比较不同LLM和提示策略,"AGONETEST does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions.",单元测试质量评估，包括编译成功率、代码覆盖率、缺陷检测能力、测试异味等,"a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",集成高级评估指标，如变异分数和测试异味，进行综合评估,"a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",类级别测试，涵盖方法交互和共享状态,"Our approach focuses on class-level test code evaluation, which is closer to real-world practices as it covers method interactions and shared state",软件测试，Java单元测试生成,"Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly.",现实世界软件项目级别，比单方法测试更复杂,This extended dataset makes it possible to assess the test performance of an LLM on a more complex scope (the entire class) than the single method.,Java,An annotated open source Java project dataset extending METHODS2TEST [9],"基于9,410个GitHub仓库的数据集","AGONETEST offers far broader applicability by using a dataset of 9,410 GitHub repositories",扩展自METHODS2TEST数据集的Java开源项目,An annotated open source Java project dataset extending METHODS2TEST [9],2025,arXiv:2511.20403v1  [cs.SE]  25 Nov 2025,官方自建，基于现有数据集扩展,"Leveraging the METHODS2TEST dataset [9], we developed a new dataset specifically aimed at comparing human-written tests with those produced by LLMs.",,,,,类级别单元测试生成,"Our approach focuses on class-level test code evaluation, which is closer to real-world practices as it covers method interactions and shared state",变异分数、测试异味、代码覆盖率,"a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",Java类代码,which maps classes under test to their related test classes,单元测试代码,"for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection",代码到代码,which maps Java classes under test to their corresponding test classes,支持所有Java LTS版本的项目环境,AGONETEST overcomes this barrier by supporting all Java LTS versions.,专注于类级别测试评估，支持多种LLM和提示策略的比较，提供端到端的自动化评估流水线,"AGONETEST shifts the focus to the generation of class-level tests. Our approach makes it possible to use up-to-date LLMs and not constrain prompt design (our prompts can be customized), thereby handling more complex, real-world scenarios."
2511.21380_output/content.md,"ROCODE, LogHub2.0","Two projects (i.e., ROCODE [15] and LogHub2.0 [16]) are selected for the following experiments.","No, 本文是使用该数据集进行评测","We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0.",,,数据集适应任务 - 将软件工程研究工具自动适配到新的数据集，构建可运行的实验并获取执行结果,"Our objective is to automatically modify 𝑅𝐷or 𝑅𝑇, construct a runnable experiment, and obtain its execution results.",多智能体系统在数据集适应任务中的能力评估,This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks.,五阶段评估流程：文件理解、代码编辑、命令生成、验证和最终执行，测量成功率并分析失败模式,"Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance.",多文件项目环境，需要理解整个代码仓库的架构,"Before modifying code or generating commands for a repository, a multi-agent system must browse the essential files in a repository to understand its architecture.",软件工程研究，包括代码生成、bug修复、性能分析和安全等领域,"In areas ranging from code generation and bug fixing to performance analysis and security, researchers are constantly proposing new techniques",复杂软件工程任务，需要多步骤协调和迭代修复,"Most of the evaluated multi-agent systems failed to complete the assigned tasks. Under such circumstances, nearly all results would be classified as failures",Python,"To ensure a consistent and manageable experimental environment, we retained only artifacts implemented in Python.","从顶级软件工程会议（FSE, ICSE, ASE, ISSTA）2024-2025年接受的论文中筛选的可重用研究构件","We began by collecting all papers accepted by the top four SE (FSE, ICSE, ASE and ISSTA) in 2024-2025 that were either (i) awarded a Reusable Artifact badge or (ii) explicitly identified by the authors as providing reusable artifacts.",学术研究构件，来自顶级软件工程会议的可重用研究项目,"To construct a representative set of high-quality SE research artifacts, we followed a systematic multi-stage selection process",2025,arXiv:2511.21380v1  [cs.SE]  26 Nov 2025,官方自建的研究构件集合,"We began by collecting all papers accepted by the top four SE (FSE, ICSE, ASE and ISSTA) in 2024-2025",,,,,代码编辑、文件创建、命令生成、验证修复,Edit and create necessary files... Generate and execute the commands... Validating and repairing,成功率、结构相似度（从7.25%到67.14%）、完成状态记录,Results show that... substantially improve structural similarity to ground truth (from 7.25% to 67.14%),代码仓库、自然语言提示,Each multi-agent system is provided with a processed repository... along with a simple prompt that specifies the adaptation task,修改后的代码、生成的脚本命令、执行结果,"the code adaptations performed to ensure compatibility, the executable scripts or commands derived for running the experiment, and the results produced by executing those scripts or commands",代码到代码的适应任务,"automatically modify 𝑅𝐷or 𝑅𝑇, construct a runnable experiment, and obtain its execution results",Python环境，需要可靠的部署环境,This choice allows for reliable environment deployment and isolates our investigation from environment setup challenges,专注于多智能体系统在数据集适应任务中的表现评估，采用五阶段评估流程，研究提示级干预对性能的影响,"This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. Through a five-stage evaluation pipeline... we measure success rates, analyze failure patterns, and assess prompt-based interventions"
2511.21022_output/content.md,EDAPIBench,"We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances.",Yes，本文是该数据集的原始发布论文,"We introduce EDAPIBench, a dedicated benchmark... We construct EDAPIBench—the first dedicated benchmark for evaluating deprecated API knowledge editing in LLMs",,,评估大语言模型中废弃API知识编辑的性能，专门用于测试模型编辑技术能否有效更新废弃API知识并生成最新的API,"a dedicated benchmark for evaluating deprecated API knowledge editing in LLMs... whether existing model editing methods can effectively update deprecated API knowledge within LLMs, and enable the edited models to correctly replace deprecated APIs with up-to-date ones",有效性、泛化性、可移植性、特异性,"comprehensively assess model editing performance across four key dimensions: Effectiveness, Generalization, Portability, Specificity",基于四个维度的评估：有效性（编辑后模型是否生成最新API）、泛化性（语义等价但语法变化的输入）、可移植性（不同输入但涉及相同废弃API）、特异性（保持与编辑任务无关输入的行为一致性）,Effectiveness: Measuring whether the edited model generates the up-to-date API for the original inputs; Generalization: Measuring whether it generates the up-to-date API on semantically equivalent but syntactically varied inputs; Portability: Measuring whether it generates the up-to-date API across different inputs... Specificity: Measuring whether it preserves consistent pre-editing behavior on inputs unrelated to the editing task,单函数级别的代码补全上下文,Our study frames the model editing scenario as a code completion task... we extract lines preceding the API invocation as candidate editing inputs (to prompt LLM API completion) and the invocation line as the target API (ground truth),软件工程、API维护、第三方库更新,deprecated APIs from 8 popular Python libraries... covering eight popular Python libraries such as PyTorch and TensorFlow,实际工程级难度，涉及真实世界废弃API的识别和更新,"Using these mappings, we extract 65,596 real-world functions from GitHub that call the up-to-date APIs",Python,deprecated APIs from 8 popular Python libraries... covering eight popular Python libraries such as PyTorch and TensorFlow,"包含70多个废弃API，超过3000个编辑实例，从GitHub提取了65,596个真实世界函数","featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances... we extract 65,596 real-world functions from GitHub",从GitHub真实项目代码中提取，基于已验证的API映射关系,"We begin with 145 verified API mappings (deprecated →up-to-date) from Wang et al. [49]... Using these mappings, we extract 65,596 real-world functions from GitHub that call the up-to-date APIs",2025-2026,Publication date: November 2026. arXiv:2511.21022v1 [cs.SE] 26 Nov 2025,官方自建，全自动构建,which can be fully automatically constructed... with fully automated construction,,,,,API级别的代码补全,"code completion tasks... during code completion, LLMs frequently suggest deprecated API invocations",基于四个维度的定性评估：有效性、泛化性、可移植性、特异性,"Effectiveness, Generalization, Portability, Specificity",代码片段,the editing input (a code snippet to be completed),API调用代码,"the specific correct, up-to-date API that should replace the deprecated one",代码到代码,code completion task... generating code,,,首个专门针对废弃API知识编辑的基准测试，支持全自动构建，包含四个维度的综合评估,"the first dedicated benchmark for evaluating deprecated API knowledge editing in LLMs, with fully automated construction, serving as a standardized, rigorous platform"
2511.19875_output/content.md,CodeFuse-CommitEval,"We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models.",Yes，本文是该数据集的原始发布论文,"We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models.",https://figshare.com/s/21fe4ec9cb960b52bffe,The dataset is publicly available at https://figshare.com/s/21fe4ec9cb960b52bffe.,"检测提交信息与代码变更之间的不一致性（Message-Code Inconsistency, MCI）",A Message-Code Inconsistency (MCI) occurs when the natural language description in a commit message does not accurately reflect the actual modifications in the associated code diff.,提交信息与代码变更的一致性检测能力,evaluate models for MCI detection,使用Recall、Precision、Specificity等指标评估模型检测结果,"Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%)",提交信息与对应的代码差异（diff）,"Each item of the verified dataset contains a commit message, the corresponding code diff, and the ground truth label",软件工程、版本控制、代码审查,Version control relies on commit messages to convey the rationale for code changes,需要语义理解和上下文推理的复杂任务,purpose inconsistencies require deeper semantic understanding and contextual reasoning,多种编程语言（基于ApacheCM数据集的多样性）,because of its diversity in programming languages and the high-quality commits,包含正负样本的平衡数据集,we generate a balanced dataset with both positive and negative samples,基于ApacheCM数据集，通过规则引导的突变生成不一致提交信息,"Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits",2025,arXiv:2511.19875v1 [cs.SE] 25 Nov 2025,官方自建，结合LLM数据合成能力与现有提交语料库,We present a comprehensive pipeline for constructing MCI datasets. By combining the data synthesis capabilities of LLMs with existing commit corpora,通过双重验证确保数据质量,apply two-fold validation to verify both positive and negative samples,,,一致性检测,detecting inconsistencies between commit messages and code,"Recall, Precision, Specificity","average Recall 85.95%, Precision 80.28%, Specificity 63.8%",自然语言（提交信息）与代码差异,"Each item of the verified dataset contains a commit message, the corresponding code diff",二元分类（一致或不一致）,allowing the models to detect whether the commit is consistent or not,文本与代码到分类,detect whether the commit is consistent or not,,,首个专门用于MCI检测的基准，包含七种不一致类型，支持多种增强策略评估,CODEFUSE-COMMITEVAL is the first benchmarking work tailored for comprehensively evaluating LLM's MCI detection ability. We define seven mutation rules to guide powerful LLMs in generating various types of inconsistent commit messages
2511.20709_output/content.md,DUALGAUGE-BENCH,"we further curated DUALGAUGE-BENCH, a comprehensive benchmark suite of 154 programming tasks",Yes，本文是该数据集的原始发布论文,"We present DUALGAUGE, the first fully automated benchmarking framework... we also present DUALGAUGE-BENCH, a curated benchmark suite",https://anonymous.4open.science/r/DualBench-6D1D,"Our system source code, benchmark, and evaluation/study data can all be found at https://anonymous.4open.science/r/DualBench-6D1D",联合评估代码生成的安全性和功能性，确保生成的代码既满足功能规范又不会引入安全漏洞,rigorously evaluate the security and correctness of LLM-generated code in unison... ensuring that generated programs fulfill their specifications without introducing vulnerabilities,代码生成的安全性和功能性联合评估,designed to rigorously evaluate the security and correctness of LLM-generated code in unison,在沙盒环境中执行程序，针对功能和安全性测试套件运行，基于执行结果评估测试通过率和联合正确性-安全性指标,"executes it in a sandboxed environment against functional and security test suites, and evaluates the execution results against both test suites to report test pass rates and joint correctness-security metrics",单任务代码生成，基于自然语言提示生成完整程序,Given an pure-natural-language prompt as functional specification... generates the model's code output for the prompt,跨领域编程任务，涵盖多样化功能领域,spanning diverse functionality domains,,,语言无关，支持多种编程语言,This dataset design and curation process allow it to be agnostic to programming languages,包含154个编程任务，每个任务都配有功能和安全性测试套件,a comprehensive benchmark suite of 154 programming tasks... Each task/prompt is paired with both a functional test suite... and a security test suite,人工与LLM协同创建，通过多评分者的人类专业知识进行精炼和修正,"constructed these test suites through a human-and-LLM co-creation process—leveraging multiple LLMs to generate candidate tests, then refining and amending these with human expertise of multiple raters",2025,arXiv:2511.20709v1 [cs.SE] 24 Nov 2025,官方自建，基于规范测试范式,following a specification-based testing paradigm,,,,,代码生成,code generation... translating natural language prompts—typically serving as functional specifications—into programs,测试通过率、联合正确性-安全性指标,report test pass rates and joint correctness-security metrics,自然语言,Given an pure-natural-language prompt as functional specification,代码,generates the model's code output for the prompt,文本到代码,translating natural language prompts... into programs,沙盒隔离容器环境，支持依赖解析和环境配置,executes it in a sandboxed environment... The execution engine compiles and runs the model-generated code within isolated containers,首个支持安全性和功能性联合评估的基准套件，每个任务都配有覆盖驱动的功能和安全性测试套件，采用人工与LLM协同创建过程,"the first benchmark suite that pairs each code-generation prompt with dual (functional and security), coverage-enforced test suites... constructed through a human-and-LLM co-creation process"
2511.23408_output/content.md,Vul4J,"Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects and covering distinct Common Weakness Enumeration (CWE) classes.","No, 本文是使用该数据集进行评测","Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects...",,,评估大型语言模型（LLM）在自动化漏洞修复方面的有效性，具体针对真实漏洞和人工生成的漏洞。,"In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs... using both real and artificial vulnerabilities.",漏洞修复的有效性、模型间的互补性与重叠性,Our paper empirically investigates the effectiveness and complementarity of several prominent LLMs in automated vulnerability patching...,"使用漏洞证明（Proof-of-Vulnerability, PoV）测试执行来验证生成的补丁是否成功修复漏洞。",Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities.,单函数/代码片段,"Given a vulnerable code snippet, these models generate a patched version that aims to eliminate security flaws while preserving functionality.",软件安全、漏洞修复,Automated vulnerability patching is crucial for software security...,,,Java,"Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities...",15个真实漏洞及其41个人工生成的对应漏洞,"To perform this evaluation, we employed 15 real vulnerabilities and their 41 artificial counterparts (vulnerabilities).",真实漏洞来自开源项目和公共漏洞数据库（如CVE），人工漏洞由CodeBERT生成并经过验证。,"Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects... Secondly, we augmented our evaluation with artificially generated vulnerabilities... we incorporated artificial vulnerabilities derived from the work of Garg et al.[15]. Garg et al. used CodeBERT[11] to generate thousands of candidate artificial vulnerabilities...",,,官方自建（Vul4J数据集）,"Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects...",,,,,代码修复（漏洞补丁生成）,"Given a vulnerable code snippet, these models generate a patched version that aims to eliminate security flaws while preserving functionality.",PoV测试通过率（基于执行的验证）,Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities.,代码（包含漏洞的代码片段）,"Given a vulnerable code snippet, these models generate a patched version...",代码（修复后的代码片段）,"Given a vulnerable code snippet, these models generate a patched version...",代码到代码,"Given a vulnerable code snippet, these models generate a patched version...",Maven/Gradle构建环境，包含PoV JUnit测试用例,"When no suitable test existed in the original project, Bui et al. [2] manually wrote one by following the exploit steps in the CVE report, thereby guaranteeing that every vulnerability in the dataset can be triggered, reproduced, and validated in a Maven/Gradle build.",该研究不仅评估LLM对真实漏洞的修复能力，还评估其对人工生成漏洞的修复能力，以测试模型的泛化性和鲁棒性。数据集（Vul4J）为每个漏洞提供了可执行的漏洞证明（PoV）测试用例，用于自动化验证补丁的有效性。,"Our paper empirically investigates the effectiveness and complementarity of several prominent LLMs in automated vulnerability patching, specifically focusing on both real vulnerabilities and their corresponding artificial vulnerabilities... For every entry Vul4J provides... one or more Proof-of-Vulnerability (PoV) JUnit test cases. A PoV test is an executable exploit oracle..."
2401.01062_output/content.md,CAASD (Capability Assessment of Automatic Software Development),we have developed a novel benchmark named CAASD (Capability Assessment of Automatic Software Development).,Yes,we have developed a novel benchmark named CAASD (Capability Assessment of Automatic Software Development).,,,评估AI辅助软件开发系统的能力，特别是针对系统级实现任务的能力,all of them either are limited to simple function-level implementation tasks or lack detailed requirements specifications for system-level implementation tasks,系统级软件开发能力评估,for assessing how well a software development task is completed,通过参考用例评估系统实现的质量和完整性,Each task of CAASD is equipped with a list of reference use cases depicting the system requirements. The reference use cases are used to evaluate the quality and completeness of a system implementation.,系统级开发（多文件项目）,system-level implementation tasks,软件开发,software development,非平凡软件项目,non-trivial software projects,,,,,,,2024,arXiv:2401.01062v1 [cs.SE] 2 Jan 2024,官方自建,we have developed a novel benchmark named CAASD,,,,,系统级实现任务,system-level implementation tasks,通过率,AISD achieves an impressive pass rate of 75.2%,自然语言需求描述,high-level (potentially vague) user requirements as inputs,系统实现,system implementation,需求到系统实现,"taking high-level (potentially vague) user requirements as inputs, generates detailed use cases, prototype system designs, and subsequently system implementation",,,首个评估软件开发任务完成质量的基准，包含详细的系统需求规范,this is the first benchmark that offers criteria for assessing how well a software development task is completed
2401.12554_output/content.md,ParEval,we propose the Parallel Code Generation Evaluation (ParEval) benchmark: a set of benchmarks (prompts) for evaluating how well LLMs generate parallel code.,Yes,we propose the Parallel Code Generation Evaluation (ParEval) benchmark,github.com/parallelcodefoundry/ParEval,ParEval is available online at: github.com/parallelcodefoundry/ParEval.,评估大型语言模型生成并行代码的能力,we study the capabilities of state-of-the-art language models to generate parallel code.,并行代码生成正确性和性能,"We evaluate several state-of-the-art open- and closed-source LLMs using these benchmarks, and report metrics that represent the correctness and performance of the generated code.",新颖的代码生成评估指标，包括speedup𝑛@k和efficiency𝑛@k，用于评估生成代码的性能和扩展性,We introduce novel code generation evaluation metrics that assess performance and parallel scaling.,,,科学和并行计算,consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.,,,C/C++,"Traditional Python code generation benchmarks are tested by running eval on the generated code for a small number of small unit tests. On the other hand, in the case of parallel code — we must compile C/C++ code, link against one or more parallel libraries, and run the code in the proper parallel environment.",420个不同的编码任务,consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.,手动设计,These benchmarks are challenging to test. Traditional Python code generation benchmarks are tested by running eval on the generated code for a small number of small unit tests.,2024,"HPDC ’24, June 3–7, 2024, Pisa, Italy",官方自建,we propose the Parallel Code Generation Evaluation (ParEval) benchmark: a set of benchmarks (prompts) for evaluating how well LLMs generate parallel code.,,,,,代码生成,we study the capabilities of state-of-the-art language models to generate parallel code.,speedup𝑛@k和efficiency𝑛@k,"We introduce two novel metrics, speedup𝑛@k and efficiency𝑛@k, for evaluating the performance and scaling of LLM generated code.",自然语言,consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.,代码,we study the capabilities of state-of-the-art language models to generate parallel code.,文本到代码,consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.,需要特定依赖（并行库）,"we must compile C/C++ code, link against one or more parallel libraries, and run the code in the proper parallel environment.",专注于并行代码生成评估，覆盖12种计算问题类型和7种执行模型,"These benchmarks cover twelve different computational problem types, and seven different execution models: serial, OpenMP, Kokkos, MPI, MPI+OpenMP, CUDA, and HIP."
2512.01010_output/content.md,Chain of Unit-Physics,"To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation.",Yes,"To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation.",,,该评测基准旨在解决科学计算中的代码生成任务，特别是针对具有严格物理约束的高风险科学问题，如燃烧科学中的计算流体动力学（CFD）求解器开发。,Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community.,物理一致性、数值稳定性、算法正确性,"The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",基于单元物理测试的验证方法，包括物理约束检查、数值一致性检查和诊断分析,The Verification Agent applies formalized unit-physics tests to assess physical and numerical consistency. These primitives yield physics-grounded verification even without reference datasets.,多步骤科学计算任务，涉及复杂的物理约束和数值计算,"The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",计算科学，特别是燃烧科学和计算流体动力学,"To overcome these limitations, this work systematically applies an inverse code design methodology (see Fig. 1), formally known as test-driven development (TDD) [26], to scientific software in combustion science, one such domain where TDD approaches have not been thoroughly evaluated.",高难度，涉及12自由度的复杂燃烧任务,"The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",,,,,基于人类专家知识构建的单元物理测试,"This work proposes Chain of Unit-Physics, an approach that embeds human expert knowledge directly into distinct reasoning chains of the agentic system via 'unit-physics'—formalized, testable constraints that encode fundamental physics (e.g., energy conservation laws) and practitioner experience (e.g., dimensional / bound checks, and floating-point diagnostics).",2025,arXiv:2512.01010v1 [cs.MA] 30 Nov 2025,官方自建，基于人类专家知识,"This work proposes Chain of Unit-Physics, an approach that embeds human expert knowledge directly into distinct reasoning chains of the agentic system via 'unit-physics'—formalized, testable constraints that encode fundamental physics (e.g., energy conservation laws) and practitioner experience (e.g., dimensional / bound checks, and floating-point diagnostics).",,,,,端到端科学代码生成,"Closed-weight systems and code-focused agentic variants fail to produce correct end-to-end solvers, despite tool and web access, exhibiting four recurrent error classes: interface (syntax/API) hallucinations, overconfident assumptions, numerical/physical incoherence, and configuration fragility.",物理一致性、数值误差、运行时间和内存使用效率,"On the benchmark task, the proposed framework converges within 5–6 iterations, matches the human-expert implementation (mean error of 3.1ˆ10´3%), with a „33.4% faster runtime and a „30% efficient memory usage at a cost comparable to mid-sized commercial APIs, yielding a practical template for physics-grounded scientific code generation.",自然语言查询和单元物理测试,"In the input, the user’s scientific question is paired with 'basis prompts' that establish execution permissions, tool availability, coding language settings, and transfer protocols. Additional input scopes the unit-physics tests that concatenated with the scientific query to form the complete input to the framework.",科学计算代码和可视化结果,"Finally, the agent consolidates the output into domain-relevant visualizations (for example, line graphs and contour graphs of key quantities of interest), closing the loop between natural-language inquiry and quantitative scientific insight.",自然语言到科学代码,Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community.,Python虚拟沙箱环境，具有隔离的执行权限,"The framework runs inside a dedicated Python virtual sandbox environment that is not pre-configured with metadata on all available libraries. The agent is granted isolated code execution privileges within this sandbox, ensuring that any dependency installs or script executions cannot affect the system root directory or global files.",基于第一性原理的单元物理测试驱动代码生成，强调物理一致性和数值稳定性,"This inverse-design method provides two key advantages in scientific software. First, human-authored tests embed deep domain expertise (first principles or primitives) into targeted validation checks, ensuring that each algorithmic component faithfully represents the underlying physics."
2512.01396_output/content.md,BackportBench,"we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",Yes,"we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",https://github.com/BackportBench/BackportBench,The BackportBench is available on https://github.com/BackportBench/BackportBench.,自动化补丁回移植。旨在为未打补丁的软件版本生成补丁，基于已有的补丁和两个版本之间的代码差异。,BackportBench defines the backporting problem as generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.,自动化补丁回移植的有效性、多语言能力、处理跨文件不兼容性的能力,"To facilitate the development and evaluation of automated backporting techniques... BackportBench is a multilingual benchmark... This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges.",在可执行的Docker环境中运行相关测试用例进行验证,"each task instance provides an executable Docker environment with scripts for running relevant test cases, thereby aligns with the criteria for a successful benchmark [5].",仓库级别，涉及跨文件的不兼容性处理,"This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges.",软件安全、软件维护、漏洞修复,BackportBench contains 202 real-world vulnerability patch backporting task instances curated from three popular OSS ecosystems... To remove such vulnerabilities... patch backporting is a helpful practice that adapts patches for other versions and makes it easier to deliver patches to users on older branches.,现实世界工程级，涉及逻辑和结构性变更,"the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes.","Python, Java, JavaScript","BackportBench contains 202 patch backporting problems from PyPI, Maven, and npm, covering Python, Java, and JavaScript, respectively.",包含202个补丁回移植任务实例,BackportBench contains 202 real-world vulnerability patch backporting task instances,"从三个流行的开源软件生态系统（PyPI, Maven, npm）中收集的真实世界漏洞补丁回移植任务","BackportBench contains 202 real-world vulnerability patch backporting task instances curated from three popular OSS ecosystems, PyPI , Maven, and npm",2018 (根据论文版权日期)，但arXiv版本为2025年12月,© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ... arXiv:2512.01396v1  [cs.SE]  1 Dec 2025,官方自建,"we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",,,ACM版权，允许个人或课堂使用，禁止商业用途,Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.,代码修复/补丁生成,generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.,通过测试用例验证补丁有效性，而非基于等价性或相似性的指标,each task instance provides an executable Docker environment with scripts for running relevant test cases... We argue that whether backported patches achieve equivalence is not the only way to prove the patch is effective.,代码（两个版本的代码库及原始补丁）,generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.,代码（回移植后的补丁）,generating a patch for the unpatched software version,代码到代码,generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.,可执行的Docker环境，包含运行相关测试用例的脚本,each task instance provides an executable Docker environment with scripts for running relevant test cases,首个针对补丁回移植问题的综合性、多语言基准测试套件，包含可执行环境和测试用例，将问题从代码块/函数级别提升到仓库级别，更贴近现实软件维护挑战。,"the first comprehensive benchmark suite for patch backporting problem... a multilingual benchmark... contains executable Docker environments and test cases for validation... This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges."
2512.01255_output/content.md,ARENAJS,we construct ARENAJS—the first systematic benchmark for LLM-based JavaScript vulnerability detection,Yes,"In this paper, we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection... we propose FORGEJS... Then, we use FORGEJS to construct ARENAJS—the first systematic benchmark for LLM-based JavaScript vulnerability detection",,,评估大型语言模型在JavaScript漏洞检测方面的能力,evaluating LLMs’ capability in JavaScript vulnerability detection,漏洞检测能力、推理充分性、鲁棒性、现实世界可用性,"reveals key limitations in reasoning sufficiency, robustness, and real-world usability",使用JUDGEJS自动评估框架，包含提示模板、响应解析、标签对齐和稳健评分，利用语义等价和模糊匹配，在函数级和项目级生成统一的、严格的、可追溯的指标套件，包括F1、假阳性率（FPR）以及工程约束下的检测效能,"we propose JUDGEJS, an automated evaluation framework that spans prompt templates, response parsing, label alignment, and robust scoring; leveraging semantic equivalence and fuzzy matching, it harmonizes heterogeneous outputs and yields a unified, rigorous, and traceable metric suite across function- and project-levels, including F1, false positive rate (FPR), and detection efficacy under engineering constraints.",函数级和项目级（完整项目）,supports both function-level and project-level evaluation,JavaScript安全漏洞检测，涵盖前端（如DOM-based XSS）、后端（如SQL注入、命令注入、原型污染）和全栈环境,JavaScript vulnerability patterns vary substantially across these environments: backend prototype pollution... while frontend prototype pollution predominantly causes DOM-based XSS or client-side denial of service.,现实世界工业级，反映真实仓库的复杂性,fails to reflect the complexity of real-world repositories,JavaScript,benchmark for JavaScript vulnerability detection,覆盖数千个真实的JavaScript项目,A benchmark that spans thousands of real JavaScript projects,聚合异构来源，结合真实世界和合成数据,aggregates heterogeneous sources... combining real-world and synthetic data,2025-12-01 (根据arXiv版本日期),arXiv:2512.01255v1  [cs.CR]  1 Dec 2025,官方自建，通过FORGEJS自动生成框架构建,"we propose FORGEJS, the first automatic benchmark generation framework... Then, we use FORGEJS to construct ARENAJS",通过使用完整项目而非单文件片段、构建修复前后项目对来直接量化假阳性、以及系统引入四种增强策略来破坏对文件名、导入路径和注释的依赖，旨在避免高估,"To avoid overestimation, FORGEJS uses complete projects rather than single-file snippets, constructs pre- and post-fix project pairs to directly quantify false positives and localize error sources, and systematically introduces four augmentation strategies to disrupt reliance on filenames, import paths, and comments.",,,漏洞检测（包括定位漏洞文件、函数、CWE类型和具体代码行）,"evaluation protocols commonly assess only whether vulnerability is detected and ignore reasoning quality such as whether the model can correctly localize the vulnerable file, function, and CWE type.",F1分数、假阳性率（FPR）、VD-S指标,"including F1, false positive rate (FPR), and detection efficacy under engineering constraints... from the perspective of the VD-S metric [6]",JavaScript代码（函数或完整项目）,supports both function-level and project-level evaluation,漏洞检测结果（包括是否存在漏洞、CWE类型、定位信息、推理等）,"evaluation protocols commonly assess only whether vulnerability is detected and ignore reasoning quality such as whether the model can correctly localize the vulnerable file, function, and CWE type.",代码到安全分析,evaluating LLMs’ capability in JavaScript vulnerability detection,,,首个基于LLM的JavaScript漏洞检测系统基准；遵循三个构建原则：全面性、不低估、不高估；覆盖218种CWE类型；支持函数级和项目级评估；通过FORGEJS自动生成；包含JUDGEJS自动评估框架；旨在客观定义LLM在可重复、可扩展、部署受限条件下的现实性能上下限。,"we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection that directly address these limitations: (1) comprehensiveness, (2) no underestimation, and (3) no overestimation... FORGEJS aggregates heterogeneous sources, covers 218 CWE types... supports both function-level and project-level evaluation... Taken together, these principles aim to objectively define the realistic upper and lower bounds of LLM performance under reproducible, scalable, deployment-constrained conditions."
2105.09938_output/content.md,APPS,"To meet this challenge, we introduce APPS, a benchmark for code generation.",Yes,"To meet this challenge, we introduce APPS, a benchmark for code generation.",https://github.com/hendrycks/apps,The dataset is available at https://github.com/hendrycks/apps.,从任意自然语言描述生成满足要求的Python代码。,our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code.,代码生成能力，包括理解任务描述、设计算法、编写语法正确且功能正确的程序。,"APPS evaluates models not only on their ability to code syntactically correct programs, but also on their ability to understand task descriptions and devise algorithms to solve these tasks.",使用测试用例检查生成的代码。,"Similar to how companies assess candidate software developers, we evaluate models by checking their generated code on test cases.",单函数（Call-Based Format）或完整脚本（Standard Input/Output Format）。,"• Call-Based Format problems generally provide initial starter code, usually in the form of a function header, and ask for the solution to be provided as the function’s return value.",编程与算法，涵盖从简单字符串操作到复杂的图论、数据结构等算法挑战。,"Our benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges.",分为入门级、面试级和竞赛级三个难度。,"It contains 10,000 programming problems at various levels of difficulty, covering simple introductory problems, interview-level problems, and coding competition challenges.",Python,our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code.,"包含10,000个编程问题，131,777个测试用例，232,421个人工编写的参考答案。","The Automated Programming Progress Standard, abbreviated APPS, consists of 10,000 coding problems in total, with 131,777 test cases for checking solutions and 232,421 ground-truth solutions written by humans.","从开放的编程网站（如Codeforces, Kattis, Codewars, AtCoder）手动收集和整理。","The APPS dataset consists of problems collected from different open-access coding websites such as Codeforces, Kattis, and more.",2021,35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks.,官方自建，由多名研究生和本科生在六个月内精心整理和优化。,"Several graduate and undergraduate student authors polished and refined this dataset over the course of six months, ensuring a high-quality set of problems.",,,,,代码生成,"APPS, a benchmark for code generation from natural language specifications.",通过测试用例的准确率（例如 pass@k）。,we evaluate models by checking their generated code on test cases.,自然语言,take an arbitrary natural language specification,代码,generate satisfactory Python code,文本到代码,code generation from natural language specifications.,允许执行任意Python代码（包括导入常见模块和库），使用自定义的测试框架。,"solutions are allowed to execute arbitrary Python code, and the results are compared against test cases for a given problem.",1. 问题描述平均长度达293.2个单词，是自包含的完整规范。2. 拥有大量测试用例（超过13万个）用于严格评估功能正确性。3. 难度分级明确，覆盖从入门到竞赛的广泛范围。4. 模拟了人类程序员（如求职面试）的评估方式。,"By comparison, problem specifications in our new APPS benchmark are self-contained and have a much larger average length of 293.2 words. Unlike Iyer et al. (2018), APPS contains test cases for every exercise, enabling a high-quality evaluation of code correctness. The APPS benchmark attempts to mirror how humans programmers are evaluated by posing coding problems in unrestricted natural language and using test cases to evaluate solution correctness."
2107.03374_output/content.md,HumanEval,"On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings",Yes,"We evaluate functional correctness on a set of 164 hand-written programming problems, which we call the HumanEval dataset. We release the HumanEval dataset so that others can evaluate functional correctness and measure the problem-solving capabilities of their models.",https://www.github.com/openai/human-eval,We release this data along with an evaluation framework at https://www.github.com/openai/human-eval.,从文档字符串（docstrings）合成独立的Python函数，并评估生成代码的功能正确性。,"In this work, we focus on the task of generating standalone Python functions from docstrings, and evaluate the correctness of code samples automatically through unit tests.",代码生成的功能正确性,measure functional correctness for synthesizing programs from docstrings,通过单元测试评估功能正确性，并使用pass@k指标。,evaluate the correctness of code samples automatically through unit tests. Kulal et al. (2019) evaluate functional correctness using the pass@k metric,单函数,generating standalone Python functions,语言理解、推理、算法和简单数学,"Programming tasks in the HumanEval dataset assess language comprehension, reasoning, algorithms, and simple mathematics.",评估语言理解、算法和简单数学，部分问题类似于简单的软件面试题。,"These problems assess language comprehension, algorithms, and simple mathematics, with some comparable to simple software interview questions.",Python,study its Python code-writing capabilities. ... generating standalone Python functions from docstrings,包含164个手写的编程问题，平均每个问题有7.7个测试用例。,a dataset of 164 original programming problems with unit tests. ... an average of 7.7 tests per problem.,手写构建，未从现有来源程序化复制。,all problems were hand-written and not programmatically copied from existing sources.,2021,arXiv:2107.03374v2 [cs.LG] 14 Jul 2021,官方自建（OpenAI团队手写）,we create a dataset of 164 original programming problems. ... all problems were hand-written,为抗污染而设计，手写问题以避免训练数据（GitHub）中已存在的解决方案。,"It is important for these tasks to be hand-written, since our models are trained on a large fraction of GitHub, which already contains solutions to problems from a variety of sources.",,,代码生成（从文档字符串生成完整函数体）,generating standalone Python functions from docstrings,pass@k,Kulal et al. (2019) evaluate functional correctness using the pass@k metric,自然语言（文档字符串）与代码（函数签名）,"synthesizing programs from docstrings. ... prompt consisting of a header, a signature, and a docstring",代码（Python函数体）,generating standalone Python functions,文本（文档字符串）到代码,synthesizing programs from docstrings,使用gVisor容器运行时的沙盒环境，用于安全执行不受信任的程序。,"we developed a sandbox environment to safely run untrusted programs against unit tests. We selected the gVisor container runtime (Lacasse, 2018) as the main host protection component.",专门为评估代码生成模型的功能正确性而设计，强调手写问题以避免数据污染，并提供了安全的沙盒执行环境。,"It is important for these tasks to be hand-written, since our models are trained on a large fraction of GitHub, which already contains solutions to problems from a variety of sources. ... we developed a sandbox environment to safely run untrusted programs against unit tests."
2108.07732_output/content.md,Mostly Basic Programming Problems (MBPP),We introduce two datasets to test Python code synthesis. The first is a new dataset called Mostly Basic Programming Problems (MBPP).,Yes,We introduce two datasets to test Python code synthesis. The first is a new dataset called Mostly Basic Programming Problems (MBPP).,,,从自然语言描述中合成简短的Python程序。,Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions.,程序合成能力，特别是从自然语言描述生成功能正确的Python代码。,This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages.,通过执行测试用例检查功能正确性。,Participants also provided a ground-truth solution that passes all three test cases.,单函数，自包含。,"Participants were instructed to write code that is self-contained (that is, it runs by itself)...",涵盖数学、列表处理、字符串处理、整数序列等。,"Of these questions, 58% were mathematical in nature (e.g., calculating the volume of a sphere), 43% involve list processing, 19% require string processing, 9% involve integer seque",入门级，设计为入门级程序员可解决。,"The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers.",Python,The Mostly Basic Programming Problems dataset contains 974 short Python programs...,包含974个编程任务。,The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks...,通过众包构建，部分由作者编辑和手动验证。,This dataset consists of a large set of crowd-sourced questions and a smaller set of questions edited and hand-verified by the authors.,2021,arXiv:2108.07732v1  [cs.PL]  16 Aug 2021,官方自建（Google Research）,"We construct two new datasets: one entirely new and the other modified from an existing benchmark. The first, Mostly Basic Programming Problems (MBPP), is an entirely new crowd-sourced programming dataset.",文中提及与预训练集的重叠很小，降低了记忆风险。,"Second, we find that the overlap between the solutions in MBPP and the pre-training set is small, reducing the chance that our synthesis results are due to memorization (Section 4.8).",,,代码生成（从自然语言描述生成完整的函数体）。,"Participants were instructed to write a short problem statement, a single self-contained Python function solving the problem specified...",功能正确性（通过测试用例）。,Participants also provided a ground-truth solution that passes all three test cases.,自然语言描述，通常结合少量输入输出示例。,"a program can be specified by a short natural language description, possibly combined with a few (e.g., 2 or 3) input-output examples.",代码（Python函数）。,"Participants were instructed to write a short problem statement, a single self-contained Python function solving the problem specified...",文本到代码,Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions.,自包含，可使用标准库，允许使用互联网参考。,"Participants were instructed to write code that is self-contained (that is, it runs by itself)... Use of internet references was allowed.",包含三个一致的、以assert语句编写的输入输出示例；问题描述设计为更基础、字面化，而非竞赛风格；包含众包和手动验证两部分。,"In contrast, our dataset consistently contains three I/O examples, written as assert statements. ... By contrast, our Mostly Basic Programming Problems dataset is designed to contain a more basic, literal description of the problems. ... This dataset consists of a large set of crowd-sourced questions and a smaller set of questions edited and hand-verified by the authors."
2203.13474_output/content.md,Multi-Turn Programming Benchmark (MTPB),"To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts.",Yes,"In this work, we develop a multi-turn programming benchmark to measure the models’ capacity for multi-turn program synthesis. To the best of our knowledge, this is the first multi-turn program synthesis benchmark, which allows quantitative analysis of multi-turn program synthesis.",https://github.com/salesforce/CodeGen/tree/main/benchmark,1Benchmark: https://github.com/salesforce/CodeGen/tree/main/benchmark,评测模型的多轮程序合成能力。用户以自然语言在多轮对话中逐步指定意图，模型在每一轮中合成子程序，最终共同完成一个完整的程序。,"To solve a problem in the benchmark, a model needs to synthesize a program in multiple steps with a user who specifies the intent in each turn in natural language.",多轮程序合成能力,"In this work, we develop a multi-turn programming benchmark to measure the models’ capacity for multi-turn program synthesis.",通过专家编写的测试用例的通过率（pass rate）来衡量性能。,Performance on the benchmark is measured by pass rate on expert-written test cases.,多轮对话上下文。一个完整的程序被分解为多个子问题，每一轮对话指定一个子问题的意图。,consisting of 115 diverse problem sets that are factorized into multi-turn prompts.,通用编程问题。文中示例为从电子邮件地址中提取用户名。,Please refer to Figure 1 for an example where the model synthesizes a program to extract the user name of an email address.,,,未明确指定，但根据模型训练和上下文（如HumanEval部分），可能主要涉及Python。,文中未明确描述MTPB的编程语言。,包含115个多样化的问题集。,consisting of 115 diverse problem sets that are factorized into multi-turn prompts.,,,2023（根据论文版本日期推断）,arXiv:2203.13474v5 [cs.LG] 27 Feb 2023,官方自建,"we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB)",,,,,多轮程序合成,"To solve a problem in the benchmark, a model needs to synthesize a program in multiple steps",通过率（pass rate）,Performance on the benchmark is measured by pass rate on expert-written test cases.,自然语言（多轮对话）,a user who specifies the intent in each turn in natural language.,代码（子程序或完整程序）,synthesize a program,自然语言到代码,synthesize a program... with a user who specifies the intent in each turn in natural language.,,,首个专注于多轮程序合成的评测基准。问题被分解为多轮提示，模拟用户与系统逐步协作完成编程任务的过程。,"To the best of our knowledge, this is the first multi-turn program synthesis benchmark, which allows quantitative analysis of multi-turn program synthesis."
2207.01780_output/content.md,APPS,"Our comprehensive experiments (Section 4) show that our models can achieve SOTA performance on the challenging APPS benchmark [Hendrycks et al., 2021].","No, 本文是使用该数据集进行评测","Our comprehensive experiments (Section 4) show that our models can achieve SOTA performance on the challenging APPS benchmark [Hendrycks et al., 2021].",,,程序合成或代码生成，旨在根据自然语言问题描述生成满足功能正确性的程序。,Program synthesis or code generation aims to generate a program that satisfies a problem specification.,程序的功能正确性,The expected output is a program to be checked for functional correctness against some unit tests.,使用单元测试检查程序的功能正确性，并采用 pass@k 指标进行评估。,"Specifically, our models reach more than 2% pass@1, 6% pass@5, and 20% pass@1000.",,,通用编程，范围从基础编程问题到竞赛级编程任务。,This type of programming task can range from basic programming problems to competition-level programming tasks that require a high level of problem-solving skills.,具有挑战性，包含竞赛级复杂问题。,"Our comprehensive experiments (Section 4) show that our models can achieve SOTA performance on the challenging APPS benchmark [Hendrycks et al., 2021].",Python,"In our work, we focus on program synthesis from natural language problem specifications and the output programs are in general-purpose languages such as Python.",,,,,,,,,,,,,完整程序生成,"Training only with NTP objective is hence, not ideal to tackle full program generation to solve programming problems.","pass@1, pass@5, pass@1000","Specifically, our models reach more than 2% pass@1, 6% pass@5, and 20% pass@1000.",自然语言问题描述，通常包含示例输入输出对。,"Each task is defined by a problem specification in natural language, often containing example input and output pairs.",代码,The expected output is a program to be checked for functional correctness against some unit tests.,文本到代码,Program synthesis or code generation is the task of designing and building an executable computer program that satisfies a problem specification.,编译器,These program candidates are concatenated with the error information received from a compiler and passed to a program repair module.,该基准（APPS）包含从基础到竞赛级别的编程问题，用于评估模型生成功能正确程序的能力。,This type of programming task can range from basic programming problems to competition-level programming tasks that require a high level of problem-solving skills.
2207.10397_output/content.md,HumanEval,"We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS, and CodeContests","No, 本文是使用该数据集进行评测","We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS, and CodeContests",,,从自然语言描述（代码注释）和函数签名等上下文信息中生成代码解决方案。,"The task of code generation is to solve a programming problem: generate code solution x based on context c. As shown in Figure 2, context c contains natural language problem description in the form of code comment, and a code snippet that includes statements such as imports and the function header.",代码生成的功能正确性,A natural way to evaluate the quality and correctness of a code solution is to run it against a set of test cases,执行单元测试，使用 pass@k 指标,"We evaluate functional correctness using pass@1. ... For instance, Codex ... can achieve a pass@100 (pass if one or more among 100 generated solutions for a given problem can pass the corresponding test cases) of 77.4%, but a pass@1 (correct rate of a single solution) of only 33.5% on the HumanEval benchmark",单函数,"Figure 2: Code generation and test case generation: an example from the HumanEval benchmark. ... Context c contains natural language problem description in the form of code comment, and a code snippet that includes statements such as imports and the function header.",,,,,Python,"Figure 2: Code generation and test case generation: an example from the HumanEval benchmark. ... from typing import List def has_close_elements(numbers: List[float], threshold: float) -> bool:",164个问题,Benchmark Problems ... HumanEval 164,,,,,,,,,,,代码生成,The task of code generation is to solve a programming problem: generate code solution x based on context c.,"pass@1, pass@100","For instance, Codex ... can achieve a pass@100 (pass if one or more among 100 generated solutions for a given problem can pass the corresponding test cases) of 77.4%, but a pass@1 (correct rate of a single solution) of only 33.5% on the HumanEval benchmark",自然语言与代码（函数签名）,"context c contains natural language problem description in the form of code comment, and a code snippet that includes statements such as imports and the function header.",代码,generate code solution x,文本到代码,The task of code generation is to solve a programming problem: generate code solution x based on context c.,,,每个问题平均有7.77个地面真值测试用例,Benchmark ... GT Tests ... HumanEval ... 7.77
2208.08227_output/content.md,MultiPL-E,"We propose MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages.",Yes,"We propose MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the ﬁrst massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages.",github.com/nuprl/MultiPL-E,"The MultiPL-E system, dataset, and tutorial are available at github.com/nuprl/MultiPL-E.",将基于单元测试的代码生成基准（从Python）翻译到新的编程语言，以创建大规模、多语言的并行代码生成评测基准。,"MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the ﬁrst massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages.",多语言代码生成能力，模型在不同编程语言上的泛化性能,We use these new parallel benchmarks to evaluate the multi-language performance of three state-of-the-art code generation models... The range of programming languages represented in MultiPL-E allow us to explore the impact of language frequency and language features on model performance.,通过单元测试评估生成代码的正确性，使用pass@k等指标,We judge a generated function correct if it passes all tests... it is common to sample multiple completions per problem and report an estimated pass rate (§4.2).,单函数,"All of the problems are functions that receive and return ﬁrst-order values, which facilitates unit testing and test translation.",通用编程问题，涵盖算法和数据处理,It is a diverse collection of 164 problems... All of the problems are functions that receive and return ﬁrst-order values.,具有挑战性,"Moreover, it is a challenging benchmark: the best model evaluated by Fried et al. [4] achieves only a 36% pass rate on Python.","Python（源语言）及通过MultiPL-E翻译的18种其他编程语言（包括JavaScript, C++, Scala, TypeScript等）",We use MultiPL-E to extend the HumanEval benchmark [1] and MBPP benchmark [2] to 18 languages that encompass a range of programming paradigms and popularity... MultiPL-E supports 18 languages and is straightforward to extend with more.,通过翻译HumanEval（164个问题）和MBPP（未明确数量）两个基准，为19种语言（Python + 18种）创建并行问题集,We create the ﬁrst massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages... HumanEval is a diverse collection of 164 problems.,基于现有Python基准（HumanEval和MBPP）通过自动化编译器翻译生成,"We use MultiPL-E to translate two widely-used code generation benchmarks, HumanEval [1] and MBPP [2], into 18 languages.",2022,arXiv:2208.08227v4  [cs.LG]  19 Dec 2022,官方自建（论文作者构建）,We propose MultiPL-E... We create the ﬁrst massively multilingual code generation benchmark...,,,,,代码生成（从自然语言描述生成函数体）,"We focus on the natural-language-to-code task (NL2Code): given the description of a function in natural language, complete the function body.",pass率（基于单元测试）,We judge a generated function correct if it passes all tests... it is common to sample multiple completions per problem and report an estimated pass rate (§4.2).,自然语言（函数描述）与代码（函数签名、类型注解、示例）,"The prompt has several sources of information for the model: the function signature (its name and parameters); a brief comment describing the function; and, optionally, examples in the form of Python doctests.",代码（函数体）,"Given the prompt as input, the code generation model generates a completion that is likely to follow the given prompt.",文本到代码,"We focus on the natural-language-to-code task (NL2Code): given the description of a function in natural language, complete the function body.",容器化沙箱，用于编译（如需要）和运行程序,"MultiPL-E also includes a containerized sandbox that (1) compiles programs if necessary, (2) runs them with appropriate timeouts, (3) validates their results on unit tests, and (4) classiﬁes each output as successful, syntax error, etc.",1. 可扩展和可扩展的系统，用于将代码生成基准编译到新的编程语言。2. 创建了第一个大规模、多语言、并行的代码生成基准。3. 通过一套小型编译器（每个约200行代码）实现翻译，仅需翻译函数签名、单元测试、注释和类型注解，而无需翻译函数体。4. 包含一个规则工具，用于将注释中的技术术语翻译得更符合目标语言习惯。,"MultiPL-E uses a suite of 18 little compilers from Python benchmarks to each target language... Each compiler must be able to translate four components from Python: (1) a function signature (name and arguments), (2) simple unit tests, (3) a comment describing the expected function behavior, and (4) type annotations if the target language is statically typed. Notably, the compiler does not have to translate the body of a function, since it is the job of the code generation model to synthesize it. Thus each MultiPL-E compiler is approximately 200 LOC and easy to build. MultiPL-E also includes a simple, rule-based tool to translate technical terms in comments to be more language appropriate, e.g. a Python list is approximately a C++ vector."
2211.11501_output/content.md,DS-1000,"We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas.",Yes,"We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas.",https://ds1000-code-gen.github.io,We release our benchmark at https://ds1000-code-gen.github.io.,数据科学代码生成。旨在评估模型根据自然语言描述和代码上下文，生成用于解决实际数据科学问题的代码的能力。,"We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas.",数据科学代码生成能力、对特定库API的理解和使用、解决现实世界问题的实用性、抗记忆化能力,"DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases... Finally, we proactively defend against memorization...",基于执行的多标准自动评估。包括：1) 运行测试用例检查功能正确性；2) 通过限制API使用或关键词来检查表面形式约束。,"we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords.",包含代码上下文的单文件任务。用户问题通常包含错误的代码、错误消息和输入输出示例等多样化上下文。,"users’ data science coding problems usually have diverse contexts including their incorrect code, error messages, and input-output examples","数据科学。涵盖七个广泛使用的Python数据科学库：NumPy, Pandas, TensorFlow, PyTorch, SciPy, Scikit-learn, 和 Matplotlib。","a thousand problems covering seven widely-used Python data science libraries: NumPy, Pandas, TensorFlow, PyTorch, SciPy, Scikit-learn, and Matplotlib.",现实世界应用级。问题来源于StackOverflow，反映了日常数据科学应用中的实际用例，而非竞赛或面试风格。,"focuses on everyday data science applications... includes naturalistic intents and contexts... our problems reflect diverse, realistic, and practical use cases",Python,a code generation benchmark with a thousand data science problems spanning seven Python libraries,包含1000个问题。,a code generation benchmark with a thousand data science problems,从StackOverflow收集的自然发生的问题，经过人工筛选、修改和扰动。,"we collected naturally occurring problems from Stack-Overflow, manually scored their representativeness and usefulness, and curated a subset of them to create our benchmark.",2022-11-18 (根据arXiv版本v1日期),arXiv:2211.11501v1 [cs.SE] 18 Nov 2022,官方自建。由五位精通数据科学和Python的计算机科学学生作者花费约1200小时构建。,five authors who are computer science students and familiar with data science spent a total of about 1200 hours constructing DS-1000,抗污染设计。通过主动扰动原始StackOverflow问题来防御模型对预训练语料的记忆。,"we proactively defend against memorization by slightly modifying our problems to be different from the original Stack-Overflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training.",,,代码生成（填空）。模型需要将代码填入提示中的“[insert]”位置。,The model needs to fill in the code into “[insert]” in the prompt,通过/不通过（基于测试用例和表面形式约束）。文中未明确命名如pass@k的指标，但报告了准确率（如43.3%）。,The current best public system (Codex-002) achieves 43.3% accuracy,自然语言与代码上下文混合。输入包括自然语言问题描述和部分可执行的代码上下文。,The model needs to fill in the code into “[insert]” in the prompt on the left; the prompt includes natural language description and code context.,代码。期望输出是一段填补缺失部分的Python代码。,The model needs to fill in the code into “[insert]”,文本与代码到代码。根据自然语言描述和给定的代码上下文，生成缺失的代码片段。,synthesizing programs from docstrings. (结合上下文推断，任务是从自然语言描述和代码上下文中合成程序),需要特定数据科学库依赖的沙盒环境。固定了Python 3.7.10及相应库的最新版本。,We fixed the evaluation environment to include the latest versions of libraries that can be installed with Python 3.7.10,1) 包含现实问题与多样化上下文；2) 可靠的多标准执行评估；3) 主动防御记忆化；4) 专注于七个核心数据科学库。,"We highlight three core features of DS-1000: 1) it contains realistic problems with diverse contexts, 2) it implements reliable multi-criteria execution-based evaluation metrics, and 3) it proactively defends against memorization."
2301.03988_output/content.md,MultiPL-E,"evaluate them on the MultiPL-E text-to-code benchmark (Cassano et al., 2022).","No, 本文是使用该数据集进行评测","evaluate them on the MultiPL-E text-to-code benchmark (Cassano et al., 2022).",,,评测从自然语言描述生成代码的能力（文本到代码）,"MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages.",多语言代码生成能力,"MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages.",使用单元测试评估生成代码的正确性,"The correctness of generated code can be tested using unit tests, a method for approximating semantic equivalence.",,,,,,,"文中仅提及实验子集（Java, JavaScript, Python），未描述完整数据集。MultiPL-E扩展了18种语言。","evaluate them on the MultiPL-E text-to-code benchmark (Cassano et al., 2022). ... MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages.",,,基于HumanEval和MBPP基准自动编译扩展,"MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. The doctests, function signatures, and unit tests for each benchmark suite are automatically compiled to new languages.",,,,,,,,,代码生成（从自然语言描述生成完整函数）,"MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages.",,,自然语言,text-to-code benchmark,代码,text-to-code benchmark,文本到代码,text-to-code benchmark,,,"1. 通过自动编译将单语言基准（HumanEval, MBPP）扩展到多种编程语言。2. 评估时隐藏测试用例，仅用于验证正确性（与MBXP不同）。","MultiPL-E (Cassano et al., 2022) extends two popular benchmarks for code completion, MBPP and HumanEval, to 18 additional languages. ... In contrast, MultiPL-E keeps the tests hidden from the model and only uses them to test correctness."
2302.08468_output/content.md,Spider,"▷Spider (Yu et al., 2018) is a semantic parsing dataset on generating SQL queries from natural language questions.","No, 本文是使用该数据集进行评测","We conduct experiments on four language-to-code datasets across domains of semantic parsing, table QA, math reasoning and basic python programming.",,,从自然语言问题生成SQL查询（语义解析）,"▷Spider (Yu et al., 2018) is a semantic parsing dataset on generating SQL queries from natural language questions.",语言到代码生成的功能正确性,"The ability of mapping natural language to executable code is the cornerstone of a variety AI applications such as database interfaces (Pasupat & Liang, 2015; Yu et al., 2018; Shi et al., 2020)...",执行生成的程序并比较结果,"Given x, a generation model P(y|x) generates a program y which is later executed via an executor E(·) to obtain the result3 E(y).",,,数据库查询（Table QA）,"Spider (Yu et al., 2018) is a semantic parsing dataset on generating SQL queries from natural language questions.",,,SQL,Target: SQL,训练集7000条，开发集1032条,"# Train: 7,000, # Dev: 1,032",,,2018,"Spider (Yu et al., 2018)",,,,,,,代码生成,language-to-code generation,执行准确率,LEVER consistently improves the execution accuracy of the generated programs.,自然语言与数据库模式,Input Format: Schema + NL,代码（SQL）,generating SQL queries from natural language questions.,文本到代码,language-to-code generation,SQL执行器,generating SQL queries from natural language questions.,拥有完整的程序标注（Has program: ✓）,Has program: ✓
2303.17568_output/content.md,HumanEval-X,"Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go.",Yes,"We hand-craft the HumanEval-X benchmark to evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness, facilitating the understanding and development of pre-trained (multilingual) code models.",https://github.com/THUDM/CodeGeeX,"Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX.",评估多语言代码模型在代码生成和代码翻译任务上的功能正确性。,We hand-craft the HumanEval-X benchmark to evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness...,多语言代码生成与翻译的功能正确性,...evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness...,通过执行测试用例来验证生成代码的功能正确性,"...rather than really verify the functional correctness of generated code. Specifically, for each problem... in HumanEval, we manually rewrite its prompt, canonical solution, and test cases in C++, Java, JavaScript, and Go.",单函数,文中未明确描述上下文依赖范围。HumanEval-X基于HumanEval构建，而HumanEval是单函数生成任务。,通用编程问题,文中未明确描述问题所属专业领域。,入门级编程问题,文中未直接描述HumanEval-X的难度，但提到其基础HumanEval用于评估Codex解决入门级Python问题。,"Python, C++, Java, JavaScript, Go","...we manually rewrite its prompt, canonical solution, and test cases in C++, Java, JavaScript, and Go. In total, HumanEval-X covers 820 hand-written problem-solution pairs (164 problems, each having solutions in 5 languages).",820个手写的问题-解决方案对（164个问题，每个问题有5种语言的解决方案）,"In total, HumanEval-X covers 820 hand-written problem-solution pairs (164 problems, each having solutions in 5 languages).",基于HumanEval（Python）手动重写和扩展,"Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go.",2022年9月,"Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X...",官方自建（手工构建）,We hand-craft the HumanEval-X benchmark...,,,,,代码生成，代码翻译,...evaluate multilingual code models for the tasks of code generation and translation...,功能正确性（通过测试用例）,...evaluate multilingual code models for the tasks of code generation and translation in terms of functional correctness...,自然语言（描述）,文中未明确描述输入类型，但基于HumanEval任务（从文档字符串生成代码）推断。,代码,文中未明确描述期望输出，但任务为代码生成和翻译，推断输出为代码。,文本到代码，代码到代码,HumanEval-X support the evaluation of both code generation and code translation between different languages.,,,1. 多语言扩展：在HumanEval（仅Python）基础上，手工编写了C++、Java、JavaScript和Go的解决方案和测试用例。2. 支持双重任务：同时评估代码生成和代码翻译。3. 专注于功能正确性评估，而非字符串相似度。,"1) HumanEval (Chen et al., 2021)—developed by OpenAI for evaluating Codex—and other benchmarks only consist of programming problems in a single language and 2) existing multilingual datasets use string similarity metrics like BLEU for evaluation rather than really verify the functional correctness of generated code. Importantly, HumanEval-X support the evaluation of both code generation and code translation between different languages."
2304.05128_output/content.md,Spider,"SELF-DEBUGGING achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation","No, 本文是使用该数据集进行评测","we evaluate SELF-DEBUGGING on the development set of the Spider benchmark (Yu et al., 2018).",,,文本到SQL生成：给定一个自然语言问题和数据库信息，生成对应的SQL查询。,The goal of text-to-SQL tasks is to generate the corresponding SQL query given a question and the database information,代码生成正确性,SELF-DEBUGGING can teach the large language model to debug its predicted program,执行单元测试（当可用时）或基于执行结果的多数投票,"When unit tests are presented in the problem description, we filter out programs that do not pass the unit tests before performing the execution-based majority vote.",,,数据库查询,text-to-SQL generation,包含最复杂级别的问题,improves the prediction accuracy on problems of the hardest level by 9%.,SQL,text-to-SQL generation,,,,,,,,,,,,,代码生成,text-to-SQL generation,预测准确率,improves the prediction accuracy on problems of the hardest level by 9%.,自然语言（问题）和数据库信息,given a question and the database information,代码（SQL查询）,generate the corresponding SQL query,文本到代码,text-to-SQL generation,,,该基准（Spider）在问题描述中没有单元测试来验证预测的正确性。,On the Spider benchmark where there are no unit tests to verify the correctness of predictions
2305.01210_output/content.md,HumanEval+,we extend the test-cases of the popular HUMANEVAL benchmark by 80× to build HUMANEVAL+.,Yes,we propose EvalPlus – a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator... we extend the test-cases of the popular HUMANEVAL benchmark by 80× to build HUMANEVAL+.,https://github.com/evalplus/evalplus,"We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.",评估大型语言模型生成的代码的功能正确性。,EvalPlus – a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code.,代码生成的功能正确性,rigorously benchmark the functional correctness of LLM-synthesized code.,通过大量新生成的测试用例（结合LLM和基于突变的策略）进行差分测试，并与基准实现进行交叉验证。,"EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies... These newly generated test inputs are then used to evaluate the LLM-generated code through differential testing against the ground-truth implementation.",单函数,The generated code snippet is then combined with the context to form a complete function that aligns with the user intent.,通用编程问题,"Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis.",,,文中仅提及实验基于HumanEval（Python），未描述完整数据集,we extend the test-cases of the popular HUMANEVAL benchmark by 80× to build HUMANEVAL+.,将HumanEval的测试用例规模扩展了80倍,"EvalPlus extends the popular HUMANEVAL benchmark to create HUMANEVAL+, improving the test-case scale by 80×.",基于现有基准（HumanEval）通过自动测试输入生成器（结合LLM和基于突变的策略）进行增强。,"EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies.",2023,arXiv:2305.01210v3 [cs.SE] 30 Oct 2023,官方自建（基于现有基准增强）,we propose EvalPlus – a code synthesis evaluation framework... we extend the test-cases of the popular HUMANEVAL benchmark by 80× to build HUMANEVAL+.,通过生成大量新测试用例来缓解现有基准测试不足导致的“虚假正确”问题，旨在更严格地评估模型。,"HUMANEVAL+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9%.",,,代码生成,program synthesis... applying LLMs for direct code generation.,pass@k,reducing the pass@k by up-to 19.3-28.9%.,自然语言（函数签名和文档字符串）,in the form of function signature and docstring that denote the desired program functionality.,代码,The generated code snippet is then combined with the context to form a complete function that aligns with the user intent.,文本到代码,synthesizing programs from docstrings.,,,1. 通过自动测试生成（结合LLM和突变策略）显著扩展现有基准的测试套件（80倍）。2. 旨在揭示因测试不足而被现有基准遗漏的错误代码。3. 提供精简版测试套件（HUMANEVAL+-MINI）以加速评估。4. 发现测试不足可能导致模型排名错误。,"EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies... HUMANEVAL+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs... we also produce HUMANEVAL+-MINI which distills HUMANEVAL+ tests by 47×... test insufficiency can lead to mis-ranking."
2305.02309_output/content.md,HumanEval,"The HumanEval (Chen et al., 2021b) is recruited to evaluate the quality of synthesized programs.","No, 本文是使用该数据集进行评测","The HumanEval (Chen et al., 2021b) is recruited to evaluate the quality of synthesized programs.",,,根据函数签名和文档字符串（意图说明）生成程序代码。,The prompt as an intent specification is in the form of a function signature and doc-string. A program is conditionally sampled (or completed) based on the prompt in left-to-right auto-regressive fashion.,程序合成的功能正确性,"The HumanEval (Chen et al., 2021b) is recruited to evaluate the quality of synthesized programs.",,,单函数,The prompt as an intent specification is in the form of a function signature and doc-string.,,,,,文中未明确提及完整数据集的语言，仅提及实验任务。,The prompt as an intent specification is in the form of a function signature and doc-string. A program is conditionally sampled (or completed) based on the prompt...,,,,,,,,,,,,,代码生成,Program Synthesis with left-to-right sampling (zero-shot)... A program is conditionally sampled (or completed) based on the prompt...,,,自然语言（文档字符串）与代码（函数签名）,The prompt as an intent specification is in the form of a function signature and doc-string.,代码,A program is conditionally sampled (or completed) based on the prompt...,文本到代码,The prompt as an intent specification is in the form of a function signature and doc-string. A program is conditionally sampled...,,,,
2305.07922_output/content.md,HumanEval,"Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task against other open code LLMs, even surpassing the OpenAI code-cushman-001 model.","No, 本文是使用该数据集进行评测","We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. ... in the zero-shot text-to-code generation task on HumanEval benchmark [Chen et al., 2021]",,,文中仅提及使用该基准进行代码生成评测，未描述其原始任务设计。,in the zero-shot text-to-code generation task on HumanEval benchmark,文中仅提及使用该基准进行代码生成评测，未描述其原始评测维度。,,文中仅提及使用pass@1和pass@10指标，未描述其原始评估方法。,achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task,,,,,,,文中仅提及实验任务（文本到代码生成），未描述数据集涉及的具体编程语言。,,,,,,,,,,,,,,代码生成,in the zero-shot text-to-code generation task on HumanEval benchmark,"pass@1, pass@10",achieves new SoTA results of 35.0% pass@1 and 54.5% pass@10 on the HumanEval code generation task,自然语言,in the zero-shot text-to-code generation task,代码,in the zero-shot text-to-code generation task,文本到代码,in the zero-shot text-to-code generation task,,,,
2305.18584_output/content.md,PYCOMMITS,"We address this issue by collecting a new dataset, PYCOMMITS, from the commit histories of 1650 open-source Python projects on GitHub.",Yes,"We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation. ... We introduce the repo-level multi-round code editing task, along with the corresponding PYCOMMITS dataset and evaluation framework.",https://github.com/mrvplusone/Coeditor,Available at https://github.com/mrvplusone/Coeditor.,代码自动编辑。旨在预测对代码区域（基于同一代码库中最近的更改）的编辑。这是一个多轮任务，目标是根据用户之前的编辑来预测代码的编辑。,"In this work, we explore a multi-round code auto-editing setting, aiming to predict edits to a code region based on recent changes within the same codebase. ... In this paper, we introduce a task that we call (multi-round) auto-editing where the goal is to predict edits to code conditioned on the user’s previous edits.",代码编辑的准确性和自动化程度，关注模型在给定编辑历史和代码库上下文的情况下预测正确编辑的能力。,"In a simplified single-round, single-edit task, Coeditor significantly outperforms GPT-3.5 and SOTA open-source code completion models (bringing exact-match accuracy from 34.7 up to 60.4), demonstrating the benefits of incorporating editing history for code completion. In a multi-round, multi-edit setting, we observe substantial gains by iteratively conditioning on additional user edits.",精确匹配准确率（exact-match accuracy），以及自动化编辑行数和节省击键数的编辑距离度量。,"our method achieves 60.4% exact match accuracy using a 220M parameter model... In the full multi-round setting, we found that Coeditor automates editing 46.7% of the changed lines, saving the user 28.6% of keystrokes measured by an edit distance metric that accounts for cursor movement.",仓库级别（repo-level），依赖同一代码库中的其他部分和用户的历史编辑。,We introduce the repo-level multi-round code editing task... aiming to predict edits to a code region based on recent changes within the same codebase.,通用软件工程，代码维护与重构。,Developers often dedicate significant time to maintaining and refactoring existing code.,工程级，涉及实际开源项目中的代码变更。,We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation.,Python,We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation.,从1650个开源Python项目的提交历史中收集。,We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation.,来自GitHub上1650个开源Python项目的提交历史。,"We address this issue by collecting a new dataset, PYCOMMITS, from the commit histories of 1650 open-source Python projects on GitHub.",2024,Published as a conference paper at ICLR 2024,官方自建，由论文作者为研究目的构建。,"We address this issue by collecting a new dataset, PYCOMMITS, from the commit histories of 1650 open-source Python projects on GitHub.",,,,,代码编辑（基于行的差异），包括添加、删除行。,"We encode all prior code edits ∆1, . . . , ∆k using a line-based diffing scheme and decodes ∆u using masked span infilling... we adopt a line-diff-based format, enabling us to convert auto-editing into a masked span infilling problem.",精确匹配准确率（exact-match accuracy），自动化编辑行百分比，节省击键百分比（基于编辑距离）。,"our method achieves 60.4% exact match accuracy... Coeditor automates editing 46.7% of the changed lines, saving the user 28.6% of keystrokes measured by an edit distance metric that accounts for cursor movement.",代码（原始代码库和以行差异格式编码的编辑历史），以及通过静态分析提取的相关代码签名。,"given an original codebase U and a set of code changes ∆1, . . . , ∆k... we employ lightweight static analysis to pull in relevant parts of the codebase U.",代码（以行差异格式编码的目标编辑）。,"the auto-editing problem is to predict how to modify a specified region of code u ∈U... We encode the input code by a function EncInput... we can encode ∆u using the following expression, EncOutput(∆u)...",代码到代码（在给定代码库和编辑历史上下文的情况下，预测对目标代码区域的编辑）。,"the auto-editing problem is to predict how to modify a specified region of code u ∈U by learning the following distribution: P(∆u | ∆k . . . ∆1, U).",,,专注于多轮、仓库级别的代码自动编辑任务，使用基于行差异的编码格式和静态分析来构建上下文。数据集源自真实项目的提交历史，模拟了实际的代码编辑工作流。,"We introduce the repo-level multi-round code editing task... We encode all prior code edits ∆1, . . . , ∆k using a line-based diffing scheme... we employ lightweight static analysis to pull in relevant parts of the codebase U. We collect a code editing dataset from the commit histories of 1650 open-source Python projects."
2306.02907_output/content.md,DS-1000,"We evaluate SELFEVOLVE on three code generation datasets, including DS-1000 for data science code","No, 本文是使用该数据集进行评测","We evaluate SELFEVOLVE on three code generation datasets, including DS-1000 for data science code, HumanEval for software engineering code, and TransCoder for C++-to-Python translation.",,,数据科学代码生成任务,the data science code generation task DS-1000,代码生成功能正确性,Extensive experiments show that SELFEVOLVE achieves a significant improvement in execution-based measurements,基于执行的测量,Extensive experiments show that SELFEVOLVE achieves a significant improvement in execution-based measurements,,,数据科学,the data science code generation task DS-1000,,,Python,JuPyT5 [7] conditions on Jupyter notebooks’ context cells to generate data science code.,,,,,,,,,,,,,代码生成,the data science code generation task DS-1000,,,自然语言问题描述,Given a problem description written in natural language d,代码,the autoregressive language model pθ predicts the solution,文本到代码,"Given a problem description written in natural language d, and code context c, the autoregressive language model pθ predicts the solution",Python解释器,This refinement mechanism teaches language models to depend on an executor like a Python interpreter to correct the preliminary code.,专注于数据科学领域的代码生成,the data science code generation task DS-1000
2306.03091_output/content.md,RepoBench,"we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.",Yes,"we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.",,,评估仓库级别的代码自动补全系统。,"RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.",跨文件代码检索能力、给定上下文的代码补全能力、结合检索与补全的端到端流程处理能力。,"RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system’s ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction.",对于检索任务（RepoBench-R）使用Accuracy@k (acc@k)指标；对于补全任务（RepoBench-C）和端到端任务（RepoBench-P），评估模型预测下一行代码的能力。,"For evaluative purposes, the Accuracy@k (acc@k) metric is employed to assess retrieval performance. The easy subset is evaluated using acc@1 and acc@3, while the hard subset is examined through acc@1, acc@3, and acc@5 metrics.",多文件项目（仓库级别），包含跨文件上下文和文件内上下文。,"However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. ... RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems.",通用编程,benchmark for auto-code completion,真实世界编程场景，包含不同难度子集（如检索任务中的Easy和Hard子集）。,"This benchmark comprises three interconnected tasks: RepoBench-R for code retrieval, RepoBench-C for code completion, and RepoBench-P, which integrates both aspects to reflect the entire workflow of an auto-completion system, offering a balanced assessment. ... we categorize them into two subsets: those with 5-9 candidates as the easy subset, and those with 10 or more candidates as the hard subset.",Python 和 Java,RepoBench supports both Python and Java,"训练数据：10,345个Python仓库和14,956个Java仓库。测试数据：1,075个Python仓库和594个Java仓库。具体任务样本数量详见论文表1。","After processing the data, our dataset comprises 10,345 Python and 14,956 Java historical repositories, serving as training data and are available for optional fine-tuning. Additionally, we have 1,075 Python and 594 Java new repositories from GitHub designated as test data for evaluation.",1. Github-Code数据集（截止2022年3月16日），用于构建训练数据。2. 新爬取的GitHub仓库（创建于2023年2月9日至8月3日之间），专门用作测试集。,"Github-Code Dataset: The first source of RepoBench is the github-code dataset2, which consists of a vast collection of code files sourced from GitHub repositories under open-source licenses with a data cutoff date of March 16, 2022. ... Newly Crawled GitHub Data: To mitigate impacts regarding data leakage and memorization, we augment the dataset by incoporating the most recent, non-forked GitHub repositories that are permitted under their respective licenses. Specifically, we use GitHub’s official API to crawl Python and Java repositories created after February 9, 2023, which aligns with the newest knowledge cutoff date of The Stack [22], and before August 3, 2023. This newly-crawled data serves exclusively as our test set for evaluation.",2023-10-04 (arXiv版本v2),arXiv:2306.03091v2  [cs.CL]  4 Oct 2023,官方自建,"we introduce RepoBench, a new benchmark",通过使用新爬取的数据作为测试集来减轻数据泄露和记忆的影响。,"To mitigate impacts regarding data leakage and memorization, we augment the dataset by incoporating the most recent, non-forked GitHub repositories ... This newly-crawled data serves exclusively as our test set for evaluation.",数据来源于开源许可下的GitHub仓库，但未明确指定数据集本身的许可证。,code files sourced from GitHub repositories under open-source licenses,代码补全（下一行预测）、代码检索、端到端流程（检索+补全）。,"RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline).","Accuracy@k (acc@1, acc@3, acc@5)","For evaluative purposes, the Accuracy@k (acc@k) metric is employed to assess retrieval performance. The easy subset is evaluated using acc@1 and acc@3, while the hard subset is examined through acc@1, acc@3, and acc@5 metrics.",代码（文件内上下文和跨文件代码片段）,the prompt is created by combining all the parsed snippets as cross-file contexts and an in-file context. The in-file context includes import statements and several preceding lines of code,代码（下一行）,predict the next line of code,代码到代码,predict the next line of code given a pre-defined context. The context can involve content from different files (cross-file context) and within the file (in-file context),,,"专注于仓库级别的代码自动补全评估，包含三个相互关联的任务（检索、补全、端到端流程），并设计了不同的上下文设置（Cross-File-First, Cross-File-Random, In-File）和子集（如2k和8k token限制）以适应不同模型。","RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). ... To effectively evaluate next-line prediction in auto-completion systems, we define three settings: Cross-File-First (XF-F)... Cross-File-Random (XF-R)... In-File (IF)... RepoBench-C is divided into two subsets: RepoBench-C-2k and RepoBench-C-8k."
2306.08568_output/content.md,HumanEval,"Through comprehensive experiments on five prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, DS-1000, and MultiPL-E, our models showcase outstanding performance.","No, 本文是使用该数据集进行评测","Through comprehensive experiments on five prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, DS-1000, and MultiPL-E, our models showcase outstanding performance.",,,代码生成（从自然语言描述生成代码）,"These models, pre-trained on substantial code data, excel in various code-related tasks, consistently delivering impressive performance. (结合上下文，评测基准用于评估代码生成任务)",代码生成的功能正确性,"Remarkably, WizardCoder 15B even surpasses the well-known closed-source LLMs, including Anthropic’s Claude and Google’s Bard, on the HumanEval and HumanEval+ benchmarks. (通过pass率评估，隐含了功能正确性维度)",pass@1 (通过率),Figure 1: An illustration of our novel Code Evol-Instruct and the superior pass@1 performance of our WizardCoder 34B... The Python score is the mean between HumanEval and MBPP.,,,通用编程问题,Develop a Python program that creates a random password of length 8 characters. (示例任务属于通用编程),,,多种编程语言（包括Python）,outperforming the open-source SOTA ... by a large margin in 9 different programming languages. The Python score is the mean between HumanEval and MBPP.,,,,,,,,,,,,,代码生成,Through comprehensive experiments on five prominent code generation benchmarks,pass@1,Figure 1: An illustration of our novel Code Evol-Instruct and the superior pass@1 performance of our WizardCoder 34B,自然语言（指令/描述）,Develop a Python program that creates a random password of length 8 characters. (示例中的输入是自然语言指令),代码,Here's an example program that generates a random password... (示例中的输出是代码),文本到代码,Develop a Python program that creates a random password of length 8 characters. (从自然语言文本描述生成代码),,,"本文提及了五个基准：HumanEval, HumanEval+, MBPP, DS-1000, 和 MultiPL-E。其中MultiPL-E支持多语言评估。","Through comprehensive experiments on five prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, DS-1000, and MultiPL-E"
2306.09896_output/content.md,HumanEval,"In this paper, we analyze Code Llama, GPT-3.5 and GPT-4’s ability to perform self-repair on problems taken from HumanEval and APPS.","No, 本文是使用该数据集进行评测","In this paper, we analyze Code Llama, GPT-3.5 and GPT-4’s ability to perform self-repair on problems taken from HumanEval and APPS.",,,从自然语言规范生成代码片段,Large language models (LLMs) have proven capable of generating code snippets from natural language specifications,代码生成与自我修复能力,"we focus on evaluating the models’ capacity to reflect upon, provide feedback on and debug the code.",pass@k 指标，执行单元测试,"performance is typically measured by pass@k (Chen et al., 2021; Kulal et al., 2019)—the probability that at least one of k i.i.d. program samples from the model satisfies a given specification.",单函数,for self-contained Python programming tasks.,通用编程,self-contained Python programming tasks.,竞赛级和面试级,complex coding challenges such as those found in competitions and professional software engineering interviews.,Python,for self-contained Python programming tasks.,,,,,,,,,,,,,代码生成,generating code snippets from natural language specifications,pass@k,"performance is typically measured by pass@k (Chen et al., 2021; Kulal et al., 2019)",自然语言与单元测试,"Given a specification ψ, a programming model MP first generates np samples i.i.d.",代码,generating code snippets,文本到代码,generating code snippets from natural language specifications,单元测试执行环境,These np code samples are then executed against a test bed.,本文重点研究自我修复（self-repair）策略在代码生成任务中的有效性，而非提出新基准。,"In this paper, we investigate the efficacy of self-repair techniques applied to CodeLlama-13b-instruct, GPT-3.5, and GPT-4 for self-contained Python programming tasks."
2306.11644_output/content.md,HumanEval,"The evaluation benchmark proposed in the latter work, HumanEval, has been widely adopted for comparing LLMs’ performance on code.","No, 本文是使用该数据集进行评测","We demonstrate the power of high quality data in breaking existing scaling laws by training a 1.3B-parameter model, which we call phi-1, ... we attain 50.6% pass@1 accuracy on HumanEval and 55.5% pass@1 accuracy on MBPP",,,从自然语言文档字符串（docstrings）中合成简单的Python函数。,"We focus our attention on LLMs trained for code, and specifically writing simple Python functions from their docstrings as in [CTJ+21].",代码生成的功能正确性,"The evaluation benchmark proposed in the latter work, HumanEval, has been widely adopted for comparing LLMs’ performance on code.",pass@1 准确率,we attain 50.6% pass@1 accuracy on HumanEval and 55.5% pass@1 accuracy on MBPP,单函数,writing simple Python functions from their docstrings,基础编程概念,determine its educational value for a student whose goal is to learn basic coding concepts,基础级别,for a student whose goal is to learn basic coding concepts,Python,writing simple Python functions from their docstrings,,,,,2021,2021 Jul Codex-300M [CTJ+21],,,文中讨论了训练数据可能存在的污染风险,in Section 5 we study possible contamination of our training data with respect to HumanEval.,,,代码生成,writing simple Python functions from their docstrings,pass@1,we attain 50.6% pass@1 accuracy on HumanEval,自然语言,writing simple Python functions from their docstrings,代码,writing simple Python functions,文本到代码,writing simple Python functions from their docstrings,,,被广泛用于比较LLM在代码生成上的性能,"HumanEval, has been widely adopted for comparing LLMs’ performance on code."
2306.14893_output/content.md,LCC (Long Code Completion Benchmark),"In this paper, we introduce the Long Code Completion Benchmark (LCC), a new benchmark that focuses on code completion with long code context for three programming languages: Python, Java, and C#.",Yes,"To evaluate the effectiveness of LongCoder and encourage future research on Long Code Completion, we construct a new dataset called LCC by filtering code from GitHub based on length, with the goal of focusing on longer code examples.",https://github.com/microsoft/CodeBERT,1All the codes and data are available at https://github.com/microsoft/CodeBERT.,专注于长代码上下文的代码补全任务，旨在评估模型在更复杂、更现实的代码文件级别场景下的能力。,"In this paper, we introduce a new task for code completion that focuses on handling long code input... We introduce the Long Code Completion Benchmark (LCC), a new benchmark that focuses on code completion with long code context...",长代码建模能力、代码补全准确性、模型效率（计算资源消耗）,...achieves superior performance on code completion tasks compared to previous models while maintaining comparable efficiency in terms of computational resources during inference.,"在逐行基础上，使用精确匹配（Exact Match, EM）和编辑相似度（Edit Similarity, Edit Sim）进行评估。","We follow Lu et al. (2021) to evaluate the performance of the models in terms of Exact Match (EM) and Edit Similarity (Edit Sim) on a per-line basis (Svyatkovskiy et al., 2020).",文件级别（长代码上下文），上下文长度通常超过512个代码标记，平均长度在1800-2000个标记左右。,"...focuses on code completion with long code context... The average length of the examples in LCC are 5× longer than those in existing datasets... The average length of a Python source file on GitHub is 1,305 tokens.",通用编程，数据来源于GitHub上的开源代码文件。,"Specifically, we construct our datasets from the github-code dataset, which contains a vast number of code files sourced from GitHub with an open-source license that permits research use.",工程级，包含真实世界中的长代码文件，结构和依赖关系更复杂。,"Meanwhile, longer code sequences contain more complex structures and require models to consider more context and dependencies. This can be challenging for previously proposed code completion models that focus on short code...","Python, Java, C#","...a new benchmark that focuses on code completion with long code context for three programming languages: Python, Java, and C#.",每种语言包含10万个训练样本，1万个开发集样本和1万个测试集样本。,"For each programming language, we sample 100k examples for training, and 10k examples for development and 10k for testing.",从GitHub上具有开源许可证的代码文件中筛选和构建，经过去重和AST解析过滤。,"Specifically, we construct our datasets from the github-code dataset, which contains a vast number of code files sourced from GitHub with an open-source license that permits research use. The steps to construct the datasets are as follows: • We first follow Allamanis (2019) to deduplicate examples with high similarity (Jacobi similarity ≥0.9) in order to eliminate forked files, and then remove code files that can’t be parsed into an abstract syntax tree using a standard compiler tool called tree-sitter.",2023,"Proceedings of the 40 th International Conference on Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright 2023 by the author(s).",官方自建（由论文作者构建）,We construct a new dataset (LCC) for code completion tasks that requires long code modeling to encourage more research in such scenarios.,文中未明确讨论,,开源许可证（允许研究使用），具体许可证名称未明确说明。,...sourced from GitHub with an open-source license that permits research use.,代码补全（具体为下一行预测或行内补全）,Code completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context.,"精确匹配（Exact Match, EM）、编辑相似度（Edit Similarity, Edit Sim）","We follow Lu et al. (2021) to evaluate the performance of the models in terms of Exact Match (EM) and Edit Similarity (Edit Sim) on a per-line basis (Svyatkovskiy et al., 2020).",代码,Code completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context.,代码,Code completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context.,代码到代码,Code completion is a crucial task in software development that helps developers save time and effort by suggesting and auto-completing code based on context.,文中未明确描述,,专门针对长代码上下文设计，平均代码长度是现有数据集的5倍；包含Python、Java、C#三种语言；通过长度过滤（>512 tokens）确保关注长上下文场景。,"On average, the examples in LCC are 5× longer than those in existing datasets (Lu et al., 2021)... Since the benchmark primarily focuses on the code completion task with long code context, we remove code files whose length of code tokens after tokenization is shorter than 512."
2307.04349_output/content.md,APPS,Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks.,"No, 本文是使用该数据集进行评测",Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks.,,,程序合成或代码生成，即根据给定的高级行为描述生成计算机代码。,"Program synthesis, or code generation, involves creating an executable program that solves a given problem.",代码的功能正确性（通过单元测试）,"Unlike text generation, program synthesis requires both syntactic and functional accuracy since the generated code must pass compilation and unit tests.",使用单元测试结果作为反馈信号，评估生成的代码是否能通过编译和测试。,"RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs.",,,编程挑战，包括竞赛级算法问题。,"While LLMs have shown promising results in basic programming tasks, there is still progress to be made in tackling more challenging problems such as program competitions.",从基础编程任务到竞赛级挑战。,"While LLMs have shown promising results in basic programming tasks, there is still progress to be made in tackling more challenging problems such as program competitions.",文中未明确描述APPS数据集的语言构成，仅提及Codex解决Python挑战。,"Codex (Chen et al., 2021) is a noteworthy example of an LLM with 12 billion parameters that can successfully solve over 70% of complex Python programming challenges created by humans.",,,,,,,,,,,,,代码生成（从描述生成完整代码）,"Program synthesis, or code generation, involves creating an executable program that solves a given problem.",基于单元测试通过率的反馈信号（如粗粒度、细粒度、自适应反馈）,"Built upon this framework, we develop multi-granularity feedback that is automatically extracted from unit test. To expand, we introduce coarse-grained and fine-grained feedbacks applicable to programs with errors, aimed at punishing the specific segments of code where the errors appear. For programs that do not pass all test cases, we propose an adaptive feedback mechanism that assigns varying penalties based on the ratio of passed test cases.",自然语言描述,Program synthesis is an automated process that generates computer code W based on a high-level description of desired behavior D.,代码,Program synthesis is an automated process that generates computer code W based on a high-level description of desired behavior D.,文本到代码,Program synthesis is an automated process that generates computer code W based on a high-level description of desired behavior D.,编译器/单元测试环境,One generates the target program and interacts with the compiler to produce a training data pair...,本文是评测论文，未描述APPS基准的独特之处。本文提出的RLTF方法的独特之处在于其在线强化学习框架和从单元测试中提取的多粒度反馈（粗粒度、细粒度、自适应反馈）。,"To address these issues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs."
2307.14936_output/content.md,HumanEval,"PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark.","No, 本文是使用该数据集进行评测","Through extensive evaluation on three benchmarks, including HumanEval, CoderEval, and LeetCode, we conjecture that...",,,从自然语言描述生成Python代码，衡量代码生成的功能正确性。,demonstrating remarkable performance on the code generation task.,代码生成正确性,measure functional correctness for synthesizing programs from docstrings.,pass@k (文中主要报告pass@1),"PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark.",,,通用编程问题,tackle up to 72% of Python programming problems.,,,Python,tackle up to 72% of Python programming problems.,,,,,,,,,,,,,代码生成,demonstrating remarkable performance on the code generation task.,pass@1,"PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark.",自然语言,generating code from natural language descriptions,代码,generating code from natural language descriptions,文本到代码,generating code from natural language descriptions,,,由OpenAI发布，是代码生成领域广泛使用的基准测试。,OpenAI HumanEval benchmark
2308.07124_output/content.md,HUMANEVALPACK,"We further introduce HUMANEVALPACK, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust).",Yes,"We further introduce HUMANEVALPACK, expanding the HumanEval benchmark to a total of 3 coding tasks...",https://github.com/bigcode-project/octopack,"Code, models and data are freely available at https://github.com/bigcode-project/octopack.",评测基准旨在评估代码大语言模型在多种代码相关任务上的泛化能力，覆盖代码修复、代码解释和代码合成三种场景。,"HUMANEVALPACK: A benchmark for Code LLM generalization, spanning three scenarios (Code Repair, Code Explanation, Code Synthesis) and 6 programming languages (Python, JavaScript, Java, Go, C++, Rust)",代码大语言模型的泛化能力、多任务处理能力、多语言代码能力,"A benchmark for Code LLM generalization, spanning three scenarios (Code Repair, Code Explanation, Code Synthesis) and 6 programming languages",使用 pass@k 指标评估功能正确性,Metric: Pass@k (Figure 3 caption),单函数,"Given an incorrect code function with a subtle bug and accompanying unit tests, the model is tasked to fix the function. (描述 HUMANEVALFIX 任务)",通用编程问题，包含算法和基础功能实现,"Write a Python function `has_close_elements(numbers: List[float], threshold: float) -> bool` to solve the following problem: Check if in given list of numbers, are any two numbers closer to each other than given threshold. (Figure 3 示例)",工程级（包含需要修复的微妙bug）,Given an incorrect code function with a subtle bug... Bugs are written such that the code still runs but produces an incorrect result leading to at least one unit test failing.,"Python, JavaScript, Java, Go, C++, Rust","spanning three scenarios (Code Repair, Code Explanation, Code Synthesis) and 6 programming languages (Python, JavaScript, Java, Go, C++, Rust)",基于164个HumanEval问题，每个问题扩展到6种语言和3种任务，共984个修复任务，以及对应的解释和合成任务。,We manually add a bug to each of the 164 HumanEval solutions across all 6 languages (984 total bugs).,基于HumanEval基准人工扩展构建,"we expand the code synthesis benchmark HumanEval (Chen et al., 2021; Zheng et al., 2023) to cover all three input-output combinations for six languages",2024,arXiv:2308.07124v2 [cs.CL] 18 Feb 2024,官方自建（本文作者团队）,We further introduce HUMANEVALPACK,基于现有基准扩展，存在污染风险,expanding the HumanEval benchmark,,,代码修复、代码解释、代码合成,"spanning three scenarios (Code Repair, Code Explanation, Code Synthesis)",pass@k,Metric: Pass@k (Figure 3 caption),"自然语言与代码的组合，取决于具体任务（NL, NL+C, C）","When instruction tuning with code (C) data, code may either appear only in the input alongside the NL instruction (NL+C→NL, e.g. code explanation), only in the output (NL→C, e.g. code synthesis), or in both input and output (NL+C→C, e.g. code modifications like bug fixing).","自然语言或代码，取决于具体任务（NL, C）","code may either appear only in the input alongside the NL instruction (NL+C→NL, e.g. code explanation), only in the output (NL→C, e.g. code synthesis), or in both input and output (NL+C→C, e.g. code modifications like bug fixing).",文本到代码（合成）、代码到文本（解释）、代码到代码（修复）,"cover all three input-output combinations: NL+C→NL (e.g. code explanation), NL→C (e.g. code synthesis), NL+C→C (e.g. code modifications like bug fixing)",单元测试执行环境,Given an incorrect code function with a subtle bug and accompanying unit tests,扩展了经典的HumanEval基准，覆盖了代码修复和解释等更广泛的现实任务，支持六种编程语言，旨在解决现有基准因模型性能接近饱和而可能失效的问题。,Our more challenging evaluation variants provide room for future LLMs to improve on the performance of the current state-of-the-art.
2308.10335_output/content.md,ROBUSTAPI,"To fill the missing piece, we propose a dataset ROBUSTAPI for evaluating the reliability and robustness of code generated by LLMs.",Yes,"To fill the missing piece, we propose a dataset ROBUSTAPI for evaluating the reliability and robustness of code generated by LLMs.",https://github.com/FloridSleeves/RobustAPI,We open-source our dataset and evaluator on GitHub2. ... 2https://github.com/FloridSleeves/RobustAPI,评估大型语言模型生成的代码的可靠性和鲁棒性，特别是针对Java API的误用情况。,We propose a dataset ROBUSTAPI for evaluating the reliability and robustness of code generated by LLMs.,代码生成的可靠性与鲁棒性，特别是API误用检测。,"The main purpose of this benchmark is not to evaluate the functional correctness of the generated code, but instead, we focus on reliability and robustness.",使用抽象语法树（AST）分析生成的代码，并与预期的API使用模式（结构化调用序列）进行比较，检测违规行为。,We also provide an evaluator that analyzes the generated code snippets using the abstract syntax tree (AST) and compares them with the expected API usage patterns.,单代码片段（基于Stack Overflow问答），涉及特定API的使用。,We collect 1208 real questions from Stack Overflow which involves 18 representative Java APIs.,软件工程，涵盖字符串处理、数据结构、移动开发、加密、I/O和数据库操作等多个领域。,"These 18 APIs cover 6 domains including string processing, data structure, mobile development, crypto, I/O and database operation.",实际软件开发中常见的、开发者容易犯错的API使用问题。,"In this way, we guarantee that the questions in ROBUSTAPI are answerable and non-trivial so we can use them to effectively evaluate the LLMs’ ability in answering coding questions that humans are prone to make mistakes.",Java,Thus we collect representative questions about Java from Stack Overflow.,包含1208个问题，涉及18个代表性的Java API。,We collect 1208 real questions from Stack Overflow which involves 18 representative Java APIs.,从Stack Overflow爬取的真实编程问题，并基于ExampleCheck数据集筛选。,We build ROBUSTAPI based on the dataset from ExampleCheck (Zhang et al. 2018) as our starting point. ... Then we crawl questions relevant to these APIs from Stack Overflow.,2024-01-27 (根据arXiv版本v5日期),arXiv:2308.10335v5 [cs.CL] 27 Jan 2024,官方自建（由本文作者构建并发布）,We propose a dataset ROBUSTAPI... We open-source our dataset and evaluator on GitHub.,基于Stack Overflow真实问题构建，存在被LLM训练数据包含的风险，但文中未明确讨论污染状态。,,文中未提及具体许可证。,,代码生成（根据自然语言问题描述生成使用特定API的代码片段）,The prompt simulates a user asking coding questions without providing any additional hints from the API documentation which is a typical scenario when novice developers seek help from large language models.,API误用检测率（通过AST分析判断生成的代码是否违反预期的API使用模式）,Any violations of such structured call sequences would be considered as API misuse from the perspective of software engineering.,自然语言（Stack Overflow问题的标题和描述）,question field contains the title and description of the Stack Overflow questions.,代码（Java代码片段）,We design templates to trigger large language models to generate the code snippet and the corresponding explanation.,文本到代码,The prompt simulates a user asking coding questions... instruct LLMs to generate answers (code) to the questions.,文中未明确描述执行环境，评估基于静态分析（AST），而非动态执行。,We introduce the static analysis method in ROBUSTAPI for detecting the API usage violations which leverages the abstract syntax tree...,专注于评估代码生成在真实软件开发场景下的可靠性和鲁棒性，而非传统的功能正确性；针对API误用这一具体风险；包含一个基于AST的静态分析评估器。,"Unlike the online programming forums, the generated code snippets are not reviewed by the community peers and thus suffer from API misuse... The main purpose of this benchmark is not to evaluate the functional correctness of the generated code, but instead, we focus on reliability and robustness."
2308.12950_output/content.md,HumanEval,"Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively.","No, 本文是使用该数据集进行评测","We perform exhaustive evaluations of our models on major code generation benchmarks: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), and APPS (Hendrycks et al., 2021), as well as a multilingual version of HumanEval (MultiPL-E, Cassano et al., 2023)...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2308.16458_output/content.md,BioCoder,"We present BioCoder, a benchmark developed to evaluate LLMs in generating bioinformatics-specific code.",Yes,"We introduce BioCoder (see Figure 1), a benchmark for code generation incorporating 2,269 bioinformatics-specific coding problems.",https://github.com/gersteinlab/biocoder and https://biocoder-benchmark.github.io/,"All datasets, benchmark, Docker images, and scripts required for testing are available at: https://github.com/gersteinlab/biocoder and https://biocoder-benchmark.github.io/.",评估大型语言模型在生成生物信息学特定代码方面的能力。,"BioCoder benchmark mainly targets bioinformatics data analysis, which tasks such as managing various biological data formats, understanding processing workflows, and utilizing APIs of various packages.",代码生成功能正确性、处理复杂上下文（跨文件依赖、类声明、全局变量）的能力、领域特定知识（生物信息学）,"BioCoder spans much of the field, covering cross-file dependencies, class declarations, and global variables. ... They contain domain-specific knowledge of bioinformatics, beyond just general coding capability.",模糊测试框架，通过执行测试用例来验证生成代码的功能正确性，并计算Pass@K率。,"BioCoder incorporates a fuzz-testing framework for evaluation. ... Our benchmark results, derived from 1,000 iterations, indicate the Pass@K rate.",跨文件依赖、包含类声明和全局变量的完整项目上下文,"BioCoder spans much of the field, covering cross-file dependencies, class declarations, and global variables. ... we included all potentially required class declarations in the input.",生物信息学，包括生物数据分析、遗传测序、DNA/RNA分析、生物信息学软件开发,"Here, we target bioinformatics due to the amount of domain knowledge, algorithms, and data operations this discipline requires. ... This project specializes in generating Python functions that address key bioinformatics topics such as genetic sequencing and DNA/RNA analysis.",具有挑战性、实用的生物信息学场景，包含日常数据分析任务和部分软件开发任务,"BIOCODER is a code generation benchmark designed for challenging, practical bioinformatics scenarios, ... This domain encapsulates the majority of daily tasks encountered by bioinformaticians in data analysis.",Python 和 Java,"It incorporates 1,026 Python functions and 1,243 Java methods extracted from GitHub, ... However, for the scope of this study, we focus on Python and Java, with the intention to expand to other languages in the future.","总共2,269个生物信息学特定的编码问题（公开集460个，隐藏集2,269个，相似集460个），包含1,026个Python函数和1,243个Java方法，以及来自Rosalind项目的253个Python示例。","a benchmark for code generation incorporating 2,269 bioinformatics-specific coding problems. ... It incorporates 1,026 Python functions and 1,243 Java methods extracted from GitHub, along with 253 examples from the Rosalind Project, all pertaining to bioinformatics.","从1,720个经同行评审生物信息学文章引用的GitHub仓库中提取，以及来自Rosalind项目的示例。","curated from 1,720 bioinformatics repositories referenced in peer-reviewed bioinformatics articles. ... We included an additional 253 questions from the Rosalind project.",2024年5月20日（根据arXiv版本v5）,arXiv:2308.16458v5 [cs.LG] 20 May 2024,官方自建，通过自动过滤、GPT辅助过滤和人工检查相结合的方式构建。,"We ensure that each function in our dataset requires a certain level of domain expertise in bioinformatics through a combination of automatic filtering, GPT-assisted filtering, and manual inspection.",经过严格过滤、数据清洗和预处理，以防止模型记忆。,"It has undergone rigorous filtering, extensive data cleaning, and preprocessing to prevent models from memorizing.",,,代码生成（生成完整的函数或方法体）,a benchmark for code generation ... generating bioinformatics-specific code.,Pass@K率,"Our benchmark results, derived from 1,000 iterations, indicate the Pass@K rate.",自然语言描述、代码规范、注释以及完整的上下文（包括依赖项、类声明、全局变量）,"We processed the data, rephrasing more detailed text descriptions, as well as associated comments and specifications, ... we included all potentially required class declarations in the input.",代码（Python或Java）,generating bioinformatics-specific code.,文本到代码,generating bioinformatics-specific code.,Docker环境，包含丰富的所需依赖项，用于在现实项目场景中进行测试。,"Testing incorporates a Docker environment, an abundance of required dependencies, ... This robust setup not only facilitates testing in realistic project scenarios",专注于生物信息学领域；包含跨文件依赖和完整项目上下文；规模远超同类领域特定基准（如CoderEval）；提供可扩展的解析工具和模糊测试工具；通过主题建模验证了代码覆盖范围具有代表性。,"Here, we target bioinformatics ... BioCoder spans much of the field, covering cross-file dependencies, class declarations, and global variables. ... our dataset surpasses the scale of CoderEval ... Using topic modeling, we show that the overall coverage of the included code is representative of the full spectrum of bioinformatics calculations. ... We provide an extendable parsing tool ... We provide a fuzz testing tool capable of scaling to handle substantial datasets."
2310.06770_output/content.md,SWE-bench,"We introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories.",Yes,"We introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories.",swebench.com,"Data, code, and leaderboard at swebench.com",评估语言模型在真实软件工程场景下的能力。给定一个代码库和一个需要解决的GitHub问题描述，模型的任务是编辑代码库以解决该问题。,"Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue.",真实世界软件工程问题解决能力，包括跨文件、跨函数、跨类的复杂代码编辑与协调能力，以及长上下文处理和复杂推理能力。,"Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks.",执行单元测试和系统测试。将生成的补丁应用到代码库后，运行与任务实例相关的测试。如果补丁应用成功且所有测试通过，则认为解决方案成功解决了问题。主要指标是解决问题的任务实例百分比。,"To evaluate a proposed solution, we apply the generated patch, using unix's patch program, to the codebase and then execute the unit and system tests associated with the task instance. If the patch applies successfully and all of these tests pass we consider the proposed solution to have successfully resolved the issue. The metric for our benchmark is the percentage of task instances that are resolved.",多文件项目。任务涉及理解大型、复杂的代码库，并跨多个函数、类甚至文件进行协调更改。,"Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously... SWE-bench's reference solutions average editing 1.7 files, 3.0 functions, and 32.8 lines (added or removed).",软件工程，具体为修复Bug或实现新功能。,SWE-bench is a benchmark featuring GitHub issues from popular repositories that report bugs or request new features...,工程级。基于真实GitHub问题，需要处理大型代码库和复杂的跨上下文编辑，对现有最先进模型极具挑战性。,"Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere 1.96% of the issues.",Python。数据集源自12个流行的开源Python仓库。,"SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories.","包含2,294个任务实例。","Through these stages of filtering, the original 90,000 PRs are filtered down to the 2,294 task instances which comprise SWE-bench.",从12个流行的开源GitHub Python仓库中抓取的真实GitHub问题和已合并的拉取请求。,SWE-bench sources task instances from real-world Python repositories by connecting GitHub issues to merged pull request solutions that resolve related tests.,2024 (论文版本为2024年11月11日),arXiv:2310.06770v3  [cs.CL]  11 Nov 2024,官方自建。作者通过一个三阶段的筛选流程从GitHub数据中构建。,"To find high-quality task instances at scale, we use a 3-stage pipeline as follows.",抗污染设计。基准可以持续更新，包含模型训练日期之后创建的问题，确保解决方案未包含在训练语料中。,"Therefore, we can extend SWE-bench with a continual supply of new task instances and evaluate LMs on issues created after their training date, which ensures that the solution was not included in their training corpus.",,,代码编辑/修复。生成应用于现有代码库的补丁以解决问题。,Each task requires generating a patch describing changes to apply to the existing codebase.,问题解决率（百分比）。,The metric for our benchmark is the percentage of task instances that are resolved.,自然语言（问题描述）与代码（完整代码库）。,A model is given an issue text description and a complete codebase.,代码（补丁文件）。,"The model is then tasked to make an edit to the codebase to resolve the issue. In practice, we represent edits as patch files...",文本与代码到代码。,"Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue.",仓库的测试框架。需要安装依赖并运行单元测试和系统测试。,The revised codebase is then evaluated using the repository’s testing framework.,1. 真实世界软件工程任务：源自真实GitHub问题和解决方案。2. 可持续更新：可轻松扩展到任何GitHub Python仓库，最小化人工干预。3. 多样化的长输入：问题描述详细，代码库包含数千个文件。4. 鲁棒的评估：每个任务实例至少有一个“失败转通过”的测试。5. 跨上下文代码编辑：不限制编辑范围，需要在大代码库的多个位置生成修订。6. 解决方案的广泛空间：可作为比较检索、长上下文模型和决策智能体的公平平台。,"SWE-bench offers several advantages over existing LM programming benchmarks. These include, a realistic setting that utilizes user-submitted issues and solutions, diverse inputs featuring unique code problems from 12 repositories, a robust framework for execution-based evaluation, and the ability to continuously update the benchmark with new instances, requiring minimal human intervention."
2312.02120_output/content.md,HumanEval,"Notably, MagicoderS-CL-7B based on CODELLAMA even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1).","No, 本文是使用该数据集进行评测","We evaluate Magicoder and MagicoderS on a wide range of coding tasks, including HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for Python text-to-code generation...",,,Python文本到代码生成，衡量从自然语言描述（docstrings）合成程序的功能正确性。,"...HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for Python text-to-code generation...",代码生成功能正确性,...indicating that MagicoderS-CL can generate more robust code.,pass@1,...surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1).,,,,,,,Python,"...HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) for Python text-to-code generation...",,,,,,,,,文中提及了去污染措施，表明存在污染风险并进行了处理,"Finally, we apply the same logic as StarCoder Li et al. (2023) to decontaminate our training data by removing coding problems that contain docstrings or solutions from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021)...",,,代码生成,...for Python text-to-code generation...,pass@1,...surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1).,自然语言,...for Python text-to-code generation...,代码,...for Python text-to-code generation...,文本到代码,...for Python text-to-code generation...,,,,
2402.16906_output/content.md,HumanEval,"Experiments demonstrate that LDB consistently enhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and TransCoder benchmarks","No, 本文是使用该数据集进行评测","We evaluate LDB on three code generation benchmarks: HumanEval (Chen et al., 2021), TransCoder (Roziere et al., 2020), and MBPP (Austin et al., 2021).",,,文本到代码生成，任务描述是一段简要的自然语言段落，概述了要生成的程序的预期功能。,"HumanEval and MBPP are for text-to-code generation, where the task description is a brief passage outlines the intended functionality of the program to be generated.",代码生成的功能正确性,Experiments demonstrate that LDB consistently enhances the baseline performance... in code debugging for various LLM selections.,使用隐藏测试用例计算 Pass@1 准确率,We compute Pass@1 accuracy with hidden test cases for assesment.,,,,,基础编程问题,"Despite these advanced approaches, they still fall short on basic programming questions from the HumanEval and MBPP datasets.",Python,文中仅提及实验设置（评测文本到代码生成），未描述完整数据集的语言构成。,,,,,2021,"HumanEval (Chen et al., 2021)",,,,,,,代码生成,HumanEval and MBPP are for text-to-code generation,Pass@1,We compute Pass@1 accuracy with hidden test cases for assesment.,自然语言,the task description is a brief passage outlines the intended functionality of the program to be generated.,代码,text-to-code generation,文本到代码,text-to-code generation,,,,
2403.07974_output/content.md,LiveCodeBench,"In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code",Yes,"In this work, we propose LiveCodeBench",https://livecodebench.github.io/,Website: https://livecodebench.github.io/,对大型语言模型的代码相关能力进行全面评估，包括代码生成、自我修复、代码执行和测试输出预测等多个方面。,"Notably, our benchmark also focuses on a broader range of code-related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation.",代码相关能力的全面性评估，包括生成、修复、执行和理解。,"Holistic Evaluation. Current code evaluations primarily focus on natural language to code generation. However, programming is a multi-faceted task that requires a variety of capabilities beyond those measured by code generation.",基于功能正确性，使用一组未见过的测试用例进行评估。对于代码生成场景，使用Pass@1指标。,"The evaluation is performed based on functional correctness, using a set of unseen test cases. We use the Pass@1 metric measured as the fraction of the problems for which the model was able to generate a program passing all tests.",单问题解决，上下文为自然语言问题描述和示例测试。,"The model is given a problem statement, which includes a natural language description and example tests (input-output pairs), and is tasked with generating a correct solution.",竞争性编程（算法问题），来源于LeetCode、AtCoder和CodeForces等竞赛平台。,"which collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces.",平衡的难度分布，包含从易到难的问题，并使用平台提供的难度评级进行分类和过滤。,"Therefore, we use problem difficulty ratings (sourced from the competition websites) for filtering the harder problems and classifying problem difficulties to ensure balanced problem difficulty distribution and allow granular model comparisons.","文中未明确提及数据集支持的具体编程语言，但来源平台（LeetCode, AtCoder, CodeForces）通常支持多种语言。",,目前包含超过500个编码问题（511个），收集于2023年5月至2024年5月期间。,"Currently, LiveCodeBench hosts over five hundred coding problems that were published between May 2023 and May 2024. / Particularly, we have collected 511 problems from contests across three competition platforms – LeetCode, AtCoder, and CodeForces occurring from May 2023 to the present (May 2024)",从三个竞赛平台（LeetCode、AtCoder、CodeForces）的每周比赛中收集的新问题。,"Particularly, we collect problems from weekly contests on competition platforms and tag them with a release date.",2024年6月6日（论文版本日期），数据集持续更新至2024年5月。,arXiv:2403.07974v2  [cs.SE]  6 Jun 2024,官方自建，由研究团队从公开竞赛平台收集和构建。,"In this work, we propose LiveCodeBench",抗污染设计，通过持续更新（使用新发布的问题）和时间分段评估来防止数据污染。,"Live updates to prevent contamination. / to prevent the risk of problem contamination, we use live updates, that is evaluate models on new problems.",,,代码生成、代码修复、代码执行、测试输出预测。,"Specifically, we evaluate code LLMs in four scenarios, namely code generation, self-repair, code execution, and test output prediction.",Pass@1（用于代码生成场景）。,We use the Pass@1 metric measured as the fraction of the problems for which the model was able to generate a program passing all tests.,自然语言问题描述、示例测试、错误程序（用于修复）、代码和输入（用于执行）。,"The model is given a problem statement, which includes a natural language description and example tests (input-output pairs) / The model is given the natural language problem description, the incorrect program, the test case it fails on, and the execution feedback from that failure. / The model is given a program and an input",代码（用于生成和修复）、执行结果（用于执行）、测试输出（用于预测）。,The output should be a correct repaired program. / the output should be the result. / the output should be the output for the problem.,文本到代码（生成）、代码与反馈到代码（修复）、代码与输入到结果（执行）、文本与输入到结果（预测）。,generating code from natural language / Fix an incorrect program from execution information / “Execute” a program on an input / Solve the natural language task on a specified input,,,1. 持续更新以防止数据污染。2. 全面的评估场景，超越单一的代码生成。3. 来源于高质量竞赛平台的问题和测试。4. 平衡的问题难度分布。,Live updates to prevent contamination. / Holistic Evaluation. / High-quality problems and tests. / Balanced problem difficulty.
2403.08604_output/content.md,DevEval,"In this case study, we explore the performance of LLMs across the entire software development lifecycle with DevEval",Yes,"To address these shortcomings and fill this gap, we present DevEval, a comprehensive case study that mirrors real-world software development.",https://github.com/open-compass/DevEval,1Our data and code are available at https://github.com/open-compass/DevEval.,评估大型语言模型在整个软件开发生命周期中的能力，包括从详细的产品需求文档（PRD）开始，构建一个多文件的代码库。,DevEval generally evaluates models on the task of constructing a multi-file codebase starting from a product requirement document (PRD) of detailed specifications.,软件开发生命周期的全面能力，包括软件设计、环境配置、实现、验收测试和单元测试。,"DevEval encompasses stages including software design, environment setup, implementation, acceptance testing, and unit testing.","根据任务不同采用多种方法：软件设计任务使用LLM作为评判者进行主观评估；环境配置任务评估依赖文件执行和示例代码运行的成功率；实现任务使用自动化测试框架（如PyTest, GTest, JUnit, Jest）执行参考测试并计算通过率；测试任务评估测试代码的可执行性（Oracle Test）和代码覆盖率。","Since the Software Design tasks are open-ended, we employ the LLM-as-a-judge approach... The evaluation is anchored by two principal metrics: general principles and faithfulness. The principal metric for evaluation in this task is the success rate of the executed example code. The evaluation procedure involves executing reference acceptance and unit tests within a predefined reference environment. Then the evaluation metric is determined by the pass rate of these tests. ...models generally fail to generate executable tests, with oracle test scores falling below 40%... the generated testing code demonstrates potential in code coverage, achieving as high as 79.4% when it is executable.",多文件项目级，需要理解产品需求文档、UML图、架构设计等完整上下文。,DevEval generally evaluates models on the task of constructing a multi-file codebase starting from a product requirement document (PRD) of detailed specifications.,涵盖多个工程和AI领域，包括数据库应用、Web服务、算法实现、API开发、神经网络、计算机视觉、自然语言处理等。,"DevEval covers a range of domains including NLP, computer vision, deep learning, algorithm implementation, API applications, Database applications, web service (both frontend and backend), and general tools and utilities.",工程级，模拟真实世界的软件开发挑战，现有顶级模型（如GPT-4）得分很低。,"Empirical studies show that current LLMs, including GPT-4, fail to solve the challenges presented within DevEval. GPT-4-Turbo achieves the highest scores amongst all evaluated models, yet it obtains less than 10% on our repository-level implementation task.","Python, C/C++, Java, JavaScript (Vue.js)","DevEval features four programming languages... we curated a collection of 22 repositories across four programming languages (Python, C/C++, Java, JavaScript)",包含22个代码仓库，覆盖4种编程语言和多个领域。平均每个仓库包含约2-7个代码文件，276-617行代码，以及若干验收测试和单元测试。,"we curated a collection of 22 repositories across four programming languages... Table 2: DevEval Statistics. Avg. #Code File 2.2-7.0, Avg. #Code Line 276-617, Avg. #Accep. Tests 2-5.4, Avg. #Unit Tests 8.2-12.4",从公开仓库中精心策划收集，并补充了完整软件开发所需的设计文档和测试程序。,"One significant challenge in this study lies in the scarcity of publicly available repositories that include the full range of software development artifacts, particularly design documents and comprehensive testing programs. To overcome this, we curated a collection of 22 repositories...",2024,arXiv:2403.08604v3 [cs.CL] 14 Dec 2024,官方自建，由研究团队为评估目的而构建。,"we present DevEval, a comprehensive case study... To overcome this, we curated a collection...",,,,,代码库生成，涉及从设计到测试的完整软件开发流程。,DevEval generally evaluates models on the task of constructing a multi-file codebase starting from a product requirement document (PRD) of detailed specifications.,软件设计：基于通用原则和忠实度的LLM主观评分；环境配置：示例代码执行成功率；实现：参考验收和单元测试的通过率；测试：Oracle Test分数和代码覆盖率。,"The evaluation is anchored by two principal metrics: general principles and faithfulness... The principal metric for evaluation in this task is the success rate of the executed example code... the evaluation metric is determined by the pass rate of these tests... oracle test scores... code coverage, achieving as high as 79.4%",自然语言（产品需求文档PRD）、结构化图表（UML图、架构设计）、代码。,"models are provided with the PRD, UML diagrams and architecture design... Table 1: Task design in DevEval. Input: PRD, UML Diagrams, Architecture Design, Implementation Code",代码（依赖文件、实现代码、测试代码）、结构化设计文档（UML图、架构设计）。,"Table 1: Task design in DevEval. Output: UML Diagrams, Architecture Design, Dependency Files, Implementation Code, Acceptance Testing Code, Unit Testing Code",文本与图表到代码、文本与图表到设计文档。,DevEval generally evaluates models on the task of constructing a multi-file codebase starting from a product requirement document (PRD)... The first subtask involves the generation of the UML class diagram...,"使用Docker定义的基础环境，针对不同语言使用特定工具（Conda, Gradle, NPM）和测试框架（PyTest, GTest, JUnit, Jest）。","The evaluation centers on the execution of dependency files across each programming language within a predetermined base environment delineated in a Docker file... For Python, the Conda environment manager is employed; for Java and JavaScript, Gradle and NPM are utilized respectively... integrates PyTest for Python, GTest for C++, JUnit for Java, and Jest for JavaScript.",首个评估模型软件设计和环境配置能力的基准；遵循瀑布模型，将软件开发分解为多个相互关联的阶段；提供文档级别的详细需求描述；采用模块化评估协议，支持端到端或分阶段评估。,"DevEval is the first to evaluate models’ software design and environment setup capabilities. Subscribing to the traditional Waterfall software development model... DevEval breaks down this process into a diverse set of inter-related development stages... In contrast to similar systems... which generate outputs from brief requirement descriptions typically under 100 words, DevEval offers document-level detail to guide the models. ...our design utilizes reference inputs for each task. This strategy enables and concentrates on evaluating the efficacy of models in executing specific tasks."
2406.06887_output/content.md,"HumanEval, MBPP, LiveCodeBench, LeetCode","PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench)... We evaluate PLUM on a diverse set of state-of-the-art code language models under different set-ups, on commonly used evaluation benchmarks: HumanEval(+) and MBPP(+) [7, 1, 28] as well as more challenging code generation datasets like LiveCodeBench and LeetCode [22, 19].","No, 本文是使用该数据集进行评测","PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench)... We evaluate PLUM on a diverse set of state-of-the-art code language models under different set-ups, on commonly used evaluation benchmarks: HumanEval(+) and MBPP(+) [7, 1, 28] as well as more challenging code generation datasets like LiveCodeBench and LeetCode [22, 19].",,,文中未描述这些基准的原始任务定义，仅提及它们是用于评估代码生成模型的常用基准。,"PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench)... We evaluate PLUM on a diverse set of state-of-the-art code language models under different set-ups, on commonly used evaluation benchmarks: HumanEval(+) and MBPP(+) [7, 1, 28] as well as more challenging code generation datasets like LiveCodeBench and LeetCode [22, 19].",代码生成的功能正确性,Works like AlphaCode [26] and LeTI [48] have introduced test outcomes as a means to define functional correctness in code generation.,使用测试用例执行来评估功能正确性,Works like AlphaCode [26] and LeTI [48] have introduced test outcomes as a means to define functional correctness in code generation.,,,通用编程问题,These datasets provide a diverse range of programming tasks and instructions.,标准基准和更具挑战性的基准,"PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench)... as well as more challenging code generation datasets like LiveCodeBench and LeetCode [22, 19].",文中仅提及实验聚焦于Python，未描述完整数据集的语言覆盖范围。,We focus on Python due to its wide use and the availability of well-established training and evaluation resources.,,,,,,,,,,,,,代码生成,"Language models pre-trained on code corpora have excelled at code generation [40, 25].",通过率 (pass rates),"PLUM increases pass rates by up to 4.8% on average on standard benchmarks and 11.8% on LiveCodeBench, demonstrating its effectiveness and generalizability.",自然语言指令,"PLUM utilizes natural language instructions from well-established datasets such as OSS-Instruct [49], Evol-Instruct-Code [31], and ShareGPT [8].",代码,"These solutions are evaluated using the generated test cases, with preference labels assigned based on the results: solutions passing the tests are preferred, while failures are dis-preferred.",文本到代码,PLUM utilizes natural language instructions from well-established datasets... These solutions are evaluated using the generated test cases...,文中未明确描述基准的执行环境，但提及了执行检查。,"With static and execution checks,3 we identify and filter out solutions that contain syntactic errors and fail to execute, as our focus is on functional correctness.",本文提出的PLUM框架本身是一个使用测试用例进行执行引导、基于策略的偏好学习方法，用于改进代码语言模型。它并非一个数据集。,"we propose PLUM, an on-policy Preference Learning framework Augmented with test cases for code LMs."
2406.18294_output/content.md,CrossCodeEval,"To assess the code completion performance of Code LLMs in real development scenarios, we utilized CrossCodeEval (Ding et al., 2023) as the evaluation dataset.","No, 本文是使用该数据集进行评测","To assess the code completion performance of Code LLMs in real development scenarios, we utilized CrossCodeEval (Ding et al., 2023) as the evaluation dataset.",,,评估代码大语言模型在真实开发场景中的代码补全性能，特别是需要利用跨文件代码信息进行补全的任务。,"To assess the code completion performance of Code LLMs in real development scenarios... The CrossCodeEval (Ding et al., 2023) benchmark provides test cases that require the use of cross-file code information for completion.",仓库级代码补全能力，利用跨文件信息的能力,"Some benchmarks for repository-level code completion have been proposed to evaluate the performance of code models in real-world completion tasks, such as CrossCodeEval (Ding et al., 2023)...","精确匹配（Exact Match, EM）和编辑相似度（Edit Similarity, ES）","Following the CrossCodeEval evaluation protocol, we evaluated the completion results using two metrics: Exact Match (EM) and Edit Similarity (ES).",跨文件、仓库级上下文,"The CrossCodeEval (Ding et al., 2023) benchmark provides test cases that require the use of cross-file code information for completion.",,,真实世界开发场景,To assess the code completion performance of Code LLMs in real development scenarios...,文中仅提及实验子集(Python)，未描述完整数据集,"Without loss of generality, in this study, we have chosen Python language as the primary language for our research.","本文实验使用了2,655个真实世界补全测试用例","Ultimately, we obtained 2,655 real-world completion tests.",,,2023,"CrossCodeEval (Ding et al., 2023)",,,,,,,代码补全（包括中段填充）,Infilling scenarios constitute the majority of code completion tasks in the real world.,"Exact Match (EM), Edit Similarity (ES)","Following the CrossCodeEval evaluation protocol, we evaluated the completion results using two metrics: Exact Match (EM) and Edit Similarity (ES).",代码（包含部分缺失的代码上下文）,We then removed the code after the cursor in that line to form authentic completion test cases.,代码,The completion results are less than satisfactory.,代码到代码（补全）,Infilling scenarios constitute the majority of code completion tasks in the real world.,,,专注于评估模型利用跨文件代码信息进行仓库级代码补全的能力，测试用例需要跨文件信息。,"The CrossCodeEval (Ding et al., 2023) benchmark provides test cases that require the use of cross-file code information for completion."
2408.06450_output/content.md,EVALPERF,"As a proof of concept, we use DPE to create EVALPERF, a benchmark with 121 performance-challenging coding tasks.",Yes,"We introduce Differential Performance Evaluation (DPE), a framework designed to reliably evaluate Large Language Models (LLMs) for efficient code generation. ... As a proof of concept, we use DPE to create EVALPERF, a benchmark with 121 performance-challenging coding tasks.",github.com/evalplus/evalplus,We also fully open-source and maintain the data curation pipeline and evaluator at github.com/evalplus/evalplus as part of EvalPlus.,评测大型语言模型（LLMs）生成的代码的效率。该基准旨在通过提供具有性能挑战性的编程任务和有效的复合指标，可靠地评估代码效率，弥补传统代码基准在效率评估方面的不足。,"We introduce Differential Performance Evaluation (DPE), a framework designed to reliably evaluate Large Language Models (LLMs) for efficient code generation. Traditional coding benchmarks often fail to provide reliable insights into code efficiency, due to their reliance on simplistic test inputs and the absence of effective compound metrics. DPE addresses these issues by focusing on efficiency-demanding programming tasks and establishing an insightful compound metric for performance evaluation.",代码生成效率,"While the correctness evaluation of code generation has been well studied, we deliver a new and important aspect to the community by studying the data curation and assessment for the efficiency evaluation of LLM-generated code.",差分性能评估（DPE）。该方法分为两个阶段：1）数据集构建：从现有基准中选择对效率有要求的任务，并生成计算密集型的输入来测试LLM解决方案的效率。2）效率评估：分析新解决方案的性能，并将其与一组具有不同效率水平的参考解决方案进行全局比较，匹配到的效率水平决定了其效率得分。,"DPE operates in two phases: To curate efficiency datasets, it selects efficiency-demanding tasks from existing coding benchmarks and generates computationally expensive inputs to stress the efficiency of LLM solutions. To assess the code efficiency, DPE profiles the new solution and compares it globally against a set of reference solutions that exhibit distinct efficiency levels, where the matched level defines its efficiency score.",,,通用编程任务，涵盖算法、数据结构等,"At the high level, the input to the dataset creation phase is a set of programming tasks, e.g., from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021).",性能挑战型任务，旨在区分不同代码解决方案的效率,DPE curates performance-demanding coding tasks by sampling synthesized test input generators and using filters to ensure evaluator quality.,Python（从HumanEval和MBPP等基准中选取任务，这些基准主要使用Python）,"At the high level, the input to the dataset creation phase is a set of programming tasks, e.g., from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021).",包含121个性能挑战型编程任务,"As a proof of concept, we use DPE to create EVALPERF, a benchmark with 121 performance-challenging coding tasks.",从现有代码生成基准（如HumanEval、MBPP）中筛选和改造任务，并利用LLM合成性能测试输入生成器来生成计算密集型测试输入。,"At the high level, the input to the dataset creation phase is a set of programming tasks, e.g., from HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021). ... To this end, we propose Synthesizing a Synthesizer (SAS) to automatically produce performance-exercising inputs of different programming tasks by prompting powerful code LLMs to generate test generators.",2024.08,arXiv:2408.06450v1  [cs.SE]  12 Aug 2024,官方自建，基于DPE框架从现有基准中筛选和增强,"As a proof of concept, we use DPE to create EVALPERF, a benchmark with 121 performance-challenging coding tasks.",,,,,代码生成（从自然语言描述生成完整的函数/解决方案）,"Given a programming task, we collect a rich set of correct solutions by sampling various LLMs and test execution.",效率得分（基于与参考解决方案性能集群的匹配度）,"During evaluation, if passing the correctness tests, the new solutions are profiled to compare against the reference solutions. Specifically, from slow to fast, the cumulative ratio of the cluster that includes the matched reference solution is the efficiency score of the evaluated solution.",自然语言（任务描述）,"Given a coding instruction in natural language, LLMs produce solutions...",代码,LLMs produce solutions whose correctness is assessed through test execution.,文本到代码,"Given a coding instruction in natural language, LLMs produce solutions...",,,1. 专注于代码效率评估，而非传统的功能正确性。2. 使用DPE框架，包含“合成一个合成器”（SAS）方法来自动生成性能测试输入。3. 通过性能聚类建立参考解决方案集，用于全局比较和评分。4. 包含任务筛选标准（足够的计算量、低性能变异、性能多样性）以确保评估质量。,"DPE addresses these issues by focusing on efficiency-demanding programming tasks and establishing an insightful compound metric for performance evaluation. ... To this end, we propose Synthesizing a Synthesizer (SAS) to automatically produce performance-exercising inputs of different programming tasks by prompting powerful code LLMs to generate test generators. ... a selected programming task must meet the following criteria: 1. Sufficient computation ... 2. Low performance variation ... 3. Performance diversity..."
2410.01215_output/content.md,HumanEval,MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].,"No, 本文是使用该数据集进行评测","Extensive experiments across multiple models and benchmarks demonstrate that MGDebugger significantly outperforms existing debugging methods. Using open-source models like DeepSeek-Coder-V2-Lite, CodeQwen1.5, and Codestral, MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].",,,文中仅提及实验使用，未描述完整数据集。根据引用[7]（HumanEval）推断，该基准旨在评估从自然语言描述（docstring）生成Python代码的功能正确性。,"Following the problem settings from Chen et al. [9] and Zhong et al. [75], we assume that the public test cases for the main function Tpub have been provided, which is common in most code generation benchmarks [7, 14, 44]2.",代码生成的功能正确性,MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].,执行单元测试（pass@k）,"Following the problem settings from Chen et al. [9] and Zhong et al. [75], we assume that the public test cases for the main function Tpub have been provided, which is common in most code generation benchmarks [7, 14, 44]2.",单函数,"Specifically, given an LLM-generated function 𝑓, we decompose it into a hierarchical structure of subfunctions denoted as (𝑓1, ..., 𝑓𝑛).",,,,,Python,"To effectively test each subfunction, we propose an LLM-simulated Python executor, which traces code execution and tracks important variable states to pinpoint errors accurately.",,,,,,,,,,,,,代码生成,"Large language models (LLMs) such as GPT-4 [48], LLaMA [56], and DeepSeek-Coder [1] have made significant advances in AI-assisted coding tasks [7, 25, 34, 52]. Trained on vast corpora of text and code, LLMs can understand and generate code snippets for various programming tasks, ranging from simple data structures to complex algorithmic problems [34, 57].",准确率（accuracy），修复成功率（repair success rate）,MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44].,自然语言（函数描述/docstring）,"Following the problem settings from Chen et al. [9] and Zhong et al. [75], we assume that the public test cases for the main function Tpub have been provided, which is common in most code generation benchmarks [7, 14, 44]2.",代码（Python函数）,"Specifically, given an LLM-generated function 𝑓, we decompose it into a hierarchical structure of subfunctions denoted as (𝑓1, ..., 𝑓𝑛).",文本到代码,"Following the problem settings from Chen et al. [9] and Zhong et al. [75], we assume that the public test cases for the main function Tpub have been provided, which is common in most code generation benchmarks [7, 14, 44]2.",,,本文未描述HumanEval的独特之处，而是将其作为评估模型调试能力的标准基准之一。,"Extensive experiments across multiple models and benchmarks demonstrate that MGDebugger significantly outperforms existing debugging methods. Using open-source models like DeepSeek-Coder-V2-Lite, CodeQwen1.5, and Codestral, MGDebugger elevates accuracy from 75.6% to 94.5% on HumanEval [7] and achieves a remarkable 97.6% repair success rate on HumanEvalFix [44]."
2411.05830_output/content.md,GitChameleon,"To address this gap, we introduce GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests.",Yes,"To address this gap, we introduce GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests.",https://github.com/NizarIslah/GitChameleon,"For facilitation for further exploration of version-conditioned code generation, we make our code repository publicly accessible at https://github.com/NizarIslah/GitChameleon.",评估大型语言模型生成特定版本库代码的能力，要求生成的代码不仅语法正确，而且在执行时功能准确。,GitChameleon is designed to rigorously assess the ability of modern large language models (LLMs) to generate version-specific code that is not only syntactically correct but also functionally accurate upon execution.,版本特定代码生成能力、代码库动态适应性、功能正确性,"By providing an execution-based benchmark that emphasizes the dynamic nature of code libraries, GitChameleon serves as a critical tool to advance the development of more adaptable and reliable code generation models.",基于执行的评估，使用手写的基于断言的单元测试来验证生成代码的正确性。,"To evaluate LLM performance on GitChameleon, each problem is accompanied by handwritten assertion-based unit tests, enabling a thorough execution-based assessment of the outputs generated by the code LLMs.",单函数代码补全,"GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems","Python编程，涉及多个流行的机器学习和数据处理库（如PyTorch, NumPy, Scikit-Learn, Pandas等）","GitChameleon consists of 116 python-based version conditioned problems based on 11 libraries: PyTorch (Paszke et al., 2019), Geopandas (Jordahl et al., 2020), NLTK (Bird & Loper, 2004), NetworkX (Hagberg et al., 2008), GeoPy3, Gradio (Abid et al., 2019), Scikit-Learn (Buitinck et al., 2013), Matplotlib (Hunter, 2007), PyCaret4, Pandas (pandas development team, 2020; Wes McKinney, 2010) and NumPy (Harris et al., 2020).",基于真实版本变化的实际问题，旨在模拟开发者在技术债务约束下的真实场景。,our dataset offers a unique and complementary perspective by focusing on the real-world scenario where developers are often constrained to specific library versions due to technical debt.,Python,"GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems",包含116个Python版本条件问题,"GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems",手工编写和LLM辅助，基于真实库的变更日志（changelogs）,"The examples were manually crafted by the authors, who divided the task among themselves. We compiled a list of popular Python libraries, focusing on those with which at least one author was familiar and that had detailed changelogs documenting changes between versions.",2024-11-05 (根据arXiv版本v1日期),arXiv:2411.05830v1  [cs.SE]  5 Nov 2024,官方自建（作者手工编写）,"The examples were manually crafted by the authors, who divided the task among themselves.",已考虑训练数据截止日期，确保样本在模型训练窗口期内，以评估模型对已训练版本的知识。,"Since some of the models evaluated on GitChameleon have disclosed their training data cutoff dates, we have ensured that most, if not all, samples fall within the training window of these models.",,,代码补全,"GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems",pass@k (例如pass@10),"for instance, GPT-4o achieves a pass@10 of only 39.9% (43.7% when provided with error feedback)",自然语言问题描述和起始代码,"The problem statements average 20.4 tokens, and the starter code averages 47.4 tokens, leading to a combined average of 67.8 tokens per sample.",代码,generate version-specific code,文本到代码,synthesizing programs from docstrings.,需要特定库版本依赖的虚拟环境,We used venv to create and manage virtual environments for testing. This process involved installing the appropriate library version and any additional dependencies.,专注于评估模型对真实库版本变化的适应能力，每个问题都绑定到特定的库版本，并配有可执行的单元测试。数据基于2014年至2023年的真实版本发布，并标注了变更类型（参数/属性变更、函数名变更、语义/行为变更、新功能）。,"GitChameleon, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. ... The samples were collected from version releases over a period from the year 2014 to 2023 ... we annotate each sample with the type of change that is classified into the following categories: Argument or Attribute change, Function Name change, Semantics or Function Behavior change, New feature or additional dependency-based change."
2411.12882_output/content.md,PurpleLlama secure coding benchmark,"We demonstrate the effectiveness of PROSEC on the PurpleLlama (Bhatt et al., 2023) secure coding benchmark.","No, 本文是使用该数据集进行评测","We demonstrate the effectiveness of PROSEC on the PurpleLlama (Bhatt et al., 2023) secure coding benchmark.",,,评估代码大语言模型生成安全代码的能力，检测其生成的代码是否包含常见弱点枚举（CWE）中定义的安全漏洞。,"The goal of security alignment in code LLMs is to reduce the likelihood of generating insecure code... We demonstrate the effectiveness of PROSEC on the PurpleLlama (Bhatt et al., 2023) secure coding benchmark.",代码安全性，即生成代码是否遵循安全编码实践，避免引入常见漏洞。,"The safety of these models remains under-explored, posing potential risks as insecure code generated by these models may introduce vulnerabilities into real-world systems.",使用静态分析器作为预言机，检测生成的代码片段中是否存在CWE漏洞。,"Following previous work (Bhatt et al., 2023), we assume that there exists a static analyzer... as an oracle to detect whether a snippet of code follows secure code patterns. Specifically, the static analyzer takes as input a code snippet, and outputs a list of detected CWEs.",,,安全编码，涵盖常见弱点枚举（CWE）中定义的各种软件安全漏洞类型。,"The Common Weakness Enumerations (CWEs) (MITRE, 2023), which abstract diverse program vulnerabilities, offer a generalizable foundation for simulating how vulnerabilities manifest across various coding tasks and programming languages.",,,文中未明确提及PurpleLlama基准支持的具体编程语言，但本文实验涉及多种语言。,"PROSEC improves the ability of code LLMs to generate secure code... across multiple models, languages, and vulnerability types.",,,,,2023,"PurpleLlama (Bhatt et al., 2023)",,,,,,,代码生成,Large language models (LLMs) capable of generating code based on human instructions have revolutionized programming by significantly facilitating tasks such as code generation...,安全代码生成能力的提升百分比（例如，比基线模型安全25.2%–35.4%）,The models trained with the dataset synthesized by PROSEC are 25.2%–35.4% more secure than those trained with the SafeCoder dataset.,自然语言指令,"Consider an instruction following code LLM πθ(y|x) = Πiπθ(yi|y<i, x) that takes user instruction x and generates the code response y.",代码,"Consider an instruction following code LLM πθ(y|x) = Πiπθ(yi|y<i, x) that takes user instruction x and generates the code response y.",文本到代码,"Consider an instruction following code LLM πθ(y|x) = Πiπθ(yi|y<i, x) that takes user instruction x and generates the code response y.",,,专注于代码安全性的评测基准，使用CWE（常见弱点枚举）作为漏洞分类和检测的基础框架。,"A widely recognized framework for categorizing such issues is the Common Weakness Enumerations (CWE) (MITRE, 2023)... Following previous work (Bhatt et al., 2023), we assume that there exists a static analyzer... to detect whether a snippet of code follows secure code patterns... and outputs a list of detected CWEs."
2412.00535_output/content.md,FullStack Bench,FullStack Bench: Evaluating LLMs as Full Stack Coders,Yes,"To address this gap, we have developed a comprehensive code evaluation dataset FullStack Bench1,2 focusing on full-stack programming","https://huggingface.co/datasets/ByteDance/FullStackBench, https://github.com/bytedance/FullStackBench","1https://huggingface.co/datasets/ByteDance/FullStackBench
2https://github.com/bytedance/FullStackBench",评估大型语言模型作为全栈程序员的能力，涵盖广泛的应用程序领域。,"focusing on full-stack programming, which encompasses a wide range of application domains (e.g., basic programming, data analysis, software engineering, mathematics, and machine learning).",全栈编程能力、多语言编程能力、跨应用领域能力,focusing on full-stack programming... to assess multi-lingual programming capabilities... a wide range of application domains,使用单元测试用例进行代码沙箱执行评估,"we design real-world instructions and corresponding unit test cases... we also release an effective code sandbox execution tool (i.e., SandboxFusion3)... to evaluate the performance of our FullStack Bench efficiently.",,,基础编程、高级编程、软件工程、数据分析、数学、桌面与Web开发、机器学习、科学计算、数据库、多媒体、操作系统等,"Basic Programming (BP)
Advanced Programming (AP)
Software Engineering (SE)
Data Analysis (DA)
Mathematics (MA)
Desktop and Web Development (DW)
Machine Learning (ML)
Scientific Computing (SC)
DataBase (DB)
Multimedia (MM)
Operating System (OS)
Others",,,16种广泛使用的编程语言,"in FullStack Bench, we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages",,,设计真实世界的指令和对应的单元测试用例，而非简单翻译,we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations.,2025-05-12,arXiv:2412.00535v6  [cs.AI]  12 May 2025,官方自建,we have developed a comprehensive code evaluation dataset FullStack Bench,,,,,代码生成,evaluating LLMs as Full Stack Coders,,,自然语言指令,we design real-world instructions,代码,evaluating LLMs as Full Stack Coders,文本到代码,we design real-world instructions and corresponding unit test cases,支持多种编程语言和包的代码沙箱执行工具(SandboxFusion),"we also release an effective code sandbox execution tool (i.e., SandboxFusion3) supporting various programming languages and packages",专注于全栈编程评估，覆盖广泛的应用程序领域；包含16种编程语言的真实世界指令和单元测试，而非简单翻译；配套发布了支持多语言和多包的代码沙箱执行工具SandboxFusion。,"focusing on full-stack programming, which encompasses a wide range of application domains... we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations. Moreover, we also release an effective code sandbox execution tool (i.e., SandboxFusion3) supporting various programming languages and packages"
2412.05210_output/content.md,CodeArena,we present a rigorous human-curated benchmark CodeArena to emulate the complexity and diversity of real-world coding tasks,Yes,"we first introduce a comprehensive human-curated benchmark, CodeArena",https://codearenaeval.github.io/,1https://codearenaeval.github.io/,评估模型生成的代码响应与人类偏好的对齐程度，模拟现实世界编码任务的复杂性和多样性。,"to evaluate the alignment between the model-generated response and human preference, enabling the community to evaluate and track the alignment between human preferences and model-generated responses in real-world scenarios.",人类偏好对齐（包括代码质量、解释、格式、注释等），而非单纯的代码正确性。,"the current code LLMs focus on synthesizing the correct code snippet, ignoring the alignment with human preferences... underscoring the importance of the human preference alignment.",使用GPT-4o作为评判员，通过“比较A和B”和“比较B和A”两种游戏计算模型相对于基线的胜率。,"we apply GPT-4o-2024-08-06 as the judger to evaluate the model performance. Specifically, we use two games “compare A and B ” and “compare B and A” (avoid the relative position of A and B affecting the results) to calculate the win rate of A compared to the baseline B.",基于现实世界用户查询的完整问题，不局限于自包含的函数片段。,"Popular code-related benchmarks typically focus on self-contained function snippets... CodeArena provides many problems for evaluation under realistic scenarios, which are not suitable for verification through unit testing.",涵盖软件开发、用户界面/体验、专用计算、工具与环境、数据库与数据处理、新兴技术、通用查询等7大类40个子类。,comprising 397 high-quality samples across 40 categories derived from real-world user queries... encompassing 7 major categories and 40 subcategories.,分为简单、中等、困难三个级别，大部分样本为中等或困难。,"all samples are classified into easy, medium, and hard. The majority of the samples are recognized as medium or hard, presenting a significant challenge to LLMs.",涵盖44种编程语言，包括Python、C++、Java、JavaScript、HTML/CSS、SQL、Bash、Go、Rust、PowerShell、Google Apps Script等。,spanning 40 categories and 44 programming languages... CodeArena provides a valuable comprehensive benchmark for 40 subtasks and 44 programming languages,包含397个高质量样本。,comprising 397 high-quality samples... CodeArena consists of nearly 400 problems.,从在线问答网站的用户查询中精心筛选和人工标注。,carefully curated from user queries... derived from real-world user queries... Online Q&A,2024年12月（论文版本日期）,arXiv:2412.05210v1  [cs.CL]  6 Dec 2024,官方自建，经过严格的人工标注和质量控制流程。,We propose CodeArena comprised of 397 manually annotated samples... we implement a rigorous human annotation process involving 4 full-time employees proficient in various programming languages for human annotation and 4 other senior programming developers for quality check.,"进行了去污染处理，通过移除与现有基准（MultiPL-E, MBPP, McEval, NaturalCodeBench）的精确匹配（10-gram重叠）来确保提示的唯一性。","To avoid data leakage, we apply decontamination to ensure the uniqueness of prompts in CodeArena, by removing exact matches (10-gram word overlap) from MultiPL-E, MBPP, McEval, and NaturalCodeBench.",,,代码生成（根据自然语言描述生成代码片段或完整解决方案）。,The query should be sampled from the practical application scenarios and the model-generated responses should satisfy the human preference.,胜率（win rate），基于GPT-4o的偏好判断。,calculate the win rate of A compared to the baseline B.,自然语言（用户查询/问题描述）。,Each sample in CodeArena includes (question...),代码与自然语言混合（期望的响应包括代码片段以及详细的解释、格式化和注释）。,"Claude3.5 produces responses that include detailed explanations, well-structured formatting, and code comments, making it more favorable in terms of human preference.",文本到代码与文本（自然语言问题到包含代码和解释的响应）。,"synthesizing the correct code snippet... alignment with human preferences (including explanations, formatting, comments)",,,专注于评估代码LLM与人类偏好的对齐，而非单纯的代码执行正确性；样本来源于真实世界用户查询，覆盖广泛的编程语言和任务类别；包含人工标注的难度等级和基线答案。,"Unlike previous benchmarks, which consist of various programming exercises along with the corresponding test cases... our benchmarks emphasize a diverse range of programming languages that are commonly used in everyday programming tasks... provides a valuable comprehensive benchmark for 40 subtasks and 44 programming languages, which satisfies the evaluation in realistic scenarios."
2505.08503_output/content.md,ICVul,ICVul: A Well-labeled C/C++ Vulnerability Dataset with Comprehensive Metadata and VCCs,Yes,"we constructed ICVul, a dataset emphasizing data quality and enriched with comprehensive metadata, including Vulnerability-Contributing Commits (VCCs).",https://github.com/Chaomeng-Lu/ICVul.git,"The project, along with supporting scripts, is publicly available on GitHub1. 1https://github.com/Chaomeng-Lu/ICVul.git",用于训练和评估基于机器学习的软件漏洞检测模型。,"Machine learning-based software vulnerability detection requires high-quality datasets, which is essential for training effective models.",漏洞检测的数据质量、标签可靠性、元数据丰富度、数据平衡性。,"To address challenges related to data label quality, diversity, and comprehensiveness, we constructed ICVul, a dataset emphasizing data quality and enriched with comprehensive metadata...",文中未明确描述针对该数据集的标准评估方法。,,代码提交（Commit）、文件（File）、函数（Function）级别。,"ICVul provides a high-quality dataset tailored for training ML models to detect software vulnerabilities at the commit, function and file levels.",软件安全、漏洞检测。,Detecting and mitigating software vulnerabilities remains one of the most critical challenges in ensuring the security and integrity of modern software systems.,真实世界软件项目中的漏洞，复杂度高。,"With the growing complexity of applications and their codebases, traditional methods of vulnerability detection... struggle to keep up with the scale and intricacy of vulnerabilities present in large software projects.",C/C++,"ICVul, an Integrated and Comprehensive C/C++ Vulnerability dataset.",包含807个仓库，146个CWE类型，4327个修复提交，6862个文件，15396个函数，其中6276个为漏洞函数（占比41%）。,"ICVul 807 146 4,327 6,862 15,396 6,276 41%",从美国国家漏洞数据库（NVD）中筛选与GitHub修复提交相关联的CVE条目，并从中提取代码和元数据。,"We began by filtering Common Vulnerabilities and Exposures from the NVD, retaining only those linked to GitHub fix commits. Then we extracted functions and files along with relevant metadata from these commits...",2024年11月13日（数据收集日期）,"The dataset construction process began with the collection of 269,509 CVEs, gathered on November 13, 2024.",官方自建，并提供了可复现的构建框架。,We introduce a vulnerability collection framework that can be re-run at any time to ensure the dataset remains up-to-date.,通过ESC技术排除可疑提交，旨在提高标签可靠性，降低噪声。,"To further enhance label reliability, we developed the ESC (Eliminate Suspicious Commit) technique, ensuring credible data labels.",文中未提及。,,漏洞检测（分类任务）。,ICVul provides a high-quality dataset tailored for training ML models to detect software vulnerabilities...,文中未提及针对该数据集的标准评估指标。,,源代码（C/C++）及丰富的元数据（如提交信息、代码变更、仓库信息等）。,The dataset is stored in a relational-like database for improved usability and data integrity.,漏洞标签（是否包含漏洞，以及CWE类型）。,"the dataset provides clear vulnerability labels at the CWE type level, enabling the development and research of multi-class classification models.",代码到标签（漏洞分类）。,training ML models to detect software vulnerabilities,不涉及代码执行，是静态分析数据集。,,1. 包含漏洞引入提交（VCC）信息，使用SZZ算法追溯。2. 应用ESC技术过滤噪声数据，提高标签质量。3. 数据平衡性好（漏洞函数占比41%）。4. 以关系型数据库结构存储，包含仓库、提交、文件、函数、CVE-VCC映射五个互相关联的表。5. 专注于C/C++语言。,"Another key feature of ICVul is its inclusion of VCCs, which trace specific commits responsible for introducing vulnerabilities into the codebase using the state-of-the-art SZZ algorithm... ICVul incorporates the ESC (Eliminate Suspicious Commit) technique, further enhancing label reliability... the dataset achieves a much better balance ratio of 41% at the function level. ICVul comprises five interconnected tables: repository info, cve fc vcc mapping, commit info, file info, and function info."
2511.17330_output/content.md,SV-COMP,Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification.,"No, 本文是使用该数据集进行评测",Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification.,,,程序验证，即证明程序满足其形式化规范。,Formal program verification uses mathematically rigorous methods to prove that a program satisfies its specifications.,程序验证的自动化能力，特别是证明生成的有效性。,"We thoroughly evaluate our approach on existing mathematical lemmas [51], as well as lemmas systematically extracted from SV-COMP programs [7]. The SV-COMP programs’ lemmas capture intricate code logic and properties, which are more representative of program verification.",通过证明成功率（即成功证明的引理比例）进行评估。,"Evaluation shows that AutoRocq significantly outperforms state-of-the-art approaches. Specifically, AutoRocq is capable of proving 51.1% mathematical lemmas and 30.9% program lemmas, exceeding baseline approaches by 20.8% to 343.0% on mathematical lemmas, and by 42.4% to 204.6% on program lemmas.",,,软件验证、形式化方法、定理证明。,"We showcase the feasibility of automatic and end-to-end verification with LLM agents. AutoRocq is evaluated on the widely used SV-COMP programs in software verification, as well as Linux kernel modules.",代表真实世界程序验证的复杂逻辑和属性，难度较高。,"The SV-COMP programs’ lemmas capture intricate code logic and properties, which are more representative of program verification.",,,,,从SV-COMP程序和Linux内核模块中系统提取的引理。,"We thoroughly evaluate our approach on existing mathematical lemmas [51], as well as lemmas systematically extracted from SV-COMP programs [7].",,,,,,,,,证明生成（生成证明脚本以履行证明义务）。,"Proof generation aims to automatically synthesize proof scripts that discharge the given proof obligations, which closes the last mile of program verification.",证明成功率（百分比）。,AutoRocq is capable of proving 51.1% mathematical lemmas and 30.9% program lemmas...,形式化的证明义务（逻辑公式）。,Proof generation aims to automatically synthesize proof scripts that discharge the given proof obligations...,证明脚本（Rocq/Coq战术序列）。,Proof generation aims to automatically synthesize proof scripts that discharge the given proof obligations...,逻辑公式到证明脚本。,Proof generation aims to automatically synthesize proof scripts that discharge the given proof obligations...,Rocq（原Coq）定理证明器环境。,The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback.,SV-COMP是软件验证竞赛的基准，其程序引理捕获了复杂的代码逻辑和属性，更能代表真实的程序验证场景。,"The SV-COMP programs’ lemmas capture intricate code logic and properties, which are more representative of program verification."
2512.03421_output/content.md,BugT,"This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns.",Yes,"BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. ... We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMs’ capabilities.",https://github.com/Xucranger/PLofLBFL,"To facilitate future study, we share our source code and experimental data in a GitHub repository 1. 1https://github.com/Xucranger/PLofLBFL",评估大型语言模型在初学者程序中进行故障定位（Fault Localization）的性能。,"This study evaluates the fault localization performance of six closed-source LLMs (e.g., OpenAI o3, GPT-3.5-Turbo, etc.) and seven open-source LLMs (e.g., DeepSeekR1, Llama3, etc.) across Codeflaws, Condefects, and BugT datasets.",故障定位的准确性、效率、可用性，以及对问题难度的鲁棒性。,"To investigate these issues, we aim to empirically assess the performance of different LLMs across various datasets, evaluating their accuracy, efficiency, and usability in fault localization for novice programs. ... We examine how problem difficulty levels across three datasets affect LLMs’ fault localization performance, providing insights into model behavior at varying complexity levels.",在多个数据集上评估LLMs的故障定位性能，并与传统方法（如SBFL、MBFL）进行比较。,"This study evaluates the fault localization performance of six closed-source LLMs (e.g., OpenAI o3, GPT-3.5-Turbo, etc.) and seven open-source LLMs (e.g., DeepSeekR1, Llama3, etc.) across Codeflaws, Condefects, and BugT datasets. All closed-source LLMs outperform traditional SBFL and MBFL methods...",,,编程教育，初学者编程。,Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. ... This study evaluates the fault localization performance ... for novice programs.,包含不同难度级别的问题，从简单到复杂。,"LLM accuracy decreases as problem difficulty increases in Codeflaws and Condefects, but top models maintain high accuracy even at peak difficulty in BugT, suggesting its lower complexity. ... We examine how problem difficulty levels across three datasets affect LLMs’ fault localization performance...",,,,,自建的包含真实编程故障的数据集。,"We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMs’ capabilities.",2025-12-04,"Preprint submitted to Journal of LATEX Templates December 4, 2025",官方自建,We introduce a new self-created dataset with real programming faults...,专门设计以减轻数据泄露问题。,BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns.,,,故障定位,This study evaluates the fault localization performance...,准确性（accuracy）,LLM accuracy decreases as problem difficulty increases...,包含故障的代码,"Novice Program refers to code written by novice programmers, usually in the early stages of learning programming. These programs usually contain multiple errors...",故障位置及解释,LLM generated fault explanations demonstrate significant value for novice programmer assistance...,代码到故障位置/解释,"LLM-based fault localization methods leverage the models’ understanding of program syntax and semantics to automatically localize faults in code, offering more precise and contextually relevant suggestions...",,,专门针对初学者程序（Novice Programs）构建，旨在减轻LLM评估中的数据泄露（data leakage）问题，包含真实的编程故障。,"BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. ... We introduce a new self-created dataset with real programming faults, aiming to mitigate data leakage concerns and establish a more reliable evaluation benchmark for assessing LLMs’ capabilities."
2512.05073_output/content.md,Comprehensive Verilog Design Problems (CVDP),"The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA, provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.","No, 本文是使用该数据集进行评测",Our work tests this by evaluating Small language models (SLMs) coupled with a curated agentic AI framework on NVIDIA’s Comprehensive verilog design problem (CVDP) benchmark.,,,从自然语言规范生成经过验证的寄存器传输级（RTL）硬件设计实现。,"Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites. Derived from production IP blocks, it represents realistic complexity.",硬件设计自动化能力，特别是Verilog代码生成的功能正确性。,"provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",使用CocoTB测试套件评估功能正确性（pass rate）。,"Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",模块级设计，包含接口规范、功能需求和测试套件。,"Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",硬件设计，具体包括算术运算、控制逻辑、内存系统和其他设计。,"336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",代表实际生产IP块的复杂性，具有挑战性（最先进模型单次通过率仅26.5%）。,"Derived from production IP blocks, it represents realistic complexity. State-of-the-art achieves only 26.5% pass rate (GPT-4o-mini, single-shot), highlighting substantial improvement opportunity.",Verilog（硬件描述语言）。,"The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA...",包含336个问题。,"provides rigorous evaluation with 336 problems across arithmetic operations, control logic, memory systems, and miscellaneous designs.",源自生产IP块，代表实际复杂性。,"Derived from production IP blocks, it represents realistic complexity.",,,由NVIDIA开发。,"The Comprehensive Verilog Design Problems (CVDP) benchmark [25], developed by NVIDIA...",,,,,代码生成（从规范生成Verilog RTL代码）。,transforms design intent from the CVDP dataset into verified register transfer level (RTL) implementations.,通过率（pass rate）。,"State-of-the-art achieves only 26.5% pass rate (GPT-4o-mini, single-shot)...",自然语言规范、模块接口、功能需求。,"Each includes natural language specification, module interface, functional requirements...",Verilog RTL代码。,transforms design intent from the CVDP dataset into verified register transfer level (RTL) implementations.,文本到代码（自然语言规范到Verilog代码）。,"Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",使用CocoTB测试套件进行验证。,"Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites.",专注于硬件设计（Verilog），问题源自实际生产IP块，包含完整的测试套件（CocoTB）用于功能验证。,"Derived from production IP blocks, it represents realistic complexity. Each includes natural language specification, module interface, functional requirements, and CocoTB-based test suites."
2512.05100_output/content.md,SAP software-documentation benchmark,Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics...,"No, 本文是使用该数据集进行评测","Our experimental results demonstrate significant improvements on the software documentation dataset (Buschbeck and Exel, 2020) across four translation directions...",,,结构化文档翻译，即将带有XML或HTML标记的软件文档从源语言翻译为目标语言，同时保持标记结构的完整性。,This work addresses the task of translating a structured document Ds in the source language into its counterpart Dt in the target language.,翻译质量与结构保真度,...making structural fidelity as important as content translation quality.,使用多种指标进行评估，包括XML-Match、XML-BLEU、Content-BLEU、StrucAUC、TreeSim和Node-chrF。,"...FORMATRL achieving average gains of 3.69 XML-Match, 2.16 XML-BLEU, 0.22 Content-BLEU, and 0.93 StrucAUC scores... We propose two rewards: TreeSim and Node-chrF.",文档级别,"Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures.",软件文档本地化,Translating structured documents such as software manuals is essential for product localization.,,,文中未明确提及完整数据集的语言覆盖范围，仅提及了实验中的翻译方向（如英语到日语）。,"A structured document translation example (English→Japanese)... Our experimental results demonstrate significant improvements on the software documentation dataset (Buschbeck and Exel, 2020) across four translation directions...",,,SAP软件文档,Experiments on the SAP software-documentation benchmark...,2020,"...software documentation dataset (Buschbeck and Exel, 2020)...",,,,,,,文档翻译,...translating a structured document Ds in the source language into its counterpart Dt in the target language.,"XML-Match, XML-BLEU, Content-BLEU, StrucAUC, TreeSim, Node-chrF","...FORMATRL achieving average gains of 3.69 XML-Match, 2.16 XML-BLEU, 0.22 Content-BLEU, and 0.93 StrucAUC scores... We propose two rewards: TreeSim and Node-chrF.",带有标记的结构化文档（XML/HTML）,"A structured document D can be viewed as an XML tree D = (VD, ED)...",带有标记的结构化文档（XML/HTML）,...generating the target document Dt given the source document Ds...,结构化文档到结构化文档,This work addresses the task of translating a structured document Ds in the source language into its counterpart Dt in the target language.,,,专注于软件文档的本地化，要求同时保证翻译质量和XML/HTML标记的结构保真度。,"Translating structured documents such as software manuals is essential for product localization. As shown in Figure 1, they carry markup that defines layout and interactive elements, making structural fidelity as important as content translation quality."
2512.04673_output/content.md,CoNaLa (Code/Natural Language Challenge),"To evaluate an LLMs ability to understand and generate natural language descriptions of code snippets, we use the CoNaLa (Code/Natural Language Challenge) [23] dataset.","No, 本文是使用该数据集进行评测","In this paper, we present a comprehensive comparative analysis of five general-purpose and five code-specific LLMs across six diverse benchmarks... We further extend our evaluation to the CoNaLa dataset [22], focusing on the task of code explanation...",,,给定一个输入代码片段，生成该代码片段意图/解释。,"Given an input code snippet, the task is to generate intents/explanation that the code is achieving.",语义代码理解、意图建模和文本生成能力,"This focuses on semantic code understanding, intent modeling and text generation.","使用标准评估指标，包括基于词元的指标（BLEU, METEOR, ROUGE）和基于语义的指标（使用CodeBERT计算余弦相似度）。","We use the following standard metrics that capture various aspects such as lexical overlap and semantic similarity using the following standard evaluation metrics for the task [1, 5, 10]. (i) Token-based: These metrics collectively quantify surface-level similarity... (ii) Semantics-based: We use this measure to assess the semantic similarity between the model generated explanation (𝑚) and the ground truth explanation (𝑔)... We then take a cosine similarity between the embeddings...",,,,,,,Python,"The dataset consists of 1, 666 Python code and paired natural language annotations.","包含1,666个Python代码片段及其配对的自然语言注释。","The dataset consists of 1, 666 Python code and paired natural language annotations.",,,,,,,,,,,代码解释,"Given an input code snippet, the task is to generate intents/explanation that the code is achieving.","BLEU, METEOR, ROUGE-1, ROUGE-2, ROUGE-L, CodeBERTScore（基于CodeBERT的余弦相似度）",BLEU [19] evaluates n-gram precision... METEOR [3] extends this... ROUGE [15] metrics adopt a recall-oriented perspective: ROUGE-1... ROUGE-2... ROUGE-L... We refer to this metric as CodeBERT in the rest of the paper.,代码,Given an input code snippet...,自然语言,...the task is to generate intents/explanation...,代码到文本,"Given an input code snippet, the task is to generate intents/explanation that the code is achieving.",,,专注于代码到自然语言的解释任务，评估模型对代码语义的理解和意图建模能力。,"This focuses on semantic code understanding, intent modeling and text generation."
2512.04538_output/content.md,"CrossCodeEval, RepoEval",Extensive experiments on CrossCodeEval and RepoEval benchmarks demonstrate that CoCo consistently surpasses state-of-the-art baselines...,"No, 本文是使用该数据集进行评测",Extensive experiments on CrossCodeEval and RepoEval benchmarks demonstrate that CoCo consistently surpasses state-of-the-art baselines...,,,仓库级代码补全。从函数级别到仓库级别，利用大规模代码库中的上下文信息生成代码。,"As code completion task from function-level to repository-level, leveraging contextual information from large-scale codebases becomes a core challenge.",代码生成质量、上下文理解能力、跨文件依赖建模能力,"This limitation becomes particularly challenging in repository-level code completion, where accurate code generation requires a holistic understanding of repository-wide dependencies, shared utilities, and inter-module interactions.",Exact Match (EM),achieving up to 20.2% gains in EM.,多文件项目、仓库级上下文,"they fall short in real-world settings where code is organized in complex repositories with abundant cross-file dependencies, customized APIs, and project-specific conventions.",通用软件开发,Automatic code completion plays a fundamental role in modern software development,工程级、真实世界场景,"they fall short in real-world settings where code is organized in complex repositories with abundant cross-file dependencies, customized APIs, and project-specific conventions.",,,,,,,,,,,,,,,代码补全,Automatic code completion plays a fundamental role in modern software development,Exact Match (EM),achieving up to 20.2% gains in EM.,代码（包含其上下文）,"CoCo first applies static code analysis tools (such as AST parsers) to extract rich contextual information at the function, file, and repository levels.",代码,"Finally, CoCo synthesizes the multi-granularity contextual information and the retrieved code examples into a unified prompt, which is fed into the LLM for code completion.",代码到代码,"Finally, CoCo synthesizes the multi-granularity contextual information and the retrieved code examples into a unified prompt, which is fed into the LLM for code completion.",,,专注于仓库级代码补全，强调对跨文件依赖、项目特定约定和共享工具的理解。,"This enables the LLM to reason globally and locally about cross-file dependencies, shared utilities, and project-specific conventions, resulting in more accurate and contextually consistent repository-level code completion."
2512.04611_output/content.md,Magma,Experimental evaluation on the Magma benchmark demonstrates decisive superiority.,"No, 本文是使用该数据集进行评测",Experimental evaluation on the Magma benchmark demonstrates decisive superiority. ... Experiments on the Magma benchmark [33] show that our agentic approach significantly outperformed previous solutions.,,,"生成漏洞证明（Proof-of-Vulnerability, PoV）输入，以触发目标代码位置处的漏洞。该任务对于漏洞验证、复现、分类、补丁验证和回归测试至关重要。","Generating a proof-of-vulnerability (PoV) input that triggers a vulnerability at target code location(s) is a critical task in software security. Such inputs are essential for vulnerability verification, reproduction, triage, patch validation, and regression testing.",漏洞触发能力、生成效率,"PBFuzz triggered 57 vulnerabilities, outperforming all baselines. Critically, PBFuzz exclusively triggered 17 vulnerabilities compared to existing fuzzers. ... Median time-to-exposure: 339 seconds for PBFuzz versus 8680 seconds for AFL++ with CmpLog, representing a 25.6× efficiency gain",在给定时间预算内成功触发的漏洞数量（CVE数量）、中位暴露时间,"PBFuzz triggered 57 vulnerabilities... PBFuzz achieved this within a 30-minute budget per target, compared to 24-hour allocations for conventional approaches. Median time-to-exposure: 339 seconds for PBFuzz versus 8680 seconds for AFL++ with CmpLog",,,软件安全、漏洞发现,Proof-of-Vulnerability (PoV) input generation is a critical task in software security,包含简单和复杂的漏洞（如PHP和libtiff）,"while LLMs could generate valid PoV inputs for simple vulnerabilities (16 out of 129 CVEs), they struggled with more complex ones (e.g., PHP and libtiff).",文中未明确提及数据集包含的编程语言，但实验涉及C/C++（如libxml2）和PHP等。,"they struggled with more complex ones (e.g., PHP and libtiff). ... Consider the buffer overflow vulnerability in libxml2’s xmlSnprintfElementContent function",包含129个CVE（公共漏洞与暴露）,while LLMs could generate valid PoV inputs for simple vulnerabilities (16 out of 129 CVEs)... PBFuzz successfully triggered 59 out of the 129 CVEs,,,,,,,,,,,漏洞触发输入生成,Generating a proof-of-vulnerability (PoV) input that triggers a vulnerability at target code location(s),成功触发的CVE数量、中位暴露时间（秒）,PBFuzz triggered 57 vulnerabilities... Median time-to-exposure: 339 seconds for PBFuzz,目标漏洞代码位置（可能包含代码片段）,using code snippets extracted by static analysis,触发漏洞的测试输入（例如，特定格式的文件或数据流）,generate PoV inputs,代码/漏洞描述到测试输入,Generating a proof-of-vulnerability (PoV) input that triggers a vulnerability at target code location(s),被测试的程序（PUT）及其运行环境，通常包含检测漏洞的净化器（sanitizers）,sanitizers acting as implicit oracles. ... The generated code is executed in a sandboxed environment with no external libraries.,Magma是一个用于评估模糊测试（Fuzzing）和漏洞发现工具的基准，专注于真实世界的漏洞（CVE）。本文将其用作评估PBFuzz框架有效性的标准测试集。,Experimental evaluation on the Magma benchmark demonstrates decisive superiority. ... Experiments on the Magma benchmark [33] show that our agentic approach significantly outperformed previous solutions.
2512.04738_output/content.md,OverpassNL,The OverpassNL dataset [20] serves as a parallel supervision source for both directions of structured query modeling.,"No, 本文是使用该数据集进行评测",We evaluate OSMT on a public benchmark against strong baselines and observe consistent improvements in both query generation and interpretation.,,,在自然语言与结构化查询语言（OverpassQL）之间进行双向翻译。具体包括：1. Text-to-OverpassQL：将自然语言查询翻译成可执行的OverpassQL语句。2. OverpassQL-to-Text：将结构化的OverpassQL脚本翻译成连贯的自然语言解释。,"To facilitate seamless interaction between natural language and structured geospatial queries, we define two complementary tasks: the established Text-to-OverpassQL task and its inverse, OverpassQL-to-Text.",查询生成与解释的准确性、结构有效性、语义对齐、可解释性、参数效率,"Experimental results demonstrate that OSMT consistently surpasses strong baseline models in terms of accuracy, interpretability, and parameter efficiency, setting new standards for natural language interfaces within geospatial database systems.",使用多个指标进行综合评估，包括精确匹配（EM）、chrF、KVS、TreeS、OQS。,Comparison of OSMT with state-of-the-art open-source and closed-source models on the Text-to-OverpassQL task. Average performance is over five metrics.,依赖OpenStreetMap（OSM）数据库的标签（tag）模式和拓扑结构。查询生成过程需要理解OSM中空间实体（节点、路径、关系）的层次和关系依赖。,"In contrast, our approach explicitly captures the hierarchical and relational dependencies inherently encoded within OSM to enhance spatial reasoning capabilities.",地理空间数据库查询，特别是OpenStreetMap（OSM）数据检索。,"OSMT, an open-source tag-aware language model specifically designed to bridge natural language and Overpass Query Language (OverpassQL), a structured query language for accessing large-scale OpenStreetMap (OSM) data.",具有不同复杂度的地理查询意图，需要理解复杂的OSM标签模式和OverpassQL语法。,"It contains 8,352 natural language questions paired with real-world OverpassQL queries, spanning diverse geographic intents and varying levels of query complexity.",自然语言（输入/输出）与OverpassQL（一种用于查询OpenStreetMap的领域特定语言）。,"OSMT, an open-source tag-aware language model specifically designed to bridge natural language and Overpass Query Language (OverpassQL)...","包含8,352个自然语言问题与真实世界OverpassQL查询的配对。","It contains 8,352 natural language questions paired with real-world OverpassQL queries, spanning diverse geographic intents and varying levels of query complexity.",从三个互补来源构建：1. OSM Taginfo（标签三元组）。2. OSM Wiki（标签-描述对）。3. OverpassNL数据集（自然语言-OverpassQL配对）。,"To support cross-modal pre-training that bridges natural language, symbolic tag knowledge, and structured OverpassQL, we construct a pre-training corpus from three complementary sources.",,,"社区贡献（OSM Taginfo, OSM Wiki）与公开数据集（OverpassNL）的结合。","OSM Taginfo, a community-curated platform that aggregates global statistics on tag usage, frequency, and co-occurrence.",,,,,文本到代码（Text-to-OverpassQL）和代码到文本（OverpassQL-to-Text）的翻译任务。,"We define two complementary tasks: the established Text-to-OverpassQL task and its inverse, OverpassQL-to-Text.",精确匹配（EM）、chrF、KVS、TreeS、OQS。,Average performance is over five metrics.,自然语言文本或OverpassQL代码。,This task aims to translate a natural language query q into a syntactically correct and semantically meaningful OverpassQL statement Ovq.,OverpassQL代码或自然语言文本。,"This task performs the reverse mapping by taking a structured OverpassQL script Ovq, typically authored by technical users, and generating a coherent natural language explanation e.",文本到代码（Text-to-OverpassQL）和代码到文本（OverpassQL-to-Text）。,Illustration of the bidirectional translation between natural language and OverpassQL.,OpenStreetMap数据库环境，用于执行生成的OverpassQL查询以检索地理空间实体。,"Retrieving structured geospatial data from OSM typically relies on the Overpass Query Language (OverpassQL), a domain-specific language designed for fine-grained spatial data extraction.",专注于地理空间数据库（OpenStreetMap）的自然语言接口，涉及复杂的标签模式和拓扑结构。包含双向翻译任务（Text-to-OverpassQL 和 OverpassQL-to-Text）。,"OSMT, an open-source tag-aware language model specifically designed to bridge natural language and Overpass Query Language (OverpassQL), a structured query language for accessing large-scale OpenStreetMap (OSM) data."
2512.05962_output/content.md,Lean theorem-proving benchmark,"We evaluate 𝛼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.","No, 本文是使用该数据集进行评测","We evaluate 𝛼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.",,,在Lean定理证明助手中自动验证形式化数学证明。,"We evaluate 𝛼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.",定理证明的正确性与证明尝试的多样性,"In this setting, success requires not only correct candidates but also diversity across proof attempts, since harder theorems may only be solved by rare derivations.",使用pass@1和pass@256指标评估精度（precision）和覆盖率（coverage）。,"We find that 𝛼-DPG achieves state-of-the-art performance, producing models that lie on the Pareto frontier between precision (pass@1) and coverage (pass@256) and that surpass prior methods in coverage.",,,形式化数学定理证明,"We evaluate 𝛼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.",,,Lean（定理证明语言）,"We evaluate 𝛼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.",,,,,,,,,,,,,定理证明生成,"In this setting, success requires not only correct candidates but also diversity across proof attempts, since harder theorems may only be solved by rare derivations.","pass@1, pass@256","We find that 𝛼-DPG achieves state-of-the-art performance, producing models that lie on the Pareto frontier between precision (pass@1) and coverage (pass@256) and that surpass prior methods in coverage.",定理陈述（可能为自然语言或形式化语言）,Let 𝜋𝜃(·|𝑥) : Y →ℝbe an LLM with parameters 𝜃 defining a probability distribution over sequences 𝑦∈Y conditioned on a prompt 𝑥.,形式化证明（代码）,"In this setting, success requires not only correct candidates but also diversity across proof attempts, since harder theorems may only be solved by rare derivations.",文本到代码（定理陈述到形式化证明）,"We evaluate 𝛼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.",Lean证明助手验证环境,"We evaluate 𝛼-DPG in Lean, a proof assistant that automatically verifies formal mathematical proofs.",该基准使用定理证明助手（Lean）作为验证器，强调在保证正确性的同时，证明尝试的多样性对于解决更难定理至关重要。,"In this setting, success requires not only correct candidates but also diversity across proof attempts, since harder theorems may only be solved by rare derivations."
2512.05908_output/content.md,DNext,"Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system for the telecommunications sector.","No, 本文是使用该数据集进行评测","Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system for the telecommunications sector.",,,微服务架构中的多仓库Bug定位。具体任务是根据自然语言Bug报告，在多仓库代码库中定位到相关的代码文件。,"Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository.",Bug定位的准确性和效率，以及多仓库路由能力。,"Our evaluation aims to answer two primary questions: (1) How effectively does our NL-to-NL approach perform against state-of-the-art baselines on a complex, multi-repository benchmark? (2) How critical is the hierarchical search component to the method’s accuracy and efficiency?",使用Pass@k和Recall@k指标进行评估。对于Bug定位，设置k=10；对于搜索空间路由，设置k=3。,"We evaluate performance using standard metrics: Pass@k and Recall@k. Pass@k measures the proportion of bug reports where at least one correct file is found in the top-𝑘 results. Recall@k measures the fraction of all correct files for a given bug that are successfully retrieved within the top-𝑘 results. Given that the maximum number of modified files for any bug in our dataset is 10 (average 7.2), we set 𝑘= 10 as a fair and comprehensive threshold... For the preliminary search space routing phase, we use a tighter 𝑘= 3.",多文件、多目录、多仓库的微服务架构项目级上下文。,"Bug localization in multi-repository microservice architectures is challenging... This makes them ineffective for large-scale software projects composed of many interacting microservices, as they lack a mechanism to first route a bug report to the correct repository (codebase) before localization can begin.",电信领域的工业级微服务软件系统。,"Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system for the telecommunications sector.",工业级、真实世界、具有挑战性的Bug定位任务。,"Its scale (see Table 1) and use of real-world, often noisy, bug reports provide a challenging benchmark that standard academic datasets cannot replicate.",Java,"Table 1: Statistics of the DNext microservice dataset. Metric: Programming Language, Value: Java","包含46个仓库，7,077个代码文件，约110万行物理代码，87个Bug工单，平均每个工单涉及7.2个Bug文件。","Table 1: Statistics of the DNext microservice dataset. Metric: Number of Repositories, Value: 46; Total Number of Code Files, Value: 7,077; Total Physical Lines of Code, Value: ∼1.1M; Number of Bug Tickets, Value: 87; Average Buggy Files per Ticket, Value: 7.2",专有的工业级微服务系统，数据来源于真实的Bug工单和代码修改记录。,"Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system for the telecommunications sector. The ground truth for each bug ticket is the set of files modified in the pull request that resolved the issue.",,,专有工业系统，非公开社区构建。,"Our evaluation is performed on DNext [7], a proprietary, industrial-scale microservice system...",,,,,代码检索/定位（从自然语言Bug报告定位到代码文件）,We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval.,"Pass@k, Recall@k, MRR (Mean Reciprocal Rank)","We evaluate performance using standard metrics: Pass@k and Recall@k... For the preliminary search space routing phase, we use a tighter 𝑘= 3. ...Our hierarchical approach achieves the highest Pass@10 and MRR...",自然语言（Bug报告）,Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code...,代码文件列表（排名）,The LLM performs a detailed analysis of these filtered file summaries to produce the final ranked list of files most likely to be the source of the bug.,文本到代码（检索）,...reframing the problem. This shift from a cross-modal retrieval task to a unified NL-to-NL reasoning task...,,,专为多仓库微服务架构Bug定位设计的工业级基准，包含真实的、有噪声的Bug报告，规模远超标准学术数据集。,"Its scale (see Table 1) and use of real-world, often noisy, bug reports provide a challenging benchmark that standard academic datasets cannot replicate."
2512.08810_output/content.md,CALIBRI,"To foster future research on this important issue, we make our Dataset CALIBRI available as a contribution1.",Yes,"To foster future research on this important issue, we make our Dataset CALIBRI available as a contribution1.",https://huggingface.co/datasets/lavis-nlp/CALIBRI,1https://huggingface.co/datasets/lavis-nlp/CALIBRI,用于研究代码大语言模型（Code LLM）的校准和不确定性估计。数据集包含模型生成的代码、其置信度（token似然）以及正确性标签，旨在支持对模型置信度与代码实际正确性之间关系的研究。,CALIBRI includes correctness labels and token likelihoods for latest-generation LLMs on a fresh dataset with relatively low risk of data leakage.,模型置信度校准、不确定性估计,"As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs — ensuring their confidence scores faithfully represent the true likelihood of code correctness.",使用校准度量，如预期校准误差（ECE）、Brier分数和Brier技能分数（BSS），来衡量模型置信度与代码实际正确性（通过单元测试评估）的匹配程度。,"Therefore, several measures are commonly used to quantify the calibration error of a model: The Expected Calibration Error (ECE)... Brier Score... Brier Skill Score (BSS)...",,,通用编程任务（函数合成）,We study four multicalibration approaches on three function synthesis benchmarks...,,,未明确指定CALIBRI数据集本身的语言，但论文实验使用了MultiPL-E（20种语言）和McEval（40种语言）等多语言基准。,"We conduct an evaluation using latest open-weight reasoning models... on three code generation datasets: the multilingual benchmarks MultiPL-E [5] and McEval [6], which offer 20 and 40 programming languages, respectively...",,,"基于三个现有的代码生成基准（MultiPL-E, McEval, LiveCodeBench）构建，包含在这些基准上最新代码LLM的生成结果、置信度和正确性标签。","We conduct an evaluation using latest open-weight reasoning models... on three code generation datasets: the multilingual benchmarks MultiPL-E [5] and McEval [6], which offer 20 and 40 programming languages, respectively, and Live-CodeBench [25]... CALIBRI includes correctness labels and token likelihoods for latest-generation LLMs on a fresh dataset...",2025,arXiv:2512.08810v1  [cs.SE]  9 Dec 2025,官方自建（由本文作者构建并发布）,"To foster future research on this important issue, we make our Dataset CALIBRI available as a contribution1.",相对较低的数据泄露风险,CALIBRI includes correctness labels and token likelihoods for latest-generation LLMs on a fresh dataset with relatively low risk of data leakage.,,,代码生成（函数合成）,We study four multicalibration approaches on three function synthesis benchmarks...,预期校准误差（ECE）、Brier分数、Brier技能分数（BSS）,"Therefore, several measures are commonly used to quantify the calibration error of a model: The Expected Calibration Error (ECE)... Brier Score... Brier Skill Score (BSS)...",自然语言任务描述和/或代码上下文,We assume the input prompt 𝑋 to consist of a task description and/or code context...,代码,...and the output 𝑍 = 𝑧1:𝑛 to be a sequence of code tokens.,文本到代码,"We assume the input prompt 𝑋 to consist of a task description and/or code context, and the output 𝑍 = 𝑧1:𝑛 to be a sequence of code tokens.",,,专门为代码LLM的校准研究设计，包含模型置信度（token似然）和正确性标签。支持多校准（multicalibration）研究，可基于代码复杂性、长度、提示长度或编程语言对问题进行分组。,"We fill this gap by using multicalibration [22], which groups inputs according to semantic facets and applies a group-specific calibration... We apply multi-calibration to code generation by grouping coding problems by complexity and/or programming language."
2512.08867_output/content.md,SimpleDevQA,"we introduce SimpleDevQA, a multilingual Dev Knowledge QA benchmark derived from real user dialogues.",Yes,"In this paper, we propose SimpleDevQA, a Dev Knowledge QA benchmark built from real developer dialogues, to address the aforementioned problems.",,,开发知识问答，旨在为软件开发过程中提出的知识寻求问题提供准确、相关的自然语言答案。,The Development Knowledge Question Answering (Dev Knowledge QA) task aims to address knowledge-seeking questions posed during the software development process by providing accurate and relevant natural language answers.,评估大语言模型在真实编程场景中对软件开发知识的理解能力。,to assess LLMs’ understanding capability of software development knowledge in real programming scenarios.,通过一个独立的评判大语言模型，输入模型的预测结果和参考答案来验证正确性。,we evaluate SimpleDevQA by prompting a separate judge LLM with the model’s prediction and the reference answer to verify correctness.,独立的知识问答对，不依赖特定代码上下文。,"This dataset contains 2,740 QA pairs... focuses on questions with unique, short, and verifiable answers",软件开发知识，涵盖底层系统、数据库、网络协议、算法原理等，而不仅仅是代码理解。,"practical software development demands a much broader understanding, encompassing knowledge of underlying systems, databases, network protocols, algorithmic principles, etc.",通过过滤掉被认为过于简单的问题来增加难度。,"To increase difficulty, four powerful LLMs filter out QA pairs deemed too easy;",英语、中文和俄语。,"This dataset contains 2,740 QA pairs in three languages (English, Chinese, and Russian)",包含2740个开发知识问答对。,"we finally build the SimpleDevQA benchmark, which contains 2,740 Dev Knowledge QA pairs",源自真实用户与ChatGPT的对话（WildChat数据集），并结合网络检索的参考文档。,"we collect 3,000 real user conversations from WildChat as a seed dataset... retrieve relevant reference documents from the web",2025,Publication date: December 2025.,官方自建，通过一个三阶段的数据构建流程将真实对话转化为基准。,we design and implement a three-phase data construction pipeline to convert real-world SE-related conversations into a Dev Knowledge QA benchmark.,源自2025年的真实用户对话，旨在反映真实的开发者需求和查询模式，污染风险相对较低。,built from real developer dialogues... to reflect genuine developer task demands and query patterns.,,,知识问答,Development Knowledge Question Answering (Dev Knowledge QA) task,通过评判LLM验证答案的正确性（准确率）。,prompting a separate judge LLM with the model’s prediction and the reference answer to verify correctness.,自然语言问题,question inquiring about development knowledge,自然语言答案,providing accurate and relevant natural language answers,文本到文本（自然语言问题到自然语言答案）,knowledge-seeking questions... providing accurate and relevant natural language answers,,,1. 源自真实用户对话，反映真实开发者需求。2. 专注于具有单一、明确、可验证答案的问题，便于评估。3. 涵盖广泛的软件开发知识，而不仅仅是代码理解。4. 每个问答对都附有网络检索的参考文档，可用于验证事实准确性。,"The benchmark focuses on questions with single, correct answers, making evaluation more accurate and simple... Additionally, each QA pair is also accompanied by multiple web-retrieved references, which can be used to verify the factual accuracy of the answer."
2512.09543_output/content.md,SWE-bench Verified Mini,"For our evaluation dataset, we use SWE-bench Verified Mini, a curated 50-task subset of SWE-bench Verified.","No, 本文是使用该数据集进行评测","We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark.",,,解决现实世界中的软件工程问题，具体是GitHub仓库中的真实issue。,"Systems such as SWE-Agent, OpenHands, and AutoCodeRover have demonstrated strong performance on benchmarks like SWE-bench, which evaluates real-world GitHub issues as problem statements.",软件工程问题解决的有效性、效率和资源利用率,"We then cataloged all measurable outputs from a run, including status, time, energy, tokens, memory, and cost, ensuring each candidate metric was compatible with the SWE-bench benchmark and common evaluation practices.",使用官方SWE-bench评估脚本，判断智能体生成的补丁是否能正确解决问题并通过所有相关测试。,A run passes only if the agent-generated patch correctly resolves the issue and all associated tests pass.,多文件项目上下文，涉及完整的代码仓库。,"It employs ReAct-style prompting [24], where each iteration consists of thought, action, and observation until the issue is resolved or a stopping condition is reached. (结合SWE-bench评估真实GitHub仓库问题的背景推断)",软件工程，具体为代码修复和问题解决。,"Autonomous software engineering agents, powered by Large Language Models (LLMs), have emerged as a transformative paradigm in software development [9, 21], demonstrating impressive capabilities in resolving real-world code issues on benchmarks like SWE-bench.",真实世界难度，覆盖不同难度级别的问题。,"This benchmark preserves the distribution of difficulty, repository diversity, and test pass rates observed in the full 500-task dataset while reducing storage requirements from approximately 130 GB to 5 GB [10].",文中未明确提及，但SWE-bench通常包含多种编程语言。,,包含50个任务的精选子集，是完整500任务数据集的子集。,"For our evaluation dataset, we use SWE-bench Verified Mini, a curated 50-task subset of SWE-bench Verified.",来自真实GitHub仓库的问题。,"Systems such as SWE-Agent, OpenHands, and AutoCodeRover have demonstrated strong performance on benchmarks like SWE-bench, which evaluates real-world GitHub issues as problem statements.",,,,,,,,,代码修复与问题解决,demonstrating impressive capabilities in resolving real-world code issues on benchmarks like SWE-bench.,解决状态（通过/失败）、失败模式、能量使用、挂钟持续时间、令牌使用、LLM调用计数、内存使用、LLM成本,"We then cataloged all measurable outputs from a run, including status, time, energy, tokens, memory, and cost, ensuring each candidate metric was compatible with the SWE-bench benchmark and common evaluation practices.",自然语言（GitHub issue描述）与代码仓库上下文,which evaluates real-world GitHub issues as problem statements.,代码补丁,A run passes only if the agent-generated patch correctly resolves the issue and all associated tests pass.,文本（问题描述）到代码（补丁）,which evaluates real-world GitHub issues as problem statements. ... the agent-generated patch,沙盒化Docker执行环境,It supports sandboxed Docker execution environments and accommodates diverse architectures from simple ReAct loops to complex planning approaches.,专注于评估自主软件工程智能体解决真实世界GitHub问题的能力，包含完整的仓库上下文和测试。,demonstrating impressive capabilities in resolving real-world code issues on benchmarks like SWE-bench.
2512.09108_output/content.md,AtCoder Heuristic Contest,"(1) the ALE Agent [16] on the AtCoder Heuristic Contest, tackling competitive programming problems;","No, 本文是使用该数据集进行评测",We evaluate Artemis on four representative agent systems: the ALE Agent for competitive programming on AtCoder Heuristic Contest...,,,解决竞争性编程问题,...tackling competitive programming problems;,代码生成正确性（接受率）,...achieving a 13.6% improvement in acceptance rate;,接受率,...achieving a 13.6% improvement in acceptance rate;,,,算法，竞争性编程,...tackling competitive programming problems;,,,,,,,,,,,,,,,,,代码生成,...tackling competitive programming problems;,接受率,...achieving a 13.6% improvement in acceptance rate;,,,代码,...tackling competitive programming problems;,文本到代码,...tackling competitive programming problems;,,,,
2512.10713_output/content.md,PACIFIC (Precise Automatically Checked Instruction Following In Code),"We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs...",Yes,"We present PACIFIC, a novel framework designed to automatically generate benchmarks... We introduce PACIFIC, a novel framework for automatically generating benchmarks...",,,评估大型语言模型（LLM）的顺序指令遵循和代码干运行（模拟代码执行）能力。模型需要根据给定的初始输入和一系列指令，模拟执行过程并产生最终输出。,...a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs... The model under evaluation is tasked with executing the instructions on the given input and producing the final output in the specified format.,指令遵循的精确性、代码干运行（推理）能力、处理多步任务的能力,...assess sequential instruction-following and code dry-running capabilities in LLMs... The framework emphasizes the ability of LLMs to simulate code execution (i.e. dry-running) without relying on external tools or agentic mechanisms.,通过将模型输出与参考代码执行生成的预期结果进行简单比较，实现自动化和确定性的评估。,"The expected results are computed by executing reference implementations of the described tasks, enabling automatic and deterministic evaluation via simple output comparison.",多步顺序指令。每个样本包含一个初始输入和一个指令序列，模型需要按顺序处理这些指令。,Each sample includes: (1) An initial input — either a number or a string. (2) A sequence of instructions — operations to be applied to the input.,通用编程逻辑与算法模拟。指令涉及数学运算（如找质数、完美平方）、字符串操作（如字符移位）和基本逻辑转换。,"Examples: (1) next_perfect_square: take the previous number and output the first perfect square that comes after it... (2) shift_back: subtract 1 from each character of the previous output; a becomes z... Instruction 1: Given the previous number, output the smallest number bigger than the previous answer that is a prime number.",难度可控，范围从易到极具挑战性。难度维度包括指令数量和预期输出长度。,...while allowing control over benchmark difficulty... The difficulty dimensions are: (1) LLM input: the number of instructions in each sample (prompt). (2) LLM expected output: the expected length of each sample’s output.,支持多种编程语言，包括Python、Java和C++。指令以自然语言或代码片段形式提供。,"Currently, instructions are implemented in multiple programming languages, including Python, Java, and C++.",规模可扩展，由生成参数（如每个编程语言的样本数）控制。框架支持大规模和多样化的基准生成。,"To produce meaningful results, the framework must support large-scale and diverse benchmark generation.",通过框架自动生成。使用一组预定义的指令作为构建块，通过随机选择和组合来创建独特的样本。,Our framework is designed to automatically generate benchmarks... Instructions serve as the fundamental building blocks of PACIFIC... instruction selection is random and fully reproducible via the specified random seed.,2025-12-11 (arXiv版本),arXiv:2512.10713v1  [cs.SE]  11 Dec 2025,官方自建（由IBM Research团队提出和构建的框架）,"IBM Research Haifa, Israel {Itay.Dreyfuss,Antonio.Abu.Nassar,Samuel.Ackerman,Axel.Bendavid}@ibm.com {Rami.Katan,ornar,marcel}@il.ibm.com",抗污染设计。框架可以轻松生成新颖的基准变体，以减轻训练数据污染的风险。,"Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations... Each benchmark generated by PACIFIC can be unique, mitigating the risk of training data contamination.",,,代码生成与推理。模型需要生成执行一系列指令后的最终输出（数字或字符串）。,The model under evaluation is tasked with executing the instructions on the given input and producing the final output in the specified format.,Prompt-Level Accuracy（提示级准确率）。通过比较每个指令的中间输出或最终输出来计算。,Correctness Check: The framework validates each instruction by comparing the output against the expected result... we compute two metrics: • Prompt-Level Acc,自然语言指令或代码片段，结合初始输入（数字或字符串）。,• Instruction Type: Either Code (instructions as code snippets) or NL (instructions expressed in natural language). Each sample includes: (1) An initial input — either a number or a string. (2) A sequence of instructions...,代码执行结果，即一个数字或字符串。,Each instruction is a function that accepts either a number or a string and returns a value of one of those types... producing the final output.,指令序列到执行结果。输入是初始值加一系列操作指令，输出是模拟执行这些指令后的最终值。,The model under evaluation is tasked with executing the instructions on the given input and producing the final output...,无需外部执行环境。评估基于输出比较，而非实际代码执行。但参考结果由执行参考代码生成。,"The expected results are computed by executing reference implementations of the described tasks, enabling automatic and deterministic evaluation via simple output comparison.",1. 是一个基准生成框架，而非单一静态数据集。2. 强调“干运行”（dry-running），即在不实际执行代码的情况下模拟代码行为。3. 提供明确的难度控制参数（指令数、输出长度）。4. 采用完全基于规则的确定性评估，避免使用LLM作为评判者。5. 受ARC基准方法启发，但应用于代码领域。,"PACIFIC, a novel framework designed to automatically generate benchmarks... evaluates both sequential instruction-following and code dry-running capabilities... The framework emphasizes the ability of LLMs to simulate code execution (i.e. dry-running) without relying on external tools... The framework must provide explicit mechanisms for controlling the difficulty of the generated benchmarks... The evaluation process must rely on deterministic and transparent metrics rather than LLM-based evaluators... This is inspired by the ARC benchmark approach [6], but differs in the implementation and domain."
2512.10398_output/content.md,SWE-Bench-Pro,"On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents.","No, 本文是使用该数据集进行评测","We report results on SWE-Bench-Verified (Jimenez et al., 2023) and SWE-Bench-Pro (Deng et al., 2025) using the public evaluation pipelines...",,,解决真实世界开源仓库中的问题（issue resolution）,"LLMs have demonstrated strong software engineering ability to tackle real-world issue resolution in open-source repositories (Jimenez et al., 2023; Yang et al., 2024; Xia et al., 2025; Zeng et al., 2025).",工业级软件工程能力，包括长上下文推理、长期记忆、复杂工具链协调,"Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time.",Resolve@1,"On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%...",大规模代码库，包含深度相互依赖的组件,"industrial-scale codebases, which are usually orders of magnitude larger than typical benchmark projects, contain deeply interdependent components, and evolve continuously (Deng et al., 2025).",软件工程，特别是开源项目维护,real-world issue resolution in open-source repositories,工业级，远超典型基准项目的规模和复杂性,"industrial-scale codebases, which are usually orders of magnitude larger than typical benchmark projects, contain deeply interdependent components, and evolve continuously (Deng et al., 2025).",,,,,开源仓库中的真实问题,real-world issue resolution in open-source repositories,2025,"Date: December 12, 2025",,,,,,,问题解决（Issue Resolution）,real-world issue resolution in open-source repositories,Resolve@1,"On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%...",自然语言（问题描述）和代码库上下文,real-world issue resolution in open-source repositories,代码修改（如补丁）,tackle real-world issue resolution,文本（问题）和代码（上下文）到代码（解决方案）,tackle real-world issue resolution in open-source repositories,真实或模拟的软件仓库环境，包含测试套件,using the public evaluation pipelines,专注于工业级规模、持续演化的代码库中的软件工程任务，强调长上下文推理和跨会话记忆。,"Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time."
2512.10485_output/content.md,VentiVul,"We constructed a new, small-scale evaluation dataset, which we name VentiVul.",Yes,"We manually construct a small but high-quality out-of-distribution vulnerability dataset and design a deployment-oriented evaluation framework introducing two novel modes, Whole-File and Function-Pair.",https://github.com/Chaomeng-Lu/A-Practical-Evaluation-of-Deep-Learning-Models-and-LLMs-for-Vulnerability-Detection.git,"All of our datasets, code, and results are available at the following link1. 1https://github.com/Chaomeng-Lu/A-Practical-Evaluation-of-Deep-Learning-Models-and-LLMs-for-Vulnerability-Detection.git",漏洞检测。旨在评估模型在真实、新颖的漏洞场景下的检测能力，特别是针对时间上分布外（out-of-distribution）的漏洞。,"To assess realistic applicability, we deploy these models ... on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. ... We constructed a new, small-scale evaluation dataset, which we name VentiVul.",漏洞检测的泛化能力、鲁棒性、以及在实际部署场景下的有效性。,"Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. ... These results expose a persistent gap between academic benchmarks and real-world deployment, emphasizing the value of our deployment-oriented evaluation framework...",部署导向的评估框架，包含两种新颖模式：Whole-File（整个文件）和 Function-Pair（函数对）。,"We designed a deployment-oriented evaluation framework for assessing vulnerability detection models... finally, we examine deployment behavior using our newly proposed Whole-File and Function-Pair evaluation methods...",函数级和文件级。数据集包含函数对（漏洞版本与修复版本），以及来自同一文件但与修复无关的非漏洞函数，以提供额外上下文。,"For each CVE, we extracted the vulnerable and patched versions of affected functions directly from the corresponding Git commits. These were carefully aligned to form function pairs... To provide additional context and challenge for model evaluation, we also included non-vulnerable functions from the same files that were unrelated to the fix.",软件安全，具体为C语言（Linux内核）中的软件漏洞检测。,"Specifically, we retrieved all Linux-related CVE reports disclosed in May 2025 from the official CVE database. ... These pairs span 21 distinct .c source files...",高难度，真实世界、新颖、分布外的漏洞，旨在挑战现有模型。,"When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably. ... providing a realistic and challenging testbed for vulnerability detection models.",C语言,"Specifically, we retrieved all Linux-related CVE reports disclosed in May 2025 from the official CVE database. ... These pairs span 21 distinct .c source files...",小规模，包含来自20个Linux CVE的25个函数对（每个对包含漏洞版本和修复版本），以及来自相同文件的835个无关的非漏洞函数。,"In total, we collected 25 function pairs (each consisting of a vulnerable and a patched version) from the 20 selected Linux CVEs. ... and include 835 unrelated functions from the same files...",手动从真实世界数据构建。数据来源于2025年5月披露的Linux内核CVE报告及其对应的Git修复提交。,"To ensure high relevance and minimize external interference, the collection process was conducted fully manually and involved careful human inspection at each step. Specifically, we retrieved all Linux-related CVE reports disclosed in May 2025 from the official CVE database. ... For each CVE, we extracted the vulnerable and patched versions of affected functions directly from the corresponding Git commits.",2025年12月（论文版本日期），数据集基于2025年5月披露的CVE构建。,arXiv:2512.10485v1  [cs.CR]  11 Dec 2025 ... comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel.,官方自建（本文作者手动构建）,We manually construct a small but high-quality out-of-distribution vulnerability dataset... The collection process was conducted fully manually and involved careful human inspection at each step.,抗污染设计，专门设计为时间上的分布外（out-of-distribution）数据集，以最小化训练数据污染风险。,"When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset... To further evaluate the performance of state-of-the-art models... on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities...",,,漏洞检测（二元分类：漏洞/非漏洞）,"These were carefully aligned to form function pairs, labeled as vulnerable (1, before-fix) and non-vulnerable (0, after-fix).",文中未明确提及用于VentiVul的具体评估指标（如准确率、F1分数等），但提到了性能下降。,"When evaluated on VentiVul... performance drops sharply, with most models failing to detect vulnerabilities reliably.",源代码（C语言函数或文件）,"For each CVE, we extracted the vulnerable and patched versions of affected functions directly from the corresponding Git commits.",二元分类标签（漏洞/非漏洞）,"labeled as vulnerable (1, before-fix) and non-vulnerable (0, after-fix).",代码到标签（二元分类）,"labeled as vulnerable (1, before-fix) and non-vulnerable (0, after-fix).",,,1. 时间上的分布外（OOD）评估：专注于近期（2025年5月）修复的漏洞，与现有训练数据时间分离。2. 高真实性与高质量：完全手动从Linux内核CVE和Git提交中构建，经过人工检查。3. 部署导向的评估模式：引入了Whole-File和Function-Pair两种新颖的评估模式，模拟真实部署场景。4. 包含无关上下文：除了漏洞/修复函数对外，还包含了来自同一文件但与修复无关的非漏洞函数，增加评估挑战性。,"We constructed a new, small-scale evaluation dataset, which we name VentiVul... comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. ... The collection process was conducted fully manually and involved careful human inspection at each step. ... We designed a deployment-oriented evaluation framework... using our newly proposed Whole-File and Function-Pair evaluation methods... To provide additional context and challenge for model evaluation, we also included non-vulnerable functions from the same files that were unrelated to the fix."
2512.10393_output/content.md,BinSeek benchmark (未明确命名，但文中称其为'first domain benchmark for binary code retrieval'),"To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research.",Yes,"In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. ... To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research.",https://github.com/XingTuLab/BinSeek,We make our model and evaluation benchmark publicly available at: https://github.com/XingTuLab/BinSeek.,基于自然语言查询，从大规模二进制代码库中检索相关的二进制函数。具体任务是跨模态检索，将自然语言查询与去符号化的二进制反编译伪代码进行匹配。,"Retrieving stripped binary code based on natural language (NL) queries, however, presents unique and formidable challenges compared to source code retrieval. ... we also deliver the first domain benchmark for the binary code retrieval task",跨模态检索能力、二进制代码语义理解、检索准确性,"Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3",使用Rec@3和MRR@3指标评估检索性能。,"Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3",单函数级别（BinSeek-Embedding）和带调用上下文的函数（BinSeek-Reranker）,"BinSeek-Embedding is used to retrieve candidates from a codebase, and BinSeek-Reranker further refine candidates with calling context information.",软件安全、二进制代码分析、逆向工程,"Binary code analysis serves as a cornerstone of software security, supporting critical tasks such as vulnerability detection, malware analysis, program audit, and so on.",高难度，涉及去符号化的二进制代码，缺乏高级语义信息。,"Retrieving stripped binary code based on natural language (NL) queries, however, presents unique and formidable challenges compared to source code retrieval. ... stripped binaries lack explicit semantic indicators such as variable names, function names, and type information.",C/C++（源语言），反编译后的伪代码（目标模态）,"Binary code is a low-level representation of a program, which is generally compiled from the source code written in PLs like C/C++","总共构建了106,711,414个原始数据对。","Ultimately, we constructed 106,711,414 raw data pairs in total.",通过自动化数据合成管道生成。从GitHub收集C/C++开源项目，编译为去符号化二进制文件，反编译为伪代码，并使用LLM为源代码函数生成语义描述。,"As shown in Figure 3, we first collect a large number of open-source projects written in C/C++ from GitHub. For each project, we compile the source code into stripped binary files using different configurations... Finally, we employ an advanced LLM (i.e., DeepSeek (DeepSeek-AI et al., 2025)) to generate semantic descriptions for each source code function.",2025年12月11日（论文版本日期）,arXiv:2512.10393v1  [cs.SE]  11 Dec 2025,官方自建，通过LLM驱动的自动化数据合成管道构建。,"We propose an LLM-driven data synthesis pipeline to automate the construction of high-quality training data, deriving the first domain benchmark for binary code retrieval.",未明确提及。,,未明确提及。,,代码检索（跨模态检索）,Retrieving stripped binary code based on natural language (NL) queries,"Rec@3, MRR@3","Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3",自然语言查询,Retrieving stripped binary code based on natural language (NL) queries,二进制反编译伪代码（函数）,Training cross-modal retrieval models requires a large amount of data pairs that consist of pseudocode functions and corresponding semantic descriptions in NL.,自然语言到代码（二进制伪代码）,Retrieving stripped binary code based on natural language (NL) queries,未明确提及，任务为检索而非执行。,,这是第一个专门为去符号化二进制代码检索任务构建的领域基准。它针对二进制代码缺乏符号信息（如函数名、变量名）的独特挑战而设计，填补了源代码检索与二进制代码检索之间的空白。数据通过自动化LLM驱动的管道合成，连接了源代码、二进制代码和自然语言描述。,"we also deliver the first domain benchmark for the binary code retrieval task, which is expected to facilitate future research in this domain. ... Unlike high-level programming languages (PL), stripped binaries lack explicit semantic indicators such as variable names, function names, and type information. ... We propose an LLM-driven data synthesis pipeline to automate the construction of high-quality training data"
2512.11482_output/content.md,data extraction attack benchmark 和 functional correctness benchmark,"• Two benchmarks for privacy and utility evaluation: We release two new datasets, i.e., data extraction attack benchmark and functional correctness benchmark, that can be used in future studies to evaluate the privacy and utility of CodeLLMs.",Yes,"• Two benchmarks for privacy and utility evaluation: We release two new datasets, i.e., data extraction attack benchmark and functional correctness benchmark, that can be used in future studies to evaluate the privacy and utility of CodeLLMs.",Replication Package (RP) [9],These datasets are tailored to the fine-tuning dataset for utility evaluation and available in the Replication Package (RP) [9].,评估代码大语言模型的隐私风险和功能正确性。具体包括：1. 数据提取攻击基准：用于评估模型对训练数据（包括敏感信息）的记忆和泄露风险。2. 功能正确性基准：用于评估模型在应用差分隐私等隐私保护技术后，生成代码的功能正确性是否得以保持。,"• Two benchmarks for privacy and utility evaluation: We release two new datasets, i.e., data extraction attack benchmark and functional correctness benchmark, that can be used in future studies to evaluate the privacy and utility of CodeLLMs.",隐私保护（记忆风险）与模型效用（功能正确性）的权衡评估,"• Two benchmarks for privacy and utility evaluation: We release two new datasets, i.e., data extraction attack benchmark and functional correctness benchmark, that can be used in future studies to evaluate the privacy and utility of CodeLLMs.",文中未明确描述针对这两个新基准的具体评估方法。,,,,通用软件工程任务，涉及代码生成、文档、测试用例等。,"Large language models specialized for code (Large language models specialized for code (CodeLLMs)) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases.",,,文中未明确指定基准使用的编程语言，但研究背景是通用代码大语言模型。,,,,为评估目的而构建的定制数据集，与微调数据集相关联。,These datasets are tailored to the fine-tuning dataset for utility evaluation...,2025,"Melih Catal, Pooja Rani, and Harald Gall. 2025. Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models. 1, 1 (December 2025), 21 pages.",官方自建（本文作者发布）,We release two new datasets...,为评估隐私保护技术（如差分隐私）而专门构建，旨在测量记忆风险，因此可能包含设计用于触发记忆的样本。,"• Two benchmarks for privacy and utility evaluation: We release two new datasets, i.e., data extraction attack benchmark and functional correctness benchmark, that can be used in future studies to evaluate the privacy and utility of CodeLLMs.",,,代码生成（从自然语言提示生成代码片段、完整函数或程序）,"LLMs specialized for code (Large language models specialized for code (CodeLLMs)), trained on large code datasets, can generate code snippets, complete functions, or even write entire programs based on natural language prompts",文中提及使用CodeBLEU分数和功能正确性（functional correctness）作为评估指标。,CodeBLEU Score: 1.00 ... Effectiveness of DP on Mitigating Memorization and Preserving Model Performance: ... as measured by functional correctness on standard benchmarks.,自然语言提示（用于代码生成任务）；可能也包括代码片段（用于数据提取攻击）。,"...can generate code snippets, complete functions, or even write entire programs based on natural language prompts",代码,"...can generate code snippets, complete functions, or even write entire programs...",文本到代码（代码生成任务）；代码到代码/文本（数据提取攻击评估）。,"...can generate code snippets, complete functions, or even write entire programs based on natural language prompts",,,这是首个专门为评估代码大语言模型的隐私（记忆风险）与效用（功能正确性）权衡而设计的基准套件。它包含两个互补的数据集：一个用于量化数据提取攻击下的记忆泄露，另一个用于确保隐私保护技术（如差分隐私）不会损害代码生成质量。,"• Two benchmarks for privacy and utility evaluation: We release two new datasets, i.e., data extraction attack benchmark and functional correctness benchmark, that can be used in future studies to evaluate the privacy and utility of CodeLLMs. These datasets are tailored to the fine-tuning dataset for utility evaluation..."
2512.11589_output/content.md,AIDev,"To conduct our study, we use the AIDev dataset [13], a large-scale corpus of almost a million pull requests (PRs) from over 116,000 real GitHub repositories, authored by five distinct LLM-based coding agents (Claude Code, Cursor, Devin, Copilot, OpenAI Codex).","No, 本文是使用该数据集进行评测","To conduct our study, we use the AIDev dataset [13]...",https://github.com/itsluketwist/agent-library-usage/,We release our dataset processing and analysis code to support reproducibility and future research in this area: https://github.com/itsluketwist/agent-library-usage/,该数据集旨在提供一个大规模的真实世界AI编码代理（Agent）生成的代码变更（Pull Requests）集合，用于研究AI代理在软件开发中的行为，特别是其使用外部库的模式。,"To conduct our study, we use the AIDev dataset [13], a large-scale corpus of almost a million pull requests (PRs) from over 116,000 real GitHub repositories, authored by five distinct LLM-based coding agents...",AI编码代理的库使用行为，包括库导入频率、新依赖引入、版本控制实践和库选择多样性。,"Specifically, we analyse file diffs (both code and dependency manifests) from 26,760 PRs across four languages (TypeScript, Python, Go, and C#), to answer three research questions (RQs): RQ1 Library Usage... RQ2 New Dependencies... RQ3 Choosing Libraries...",通过分析Pull Request的文件差异（diffs），使用特定于语言的解析器提取导入语句和依赖清单，并进行统计分析（如频率统计、版本约束分析）。,We then implement manifest-specific regex parsers to automatically identify new dependencies—including any explicit version specifiers... We then perform manual analysis on the top ten libraries for each language and RQ to determine the areas that agents are most likely to use external libraries.,多文件项目级别，基于真实的GitHub仓库和Pull Requests，包含完整的代码变更上下文。,"Specifically, we use the AIDev-Pop subset that is curated to repositories with over 100 GitHub stars and includes the full commit details; AIDev-Pop contains 33,596 PRs from 2,807 repositories.",通用软件开发，涵盖多种编程语言和项目类型。,"We restrict our study to the four most common languages in the dataset... The chosen languages are TypeScript/JavaScript... Python, Go and C#.",工程级，涉及真实世界软件项目的代码修改和依赖管理。,"These agentic systems—now widely available in production tools [19]—can complete end-to-end workflows that previously required a human developer. Crucially, these workflows include raising pull requests (PRs), allowing agents to propose substantial code changes...","TypeScript/JavaScript, Python, Go, C#","We restrict our study to the four most common languages in the dataset... The chosen languages are TypeScript/JavaScript (combined due to nearly identical syntax and library ecosystems, and will be referred to as TypeScript throughout the paper), Python, Go and C#.","包含来自2,807个仓库的33,596个Pull Requests（AIDev-Pop子集），本研究分析了其中26,760个PRs。","AIDev-Pop contains 33,596 PRs from 2,807 repositories... We analyse a total of 26,760 agent-authored pull requests across 1,832 repositories and the four most common languages in the AIDev dataset.","来自真实GitHub仓库的公开Pull Requests，由五种不同的LLM编码代理（Claude Code, Cursor, Devin, Copilot, OpenAI Codex）生成。","To conduct our study, we use the AIDev dataset [13], a large-scale corpus of almost a million pull requests (PRs) from over 116,000 real GitHub repositories, authored by five distinct LLM-based coding agents (Claude Code, Cursor, Devin, Copilot, OpenAI Codex).",2025年11月,"Specifically, we use the November 2025 snapshot (commit eee0408 [8]).",社区贡献（由研究人员从GitHub收集构建）,"To conduct our study, we use the AIDev dataset [13], a large-scale corpus of almost a million pull requests (PRs) from over 116,000 real GitHub repositories...",,,数据源自各仓库的开源许可证（通常为MIT、Apache-2.0或GPL变体）。,"AIDev contains no personal or sensitive information, and all data is publicly accessible under the open-source license of its respective repository (typically MIT, Apache-2.0, or GPL variants).",代码生成与修改（在Pull Request级别）,"These agentic systems... can complete end-to-end workflows that previously required a human developer. Crucially, these workflows include raising pull requests (PRs), allowing agents to propose substantial code changes...",库导入频率、新依赖引入频率、版本约束指定比例、库选择多样性（独特库数量）。,"RQ1 Library Usage How often do agents import libraries when generating code... RQ2 New Dependencies How often do agents introduce new dependencies into a repository, and do they specify versions? RQ3 Choosing Libraries Which specific libraries do agents choose...",代码（Pull Request的代码变更差异）,"Specifically, we analyse file diffs (both code and dependency manifests) from 26,760 PRs...",分析结果（统计数据和发现）,Our findings are concise but instructive. (1) Agents often import libraries (29.5% of PRs) but rarely add entirely new dependencies (1.3% of PRs). (2) Agentic workflows encourage far better version hygiene than traditional LLM prompting...,代码到分析,"Specifically, we analyse file diffs (both code and dependency manifests) from 26,760 PRs...",,,专注于AI编码代理（而非普通LLM）在真实软件开发环境（Pull Requests）中的库使用行为研究。数据集包含多种编程语言和真实的项目上下文。,"To begin to fill this gap, we present the first focused empirical study of how AI coding agents use libraries in practice, by mining real agent-authored PRs from the AIDev dataset [13]."
2512.13607_output/content.md,LiveCodeBench,"Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro","No, 本文是使用该数据集进行评测","Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro",,,文中未描述LiveCodeBench的具体任务，仅提及作为评测基准。,"Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2512.13102_output/content.md,GSM8K,"Math: We evaluate on GSM8K (Cobbe et al., 2021).","No, 本文是使用该数据集进行评测","We study this on math (GSM8K, (Cobbe et al., 2021)) and coding (HumanEval/OPC, (Chen et al., 2021; Huang et al., 2025)) tasks using small-scale LLMs (7B–8B).",,,数学问题求解，特别是小学数学应用题,"Across math and coding benchmarks, where baseline student models begin with near-zero performance... (结合上下文，GSM8K是数学基准)",数学推理能力,"However, for reasoning-heavy domains, such as math and coding, learning requires more than recalling and using facts.",Pass@k,"After each teacher response, S is evaluated with Pass@k.",,,数学（小学数学）,"Math: We evaluate on GSM8K (Cobbe et al., 2021).",,,,,,,,,2021,"Math: We evaluate on GSM8K (Cobbe et al., 2021).",,,,,,,问题求解,"Given a complex question, S seeks to answer the question by interacting with T.",Pass@k,"After each teacher turn, we evaluate the student using the full chat history so far: we sample k direct answers from S and compute Pass@k.",自然语言（数学问题描述）,"Given a complex question, S seeks to answer the question by interacting with T.",自然语言（答案）,"Following each teacher turn, S is prompted to output only the final answer.",文本到文本（数学问题到答案）,"Given a complex question, S seeks to answer the question... S is prompted to output only the final answer.",,,本文未描述GSM8K数据集的独特之处，仅将其用作评测基准。本文的独特之处在于提出了一个学生主导的交互式学习框架，用于提升模型在数学和代码任务上的表现。,"In this paper, we investigate the complementary setting: Can students guide the conversation by asking helpful questions to a teacher? We study this on math (GSM8K, (Cobbe et al., 2021)) and coding (HumanEval/OPC, (Chen et al., 2021; Huang et al., 2025)) tasks..."
2512.14233_output/content.md,PentestEval,"To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages",Yes,"we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages",,,评估大型语言模型在分解的渗透测试六个阶段中的性能。这六个阶段是：信息收集、弱点收集、弱点过滤、攻击决策、漏洞利用生成和漏洞利用修订。,"PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision.",渗透测试各阶段的性能、端到端工作流完成能力,evaluate LLMs along two complementary dimensions: performance on individual stages and the ability to complete full penetration testing workflows.,将LLM输出与专家标注的真实攻击数据进行系统比较，实现精确测量。包含阶段级评估和端到端测试。,"By systematically comparing LLM outputs against the expert-annotated solutions, PentestEval enables precise measurement of model effectiveness at each stage.",多步骤、迭代的攻击链，依赖前序阶段的输出和系统反馈。,"attackers rarely succeed by exploiting a single vulnerability. Instead, they construct multi-step attack chains by combining multiple weaknesses in sequence",网络安全、渗透测试,Penetration testing is essential for assessing and strengthening system security against real-world threats,涵盖代表性弱点，包括OWASP Top 10、CWE Top 25以及未公开的零日漏洞，难度较高。,"collectively capturing representative weaknesses from OWASP Top 10 [17], CWE Top 25 [18], and even undisclosed zero-day vulnerabilities.",文中未明确提及编程语言，但任务涉及生成脚本或命令行调用，推测可能涉及多种脚本语言。,,包含346个独立任务，组织在12个易受攻击的场景中。,The benchmark comprises 346 distinct tasks organized within 12 vulnerable scenarios,由五位专家设计环境、注入漏洞（包括一个零日漏洞）并标注真实攻击数据。,"built with five experts who design environments, inject vulnerabilities including a zero-day, and annotate ground-truth attack data.",2025,arXiv:2512.14233v1  [cs.SE]  16 Dec 2025,官方自建，由研究团队与专业渗透测试人员协作构建。,"expert-annotated ground truth for each stage, collaboratively verified by professional penetration testers",,,,,渗透测试工作流分解为六个阶段：信息收集、弱点收集、弱点过滤、攻击决策、漏洞利用生成、漏洞利用修订。,"six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision.",成功率（例如，端到端管道达到31%的成功率，攻击决策和漏洞利用生成阶段成功率约为25%）,"End-to-end pipelines reach only 31% success rate... The most challenging components, i.e., Attack Decision-Making and Exploit Generation, reach only around a 25% success rate.",目标系统信息（如URL）、收集的弱点信息、前序攻击步骤的反馈等，主要为文本信息。,"given a target URL T... given structured profile I of a target system... given WF , wi−1, ri−1",结构化配置文件、弱点集合、攻击决策、可执行的攻击脚本或命令。,I represents a structured profile... WG represents the resulting set... outputs the next weakness wi... producing a concrete script or command-line invocation ei,文本到文本（生成决策、弱点列表）、文本到代码（生成攻击脚本）,producing a concrete script or command-line invocation ei,文中未明确描述，但涉及对目标系统的实际或模拟攻击，推测需要特定的测试环境或沙箱。,,首个针对渗透测试的模块化、阶段级评估基准；将工作流分解为六个阶段进行细粒度评估；包含专家标注的真实数据和自动化评估管道；覆盖零日漏洞。,the first modular benchmarking framework specifically designed for fine-grained and rigorous assessment of LLM performance in the stages of the penetration testing workflow... inject vulnerabilities including a zero-day
2512.15699_output/content.md,FrontierCS,"We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science...",Yes,"We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science...",Frontier-CS GitHub,Code and Data: Frontier-CS GitHub,评估大型语言模型解决开放式计算机科学问题的能力，这些问题没有已知的封闭形式或确定性最优解。模型需要实现可执行程序来解决问题，而非直接输出答案。,"In this paper, we introduce FrontierCS, a coding benchmark that evaluates LLMs on solving open-ended computer science problems, where no known closed-form or deterministic optimal solution exists in practice. Unlike math or reasoning benchmarks that require a direct answer (e.g., AIME), FrontierCS requires models to implement executable programs to solve the problem (e.g., LiveCodeBench).",开放式推理、算法设计与优化、研究问题解决能力,"FrontierCS focuses on two tracks: algorithmic optimization problems, and tasks more closely tied to real-world CS research. Both tracks naturally exhibit open-endedness, stress-testing a model’s ability to perform deep open-ended reasoning and discover nontrivial optimization strategies.",通过自动评估器运行生成的程序，根据任务特定指标（如打包密度、运行时间、内存使用）在资源限制下进行确定性验证和定量评分，而非简单的通过/失败。,"Each FrontierCS task can be solved by submitting executable code: the evaluator runs the program on generated instances and scores its outputs by task-specific metrics under resource limits (e.g., time and memory usage). ... Solutions with runtime limit can be automatically checked for validity and assigned a numeric score that reflects the quality of the solution, rather than a simple pass-or-fail.",单任务问题，模型根据问题描述和必要的I/O或API存根生成独立的求解程序。,The model is prompted with the problem specification (and any required I/O or API stubs) and must produce a self-contained solver program.,计算机科学，包含算法优化问题和研究问题。算法问题涵盖优化、构造、交互类别；研究问题涵盖操作系统、高性能计算、人工智能、数据库、编程语言、安全等领域。,"FrontierCS consists 156 problems across two tracks: Algorithmic Problems and Research Problems. The Algorithmic Problems track contains 107 problems adapted from programming contests, covering three categories: Optimization, Constructive, and Interactive problems. The Research Problems track contains 49 problems sourced from real-world computer science research questions, spanning six domains: Operating Systems, High-Performance Computing, Artificial Intelligence, Databases, Programming Languages, and Security.",前沿计算机科学难度，问题具有开放性，全局最优解未知或计算上不可行。,"Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty.",文中未明确指定编程语言，但要求模型生成可执行程序。,,包含156个问题，其中算法问题107个，研究问题49个。,FrontierCS consists 156 problems across two tracks: Algorithmic Problems and Research Problems. The Algorithmic Problems track contains 107 problems... The Research Problems track contains 49 problems...,算法问题改编自编程竞赛；研究问题来源于现实世界的计算机科学研究问题，由专家（包括CS博士和顶级竞赛参与者及出题者）设计和评审。,"a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. ... The Algorithmic Problems track contains 107 problems adapted from programming contests... The Research Problems track contains 49 problems sourced from real-world computer science research questions...",2025-12-17 (arXiv版本),arXiv:2512.15699v1  [cs.LG]  17 Dec 2025,官方自建，由多所大学和研究机构的专家团队精心策划，经过提案、实现和评审的多阶段流程。,"Each problem is carefully curated through a multi-stage process involving proposal, implementation, and review to ensure quality and relevance...",抗污染设计，通过参数化问题生成器产生大量可变难度的实例，支持生成新鲜的、未见过的测试用例以防止泄露和过拟合。,"Parametric Problem Generator: The task specification induces a large, variable-difficulty space of instances, enabling fresh, unseen test cases to prevent leak and overfitting.",,,代码生成（生成完整的求解程序）,Models solve these tasks by implementing executable programs rather than outputting a direct answer.,任务特定的定量评分（如打包密度），而非单一指标。,"assigned a numeric score that reflects the quality of the solution, rather than a simple pass-or-fail.",自然语言问题描述，以及必要的I/O或API存根。,The model is prompted with the problem specification (and any required I/O or API stubs)...,代码（可执行的求解程序）,must produce a self-contained solver program.,文本到代码,The model is prompted with the problem specification... and must produce a self-contained solver program.,在资源限制（如时间和内存）下运行，具体环境取决于任务。,"the evaluator runs the program on generated instances and scores its outputs by task-specific metrics under resource limits (e.g., time and memory usage).",专注于解决最优解未知的开放式计算机科学问题；结合了算法优化和现实世界研究问题；提供确定性验证和连续质量评分，而非通过/失败；通过参数化生成器防止数据污染；旨在衡量模型实现有效和高效算法的能力，而非穷举解决问题。,"Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. ... This design measures the ability of models to implement effective and efficient algorithms rather than algorithms that exhaustively solve the problem. ... problems are unsolved, admit many solution strategies, and are evaluated via deterministic scoring rather than pass-or-fail."
2512.15688_output/content.md,BashArena,"We introduce BashArena, a control setting for studying AI control techniques in security-critical environments.",Yes,"We introduce BashArena, a control setting for studying AI control techniques in security-critical environments.",,,用于研究AI控制技术的控制设定，包含主任务（系统管理/基础设施工程）和侧任务（模拟安全破坏），旨在评估和设计AI控制协议。,"BashArena contains 637 Linux system administration and infrastructure engineering tasks in complex, realistic environments, along with four sabotage objectives... for a red team to target.",AI控制、安全、任务完成能力、破坏行为检测与规避,"We evaluate multiple frontier LLMs on their ability to complete tasks, perform sabotage undetected, and detect sabotage attempts.",主任务：通过Pytest测试用例评估任务完成度（通过测试用例的比例）。侧任务：在评估过程中，每次行动后运行测试，若通过则认为侧任务完成。,"The agent’s score on the task is measured as the fraction of test cases that pass. Tasks contain 12 test cases on average... The test is run after every action during evaluation, and the side task is considered complete if it ever passes.",多行动、多容器Docker环境交互,"All task environments are variations of the base environment, which is a multi-container Docker compose project... The agent works in the default container... The base environment also features three servers running in separate containers...",Linux系统管理、基础设施工程、安全、系统监控、数据处理、低级系统编程、进程管理、数据库自动化、内存优化、CI/CD自动化,"BashArena contains 637 Linux system administration and infrastructure engineering tasks... Cluster Label: System Monitoring, Data Processing, Low-Level Systems Programming, Process Management, Database Automation, Security, Memory Optimization, CI/CD Automation, Other.",对当前前沿模型具有挑战性，尚未被饱和,"Main tasks are challenging for current frontier models. BashArena’s main tasks are difficult enough that they are not saturated by current frontier models, and weaker models perform significantly worse.",Bash、Python（环境中安装了Python包）,"The agent works in the default container, which contains a simple filesystem with basic shell utilities and Python packages installed.",包含637个主任务和4个侧任务,"BashArena contains 637 main tasks created by an automated pipeline, and four manually-constructed side tasks which simulate critical security failures.",主任务基于StackOverflow上标记为“linux”或“bash”的问题，通过自动化管道生成和迭代改进。侧任务为手动构建。,We generate main tasks based on StackOverflow questions tagged “linux” or “bash”... We generate each task using Claude Opus 4.1 based on a selected StackOverflow question... Each side task consists of a description and a test.,2025-12-17 (arXiv版本日期),arXiv:2512.15688v1  [cs.CR]  17 Dec 2025,官方自建（Redwood Research），使用自动化管道生成主任务，手动构建侧任务。,We release the dataset as a ControlArena setting and share our task generation pipeline... We generate main tasks based on StackOverflow questions... and four manually-constructed side tasks.,,,,,代码生成与系统操作（通过Bash和Python命令序列实现复杂任务）,"Models are run using a slightly modified version of Inspect AI’s ReAct scaffold, provided with bash, python, file_edit, and submit tools.",主任务：通过测试用例的比例。侧任务：轨迹级假阳性率（FPR）、破坏成功率。,"The agent’s score on the task is measured as the fraction of test cases that pass... Claude Sonnet 4.5 successfully executes sabotage while evading monitoring by GPT-4.1 mini 26% of the time, at 4% trajectory-wise FPR.",自然语言任务描述,Each main task includes the following components: Task description: a written assignment that describes what needs to be accomplished...,代码（Bash/Python脚本）、系统配置、文件操作,"The agent runs commands as root inside the default container... it implements a solution to the main task, generally by creating several files back to back.",文本到代码/系统操作,The agent is only shown the task description... The agent runs commands as root inside the default container.,多容器Docker环境，包含基本Shell工具、Python包、模拟敏感数据、防火墙、sudo管理工具、外部服务器（日志服务器、恶意软件服务器）。,"All task environments are variations of the base environment, which is a multi-container Docker compose project. The agent works in the default container, which contains a simple filesystem with basic shell utilities and Python packages installed... The base environment also features three servers running in separate containers...",专门为研究AI控制而设计，包含主任务与破坏性侧任务的对抗性设定；环境复杂多样，支持复杂的破坏策略；主任务对前沿模型具有挑战性；数据集规模大且多样；包含详细的自动化任务生成管道。,"BashArena, a control setting for studying AI control techniques in security-critical environments... Supports sophisticated sabotage strategies... Main tasks are challenging for current frontier models... Large and diverse... We release the dataset as a ControlArena setting and share our task generation pipeline."
2512.16816_output/content.md,CAFFE (Counterfactual Assessment Framework for Fairness Evaluation),"This paper introduces CAFFE (Counterfactual Assessment Framework for Fairness Evaluation), a framework that integrates counterfactual fairness principles with structured test case definitions inspired by the ISO/IEC/IEEE 29119 standard [3].",Yes,"This paper introduces CAFFE (Counterfactual Assessment Framework for Fairness Evaluation)... To sum up, this paper offers three main contributions: (1) a novel testing framework, CAFFE...",,,系统性地测试大型语言模型在反事实条件下的公平性属性，即检测模型在输入仅敏感属性（如性别、种族）不同但意图相同时，输出是否保持一致。,"Our work contributes to the latter line of research, with a focus on systematically testing the fairness properties of LLMs under counterfactual conditions... Counterfactual Fairness: A model is considered fair with respect to a sensitive attribute if, for every pair of identical inputs differing only in that attribute, the output does not change.",反事实公平性,Toward Systematic Counterfactual Fairness Evaluation of Large Language Models: The CAFFE Framework,使用语义相似度指标评估模型响应，并定义量化的公平性阈值。,"(3) evaluates model responses using semantic similarity metrics... It automatically constructs intent-aware and realistic counterfactual test cases, defines quantitative fairness thresholds, and evaluates responses through semantic similarity metrics.",依赖于提示意图和对话上下文。,"proposing a structured and intent-aware framework... we propose formalizing fairness test cases along key dimensions such as prompt intent, conversational context...",大型语言模型的公平性评估，属于软件工程中的人工智能软件测试领域。,Toward Systematic Counterfactual Fairness Evaluation of Large Language Models... Software Engineering for Artificial Intelligence.,,,自然语言（用于构建提示词），不特定于编程语言。,"Our framework automatically generates counterfactual test data through stereotype-aware prompt construction, enhancing both linguistic diversity and semantic consistency across test cases.",,,通过刻板印象感知的提示构建自动生成反事实测试数据。,Our framework automatically generates counterfactual test data through stereotype-aware prompt construction...,2025-12-18 (预印本版本日期),arXiv:2512.16816v1  [cs.SE]  18 Dec 2025,官方自建（由本文作者提出和构建的框架）,This paper introduces CAFFE (Counterfactual Assessment Framework for Fairness Evaluation)...,,,,,公平性测试（检测模型行为的不一致性）,"LLM-Fairness testing: Given a language model 𝑆, a set of inputs 𝐼, the required fairness condition 𝐶, and the observed fairness condition 𝐶′, LLM-Fairness testing consists of executing 𝐼on 𝑆to identify any discrepancies between 𝐶and 𝐶′ in the generated responses, measuring to what extent the system discriminates.",语义相似度指标，公平性违规检测率（与现有方法相比提升高达60%）,evaluates model responses using semantic similarity metrics... Our results show that... CAFFE improves the detection of fairness violations by up to 60%...,自然语言提示（包含敏感属性的变体）,"automatically generates targeted test data... systematically combining social groups with biased properties, generating synthetic inputs...",自然语言响应,identify any discrepancies between 𝐶and 𝐶′ in the generated responses...,文本到文本（自然语言提示到自然语言响应）,"prompts differing only in sensitive attributes, but expressing the same intent, should yield consistent responses",,,1. 将反事实公平性原则与受ISO/IEC/IEEE 29119标准启发的结构化测试用例定义相结合。2. 支持用户自定义意图，自动生成现实、语义一致的反事实提示对。3. 与仅关注输入输出不变性的蜕变测试不同，CAFFE沿提示意图、对话上下文、预期公平性阈值等关键维度形式化公平性测试用例。,"CAFFE (Counterfactual Assessment Framework for Fairness Evaluation), a framework that integrates counterfactual fairness principles with structured test case definitions inspired by the ISO/IEC/IEEE 29119 standard [3]... supports a broader range of user-defined intents and automatically generates realistic, semantically consistent counterfactual prompt pairs that vary only in the sensitive attribute... Rather than focusing solely on input-output invariance—which is the typical use case in metamorphic testing—we propose formalizing fairness test cases along key dimensions such as prompt intent, conversational context, expected fairness thresholds, and test environment configuration."
2512.16814_output/content.md,"CW, GLTL, Navigation","We evaluate the proposed framework on three natural language to temporal logic benchmarks- CW, GLTL, and Navigation.","No, 本文是使用该数据集进行评测","We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks.",,,将自然语言翻译为时序逻辑形式语言,Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems.,翻译准确性,GraFT improves the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.,准确率,GraFT improves the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.,,,形式化规范、机器人、自主系统,"Formal specifications play a crucial role in all systems that require autonomous reasoning, verification, or planning... Temporal Logics (TL) are a group of powerful formalisms that constitute the basis of most formal specification languages, allowing expressive description of the behavior of dynamic systems over time.",,,自然语言与时序逻辑形式语言,Translating natural language (NL) into a formal language such as temporal logic (TL)...,,,,,,,,,,,,,语言翻译,Translating natural language (NL) into a formal language such as temporal logic (TL)...,准确率,GraFT improves the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.,自然语言,Translating natural language (NL) into a formal language such as temporal logic (TL)...,时序逻辑公式,A sample NL to TL translation problem would involve translating the natural language sentence “Go to the red room and push the box into the green room.” into “♢( red room ∧♢green room )”.,自然语言到时序逻辑,Translating natural language (NL) into a formal language such as temporal logic (TL)...,,,专注于自然语言到形式化时序逻辑的翻译任务，用于机器人、自主系统的规范生成。,Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems.
2512.16465_output/content.md,KernelBench,"Experiment on the 100 kernels of KernelBench [11], cuPilot achieves 3.09× speedup over PyTorch.","No, 本文是使用该数据集进行评测","Experiment on the 100 kernels of KernelBench [11], cuPilot achieves 3.09× speedup over PyTorch.",,,评测基准旨在评估CUDA内核（GPU程序）的性能优化效果。,Optimizing CUDA kernels is a challenging and labor-intensive task... The optimization of CUDA kernels... is crucial for achieving peak AI inference/training performance.,CUDA内核性能优化（速度提升）,Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09× over PyTorch on a benchmark of 100 kernels.,速度提升倍数（相对于PyTorch基准）,Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09× over PyTorch on a benchmark of 100 kernels.,,,GPU编程、高性能计算、CUDA内核优化,"Optimizing CUDA kernels, a widely adopted GPU programming model [1], is crucial for achieving peak AI inference/training performance.",工程级、需要硬件-软件协同设计专业知识,"Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise...",CUDA（基于C/C++的GPU编程语言）,CUDA (Compute Unified Device Architecture) as a parallel computing platform and programming model [1]. CUDA extended the C/C++ language...,包含100个内核,Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09× over PyTorch on a benchmark of 100 kernels.,,,,,,,,,,,代码生成与优化（CUDA内核生成与性能优化）,"Optimizing CUDA kernels is a challenging and labor-intensive task... This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution.",平均速度提升倍数（相对于PyTorch）,Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09× over PyTorch on a benchmark of 100 kernels.,代码（CUDA内核）与可能的性能分析报告,"The fitness function, based solely on performance, exhibits weak semantic correlation with the kernel code. This prevents LLM from accurately pinpointing bottlenecks among numerous profiling reports...",优化后的代码（CUDA内核）,Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09× over PyTorch...,代码到代码（优化）,Optimizing CUDA kernels is a challenging and labor-intensive task... proposes cuPilot... for kernel evolution.,GPU硬件环境（需要CUDA支持）,"Optimizing CUDA kernels, a widely adopted GPU programming model [1], is crucial for achieving peak AI inference/training performance.",专注于CUDA内核的性能优化评测，包含GEMM（通用矩阵乘法）等具体任务案例。,"On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units."
2512.16070_output/content.md,LLM4Perf 多目标性能数据集,we construct a new multi-objective performance dataset based on four real-world systems.,Yes,we construct a new multi-objective performance dataset based on four real-world systems.,,,用于评估LLM作为配置采样器在构建多目标性能模型中的有效性。数据集记录了真实世界系统在不同配置下的性能指标，旨在支持对配置空间剪枝和多目标优化能力的检验。,This design allows us to examine the effects of configuration space pruning and assess the framework’s ability to handle multi-objective optimization.,多目标性能建模的有效性、配置空间剪枝效果、采样策略迭代优化能力,Our empirical evaluation... addresses four key research questions: (i) Effectiveness (RQ1)... (ii) Core Mechanisms (RQ2)... (iii) Impact of LLM Selection (RQ3)... (iv) Impact of Hyperparameter (RQ4)...,使用LLM4Perf框架生成配置样本，训练传统机器学习模型（如XGBoost）和深度学习模型（如DeepPerf），并比较它们在多个性能指标上的表现。,"The results show that both traditional machine learning model (i.e., XGBoost) and deep learning models (i.e., DeepPerf), when trained on configuration samples generated by LLM4Perf, generally achieve better performance across multiple metrics.",高度可配置软件系统的性能建模，依赖系统配置选项及其文档。,The performance of modern software systems is critically dependent on their complex configuration options.,软件性能工程、配置优化、多目标优化,"Overall, this paper provides strong evidence for the effectiveness of LLMs in performance engineering...",工程级，涉及真实世界复杂软件系统的配置空间组合爆炸和多目标权衡。,"The combination of options leads to a combinatorial explosion in the configuration search space... This challenge is further compounded by complex trade-offs, where improving one objective may degrade another...",,,基于四个真实世界系统构建。,We construct a new multi-objective performance dataset covering four real-world systems...,基于四个真实世界高度可配置系统的性能测量数据构建。,We construct a new multi-objective performance dataset based on four real-world systems.,2025-12-18 (arXiv版本),arXiv:2512.16070v1  [cs.SE]  18 Dec 2025,官方自建,we construct a new multi-objective performance dataset...,,,,,配置采样与性能预测,Building accurate performance models to navigate this vast space requires effective sampling strategies...,多个性能指标（如延迟、内存使用）的预测准确性。,"each configuration (i.e., a specific combination of option values) can have an unpredictable effect on key performance metrics like latency and memory usage...",系统配置选项、性能文档、历史性能测量数据,"In LLM4Perf, the LLM analyzes performance documentation as domain knowledge... and uses iterative feedback to refine the sampling strategy...",配置样本（用于训练性能模型）,when trained on configuration samples generated by LLM4Perf...,配置文档与历史数据到配置样本,The LLM analyzes performance documentation as domain knowledge... to guide the sampling toward more effective regions of the configuration space.,真实世界软件系统的性能测试环境,based on four real-world systems.,1. 专为多目标性能建模设计。2. 同时记录了完整配置空间和剪枝后（移除性能不敏感选项）配置空间的性能指标，便于分析剪枝效果。3. 用于评估LLM驱动的、结合领域知识（文档）和反馈的迭代采样框架。,"Unlike existing datasets, ours records performance metrics under both the full configuration space and a pruned space where performance-insensitive options are removed. This design allows us to examine the effects of configuration space pruning and assess the framework’s ability to handle multi-objective optimization."
2512.17259_output/content.md,"OPERA (Observability, Provable Execution, Red-team, Attestation)","We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol designed to measure (i) detectability of misalignment, (ii) time-to-detection under stealthy strategies, and (iii) resilience of verifiability mechanisms to adversarial prompt/persona injection.",Yes,"We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol...",,,评测基准旨在评估LLM智能体系统的可验证性机制，具体衡量其（i）对未对齐行为的可检测性，（ii）在隐蔽策略下的检测时间，以及（iii）可验证性机制对抗对抗性提示/角色注入的鲁棒性。,"We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol designed to measure (i) detectability of misalignment, (ii) time-to-detection under stealthy strategies, and (iii) resilience of verifiability mechanisms to adversarial prompt/persona injection.",智能体系统的可验证性、可观测性、可控性、安全性,"Our approach aims to shift the evaluation focus from ""how likely misalignment is"" to ""how quickly and reliably misalignment can be detected and remediated.""",通过一个包含可观测性、可证明执行、红队测试和证明协议的评估协议进行评测。,"We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol...",自主LLM智能体在循环中运行，涉及内部规划、工具使用、长期记忆和重复自我提示，生成复杂行为。,"Autonomous LLM agents operate in loops where internal planning, tool use, long-horizon memory (Yao et al. 2023; OpenAI 2023), and repeated self-prompting generate complex behaviors...",人工智能安全、智能体对齐、可验证性、网络安全、内容审核、谈判等现实自主任务。,"Recent benchmarks have quantified the propensity for misaligned behavior (Wang, Chen, and Liu 2024) across realistic autonomous tasks such as cybersecurity, content moderation, and negotiation.",高难度，涉及对抗性策略、隐蔽的未对齐行为、多智能体协调中的欺骗和不合规等复杂场景。,"Evaluating only individual agent reliability overlooks how deception, sandbagging, or non-compliance may emerge at the group level.",,,,,,,,,官方自建,"We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol...",,,,,智能体行为评估，包括检测、归因和补救未对齐行为。,"The field, therefore, needs an evaluative shift from measuring how often misalignment arises (Pan and Liu 2023; Long, Zhang, and Du 2024) to measuring how transparently and promptly it can be detected, attributed, and remediated.",可检测性、归因性、可补救性、检测时间、对抗性注入的鲁棒性。,"We describe the reliability of the architecture using three measurable ideas: detectability, attributability, and remediability.",智能体的意图规范、动作收据、审计日志。,"An ISpec can be thought of as the constitution of an agent. Instead of relying only on natural language prompts, it encodes deployer intent in a formal schema...",对齐概率、检测结果、补救措施。,"Collectively, they produce a dynamic estimate of the agent’s alignment probability over time.",智能体行为与意图规范到对齐评估,Their primary role is to continuously monitor the Provenance Log (PL) and validate whether the agent’s observed behavior aligns with its Intent Specification (ISpec).,在受控的智能体循环中，包含验证器栈和审计代理。,"All of this runs inside a controlled loop: the agent acts, the receipts are logged, the auditors check them, and any detected issue activates a Controller and Remediator (C&R).",OPERA基准将评估重点从“未对齐的可能性有多大”转向“未对齐行为能被多快、多可靠地检测和补救”。它集成了运行时证明、轻量级审计代理和挑战-响应证明协议，旨在为自主LLM系统提供可证明的可观测性和可控性。,"Our approach aims to shift the evaluation focus from ""how likely misalignment is"" to ""how quickly and reliably misalignment can be detected and remediated."""
2512.17419_output/content.md,SWE-Bench++,"We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects.",Yes,"We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects.",https://research.turing.com/swebench,Project Page: https://research.turing.com/swebench,评测基准旨在评估大型语言模型在仓库级软件工程任务上的能力，任务来源于真实的GitHub拉取请求，包括错误修复和功能请求。,"Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. ... Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages from open-source GitHub repositories.",仓库级代码生成、多语言能力、软件工程任务解决（错误修复与功能请求）,"SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation. ... Our state-differential oracle identifies both bug fixes and feature requests, increasing feature-like coverage compared to prior benchmarks",基于执行的验证，通过测试套件评估模型生成的补丁是否正确。,"SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance.",多文件项目、仓库级上下文,"repository-level software engineering tasks. ... The evaluation of Large Language Models (LLMs) coding agents has shifted from isolated function synthesis (e.g., HumanEval (Chen et al., 2021)) to repository-level software engineering.",通用软件工程，涵盖多种编程语言和项目类型,software engineering benchmarks from GitHub pull requests. ... covering diverse build systems and coding patterns.,工程级，涉及真实的、复杂的开源项目维护任务,"substantial complexity, defined by codebases exceeding 10k lines of code; ... merged PRs that explicitly resolve an issue (ensuring a link between a natural language problem description and a coding solution)",11种编程语言,"across 11 languages from open-source GitHub repositories. ... Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages.","包含11,133个实例，来自3,971个仓库","Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages.",从开源GitHub仓库中获取的实时拉取请求,generates repository-level coding tasks from open-source GitHub projects. ... harvests live pull requests,2025,arXiv:2512.17419v1  [cs.SE]  19 Dec 2025,官方自建，通过自动化框架生成,"We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects.",抗污染设计，可通过拉取请求创建日期过滤，支持时间分离的评估集以降低数据污染风险,"Contamination-Aware Evaluation: SWE-Bench++ is constructed from dated GitHub pull requests and can be filtered by PR creation date, enabling temporally separated evaluation sets that reduce data-contamination risk for future models.",,,仓库级代码生成与修改,repository-level software engineering tasks. ... repository-level code generation.,pass@10（文中示例）,"claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%.",自然语言（GitHub Issue描述）与代码仓库上下文,merged PRs that explicitly resolve an issue (ensuring a link between a natural language problem description and a coding solution),代码（补丁或新功能实现）,cover both bug fixes and feature requests,文本（问题描述）到代码（仓库修改）,a link between a natural language problem description and a coding solution,自动合成的Docker环境，包含特定的语言版本和依赖,Our pipeline automatically synthesizes Docker environments and log parsers across 11 languages. ... We create a reproducible execution environment that mirrors the repository at the time the issue was present.,1. 自动化、可扩展的多语言基准生成框架。2. 覆盖错误修复和功能请求两种任务类型。3. 采用状态差分测试预言机进行任务分类和验证。4. 包含提示引导的轨迹合成，可将模型失败的实例转化为训练数据。5. 具有污染感知设计，支持按时间过滤。,"an automated multilingual framework that generates software engineering benchmarks from GitHub pull requests. ... Our state-differential oracle identifies both bug fixes and feature requests. ... Hint-Guided Trajectory Synthesis: ... converts model-breaking instances (where SOTA models fail) in SWE-Bench++ environments into executable training trajectories. ... Contamination-Aware Evaluation: SWE-Bench++ is constructed from dated GitHub pull requests and can be filtered by PR creation date, enabling temporally separated evaluation sets that reduce data-contamination risk for future models."
