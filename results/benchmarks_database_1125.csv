source_paper,benchmark_name,benchmark_name_quote,is_original_proposal,is_original_proposal_quote,dataset_url,dataset_url_quote,task_description,task_description_quote,dimension,dimension_quote,evaluation_method,evaluation_method_quote,context_dependency,context_dependency_quote,problem_domain,problem_domain_quote,problem_difficulty,problem_difficulty_quote,language,language_quote,data_size,data_size_quote,source_type,source_type_quote,last_updated,last_updated_quote,build_type,build_type_quote,contamination_status,contamination_status_quote,dataset_license,dataset_license_quote,task_granularity,task_granularity_quote,evaluation_metrics,evaluation_metrics_quote,input_modality,input_modality_quote,output_modality,output_modality_quote,task_io_type,task_io_type_quote,execution_environment,execution_environment_quote,unique_features,unique_features_quote
2511.20403_output/content.md,CLASSES2TEST,"We introduce the CLASSES2TEST dataset, which maps Java classes under test to their corresponding test classes",Yes，本文是该数据集的原始发布论文,"We introduce the CLASSES2TEST dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics",https://anonymous.4open.science/r/classes2test,1https://anonymous.4open.science/r/classes2test,用于评估大型语言模型生成的Java单元测试的质量，支持研究人员和开发者比较不同LLM和提示策略,"AGONETEST does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions.",单元测试质量评估，包括编译成功率、代码覆盖率、缺陷检测能力、测试异味等,"a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",集成高级评估指标，如变异分数和测试异味，进行综合评估,"a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",类级别测试，涵盖方法交互和共享状态,"Our approach focuses on class-level test code evaluation, which is closer to real-world practices as it covers method interactions and shared state",软件测试，Java单元测试生成,"Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly.",现实世界软件项目级别，比单方法测试更复杂,This extended dataset makes it possible to assess the test performance of an LLM on a more complex scope (the entire class) than the single method.,Java,An annotated open source Java project dataset extending METHODS2TEST [9],"基于9,410个GitHub仓库的数据集","AGONETEST offers far broader applicability by using a dataset of 9,410 GitHub repositories",扩展自METHODS2TEST数据集的Java开源项目,An annotated open source Java project dataset extending METHODS2TEST [9],2025,arXiv:2511.20403v1  [cs.SE]  25 Nov 2025,官方自建，基于现有数据集扩展,"Leveraging the METHODS2TEST dataset [9], we developed a new dataset specifically aimed at comparing human-written tests with those produced by LLMs.",,,,,类级别单元测试生成,"Our approach focuses on class-level test code evaluation, which is closer to real-world practices as it covers method interactions and shared state",变异分数、测试异味、代码覆盖率,"a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment",Java类代码,which maps classes under test to their related test classes,单元测试代码,"for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection",代码到代码,which maps Java classes under test to their corresponding test classes,支持所有Java LTS版本的项目环境,AGONETEST overcomes this barrier by supporting all Java LTS versions.,专注于类级别测试评估，支持多种LLM和提示策略的比较，提供端到端的自动化评估流水线,"AGONETEST shifts the focus to the generation of class-level tests. Our approach makes it possible to use up-to-date LLMs and not constrain prompt design (our prompts can be customized), thereby handling more complex, real-world scenarios."
2511.21380_output/content.md,"ROCODE, LogHub2.0","Two projects (i.e., ROCODE [15] and LogHub2.0 [16]) are selected for the following experiments.","No, 本文是使用该数据集进行评测","We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0.",,,数据集适应任务 - 将软件工程研究工具自动适配到新的数据集，构建可运行的实验并获取执行结果,"Our objective is to automatically modify 𝑅𝐷or 𝑅𝑇, construct a runnable experiment, and obtain its execution results.",多智能体系统在数据集适应任务中的能力评估,This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks.,五阶段评估流程：文件理解、代码编辑、命令生成、验证和最终执行，测量成功率并分析失败模式,"Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance.",多文件项目环境，需要理解整个代码仓库的架构,"Before modifying code or generating commands for a repository, a multi-agent system must browse the essential files in a repository to understand its architecture.",软件工程研究，包括代码生成、bug修复、性能分析和安全等领域,"In areas ranging from code generation and bug fixing to performance analysis and security, researchers are constantly proposing new techniques",复杂软件工程任务，需要多步骤协调和迭代修复,"Most of the evaluated multi-agent systems failed to complete the assigned tasks. Under such circumstances, nearly all results would be classified as failures",Python,"To ensure a consistent and manageable experimental environment, we retained only artifacts implemented in Python.","从顶级软件工程会议（FSE, ICSE, ASE, ISSTA）2024-2025年接受的论文中筛选的可重用研究构件","We began by collecting all papers accepted by the top four SE (FSE, ICSE, ASE and ISSTA) in 2024-2025 that were either (i) awarded a Reusable Artifact badge or (ii) explicitly identified by the authors as providing reusable artifacts.",学术研究构件，来自顶级软件工程会议的可重用研究项目,"To construct a representative set of high-quality SE research artifacts, we followed a systematic multi-stage selection process",2025,arXiv:2511.21380v1  [cs.SE]  26 Nov 2025,官方自建的研究构件集合,"We began by collecting all papers accepted by the top four SE (FSE, ICSE, ASE and ISSTA) in 2024-2025",,,,,代码编辑、文件创建、命令生成、验证修复,Edit and create necessary files... Generate and execute the commands... Validating and repairing,成功率、结构相似度（从7.25%到67.14%）、完成状态记录,Results show that... substantially improve structural similarity to ground truth (from 7.25% to 67.14%),代码仓库、自然语言提示,Each multi-agent system is provided with a processed repository... along with a simple prompt that specifies the adaptation task,修改后的代码、生成的脚本命令、执行结果,"the code adaptations performed to ensure compatibility, the executable scripts or commands derived for running the experiment, and the results produced by executing those scripts or commands",代码到代码的适应任务,"automatically modify 𝑅𝐷or 𝑅𝑇, construct a runnable experiment, and obtain its execution results",Python环境，需要可靠的部署环境,This choice allows for reliable environment deployment and isolates our investigation from environment setup challenges,专注于多智能体系统在数据集适应任务中的表现评估，采用五阶段评估流程，研究提示级干预对性能的影响,"This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. Through a five-stage evaluation pipeline... we measure success rates, analyze failure patterns, and assess prompt-based interventions"
2511.21022_output/content.md,EDAPIBench,"We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances.",Yes，本文是该数据集的原始发布论文,"We introduce EDAPIBench, a dedicated benchmark... We construct EDAPIBench—the first dedicated benchmark for evaluating deprecated API knowledge editing in LLMs",,,评估大语言模型中废弃API知识编辑的性能，专门用于测试模型编辑技术能否有效更新废弃API知识并生成最新的API,"a dedicated benchmark for evaluating deprecated API knowledge editing in LLMs... whether existing model editing methods can effectively update deprecated API knowledge within LLMs, and enable the edited models to correctly replace deprecated APIs with up-to-date ones",有效性、泛化性、可移植性、特异性,"comprehensively assess model editing performance across four key dimensions: Effectiveness, Generalization, Portability, Specificity",基于四个维度的评估：有效性（编辑后模型是否生成最新API）、泛化性（语义等价但语法变化的输入）、可移植性（不同输入但涉及相同废弃API）、特异性（保持与编辑任务无关输入的行为一致性）,Effectiveness: Measuring whether the edited model generates the up-to-date API for the original inputs; Generalization: Measuring whether it generates the up-to-date API on semantically equivalent but syntactically varied inputs; Portability: Measuring whether it generates the up-to-date API across different inputs... Specificity: Measuring whether it preserves consistent pre-editing behavior on inputs unrelated to the editing task,单函数级别的代码补全上下文,Our study frames the model editing scenario as a code completion task... we extract lines preceding the API invocation as candidate editing inputs (to prompt LLM API completion) and the invocation line as the target API (ground truth),软件工程、API维护、第三方库更新,deprecated APIs from 8 popular Python libraries... covering eight popular Python libraries such as PyTorch and TensorFlow,实际工程级难度，涉及真实世界废弃API的识别和更新,"Using these mappings, we extract 65,596 real-world functions from GitHub that call the up-to-date APIs",Python,deprecated APIs from 8 popular Python libraries... covering eight popular Python libraries such as PyTorch and TensorFlow,"包含70多个废弃API，超过3000个编辑实例，从GitHub提取了65,596个真实世界函数","featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances... we extract 65,596 real-world functions from GitHub",从GitHub真实项目代码中提取，基于已验证的API映射关系,"We begin with 145 verified API mappings (deprecated →up-to-date) from Wang et al. [49]... Using these mappings, we extract 65,596 real-world functions from GitHub that call the up-to-date APIs",2025-2026,Publication date: November 2026. arXiv:2511.21022v1 [cs.SE] 26 Nov 2025,官方自建，全自动构建,which can be fully automatically constructed... with fully automated construction,,,,,API级别的代码补全,"code completion tasks... during code completion, LLMs frequently suggest deprecated API invocations",基于四个维度的定性评估：有效性、泛化性、可移植性、特异性,"Effectiveness, Generalization, Portability, Specificity",代码片段,the editing input (a code snippet to be completed),API调用代码,"the specific correct, up-to-date API that should replace the deprecated one",代码到代码,code completion task... generating code,,,首个专门针对废弃API知识编辑的基准测试，支持全自动构建，包含四个维度的综合评估,"the first dedicated benchmark for evaluating deprecated API knowledge editing in LLMs, with fully automated construction, serving as a standardized, rigorous platform"
2511.19875_output/content.md,CodeFuse-CommitEval,"We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models.",Yes，本文是该数据集的原始发布论文,"We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models.",https://figshare.com/s/21fe4ec9cb960b52bffe,The dataset is publicly available at https://figshare.com/s/21fe4ec9cb960b52bffe.,"检测提交信息与代码变更之间的不一致性（Message-Code Inconsistency, MCI）",A Message-Code Inconsistency (MCI) occurs when the natural language description in a commit message does not accurately reflect the actual modifications in the associated code diff.,提交信息与代码变更的一致性检测能力,evaluate models for MCI detection,使用Recall、Precision、Specificity等指标评估模型检测结果,"Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%)",提交信息与对应的代码差异（diff）,"Each item of the verified dataset contains a commit message, the corresponding code diff, and the ground truth label",软件工程、版本控制、代码审查,Version control relies on commit messages to convey the rationale for code changes,需要语义理解和上下文推理的复杂任务,purpose inconsistencies require deeper semantic understanding and contextual reasoning,多种编程语言（基于ApacheCM数据集的多样性）,because of its diversity in programming languages and the high-quality commits,包含正负样本的平衡数据集,we generate a balanced dataset with both positive and negative samples,基于ApacheCM数据集，通过规则引导的突变生成不一致提交信息,"Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits",2025,arXiv:2511.19875v1 [cs.SE] 25 Nov 2025,官方自建，结合LLM数据合成能力与现有提交语料库,We present a comprehensive pipeline for constructing MCI datasets. By combining the data synthesis capabilities of LLMs with existing commit corpora,通过双重验证确保数据质量,apply two-fold validation to verify both positive and negative samples,,,一致性检测,detecting inconsistencies between commit messages and code,"Recall, Precision, Specificity","average Recall 85.95%, Precision 80.28%, Specificity 63.8%",自然语言（提交信息）与代码差异,"Each item of the verified dataset contains a commit message, the corresponding code diff",二元分类（一致或不一致）,allowing the models to detect whether the commit is consistent or not,文本与代码到分类,detect whether the commit is consistent or not,,,首个专门用于MCI检测的基准，包含七种不一致类型，支持多种增强策略评估,CODEFUSE-COMMITEVAL is the first benchmarking work tailored for comprehensively evaluating LLM's MCI detection ability. We define seven mutation rules to guide powerful LLMs in generating various types of inconsistent commit messages
2511.20709_output/content.md,DUALGAUGE-BENCH,"we further curated DUALGAUGE-BENCH, a comprehensive benchmark suite of 154 programming tasks",Yes，本文是该数据集的原始发布论文,"We present DUALGAUGE, the first fully automated benchmarking framework... we also present DUALGAUGE-BENCH, a curated benchmark suite",https://anonymous.4open.science/r/DualBench-6D1D,"Our system source code, benchmark, and evaluation/study data can all be found at https://anonymous.4open.science/r/DualBench-6D1D",联合评估代码生成的安全性和功能性，确保生成的代码既满足功能规范又不会引入安全漏洞,rigorously evaluate the security and correctness of LLM-generated code in unison... ensuring that generated programs fulfill their specifications without introducing vulnerabilities,代码生成的安全性和功能性联合评估,designed to rigorously evaluate the security and correctness of LLM-generated code in unison,在沙盒环境中执行程序，针对功能和安全性测试套件运行，基于执行结果评估测试通过率和联合正确性-安全性指标,"executes it in a sandboxed environment against functional and security test suites, and evaluates the execution results against both test suites to report test pass rates and joint correctness-security metrics",单任务代码生成，基于自然语言提示生成完整程序,Given an pure-natural-language prompt as functional specification... generates the model's code output for the prompt,跨领域编程任务，涵盖多样化功能领域,spanning diverse functionality domains,,,语言无关，支持多种编程语言,This dataset design and curation process allow it to be agnostic to programming languages,包含154个编程任务，每个任务都配有功能和安全性测试套件,a comprehensive benchmark suite of 154 programming tasks... Each task/prompt is paired with both a functional test suite... and a security test suite,人工与LLM协同创建，通过多评分者的人类专业知识进行精炼和修正,"constructed these test suites through a human-and-LLM co-creation process—leveraging multiple LLMs to generate candidate tests, then refining and amending these with human expertise of multiple raters",2025,arXiv:2511.20709v1 [cs.SE] 24 Nov 2025,官方自建，基于规范测试范式,following a specification-based testing paradigm,,,,,代码生成,code generation... translating natural language prompts—typically serving as functional specifications—into programs,测试通过率、联合正确性-安全性指标,report test pass rates and joint correctness-security metrics,自然语言,Given an pure-natural-language prompt as functional specification,代码,generates the model's code output for the prompt,文本到代码,translating natural language prompts... into programs,沙盒隔离容器环境，支持依赖解析和环境配置,executes it in a sandboxed environment... The execution engine compiles and runs the model-generated code within isolated containers,首个支持安全性和功能性联合评估的基准套件，每个任务都配有覆盖驱动的功能和安全性测试套件，采用人工与LLM协同创建过程,"the first benchmark suite that pairs each code-generation prompt with dual (functional and security), coverage-enforced test suites... constructed through a human-and-LLM co-creation process"
2511.23408_output/content.md,Vul4J,"Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects and covering distinct Common Weakness Enumeration (CWE) classes.","No, 本文是使用该数据集进行评测","Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects...",,,评估大型语言模型（LLM）在自动化漏洞修复方面的有效性，具体针对真实漏洞和人工生成的漏洞。,"In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs... using both real and artificial vulnerabilities.",漏洞修复的有效性、模型间的互补性与重叠性,Our paper empirically investigates the effectiveness and complementarity of several prominent LLMs in automated vulnerability patching...,"使用漏洞证明（Proof-of-Vulnerability, PoV）测试执行来验证生成的补丁是否成功修复漏洞。",Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities.,单函数/代码片段,"Given a vulnerable code snippet, these models generate a patched version that aims to eliminate security flaws while preserving functionality.",软件安全、漏洞修复,Automated vulnerability patching is crucial for software security...,,,Java,"Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities...",15个真实漏洞及其41个人工生成的对应漏洞,"To perform this evaluation, we employed 15 real vulnerabilities and their 41 artificial counterparts (vulnerabilities).",真实漏洞来自开源项目和公共漏洞数据库（如CVE），人工漏洞由CodeBERT生成并经过验证。,"Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects... Secondly, we augmented our evaluation with artificially generated vulnerabilities... we incorporated artificial vulnerabilities derived from the work of Garg et al.[15]. Garg et al. used CodeBERT[11] to generate thousands of candidate artificial vulnerabilities...",,,官方自建（Vul4J数据集）,"Firstly, we utilized the Vul4J dataset [2], a carefully-curated benchmark of reproducible Java vulnerabilities drawn from open-source projects...",,,,,代码修复（漏洞补丁生成）,"Given a vulnerable code snippet, these models generate a patched version that aims to eliminate security flaws while preserving functionality.",PoV测试通过率（基于执行的验证）,Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities.,代码（包含漏洞的代码片段）,"Given a vulnerable code snippet, these models generate a patched version...",代码（修复后的代码片段）,"Given a vulnerable code snippet, these models generate a patched version...",代码到代码,"Given a vulnerable code snippet, these models generate a patched version...",Maven/Gradle构建环境，包含PoV JUnit测试用例,"When no suitable test existed in the original project, Bui et al. [2] manually wrote one by following the exploit steps in the CVE report, thereby guaranteeing that every vulnerability in the dataset can be triggered, reproduced, and validated in a Maven/Gradle build.",该研究不仅评估LLM对真实漏洞的修复能力，还评估其对人工生成漏洞的修复能力，以测试模型的泛化性和鲁棒性。数据集（Vul4J）为每个漏洞提供了可执行的漏洞证明（PoV）测试用例，用于自动化验证补丁的有效性。,"Our paper empirically investigates the effectiveness and complementarity of several prominent LLMs in automated vulnerability patching, specifically focusing on both real vulnerabilities and their corresponding artificial vulnerabilities... For every entry Vul4J provides... one or more Proof-of-Vulnerability (PoV) JUnit test cases. A PoV test is an executable exploit oracle..."
2401.01062_output/content.md,CAASD (Capability Assessment of Automatic Software Development),we have developed a novel benchmark named CAASD (Capability Assessment of Automatic Software Development).,Yes,we have developed a novel benchmark named CAASD (Capability Assessment of Automatic Software Development).,,,评估AI辅助软件开发系统的能力，特别是针对系统级实现任务的能力,all of them either are limited to simple function-level implementation tasks or lack detailed requirements specifications for system-level implementation tasks,系统级软件开发能力评估,for assessing how well a software development task is completed,通过参考用例评估系统实现的质量和完整性,Each task of CAASD is equipped with a list of reference use cases depicting the system requirements. The reference use cases are used to evaluate the quality and completeness of a system implementation.,系统级开发（多文件项目）,system-level implementation tasks,软件开发,software development,非平凡软件项目,non-trivial software projects,,,,,,,2024,arXiv:2401.01062v1 [cs.SE] 2 Jan 2024,官方自建,we have developed a novel benchmark named CAASD,,,,,系统级实现任务,system-level implementation tasks,通过率,AISD achieves an impressive pass rate of 75.2%,自然语言需求描述,high-level (potentially vague) user requirements as inputs,系统实现,system implementation,需求到系统实现,"taking high-level (potentially vague) user requirements as inputs, generates detailed use cases, prototype system designs, and subsequently system implementation",,,首个评估软件开发任务完成质量的基准，包含详细的系统需求规范,this is the first benchmark that offers criteria for assessing how well a software development task is completed
2401.12554_output/content.md,ParEval,we propose the Parallel Code Generation Evaluation (ParEval) benchmark: a set of benchmarks (prompts) for evaluating how well LLMs generate parallel code.,Yes,we propose the Parallel Code Generation Evaluation (ParEval) benchmark,github.com/parallelcodefoundry/ParEval,ParEval is available online at: github.com/parallelcodefoundry/ParEval.,评估大型语言模型生成并行代码的能力,we study the capabilities of state-of-the-art language models to generate parallel code.,并行代码生成正确性和性能,"We evaluate several state-of-the-art open- and closed-source LLMs using these benchmarks, and report metrics that represent the correctness and performance of the generated code.",新颖的代码生成评估指标，包括speedup𝑛@k和efficiency𝑛@k，用于评估生成代码的性能和扩展性,We introduce novel code generation evaluation metrics that assess performance and parallel scaling.,,,科学和并行计算,consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.,,,C/C++,"Traditional Python code generation benchmarks are tested by running eval on the generated code for a small number of small unit tests. On the other hand, in the case of parallel code — we must compile C/C++ code, link against one or more parallel libraries, and run the code in the proper parallel environment.",420个不同的编码任务,consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.,手动设计,These benchmarks are challenging to test. Traditional Python code generation benchmarks are tested by running eval on the generated code for a small number of small unit tests.,2024,"HPDC ’24, June 3–7, 2024, Pisa, Italy",官方自建,we propose the Parallel Code Generation Evaluation (ParEval) benchmark: a set of benchmarks (prompts) for evaluating how well LLMs generate parallel code.,,,,,代码生成,we study the capabilities of state-of-the-art language models to generate parallel code.,speedup𝑛@k和efficiency𝑛@k,"We introduce two novel metrics, speedup𝑛@k and efficiency𝑛@k, for evaluating the performance and scaling of LLM generated code.",自然语言,consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.,代码,we study the capabilities of state-of-the-art language models to generate parallel code.,文本到代码,consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing.,需要特定依赖（并行库）,"we must compile C/C++ code, link against one or more parallel libraries, and run the code in the proper parallel environment.",专注于并行代码生成评估，覆盖12种计算问题类型和7种执行模型,"These benchmarks cover twelve different computational problem types, and seven different execution models: serial, OpenMP, Kokkos, MPI, MPI+OpenMP, CUDA, and HIP."
2512.01010_output/content.md,Chain of Unit-Physics,"To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation.",Yes,"To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation.",,,该评测基准旨在解决科学计算中的代码生成任务，特别是针对具有严格物理约束的高风险科学问题，如燃烧科学中的计算流体动力学（CFD）求解器开发。,Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community.,物理一致性、数值稳定性、算法正确性,"The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",基于单元物理测试的验证方法，包括物理约束检查、数值一致性检查和诊断分析,The Verification Agent applies formalized unit-physics tests to assess physical and numerical consistency. These primitives yield physics-grounded verification even without reference datasets.,多步骤科学计算任务，涉及复杂的物理约束和数值计算,"The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",计算科学，特别是燃烧科学和计算流体动力学,"To overcome these limitations, this work systematically applies an inverse code design methodology (see Fig. 1), formally known as test-driven development (TDD) [26], to scientific software in combustion science, one such domain where TDD approaches have not been thoroughly evaluated.",高难度，涉及12自由度的复杂燃烧任务,"The framework is evaluated on a nontrivial combustion task (12 degrees-of-freedom), used here as a representative benchmark for scientific problem with realistic physical constraints.",,,,,基于人类专家知识构建的单元物理测试,"This work proposes Chain of Unit-Physics, an approach that embeds human expert knowledge directly into distinct reasoning chains of the agentic system via 'unit-physics'—formalized, testable constraints that encode fundamental physics (e.g., energy conservation laws) and practitioner experience (e.g., dimensional / bound checks, and floating-point diagnostics).",2025,arXiv:2512.01010v1 [cs.MA] 30 Nov 2025,官方自建，基于人类专家知识,"This work proposes Chain of Unit-Physics, an approach that embeds human expert knowledge directly into distinct reasoning chains of the agentic system via 'unit-physics'—formalized, testable constraints that encode fundamental physics (e.g., energy conservation laws) and practitioner experience (e.g., dimensional / bound checks, and floating-point diagnostics).",,,,,端到端科学代码生成,"Closed-weight systems and code-focused agentic variants fail to produce correct end-to-end solvers, despite tool and web access, exhibiting four recurrent error classes: interface (syntax/API) hallucinations, overconfident assumptions, numerical/physical incoherence, and configuration fragility.",物理一致性、数值误差、运行时间和内存使用效率,"On the benchmark task, the proposed framework converges within 5–6 iterations, matches the human-expert implementation (mean error of 3.1ˆ10´3%), with a „33.4% faster runtime and a „30% efficient memory usage at a cost comparable to mid-sized commercial APIs, yielding a practical template for physics-grounded scientific code generation.",自然语言查询和单元物理测试,"In the input, the user’s scientific question is paired with 'basis prompts' that establish execution permissions, tool availability, coding language settings, and transfer protocols. Additional input scopes the unit-physics tests that concatenated with the scientific query to form the complete input to the framework.",科学计算代码和可视化结果,"Finally, the agent consolidates the output into domain-relevant visualizations (for example, line graphs and contour graphs of key quantities of interest), closing the loop between natural-language inquiry and quantitative scientific insight.",自然语言到科学代码,Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community.,Python虚拟沙箱环境，具有隔离的执行权限,"The framework runs inside a dedicated Python virtual sandbox environment that is not pre-configured with metadata on all available libraries. The agent is granted isolated code execution privileges within this sandbox, ensuring that any dependency installs or script executions cannot affect the system root directory or global files.",基于第一性原理的单元物理测试驱动代码生成，强调物理一致性和数值稳定性,"This inverse-design method provides two key advantages in scientific software. First, human-authored tests embed deep domain expertise (first principles or primitives) into targeted validation checks, ensuring that each algorithmic component faithfully represents the underlying physics."
2512.01396_output/content.md,BackportBench,"we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",Yes,"we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",https://github.com/BackportBench/BackportBench,The BackportBench is available on https://github.com/BackportBench/BackportBench.,自动化补丁回移植。旨在为未打补丁的软件版本生成补丁，基于已有的补丁和两个版本之间的代码差异。,BackportBench defines the backporting problem as generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.,自动化补丁回移植的有效性、多语言能力、处理跨文件不兼容性的能力,"To facilitate the development and evaluation of automated backporting techniques... BackportBench is a multilingual benchmark... This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges.",在可执行的Docker环境中运行相关测试用例进行验证,"each task instance provides an executable Docker environment with scripts for running relevant test cases, thereby aligns with the criteria for a successful benchmark [5].",仓库级别，涉及跨文件的不兼容性处理,"This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges.",软件安全、软件维护、漏洞修复,BackportBench contains 202 real-world vulnerability patch backporting task instances curated from three popular OSS ecosystems... To remove such vulnerabilities... patch backporting is a helpful practice that adapts patches for other versions and makes it easier to deliver patches to users on older branches.,现实世界工程级，涉及逻辑和结构性变更,"the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes.","Python, Java, JavaScript","BackportBench contains 202 patch backporting problems from PyPI, Maven, and npm, covering Python, Java, and JavaScript, respectively.",包含202个补丁回移植任务实例,BackportBench contains 202 real-world vulnerability patch backporting task instances,"从三个流行的开源软件生态系统（PyPI, Maven, npm）中收集的真实世界漏洞补丁回移植任务","BackportBench contains 202 real-world vulnerability patch backporting task instances curated from three popular OSS ecosystems, PyPI , Maven, and npm",2018 (根据论文版权日期)，但arXiv版本为2025年12月,© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ... arXiv:2512.01396v1  [cs.SE]  1 Dec 2025,官方自建,"we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem.",,,ACM版权，允许个人或课堂使用，禁止商业用途,Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.,代码修复/补丁生成,generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.,通过测试用例验证补丁有效性，而非基于等价性或相似性的指标,each task instance provides an executable Docker environment with scripts for running relevant test cases... We argue that whether backported patches achieve equivalence is not the only way to prove the patch is effective.,代码（两个版本的代码库及原始补丁）,generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.,代码（回移植后的补丁）,generating a patch for the unpatched software version,代码到代码,generating a patch for the unpatched software version based on the existing patch and the code differences between the two versions.,可执行的Docker环境，包含运行相关测试用例的脚本,each task instance provides an executable Docker environment with scripts for running relevant test cases,首个针对补丁回移植问题的综合性、多语言基准测试套件，包含可执行环境和测试用例，将问题从代码块/函数级别提升到仓库级别，更贴近现实软件维护挑战。,"the first comprehensive benchmark suite for patch backporting problem... a multilingual benchmark... contains executable Docker environments and test cases for validation... This formulation shifts the patch porting problem from code-hunk level or function level to repository level, offering a more realistic representation of software maintenance challenges."
