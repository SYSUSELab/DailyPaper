[
  {
    "id": "2305.06161",
    "title": "StarCoder: may the source be with you!",
    "abstract": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.",
    "arxiv_url": "https://arxiv.org/abs/2305.06161",
    "authors": [
      "Raymond Li",
      "Loubna Ben Allal",
      "Yangtian Zi",
      "Niklas Muennighoff",
      "Denis Kocetkov",
      "Chenghao Mou",
      "Marc Marone",
      "Christopher Akiki",
      "Jia Li",
      "Jenny Chim",
      "Qian Liu",
      "Evgenii Zheltonozhskii",
      "Terry Yue Zhuo",
      "Thomas Wang",
      "Olivier Dehaene",
      "Mishig Davaadorj",
      "Joel Lamy-Poirier",
      "João Monteiro",
      "Oleh Shliazhko",
      "Nicolas Gontier",
      "Nicholas Meade",
      "Armel Zebaze",
      "Ming-Ho Yee",
      "Logesh Kumar Umapathi",
      "Jian Zhu",
      "Benjamin Lipkin",
      "Muhtasham Oblokulov",
      "Zhiruo Wang",
      "Rudra Murthy",
      "Jason Stillerman",
      "Siva Sankalp Patel",
      "Dmitry Abulkhanov",
      "Marco Zocca",
      "Manan Dey",
      "Zhihan Zhang",
      "Nour Fahmy",
      "Urvashi Bhattacharyya",
      "Wenhao Yu",
      "Swayam Singh",
      "Sasha Luccioni",
      "Paulo Villegas",
      "Maxim Kunakov",
      "Fedor Zhdanov",
      "Manuel Romero",
      "Tony Lee",
      "Nadav Timor",
      "Jennifer Ding",
      "Claire Schlesinger",
      "Hailey Schoelkopf",
      "Jan Ebert",
      "Tri Dao",
      "Mayank Mishra",
      "Alex Gu",
      "Jennifer Robinson",
      "Carolyn Jane Anderson",
      "Brendan Dolan-Gavitt",
      "Danish Contractor",
      "Siva Reddy",
      "Daniel Fried",
      "Dzmitry Bahdanau",
      "Yacine Jernite",
      "Carlos Muñoz Ferrandis",
      "Sean Hughes",
      "Thomas Wolf",
      "Arjun Guha",
      "Leandro von Werra",
      "Harm de Vries"
    ],
    "first_author": "Raymond Li",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Pre-Training",
    "summary": "本文发布并评估了开源代码语言模型StarCoder与StarCoderBase（15.5B参数、8K上下文、填空能力与多查询注意力），在大规模许可代码语料上预训练并对Python进行微调，同时提供PII脱敏与归属追踪工具，且在多项基准上优于其他开源代码模型。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2305.06161v2",
    "published": "2023-05-09",
    "update_time": "2023-12-13",
    "download_time": "2025-12-04 23:09:51"
  },
  {
    "id": "2305.07922",
    "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation",
    "abstract": "Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.",
    "arxiv_url": "https://arxiv.org/abs/2305.07922",
    "authors": [
      "Yue Wang",
      "Hung Le",
      "Akhilesh Deepak Gotmare",
      "Nghi D. Q. Bui",
      "Junnan Li",
      "Steven C. H. Hoi"
    ],
    "first_author": "Yue Wang",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Pre-Training",
    "summary": "本文提出CodeT5+，一种可灵活组合组件的encoder-decoder代码大模型族，通过混合预训练目标（span denoising、对比学习、文本-代码匹配与因果语言建模）、冻结大解码器并仅训练浅编码器与交叉注意力以高效扩展，并结合instruction-tuning，在多项代码理解与生成基准上取得显著提升并在HumanEval上达成新SOTA。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2305.07922v2",
    "published": "2023-05-13",
    "update_time": "2023-05-20",
    "download_time": "2025-12-04 23:13:45"
  },
  {
    "id": "2305.02309",
    "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages",
    "abstract": "Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly.   In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a \"free lunch\" hypothesis. For data distributions, the effect of a mixture distribution and multi-epoch training of programming and natural languages on model performance is explored.   We conduct a comprehensive series of empirical experiments on 1B LLMs, for which failures and successes of this exploration are distilled into five lessons. We will provide a final recipe for training and release CodeGen2 models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training framework as open-source: https://github.com/salesforce/CodeGen.",
    "arxiv_url": "https://arxiv.org/abs/2305.02309",
    "authors": [
      "Erik Nijkamp",
      "Hiroaki Hayashi",
      "Caiming Xiong",
      "Silvio Savarese",
      "Yingbo Zhou"
    ],
    "first_author": "Erik Nijkamp",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Pre-Training",
    "summary": "该工作通过对模型架构、学习目标、infill 采样和数据分布的大规模消融实验，提出并验证了用于代码与自然语言统一预训练的简洁混合目标与训练配方，并开源了 CodeGen2 模型与训练框架。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2305.02309v2",
    "published": "2023-05-03",
    "update_time": "2023-07-11",
    "download_time": "2025-12-04 23:14:12"
  },
  {
    "id": "2305.01210",
    "title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation",
    "abstract": "Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code. Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis. However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies. While EvalPlus is general, we extend the test-cases of the popular HumanEval benchmark by 80x to build HumanEval+. Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that test insufficiency can lead to mis-ranking. For example, both WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+, while none of them could on HumanEval. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis, but also opens up a new direction to improve such programming benchmarks through automated testing. We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.",
    "arxiv_url": "https://arxiv.org/abs/2305.01210",
    "authors": [
      "Jiawei Liu",
      "Chunqiu Steven Xia",
      "Yuyao Wang",
      "Lingming Zhang"
    ],
    "first_author": "Jiawei Liu",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "summary": "本文提出EvalPlus，通过结合LLM驱动的种子输入生成与类型感知变异的大规模测试输入扩充并将HUMANEVAL扩展为HUMANEVAL+，从而更严格地评估LLM生成代码的功能正确性并揭示大量先前未被检测的错误与模型排名变化。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2305.01210v3",
    "published": "2023-05-02",
    "update_time": "2023-10-30",
    "download_time": "2025-12-04 23:29:49"
  },
  {
    "id": "2305.18584",
    "title": "Coeditor: Leveraging Contextual Changes for Multi-round Code Auto-editing",
    "abstract": "Developers often dedicate significant time to maintaining and refactoring existing code. However, most prior work on generative models for code focuses solely on creating new code, overlooking the distinctive needs of editing existing code. In this work, we explore a multi-round code auto-editing setting, aiming to predict edits to a code region based on recent changes within the same codebase. Our model, Coeditor, is a fine-tuned language model specifically designed for code editing tasks. We represent code changes using a line diff format and employ static analysis to form large customized model contexts, ensuring the availability of appropriate information for prediction. We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation. In a simplified single-round, single-edit task, Coeditor significantly outperforms GPT-3.5 and SOTA open-source code completion models (bringing exact-match accuracy from 34.7 up to 60.4), demonstrating the benefits of incorporating editing history for code completion. In a multi-round, multi-edit setting, we observe substantial gains by iteratively conditioning on additional user edits. We have open-sourced our code, data, and model weights to encourage future research and have released a VSCode extension powered by our model for interactive IDE usage.",
    "arxiv_url": "https://arxiv.org/abs/2305.18584",
    "authors": [
      "Jiayi Wei",
      "Greg Durrett",
      "Isil Dillig"
    ],
    "first_author": "Jiayi Wei",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "summary": "该论文提出Coeditor——一种基于repo级行diff编码、扩展CodeT5并采用块稀疏注意力的多轮代码自动编辑模型，同时构建并开源了PYCOMMITS数据集与评测框架，并在多轮编辑任务上显著优于现有基线。",
    "quality": "High",
    "conference": "The Twelfth International Conference on Learning Representations 2024",
    "pdf_url": "https://arxiv.org/pdf/2305.18584v2",
    "published": "2023-05-29",
    "update_time": "2024-04-28",
    "download_time": "2025-12-04 23:30:18"
  },
  {
    "id": "2305.05959",
    "title": "Survey of Code Search Based on Deep Learning",
    "abstract": "Code writing is repetitive and predictable, inspiring us to develop various code intelligence techniques. This survey focuses on code search, that is, to retrieve code that matches a given query by effectively capturing the semantic similarity between the query and code. Deep learning, being able to extract complex semantics information, has achieved great success in this field. Recently, various deep learning methods, such as graph neural networks and pretraining models, have been applied to code search with significant progress. Deep learning is now the leading paradigm for code search. In this survey, we provide a comprehensive overview of deep learning-based code search. We review the existing deep learning-based code search framework which maps query/code to vectors and measures their similarity. Furthermore, we propose a new taxonomy to illustrate the state-of-the-art deep learning-based code search in a three-steps process: query semantics modeling, code semantics modeling, and matching modeling which involves the deep learning model training. Finally, we suggest potential avenues for future research in this promising field.",
    "arxiv_url": "https://arxiv.org/abs/2305.05959",
    "authors": [
      "Yutao Xie",
      "Jiayi Lin",
      "Hande Dong",
      "Lei Zhang",
      "Zhonghai Wu"
    ],
    "first_author": "Yutao Xie",
    "category": [
      "Survey"
    ],
    "field": "Coding Assistant",
    "task": "Code Retrieval",
    "tags": [
      "NL-to-code retrieval",
      "Query intent modeling",
      "Code semantic representation",
      "Graph-based code representations",
      "Transformer-based code encoders",
      "Contrastive and ranking training objectives",
      "Evaluation metrics and benchmarking",
      "Training / pretraining strategies for retrieval"
    ],
    "summary": "本文综述了基于深度学习的代码检索研究，提出了查询语义建模、代码语义建模与匹配建模的三步分类法，并总结了现有方法、评估指标与未来研究方向。",
    "quality": "High",
    "conference": "ACM Transactions on Software Engineering and Methodology 2023",
    "pdf_url": "https://arxiv.org/pdf/2305.05959v2",
    "published": "2023-05-10",
    "update_time": "2023-12-13",
    "download_time": "2025-12-11 17:43:30"
  },
  {
    "id": "2305.17145",
    "title": "Type Prediction With Program Decomposition and Fill-in-the-Type Training",
    "abstract": "TypeScript and Python are two programming languages that support optional type annotations, which are useful but tedious to introduce and maintain. This has motivated automated type prediction: given an untyped program, produce a well-typed output program. Large language models (LLMs) are promising for type prediction, but there are challenges: fill-in-the-middle performs poorly, programs may not fit into the context window, generated types may not type check, and it is difficult to measure how well-typed the output program is. We address these challenges by building OpenTau, a search-based approach for type prediction that leverages large language models. We propose a new metric for type prediction quality, give a tree-based program decomposition that searches a space of generated types, and present fill-in-the-type fine-tuning for LLMs. We evaluate our work with a new dataset for TypeScript type prediction, and show that 47.4% of files type check (14.5% absolute improvement) with an overall rate of 3.3 type errors per file. All code, data, and models are available at: https://github.com/GammaTauAI/opentau.",
    "arxiv_url": "https://arxiv.org/abs/2305.17145",
    "authors": [
      "Federico Cassano",
      "Ming-Ho Yee",
      "Noah Shinn",
      "Arjun Guha",
      "Steven Holtzen"
    ],
    "first_author": "Federico Cassano",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Tree-based program decomposition",
      "Fill-in-the-type fine-tuning",
      "Typedness evaluation metric",
      "Search over type candidates",
      "Type-checking guided ranking",
      "Local type inference",
      "TypeScript gradual migration"
    ],
    "summary": "本文提出OPENTAU，通过将程序递归分解为树状代码块、基于搜索的候选类型生成与一种名为fill-in-the-type的微调方法，并辅以新的typedness评估和TypeScript数据集，有效提升了自动类型注入的可检类型率与类型精确度。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2305.17145v1",
    "published": "2023-05-25",
    "update_time": "2023-05-25",
    "download_time": "2025-12-11 17:53:38"
  }
]