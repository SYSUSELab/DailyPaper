[
  {
    "id": "2409.04183",
    "title": "GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding",
    "abstract": "Programming languages possess rich semantic information - such as data flow - that is represented by graphs and not available from the surface form of source code. Recent code language models have scaled to billions of parameters, but model source code solely as text tokens while ignoring any other structural information. Conversely, models that do encode structural information of code make modifications to the Transformer architecture, limiting their scale and compatibility with pretrained LLMs. In this work, we take the best of both worlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph neural networks and cross-modal alignment technologies to inject the structural information of code into LLMs as an auxiliary task during finetuning. This framework is both model-agnostic and task-agnostic, as it can be applied to any code LLM for any code downstream task, and requires the structural graph data only at training time from a corpus unrelated to the finetuning data, while incurring no cost at inference time over the baseline LLM. Experiments on five code tasks with seven different baseline LLMs ranging in size from 350M to 14B validate the effectiveness of GALLa, demonstrating consistent improvement over the baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder.",
    "arxiv_url": "https://arxiv.org/abs/2409.04183",
    "authors": [
      "Ziyin Zhang",
      "Hang Yu",
      "Shijie Li",
      "Peng Di",
      "Jianguo Li",
      "Rui Wang"
    ],
    "first_author": "Ziyin Zhang",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Graph Alignment",
      "AST Integration",
      "DFG Integration",
      "GNN Adapters",
      "Cross‑Modal Representation"
    ],
    "summary": "该论文提出GALLa框架利用图结构信息对代码LLM进行对齐，从而在多种代码理解与生成任务中提升模型性能。",
    "quality": "High",
    "conference": "ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2409.04183v4",
    "published": "2024-09-06",
    "update_time": "2025-09-23",
    "download_time": "2025-12-10 15:35:38"
  },
  {
    "id": "2409.02038",
    "title": "BEAVER: An Enterprise Benchmark for Text-to-SQL",
    "abstract": "Existing text-to-SQL benchmarks have largely been constructed from web tables with human-generated question-SQL pairs. LLMs typically show strong results on these benchmarks, leading to a belief that LLMs are effective at text-to-SQL tasks. However, how these results transfer to enterprise settings is unclear because tables in enterprise databases might differ substantially from web tables in structure and content. To contend with this problem, we introduce a new dataset BEAVER, the first enterprise text-to-SQL benchmark sourced from real private enterprise data warehouses. This dataset includes natural language queries and their correct SQL statements, which we collected from actual query logs. We then benchmark off-the-shelf LLMs on this dataset. LLMs perform poorly, even when augmented with standard prompt engineering and RAG techniques. We identify three main reasons for the poor performance: (1) schemas of enterprise tables are more complex than the schemas in public data, resulting in SQL-generation tasks intrinsically harder; (2) business-oriented questions are often more complex, requiring joins over multiple tables, aggregations, and nested queries; (3) public LLMs cannot train on private enterprise data warehouses that are not publicly accessible, and therefore it is difficult for the model to learn to solve (1) and (2). We believe BEAVER will facilitate future research in building text-to-SQL systems that perform better in enterprise settings.",
    "arxiv_url": "https://arxiv.org/abs/2409.02038",
    "authors": [
      "Peter Baile Chen",
      "Fabian Wenz",
      "Yi Zhang",
      "Devin Yang",
      "Justin Choi",
      "Nesime Tatbul",
      "Michael Cafarella",
      "Çağatay Demiralp",
      "Michael Stonebraker"
    ],
    "first_author": "Peter Baile Chen",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "Enterprise data warehouse schemas",
      "Anonymized real-user SQL query logs",
      "Column-to-question mapping annotation",
      "Table retrieval / selection for SQL generation",
      "Query complexity analysis (joins, aggregations, nesting)",
      "Retrieval-augmented generation (RAG) evaluation",
      "Error analysis of SQL generation (incorrect columns/values)",
      "Challenges from private-data domain shift"
    ],
    "summary": "本文提出BEAVER——来自真实私有企业数据仓库的首个企业级Text-to-SQL基准数据集，并通过对现成模型的大规模评测与误差分析揭示了企业场景中架构复杂性、复杂业务查询与私有数据不可访问性导致的性能崩溃问题。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2409.02038v2",
    "published": "2024-09-03",
    "update_time": "2025-01-20",
    "download_time": "2025-12-12 22:23:35"
  },
  {
    "id": "2409.12866",
    "title": "SpecEval: Evaluating Code Comprehension in Large Language Models via Program Specifications",
    "abstract": "Large Language models have achieved impressive performance in automated software engineering. Extensive efforts have been made to evaluate the abilities of code LLMs in various aspects, with an increasing number of benchmarks and evaluation frameworks proposed. Apart from the most sought-after capability of code generation, the capability of code comprehension is being granted growing attention. Nevertheless, existing works assessing the code comprehension capability of LLMs exhibit varied limitations. Evaluation frameworks like CRUXEval and REval usually focus on code reasoning tasks over a certain input case, leading to a limited range of execution traces covered, resulting in a loss in code semantics examined and the inability to assess the comprehensive understanding of LLMs concerning the target program. To tackle these challenges, we propose SpecEval, a novel black-box evaluation framework to evaluate code comprehension in LLMs via program specifications. Inspired by the idea that specifications can act as a comprehensive articulation of program behaviors concerning all possible execution traces, we employ formalized program specifications to represent program semantics and perform comprehensive evaluations. In particular, four specification-related tasks are designed meticulously to assess the capability of LLMs from basic to advanced levels. Counterfactual analysis is further conducted to study the performance variance of LLMs under semantics-preserving perturbations. Systematic experiments are conducted on six state-of-the-art LLMs. Extensive experimental results present a below-satisfactory performance of LLMs on specification-related tasks, revealing the limitations of existing LLMs in terms of articulating program semantics with formal specifications. Counterfactual analysis also reveals the sensitivity of LLMs towards semantic-preserving perturbations.",
    "arxiv_url": "https://arxiv.org/abs/2409.12866",
    "authors": [
      "Lezhi Ma",
      "Shangqing Liu",
      "Lei Bu",
      "Shangru Li",
      "Yida Wang",
      "Yang Liu"
    ],
    "first_author": "Lezhi Ma",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Understanding",
    "tags": [
      "Formal program specifications",
      "JML specifications",
      "Specification correctness judgment",
      "Specification selection",
      "Specification infilling",
      "Specification generation",
      "Black-box evaluation",
      "Counterfactual semantic-preserving perturbations",
      "Mutation operators",
      "Semantics-equivalent program variants"
    ],
    "summary": "本文提出SpecEval，一个基于形式化程序规格（JML）的黑盒评估框架，设计四类规格相关任务并构建含204个带可验证规格的Java程序基准，通过对六种主流LLM的系统性实验及反事实扰动分析，揭示模型在用形式化规格表述程序语义方面的不足及对语义保持扰动的敏感性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2409.12866v2",
    "published": "2024-09-19",
    "update_time": "2025-03-22",
    "download_time": "2025-12-12 23:34:48"
  }
]