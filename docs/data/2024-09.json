[
  {
    "id": "2409.12186",
    "title": "Qwen2.5-Coder Technical Report",
    "abstract": "In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes six models: Qwen2.5-Coder-(0.5B/1.5B/3B/7B/14B/32B). As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general and math skills. These models have been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will advance research in code intelligence and, with its permissive licensing, support wider adoption by developers in real-world applications.",
    "arxiv_url": "https://arxiv.org/abs/2409.12186",
    "authors": [
      "Binyuan Hui",
      "Jian Yang",
      "Zeyu Cui",
      "Jiaxi Yang",
      "Dayiheng Liu",
      "Lei Zhang",
      "Tianyu Liu",
      "Jiajun Zhang",
      "Bowen Yu",
      "Keming Lu",
      "Kai Dang",
      "Yang Fan",
      "Yichang Zhang",
      "An Yang",
      "Rui Men",
      "Fei Huang",
      "Bo Zheng",
      "Yibo Miao",
      "Shanghaoran Quan",
      "Yunlong Feng",
      "Xingzhang Ren",
      "Xuancheng Ren",
      "Jingren Zhou",
      "Junyang Lin"
    ],
    "first_author": "Binyuan Hui",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Pre-Training",
    "summary": "本文介绍了Qwen2.5-Coder系列（0.5B–32B），在继承Qwen2.5架构的基础上用超过5.5万亿token的代码与相关文本进行大规模预训练并通过精心设计的指令微调与数据清洗策略显著提升代码生成、补全与推理任务的性能并开源发布。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2409.12186v3",
    "published": "2024-09-18",
    "update_time": "2024-11-12",
    "download_time": "2025-12-04 23:08:44"
  },
  {
    "id": "2409.04183",
    "title": "GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding",
    "abstract": "Programming languages possess rich semantic information - such as data flow - that is represented by graphs and not available from the surface form of source code. Recent code language models have scaled to billions of parameters, but model source code solely as text tokens while ignoring any other structural information. Conversely, models that do encode structural information of code make modifications to the Transformer architecture, limiting their scale and compatibility with pretrained LLMs. In this work, we take the best of both worlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph neural networks and cross-modal alignment technologies to inject the structural information of code into LLMs as an auxiliary task during finetuning. This framework is both model-agnostic and task-agnostic, as it can be applied to any code LLM for any code downstream task, and requires the structural graph data only at training time from a corpus unrelated to the finetuning data, while incurring no cost at inference time over the baseline LLM. Experiments on five code tasks with seven different baseline LLMs ranging in size from 350M to 14B validate the effectiveness of GALLa, demonstrating consistent improvement over the baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder.",
    "arxiv_url": "https://arxiv.org/abs/2409.04183",
    "authors": [
      "Ziyin Zhang",
      "Hang Yu",
      "Shijie Li",
      "Peng Di",
      "Jianguo Li",
      "Rui Wang"
    ],
    "first_author": "Ziyin Zhang",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Graph Alignment",
      "AST Integration",
      "DFG Integration",
      "GNN Adapters",
      "Cross‑Modal Representation"
    ],
    "summary": "该论文提出GALLa框架利用图结构信息对代码LLM进行对齐，从而在多种代码理解与生成任务中提升模型性能。",
    "quality": "High",
    "conference": "ACL 2025",
    "pdf_url": "https://arxiv.org/pdf/2409.04183v4",
    "published": "2024-09-06",
    "update_time": "2025-09-23",
    "download_time": "2025-12-10 15:35:38"
  }
]