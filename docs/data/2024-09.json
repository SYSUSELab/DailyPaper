[
  {
    "id": "2409.12186",
    "title": "Qwen2.5-Coder Technical Report",
    "abstract": "In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes six models: Qwen2.5-Coder-(0.5B/1.5B/3B/7B/14B/32B). As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general and math skills. These models have been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will advance research in code intelligence and, with its permissive licensing, support wider adoption by developers in real-world applications.",
    "arxiv_url": "https://arxiv.org/abs/2409.12186",
    "authors": [
      "Binyuan Hui",
      "Jian Yang",
      "Zeyu Cui",
      "Jiaxi Yang",
      "Dayiheng Liu",
      "Lei Zhang",
      "Tianyu Liu",
      "Jiajun Zhang",
      "Bowen Yu",
      "Keming Lu",
      "Kai Dang",
      "Yang Fan",
      "Yichang Zhang",
      "An Yang",
      "Rui Men",
      "Fei Huang",
      "Bo Zheng",
      "Yibo Miao",
      "Shanghaoran Quan",
      "Yunlong Feng",
      "Xingzhang Ren",
      "Xuancheng Ren",
      "Jingren Zhou",
      "Junyang Lin"
    ],
    "first_author": "Binyuan Hui",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "tag": "Code Pre-Training",
    "summary": "本文介绍了Qwen2.5-Coder系列（0.5B–32B），在继承Qwen2.5架构的基础上用超过5.5万亿token的代码与相关文本进行大规模预训练并通过精心设计的指令微调与数据清洗策略显著提升代码生成、补全与推理任务的性能并开源发布。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2409.12186v3",
    "published": "2024-09-18",
    "update_time": "2024-11-12",
    "download_time": "2025-12-04 23:08:44"
  }
]