[
  {
    "id": "2502.12115",
    "title": "SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?",
    "abstract": "We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \\$1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from \\$50 bug fixes to \\$32,000 feature implementations--and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (https://github.com/openai/SWELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development.",
    "arxiv_url": "https://arxiv.org/abs/2502.12115",
    "authors": [
      "Samuel Miserendino",
      "Michele Wang",
      "Tejal Patwardhan",
      "Johannes Heidecke"
    ],
    "first_author": "Samuel Miserendino",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Freelance job-derived benchmark",
      "End-to-end browser automation tests",
      "Economic payout mapping",
      "Managerial proposal selection evaluation",
      "Full-stack repository-level patching",
      "Triple-verified test validation",
      "Public/private evaluation splits"
    ],
    "summary": "本文构建并公开了一个基于真实Upwork自由职业软件工程任务（共1,488个、总计约100万美元报酬）的基准，采用专业工程师编写并三重验证的端到端测试评估模型对真实全栈修补与管理决策的能力，并报告前沿模型在该基准上的表现。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2502.12115v4",
    "published": "2025-02-17",
    "update_time": "2025-05-29",
    "download_time": "2025-12-11 16:43:43"
  },
  {
    "id": "2502.12466",
    "title": "EquiBench: Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking",
    "abstract": "As large language models (LLMs) become integral to code-related tasks, a central question emerges: Do LLMs truly understand program semantics? We introduce EquiBench, a new benchmark for evaluating LLMs through equivalence checking, i.e., determining whether two programs produce identical outputs for all possible inputs. Unlike prior code generation benchmarks, this task directly tests a model's ability to reason about program semantics. EquiBench consists of 2400 program pairs across four languages and six categories. These pairs are generated through program analysis, compiler scheduling, and superoptimization, ensuring high-confidence labels, nontrivial difficulty, and full automation. We evaluate 19 state-of-the-art LLMs and find that in the most challenging categories, the best accuracies are 63.8% and 76.2%, only modestly above the 50% random baseline. Further analysis reveals that models often rely on syntactic similarity rather than exhibiting robust reasoning about program semantics, highlighting current limitations. Our code and dataset are publicly available at https://github.com/Anjiang-Wei/equibench",
    "arxiv_url": "https://arxiv.org/abs/2502.12466",
    "authors": [
      "Anjiang Wei",
      "Jiannan Cao",
      "Ran Li",
      "Hongyu Chen",
      "Yuhui Zhang",
      "Ziheng Wang",
      "Yuan Liu",
      "Thiago S. F. X. Teixeira",
      "Diyi Yang",
      "Ke Wang",
      "Alex Aiken"
    ],
    "first_author": "Anjiang Wei",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Program Equivalence",
      "Equivalence-checking Benchmark",
      "Dead Code Elimination",
      "Superoptimization-generated Assembly Pairs",
      "Compiler Scheduling Transformations",
      "CUDA tensor scheduling with FP tolerance",
      "Algorithmic/variable-renaming transformations (OJ)",
      "Automated transformation and labeling pipeline",
      "Syntactic-similarity bias analysis",
      "Evaluation of prompting (few-shot, CoT) on semantic reasoning"
    ],
    "summary": "本文提出 EquiBench：一个包含2400对跨四种语言与六类等价/不等价程序对的自动构造基准，用以衡量大模型对程序语义（等价性判定）的推理能力，并通过对19个模型的评估揭示模型常依赖句法相似性而非稳健语义推理，且在最难类别上性能接近随机基线。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2502.12466v3",
    "published": "2025-02-18",
    "update_time": "2025-09-19",
    "download_time": "2025-12-12 21:32:49"
  }
]