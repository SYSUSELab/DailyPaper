[
  {
    "id": "2312.02120",
    "title": "Magicoder: Empowering Code Generation with OSS-Instruct",
    "abstract": "We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open-source references for the production of more realistic and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a new direction for crafting diverse synthetic instruction data for code using abundant open-source references.",
    "arxiv_url": "https://arxiv.org/abs/2312.02120",
    "authors": [
      "Yuxiang Wei",
      "Zhe Wang",
      "Jiawei Liu",
      "Yifeng Ding",
      "Lingming Zhang"
    ],
    "first_author": "Yuxiang Wei",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "tag": "Code Instruction-Tuning",
    "summary": "本文提出OSS-INSTRUCT方法从开源代码片段自动生成多样且可控的指令微调数据，并基于此训练并开源了Magicoder系列7B级代码生成模型，在多项代码生成基准上显著提升表现且部分模型超越ChatGPT。",
    "quality": "High",
    "conference": "ICML 2024",
    "pdf_url": "https://arxiv.org/pdf/2312.02120v2",
    "published": "2023-12-04",
    "update_time": "2024-06-07",
    "download_time": "2025-12-04 23:16:40"
  }
]