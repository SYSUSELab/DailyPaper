[
  {
    "id": "2312.02120",
    "title": "Magicoder: Empowering Code Generation with OSS-Instruct",
    "abstract": "We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open-source references for the production of more realistic and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a new direction for crafting diverse synthetic instruction data for code using abundant open-source references.",
    "arxiv_url": "https://arxiv.org/abs/2312.02120",
    "authors": [
      "Yuxiang Wei",
      "Zhe Wang",
      "Jiawei Liu",
      "Yifeng Ding",
      "Lingming Zhang"
    ],
    "first_author": "Yuxiang Wei",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Instruction-Tuning",
    "summary": "本文提出OSS-INSTRUCT方法从开源代码片段自动生成多样且可控的指令微调数据，并基于此训练并开源了Magicoder系列7B级代码生成模型，在多项代码生成基准上显著提升表现且部分模型超越ChatGPT。",
    "quality": "High",
    "conference": "ICML 2024",
    "pdf_url": "https://arxiv.org/pdf/2312.02120v2",
    "published": "2023-12-04",
    "update_time": "2024-06-07",
    "download_time": "2025-12-04 23:16:40"
  },
  {
    "id": "2312.15223",
    "title": "A Survey on Large Language Models for Software Engineering",
    "abstract": "Software Engineering (SE) is the systematic design, development, maintenance, and management of software applications underpinning the digital infrastructure of our modern world. Very recently, the SE community has seen a rapidly increasing number of techniques employing Large Language Models (LLMs) to automate a broad range of SE tasks. Nevertheless, existing information of the applications, effects, and possible limitations of LLMs within SE is still not well-studied.   In this paper, we provide a systematic survey to summarize the current state-of-the-art research in the LLM-based SE community. We summarize 62 representative LLMs of Code across three model architectures, 15 pre-training objectives across four categories, and 16 downstream tasks across five categories. We then present a detailed summarization of the recent SE studies for which LLMs are commonly utilized, including 947 studies for 112 specific code-related tasks across five crucial phases within the SE workflow. We also discuss several critical aspects during the integration of LLMs into SE, such as empirical evaluation, benchmarking, security and reliability, domain tuning, compressing and distillation. Finally, we highlight several challenges and potential opportunities on applying LLMs for future SE studies, such as exploring domain LLMs and constructing clean evaluation datasets. Overall, our work can help researchers gain a comprehensive understanding about the achievements of the existing LLM-based SE studies and promote the practical application of these techniques. Our artifacts are publicly available and will be continuously updated at the living repository: https://github.com/iSEngLab/AwesomeLLM4SE.",
    "arxiv_url": "https://arxiv.org/abs/2312.15223",
    "authors": [
      "Quanjun Zhang",
      "Chunrong Fang",
      "Yang Xie",
      "Yaxin Zhang",
      "Yun Yang",
      "Weisong Sun",
      "Shengcheng Yu",
      "Zhenyu Chen"
    ],
    "first_author": "Quanjun Zhang",
    "category": [
      "Survey"
    ],
    "field": "LLMs for Software Engineering",
    "task": "Comprehensive survey of LLM applications across the SE workflow",
    "tags": [
      "Model architecture taxonomy for code LLMs",
      "Pre-training objective categorization",
      "Downstream code-task taxonomy",
      "Mapping of 112 code-related tasks to SE phases",
      "Evaluation and benchmarking practices for code LLMs",
      "Security and reliability concerns in LLM4SE",
      "Domain adaptation / domain tuning for code models",
      "Model compression and distillation for code LLMs",
      "Living repository for tracking LLM4SE research"
    ],
    "summary": "本文对将大语言模型应用于软件工程的研究进行了系统综述，汇总了代码领域的模型类别、预训练目标、下游任务与112项具体任务，讨论了评估、基准、可靠性、领域调优与模型压缩等整合问题并提供持续更新的资源库与未来研究方向。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2312.15223v2",
    "published": "2023-12-23",
    "update_time": "2024-09-08",
    "download_time": "2025-12-10 16:16:35"
  },
  {
    "id": "2401.00288",
    "title": "Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit",
    "abstract": "Code intelligence leverages machine learning techniques to extract knowledge from extensive code corpora, with the aim of developing intelligent tools to improve the quality and productivity of computer programming. Currently, there is already a thriving research community focusing on code intelligence, with efforts ranging from software engineering, machine learning, data mining, natural language processing, and programming languages. In this paper, we conduct a comprehensive literature review on deep learning for code intelligence, from the aspects of code representation learning, deep learning techniques, and application tasks. We also benchmark several state-of-the-art neural models for code intelligence, and provide an open-source toolkit tailored for the rapid prototyping of deep-learning-based code intelligence models. In particular, we inspect the existing code intelligence models under the basis of code representation learning, and provide a comprehensive overview to enhance comprehension of the present state of code intelligence. Furthermore, we publicly release the source code and data resources to provide the community with a ready-to-use benchmark, which can facilitate the evaluation and comparison of existing and future code intelligence models (https://xcodemind.github.io). At last, we also point out several challenging and promising directions for future research.",
    "arxiv_url": "https://arxiv.org/abs/2401.00288",
    "authors": [
      "Yao Wan",
      "Yang He",
      "Zhangqian Bi",
      "Jianguo Zhang",
      "Hongyu Zhang",
      "Yulei Sui",
      "Guandong Xu",
      "Hai Jin",
      "Philip S. Yu"
    ],
    "first_author": "Yao Wan",
    "category": [
      "Survey",
      "Benchmark",
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Representation Learning",
    "tags": [
      "Code Representation Learning",
      "Pre-trained Code Language Models",
      "Open-source Benchmarking Toolkit",
      "Cross-task Evaluation (summarization, search, completion, type inference)",
      "AST-path and Structural Code Features",
      "Evaluation Protocols and Metrics"
    ],
    "summary": "本文全面综述了基于深度学习的代码智能研究，提出并开源了用于快速原型和评测的工具平台（NaturalCC），并对多种代码预训练模型在代码摘要、代码检索、代码补全与类型推断等任务上进行了统一基准评测。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2401.00288v1",
    "published": "2023-12-30",
    "update_time": "2023-12-30",
    "download_time": "2025-12-10 16:17:26"
  }
]