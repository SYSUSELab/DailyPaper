[
  {
    "id": "2212.09420",
    "title": "Large Language Models Meet NL2Code: A Survey",
    "abstract": "The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are \"Large Size, Premium Data, Expert Tuning\". In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.",
    "arxiv_url": "https://arxiv.org/abs/2212.09420",
    "authors": [
      "Daoguang Zan",
      "Bei Chen",
      "Fengji Zhang",
      "Dianjie Lu",
      "Bingchao Wu",
      "Bei Guan",
      "Yongji Wang",
      "Jian-Guang Lou"
    ],
    "first_author": "Daoguang Zan",
    "category": [
      "Survey"
    ],
    "field": "Coding Assistant",
    "task": "NL2Code",
    "tags": [
      "NL2Code",
      "Code Generation",
      "Model Comparison",
      "Benchmark Analysis"
    ],
    "summary": "该论文系统综述了27种面向NL2Code任务的大语言模型，总结其技术特征、性能表现与未来挑战。",
    "quality": "High",
    "conference": "ACL 2023",
    "pdf_url": "https://arxiv.org/pdf/2212.09420v2",
    "published": "2022-12-19",
    "update_time": "2023-05-08",
    "download_time": "2025-12-10 15:35:55"
  },
  {
    "id": "2212.10079",
    "title": "A Survey on Pretrained Language Models for Neural Code Intelligence",
    "abstract": "As the complexity of modern software continues to escalate, software engineering has become an increasingly daunting and error-prone endeavor. In recent years, the field of Neural Code Intelligence (NCI) has emerged as a promising solution, leveraging the power of deep learning techniques to tackle analytical tasks on source code with the goal of improving programming efficiency and minimizing human errors within the software industry. Pretrained language models have become a dominant force in NCI research, consistently delivering state-of-the-art results across a wide range of tasks, including code summarization, generation, and translation. In this paper, we present a comprehensive survey of the NCI domain, including a thorough review of pretraining techniques, tasks, datasets, and model architectures. We hope this paper will serve as a bridge between the natural language and programming language communities, offering insights for future research in this rapidly evolving field.",
    "arxiv_url": "https://arxiv.org/abs/2212.10079",
    "authors": [
      "Yichen Xu",
      "Yanqiao Zhu"
    ],
    "first_author": "Yichen Xu",
    "category": [
      "Survey"
    ],
    "field": "Coding Assistant",
    "task": "Code Pre-Training",
    "tags": [
      "Tokenization Strategies",
      "Structure Extraction",
      "Pretraining Paradigms",
      "Neural Code Intelligence"
    ],
    "summary": "该论文系统综述了面向神经代码智能的预训练语言模型在预处理、模型设计与下游任务中的方法与进展。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2212.10079v1",
    "published": "2022-12-20",
    "update_time": "2022-12-20",
    "download_time": "2025-12-10 15:36:12"
  }
]