[
  {
    "id": "2302.08468",
    "title": "LEVER: Learning to Verify Language-to-Code Generation with Execution",
    "abstract": "The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generation probability, and marginalizing over programs with the same execution results. On four datasets across the domains of table QA, math QA and basic Python programming, LEVER consistently improves over the base code LLMs(4.6% to 10.9% with code-davinci-002) and achieves new state-of-the-art results on all of them.",
    "arxiv_url": "https://arxiv.org/abs/2302.08468",
    "authors": [
      "Ansong Ni",
      "Srini Iyer",
      "Dragomir Radev",
      "Ves Stoyanov",
      "Wen-tau Yih",
      "Sida I. Wang",
      "Xi Victoria Lin"
    ],
    "first_author": "Ansong Ni",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "summary": "本文提出LEVER，一种通过对代码LLM生成的候选程序结合其执行结果训练判别器并据此重排序的方法，以提高语言到代码生成的执行准确率，在四个基准上显著优于基线并达成SOTA。",
    "quality": "High",
    "conference": "ICML 2023",
    "pdf_url": "https://arxiv.org/pdf/2302.08468v3",
    "published": "2023-02-16",
    "update_time": "2023-09-01",
    "download_time": "2025-12-04 23:22:58"
  },
  {
    "id": "2302.04026",
    "title": "An Empirical Comparison of Pre-Trained Models of Source Code",
    "abstract": "While a large number of pre-trained models of source code have been successfully developed and applied to a variety of software engineering (SE) tasks in recent years, our understanding of these pre-trained models is arguably fairly limited. With the goal of advancing our understanding of these models, we perform the first systematic empirical comparison of 19 recently-developed pre-trained models of source code on 13 SE tasks. To gain additional insights into these models, we adopt a recently-developed 4-dimensional categorization of pre-trained models, and subsequently investigate whether there are correlations between different categories of pre-trained models and their performances on different SE tasks.",
    "arxiv_url": "https://arxiv.org/abs/2302.04026",
    "authors": [
      "Changan Niu",
      "Chuanyi Li",
      "Vincent Ng",
      "Dongxiao Chen",
      "Jidong Ge",
      "Bin Luo"
    ],
    "first_author": "Changan Niu",
    "category": [
      "Empirical"
    ],
    "field": "New Field: Model Evaluation for SE",
    "task": "New Task: Pre-trained Model Comparison",
    "tags": [
      "Empirical Comparison",
      "Code Pre-trained Models",
      "Cross-task Evaluation",
      "Model Categorization"
    ],
    "summary": "该论文系统性地对19种源码预训练模型在13项软件工程任务上的表现进行了实证比较，并分析其类别与性能之间的关联。",
    "quality": "High",
    "conference": "ICSE 2023",
    "pdf_url": "https://arxiv.org/pdf/2302.04026v1",
    "published": "2023-02-08",
    "update_time": "2023-02-08",
    "download_time": "2025-12-10 15:36:31"
  },
  {
    "id": "2302.12163",
    "title": "Do Machine Learning Models Produce TypeScript Types That Type Check?",
    "abstract": "Type migration is the process of adding types to untyped code to gain assurance at compile time. TypeScript and other gradual type systems facilitate type migration by allowing programmers to start with imprecise types and gradually strengthen them. However, adding types is a manual effort and several migrations on large, industry codebases have been reported to have taken several years. In the research community, there has been significant interest in using machine learning to automate TypeScript type migration. Existing machine learning models report a high degree of accuracy in predicting individual TypeScript type annotations. However, in this paper we argue that accuracy can be misleading, and we should address a different question: can an automatic type migration tool produce code that passes the TypeScript type checker?   We present TypeWeaver, a TypeScript type migration tool that can be used with an arbitrary type prediction model. We evaluate TypeWeaver with three models from the literature: DeepTyper, a recurrent neural network; LambdaNet, a graph neural network; and InCoder, a general-purpose, multi-language transformer that supports fill-in-the-middle tasks. Our tool automates several steps that are necessary for using a type prediction model, (1) importing types for a project's dependencies; (2) migrating JavaScript modules to TypeScript notation; (3) inserting predicted type annotations into the program to produce TypeScript when needed; and (4) rejecting non-type predictions when needed.   We evaluate TypeWeaver on a dataset of 513 JavaScript packages, including packages that have never been typed before. With the best type prediction model, we find that only 21% of packages type check, but more encouragingly, 69% of files type check successfully.",
    "arxiv_url": "https://arxiv.org/abs/2302.12163",
    "authors": [
      "Ming-Ho Yee",
      "Arjun Guha"
    ],
    "first_author": "Ming-Ho Yee",
    "category": [
      "Technical",
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "Type migration automation",
      "Type annotation weaving",
      "Type checking validation",
      "CommonJS to ECMAScript module conversion",
      "Dependency .d.ts importing",
      "Filtering non-type model outputs",
      "Empirical evaluation on npm packages",
      "Analysis of trivial vs. meaningful annotations"
    ],
    "summary": "本文提出 TypeWeaver 工具并在 513 个 npm 包上评估多种机器学习类型预测模型，展示了在实际迁移中需要处理依赖类型导入、模块系统转换和非类型预测过滤等工程步骤，且尽管单注解准确率高，只有 21% 的包（69% 的文件）最终通过 TypeScript 类型检查。",
    "quality": "High",
    "conference": "European Conference on Object-Oriented Programming (ECOOP) 2023",
    "pdf_url": "https://arxiv.org/pdf/2302.12163v2",
    "published": "2023-02-23",
    "update_time": "2023-07-11",
    "download_time": "2025-12-11 17:52:33"
  }
]