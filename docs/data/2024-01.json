[
  {
    "id": "2401.04621",
    "title": "DebugBench: Evaluating Debugging Capability of Large Language Models",
    "abstract": "Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and four open-source models in a zero-shot scenario. We find that (1) while closed-source models exhibit inferior debugging performance compared to humans, open-source models relatively lower pass rate scores; (2) the complexity of debugging notably fluctuates depending on the bug category; (3) incorporating runtime feedback has a clear impact on debugging performance which is not always helpful. As an extension, we also compare LLM debugging and code generation, revealing a strong correlation between them for closed-source models. These findings will benefit the development of LLMs in debugging.",
    "arxiv_url": "https://arxiv.org/abs/2401.04621",
    "authors": [
      "Runchu Tian",
      "Yining Ye",
      "Yujia Qin",
      "Xin Cong",
      "Yankai Lin",
      "Yinxu Pan",
      "Yesai Wu",
      "Haotian Hui",
      "Weichuan Liu",
      "Zhiyuan Liu",
      "Maosong Sun"
    ],
    "first_author": "Runchu Tian",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Bug Repair",
    "tags": [
      "Debugging benchmark",
      "Fine-grained bug taxonomy",
      "Automated bug implantation",
      "Data-leakage-aware curation",
      "Multi-language code snippets",
      "Runtime-feedback evaluation",
      "Zero-shot LLM assessment",
      "Error-type difficulty analysis",
      "Debugging vs. code-generation correlation",
      "Test-suite-based automatic evaluation"
    ],
    "summary": "该论文提出DebugBench——一个包含4,253个实例、覆盖18种细分错误并在C++/Java/Python上构建的数据集，用以无泄露地评估LLM的调试能力，并通过零样本实验分析不同错误类别与运行时反馈对调试性能的影响。",
    "quality": "High",
    "conference": "ACL 2024",
    "pdf_url": "https://arxiv.org/pdf/2401.04621v3",
    "published": "2024-01-09",
    "update_time": "2024-06-06",
    "download_time": "2025-12-11 17:01:33"
  },
  {
    "id": "2401.03065",
    "title": "CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution",
    "abstract": "We present CRUXEval (Code Reasoning, Understanding, and eXecution Evaluation), a benchmark consisting of 800 Python functions (3-13 lines). Each function comes with an input-output pair, leading to two natural tasks: input prediction and output prediction. First, we propose a generic recipe for generating our execution benchmark which can be used to create future variation of the benchmark. Second, we evaluate twenty code models on our benchmark and discover that many recent high-scoring models on HumanEval do not show the same improvements on our benchmark. Third, we show that simple CoT and fine-tuning schemes can improve performance on our benchmark but remain far from solving it. The best setup, GPT-4 with chain of thought (CoT), achieves a pass@1 of 75% and 81% on input and output prediction, respectively. In contrast, Code Llama 34B achieves a pass@1 of 50% and 46% on input and output prediction, highlighting the gap between open and closed source models. As no model is close to acing CRUXEval, we provide examples of consistent GPT-4 failures on simple programs as a lens into its code reasoning capabilities and areas for improvement.",
    "arxiv_url": "https://arxiv.org/abs/2401.03065",
    "authors": [
      "Alex Gu",
      "Baptiste Rozière",
      "Hugh Leather",
      "Armando Solar-Lezama",
      "Gabriel Synnaeve",
      "Sida I. Wang"
    ],
    "first_author": "Alex Gu",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Output prediction (execution)",
      "Input prediction (reverse execution)",
      "Execution tracing of short Python programs",
      "Generate-and-filter benchmark construction",
      "Chain-of-thought prompting evaluation",
      "Fine-tuning on I/O assertion data",
      "Open-source vs closed-source model performance gap",
      "Analysis of consistent model failure cases"
    ],
    "summary": "本文提出CRUXEval，一个包含800个短Python函数的基准（包含输出预测与输入预测两项任务），并在20个代码模型上评估，展示了模型在代码执行推理上的短板以及链式思维与微调的改进效果。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2401.03065v1",
    "published": "2024-01-05",
    "update_time": "2024-01-05",
    "download_time": "2025-12-12 23:29:31"
  }
]