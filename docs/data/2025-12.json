[
  {
    "id": "2512.01939",
    "title": "An Empirical Study of Agent Developer Practices in AI Agent Frameworks",
    "abstract": "The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers.",
    "arxiv_url": "https://arxiv.org/abs/2512.01939",
    "authors": [
      "Yanlin Wang",
      "Xinyi Xu",
      "Jiachi Chen",
      "Tingting Bi",
      "Wenchao Gu",
      "Zibin Zheng"
    ],
    "first_author": "Yanlin Wang",
    "category": [
      "Empirical"
    ],
    "field": "Agent Frameworks & Developer Practices",
    "task": "Developer practices and SDLC challenges in LLM-based agent frameworks",
    "tags": [
      "Mining GitHub discussions",
      "Agent framework adoption patterns",
      "SDLC challenges taxonomy",
      "Logic control & task termination",
      "Tool integration and API compatibility",
      "Context retention and memory management",
      "Version and dependency compatibility",
      "Comparative framework evaluation (learning cost, efficiency, abstraction, performance, maintainability)",
      "Multi-framework composition"
    ],
    "summary": "本文通过分析1,575个基于LLM的代理项目及近1.2万条开发者讨论，构建了代理开发面向SDLC的挑战分类、识别十个代表性框架并基于五维度比较其对开发者需求的满足情况，揭示常见问题并提出改进建议。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.01939v1",
    "published": "2025-12-01",
    "update_time": "2025-12-01",
    "download_time": "2025-12-16 14:30:54"
  },
  {
    "id": "2512.01690",
    "title": "Generating REST API Tests With Descriptive Names",
    "abstract": "Automated test generation has become a key technique for ensuring software quality, particularly in modern API-based architectures. However, automatically generated test cases are typically assigned non-descriptive names (e.g., test0, test1), which reduces their readability and hinders their usefulness during comprehension and maintenance. In this work, we present three novel deterministic techniques to generate REST API test names. We then compare eight techniques in total for generating descriptive names for REST API tests automatically produced by the fuzzer EvoMaster, using 10 test cases generated for 9 different open-source APIs. The eight techniques include rule-based heuristics and large language model (LLM)-based approaches. Their effectiveness was empirically evaluated through two surveys (involving up to 39 people recruited via LinkedIn). Our results show that a rule-based approach achieves the highest clarity ratings among deterministic methods, performs on par with state-of-the-art LLM-based models such as Gemini and GPT-4o, and significantly outperforms GPT-3.5.   To further evaluate the practical impact of our results, an industrial case study was carried out with practitioners who actively use EvoMaster at Volkswagen AG. A developer questionnaire was then carried out based on the use of EvoMaster on four different APIs by four different users, for a total of 74 evaluated test cases. Feedback from practitioners further confirms that descriptive names produced by this approach improve test suite readability.   These findings highlight that lightweight, deterministic techniques can serve as effective alternatives to computationally expensive and security-sensitive LLM-based approaches for automated system-level test naming, providing a practical step toward more developer-friendly API test generation.",
    "arxiv_url": "https://arxiv.org/abs/2512.01690",
    "authors": [
      "Philip Garrett",
      "Juan P. Galeotti",
      "Andrea Arcuri",
      "Alexander Poth",
      "Olsi Rrjolli"
    ],
    "first_author": "Philip Garrett",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Test case naming",
      "Deterministic naming heuristics",
      "REST API semantics (endpoints, methods, params, status codes)",
      "EvoMaster integration",
      "Human readability evaluation",
      "LLM-based naming comparison",
      "Test-suite organization and sorting",
      "Industrial case study / practitioner feedback"
    ],
    "summary": "本文提出三种确定性方法为自动生成的REST API测试用例生成描述性名称，并通过问卷用户研究与大众及工业案例将这些规则方法与多种基于LLM的方法对比，结果表明规则方法在可读性上优于GPT‑3.5、与更先进模型相当且已集成到EvoMaster中提升了测试套件的可读性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.01690v1",
    "published": "2025-12-01",
    "update_time": "2025-12-01",
    "download_time": "2025-12-16 14:31:28"
  },
  {
    "id": "2512.01609",
    "title": "GPTrace: Effective Crash Deduplication Using LLM Embeddings",
    "abstract": "Fuzzing is a highly effective method for uncovering software vulnerabilities, but analyzing the resulting data typically requires substantial manual effort. This is amplified by the fact that fuzzing campaigns often find a large number of crashing inputs, many of which share the same underlying bug. Crash deduplication is the task of finding such duplicate crashing inputs and thereby reducing the data that needs to be examined. Many existing deduplication approaches rely on comparing stack traces or other information that is collected when a program crashes. Although various metrics for measuring the similarity of such pieces of information have been proposed, many do not yield satisfactory deduplication results. In this work, we present GPTrace, a deduplication workflow that leverages a large language model to evaluate the similarity of various data sources associated with crashes by computing embedding vectors and supplying those as input to a clustering algorithm. We evaluate our approach on over 300 000 crashing inputs belonging to 50 ground truth labels from 14 different targets. The deduplication results produced by GPTrace show a noticeable improvement over hand-crafted stack trace comparison methods and even more complex state-of-the-art approaches that are less flexible.",
    "arxiv_url": "https://arxiv.org/abs/2512.01609",
    "authors": [
      "Patrick Herter",
      "Vincent Ahlrichs",
      "Ridvan Açilan",
      "Julian Horsch"
    ],
    "first_author": "Patrick Herter",
    "category": [
      "Technical"
    ],
    "field": "Software Testing",
    "task": "Testing automation",
    "tags": [
      "Crash Deduplication",
      "Fuzzing Crash Triage",
      "Stack Trace Preprocessing",
      "ASan Report Sanitization",
      "Embedding-based Crash Similarity",
      "Summed Normalized Embeddings",
      "HDBSCAN-DBSCAN Hybrid Clustering"
    ],
    "summary": "GPTrace提出通过对栈跟踪和ASan报告计算并归一化求和的嵌入向量，结合密度聚类实现模糊测试崩溃去重，从而显著优于传统手工栈跟踪比对方法。",
    "quality": "High",
    "conference": "IEEE/ACM International Conference on Software Engineering (ICSE 2026)",
    "pdf_url": "https://arxiv.org/pdf/2512.01609v1",
    "published": "2025-12-01",
    "update_time": "2025-12-01",
    "download_time": "2025-12-16 14:32:38"
  },
  {
    "id": "2512.01396",
    "title": "BackportBench: A Multilingual Benchmark for Automated Backporting of Patches",
    "abstract": "Many modern software projects evolve rapidly to incorporate new features and security patches. It is important for users to update their dependencies to safer versions, but many still use older, vulnerable package versions because upgrading can be difficult and may break their existing codebase. Software developers can mitigate this problem by backporting security patches to older releases. However, manually backporting is time-consuming and error-prone. The effectiveness of existing automated backporting techniques on general software remains unclear since they typically target only code-hunk or function-level patch porting scenarios and are evaluated with imperfect metrics.   To facilitate the development and evaluation of automated backporting techniques, we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem. BackportBench is a multilingual benchmark that contains 202 patch backporting problems from PyPI, Maven, and npm, each with executable Docker environments and relevant test cases. We evaluated existing patch porting methods and LLM-based techniques that have the potential to adapt to this task using BackportBench. The results show that the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes. However, the performance varies across different programming languages. Based on the findings, we draw several implications for researchers and software practitioners in future work on automated backporting.",
    "arxiv_url": "https://arxiv.org/abs/2512.01396",
    "authors": [
      "Zhiqing Zhong",
      "Jiaming Huang",
      "Pinjia He"
    ],
    "first_author": "Zhiqing Zhong",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Repair",
    "tags": [
      "Patch Backporting",
      "Multilingual (Python/Java/JavaScript)",
      "Repository-level Patch Adaptation",
      "Executable Docker Environments",
      "Test-suite Validation",
      "Agentic LLM Workflows",
      "Cross-file Incompatibility Handling",
      "Evaluation Metrics for Backporting"
    ],
    "summary": "本文提出BackportBench——一个包含来自PyPI、Maven和npm的202个多语言可执行补丁回移任务的基准，并用测试用例评估传统补丁迁移方法与多种（包括agentic）LLM驱动方法的效果与差异。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.01396v1",
    "published": "2025-12-01",
    "update_time": "2025-12-01",
    "download_time": "2025-12-16 14:33:18"
  },
  {
    "id": "2512.01356",
    "title": "LAURA: Enhancing Code Review Generation with Context-Enriched Retrieval-Augmented LLM",
    "abstract": "Code review is critical for ensuring software quality and maintainability. With the rapid growth in software scale and complexity, code review has become a bottleneck in the development process because of its time-consuming and knowledge-intensive nature and the shortage of experienced developers willing to review code. Several approaches have been proposed for automatically generating code reviews based on retrieval, neural machine translation, pre-trained models, or large language models (LLMs). These approaches mainly leverage historical code changes and review comments. However, a large amount of crucial information for code review, such as the context of code changes and prior review knowledge, has been overlooked. This paper proposes an LLM-based review knowledge-augmented, context-aware framework for code review generation, named LAURA. The framework integrates review exemplar retrieval, context augmentation, and systematic guidance to enhance the performance of ChatGPT-4o and DeepSeek v3 in generating code review comments. Besides, given the extensive low-quality reviews in existing datasets, we also constructed a high-quality dataset. Experimental results show that for both models, LAURA generates review comments that are either completely correct or at least helpful to developers in 42.2% and 40.4% of cases, respectively, significantly outperforming SOTA baselines. Furthermore, our ablation studies demonstrate that all components of LAURA contribute positively to improving comment quality.",
    "arxiv_url": "https://arxiv.org/abs/2512.01356",
    "authors": [
      "Yuxin Zhang",
      "Yuxia Zhang",
      "Zeyu Sun",
      "Yanjie Jiang",
      "Hui Liu"
    ],
    "first_author": "Yuxin Zhang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Maintenance",
    "task": "Code Review",
    "tags": [
      "Context-extended diffs",
      "PR metadata augmentation",
      "Review exemplar retrieval",
      "Retrieval-augmented generation (RAG) for reviews",
      "Transformer-based code embeddings",
      "AST-based context expansion",
      "Systematic review prompting",
      "High-quality diff-comment dataset construction",
      "Human + LLM dual evaluation (I-Score/IH-Score)",
      "Ablation study of components"
    ],
    "summary": "本文提出LAURA框架，通过补充PR上下文、检索相似变更及其审查范例并结合系统化提示来增强LLM生成代码审查注释，并构建并公开了一个高质量的diff‑comment检索数据库以验证性能提升。",
    "quality": "High",
    "conference": "IEEE/ACM International Conference on Automated Software Engineering (ASE 2025)",
    "pdf_url": "https://arxiv.org/pdf/2512.01356v1",
    "published": "2025-12-01",
    "update_time": "2025-12-01",
    "download_time": "2025-12-16 14:33:59"
  },
  {
    "id": "2512.01255",
    "title": "Large Language Models Cannot Reliably Detect Vulnerabilities in JavaScript: The First Systematic Benchmark and Evaluation",
    "abstract": "Researchers have proposed numerous methods to detect vulnerabilities in JavaScript, especially those assisted by Large Language Models (LLMs). However, the actual capability of LLMs in JavaScript vulnerability detection remains questionable, necessitating systematic evaluation and comprehensive benchmarks. Unfortunately, existing benchmarks suffer from three critical limitations: (1) incomplete coverage, such as covering a limited subset of CWE types; (2) underestimation of LLM capabilities caused by unreasonable ground truth labeling; and (3) overestimation due to unrealistic cases such as using isolated vulnerable files rather than complete projects.   In this paper, we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection that directly address these limitations: (1) comprehensiveness, (2) no underestimation, and (3) no overestimation. Guided by these principles, we propose FORGEJS, the first automatic benchmark generation framework for evaluating LLMs' capability in JavaScript vulnerability detection. Then, we use FORGEJS to construct ARENAJS-the first systematic benchmark for LLM-based JavaScript vulnerability detection-and further propose JUDGEJS, an automatic evaluation framework.   We conduct the first systematic evaluation of LLMs for JavaScript vulnerability detection, leveraging JUDGEJS to assess seven popular commercial LLMs on ARENAJS. The results show that LLMs not only exhibit limited reasoning capabilities, but also suffer from severe robustness defects, indicating that reliable JavaScript vulnerability detection with LLMs remains an open challenge.",
    "arxiv_url": "https://arxiv.org/abs/2512.01255",
    "authors": [
      "Qingyuan Fei",
      "Xin Liu",
      "Song Li",
      "Shujiang Wu",
      "Jianwei Hou",
      "Ping Chen",
      "Zifeng Kang"
    ],
    "first_author": "Qingyuan Fei",
    "category": [
      "Benchmark",
      "Empirical",
      "Technical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "JavaScript vulnerability benchmark",
      "Comprehensive CWE coverage",
      "Project-level vs. function-level evaluation",
      "Automatic benchmark generation",
      "Automated evaluation pipeline",
      "Semantic equivalence / fuzzy label alignment",
      "Vulnerability localization (file/function/line)",
      "Taint-flow and dependency reasoning assessment",
      "Robustness testing via code augmentations",
      "False positive rate and deployment-oriented metrics"
    ],
    "summary": "本文提出遵循全面性、避免低估和避免高估三项原则的自动化基准生成与评估体系，并基于该体系构建了系统性JavaScript漏洞检测基准与评估框架，实证评估多款主流商业大模型后发现其在推理能力、鲁棒性和可部署性方面表现不足且不可靠。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.01255v1",
    "published": "2025-12-01",
    "update_time": "2025-12-01",
    "download_time": "2025-12-16 14:34:37"
  },
  {
    "id": "2512.02795",
    "title": "Towards Observation Lakehouses: Living, Interactive Archives of Software Behavior",
    "abstract": "Code-generating LLMs are trained largely on static artifacts (source, comments, specifications) and rarely on materializations of run-time behavior. As a result, they readily internalize buggy or mislabeled code. Since non-trivial semantic properties are undecidable in general, the only practical way to obtain ground-truth functionality is by dynamic observation of executions. In prior work, we addressed representation with Sequence Sheets, Stimulus-Response Matrices (SRMs), and Stimulus-Response Cubes (SRCs) to capture and compare behavior across tests, implementations, and contexts. These structures make observation data analyzable offline and reusable, but they do not by themselves provide persistence, evolution, or interactive analytics at scale. In this paper, therefore, we introduce observation lakehouses that operationalize continual SRCs: a tall, append-only observations table storing every actuation (stimulus, response, context) and SQL queries that materialize SRC slices on demand. Built on Apache Parquet + Iceberg + DuckDB, the lakehouse ingests data from controlled pipelines (LASSO) and CI pipelines (e.g., unit test executions), enabling n-version assessment, behavioral clustering, and consensus oracles without re-execution. On a 509-problem benchmark, we ingest $\\approx$8.6M observation rows ($<$51MiB) and reconstruct SRM/SRC views and clusters in $<$100ms on a laptop, demonstrating that continual behavior mining is practical without a distributed cluster of machines. This makes behavioral ground truth first-class alongside other run-time data and provides an infrastructure path toward behavior-aware evaluation and training. The Observation Lakehouse, together with the accompanying dataset, is publicly available as an open-source project on GitHub: https://github.com/SoftwareObservatorium/observation-lakehouse",
    "arxiv_url": "https://arxiv.org/abs/2512.02795",
    "authors": [
      "Marcus Kessel"
    ],
    "first_author": "Marcus Kessel",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Software Testing",
    "task": "Testing automation",
    "tags": [
      "Observation Lakehouse",
      "Stimulus-Response Matrix (SRM)",
      "Stimulus-Response Cube (SRC)",
      "Sequence Sheets / Invocation step records",
      "Append-only observations table",
      "Parquet + Iceberg + DuckDB stack",
      "Schema evolution for continual observations",
      "Partitioned SRM reconstruction",
      "Behavioral clustering",
      "Consensus oracle / n-version assessment",
      "Interactive SQL-based analytics"
    ],
    "summary": "本文提出Observation Lakehouse，一种基于Parquet/Iceberg/DuckDB的数据湖仓架构，用细粒度调用步记录持续存储并按需重建SRM/SRC视图，从而在单机上高效完成行为聚类、n版评估与共识判定，推动运行时行为成为训练与评估的一等公民。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.02795v1",
    "published": "2025-12-02",
    "update_time": "2025-12-02",
    "download_time": "2025-12-16 14:35:14"
  },
  {
    "id": "2512.02750",
    "title": "\"Can you feel the vibes?\": An exploration of novice programmer engagement with vibe coding",
    "abstract": "Emerging alongside generative AI and the broader trend of AI-assisted coding, the term \"vibe coding\" refers to creating software via natural language prompts rather than direct code authorship. This approach promises to democratize software development, but its educational implications remain underexplored. This paper reports on a one-day educational hackathon investigating how novice programmers and mixed-experience teams engage with vibe coding. We organized an inclusive event at a Brazilian public university with 31 undergraduate participants from computing and non-computing disciplines, divided into nine teams. Through observations, an exit survey, and semi-structured interviews, we examined creative processes, tool usage patterns, collaboration dynamics, and learning outcomes. Findings reveal that vibe coding enabled rapid prototyping and cross-disciplinary collaboration, with participants developing prompt engineering skills and delivering functional demonstrations within time constraints. However, we observed premature convergence in ideation, uneven code quality requiring rework, and limited engagement with core software engineering practices. Teams adopted sophisticated workflows combining multiple AI tools in pipeline configurations, with human judgment remaining essential for critical refinement. The short format (9 hours) proved effective for confidence-building among newcomers while accommodating participants with limited availability. We conclude that vibe coding hackathons can serve as valuable low-stakes learning environments when coupled with explicit scaffolds for divergent thinking, critical evaluation of AI outputs, and realistic expectations about production quality.",
    "arxiv_url": "https://arxiv.org/abs/2512.02750",
    "authors": [
      "Kiev Gama",
      "Filipe Calegario",
      "Victoria Jackson",
      "Alexander Nolte",
      "Luiz Augusto Morais",
      "Vinicius Garcia"
    ],
    "first_author": "Kiev Gama",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "tags": [
      "Vibe Coding",
      "Hackathon Design",
      "Prompt Engineering",
      "Novice Programmer Engagement",
      "Cross-disciplinary Collaboration",
      "Toolchain Orchestration",
      "Premature Ideation Convergence",
      "Human-in-the-loop Validation"
    ],
    "summary": "本文通过对一次为期一天、面向新手的vibe coding黑客松的观察、问卷和访谈，探讨参与者如何用自然语言提示与多种AI工具协作快速原型、学到的提示工程技能、协作动态与学习成效，发现活动能提高信心与跨学科合作但也带来早熟收敛、代码质量参差及对软件工程实践的有限参与。",
    "quality": "Middle",
    "conference": "International Conference on Software Engineering, Education Track (SEET) 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.02750v1",
    "published": "2025-12-02",
    "update_time": "2025-12-02",
    "download_time": "2025-12-16 14:35:58"
  },
  {
    "id": "2512.03421",
    "title": "Exploring the Potential and Limitations of Large Language Models for Novice Program Fault Localization",
    "abstract": "Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. Traditional methods like Spectrum-Based Fault Localization (SBFL) and Mutation-Based Fault Localization (MBFL) help identify faults but often lack the ability to understand code context, making them less effective for beginners. In recent years, Large Language Models (LLMs) have shown promise in overcoming these limitations by utilizing their ability to understand program syntax and semantics. LLM-based fault localization provides more accurate and context-aware results than traditional techniques. This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. Advanced models with reasoning capabilities, such as OpenAI o3 and DeepSeekR1, achieve superior accuracy with minimal reliance on prompt engineering. In contrast, models without reasoning capabilities, like GPT-4, require carefully designed prompts to maintain performance. While LLMs perform well in simple fault localization, their accuracy decreases as problem difficulty increases, though top models maintain robust performance in the BugT dataset. Over-reasoning is another challenge, where some models generate excessive explanations that hinder fault localization clarity. Additionally, the computational cost of deploying LLMs remains a significant barrier for real-time debugging. LLM's explanations demonstrate significant value for novice programmer assistance, with one-year experience participants consistently rating them highly. Our findings demonstrate the potential of LLMs to improve debugging efficiency while stressing the need for further refinement in their reasoning and computational efficiency for practical adoption.",
    "arxiv_url": "https://arxiv.org/abs/2512.03421",
    "authors": [
      "Hexiang Xu",
      "Hengyuan Liu",
      "Yonghao Wu",
      "Xiaolan Kang",
      "Xiang Chen",
      "Yong Liu"
    ],
    "first_author": "Hexiang Xu",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Bug Localization",
    "tags": [
      "Novice-program fault dataset construction",
      "LLM vs SBFL/MBFL comparison",
      "Prompt engineering sensitivity",
      "Difficulty-level performance analysis",
      "Over-reasoning in model explanations",
      "User study on explanation usefulness",
      "Model size versus performance analysis",
      "Computational cost and real-time feasibility"
    ],
    "summary": "本文通过构建防止数据泄露的新数据集并在多套公开与自建数据上系统评估多款闭源/开源大模型，分析其在新手程序错误定位中的性能、提示工程敏感性、问题难度影响、过度推理与计算成本等局限，并通过用户研究验证解释性输出对初学者的教学价值。",
    "quality": "High",
    "conference": "Journal of Systems and Software",
    "pdf_url": "https://arxiv.org/pdf/2512.03421v1",
    "published": "2025-12-03",
    "update_time": "2025-12-03",
    "download_time": "2025-12-16 14:37:42"
  },
  {
    "id": "2512.03420",
    "title": "HarnessAgent: Scaling Automatic Fuzzing Harness Construction with Tool-Augmented LLM Pipelines",
    "abstract": "Large language model (LLM)-based techniques have achieved notable progress in generating harnesses for program fuzzing. However, applying them to arbitrary functions (especially internal functions) \\textit{at scale} remains challenging due to the requirement of sophisticated contextual information, such as specification, dependencies, and usage examples. State-of-the-art methods heavily rely on static or incomplete context provisioning, causing failure of generating functional harnesses. Furthermore, LLMs tend to exploit harness validation metrics, producing plausible yet logically useless code. % Therefore, harness generation across large and diverse projects continues to face challenges in reliable compilation, robust code retrieval, and comprehensive validation.   To address these challenges, we present HarnessAgent, a tool-augmented agentic framework that achieves fully automated, scalable harness construction over hundreds of OSS-Fuzz targets. HarnessAgent introduces three key innovations: 1) a rule-based strategy to identify and minimize various compilation errors; 2) a hybrid tool pool for precise and robust symbol source code retrieval; and 3) an enhanced harness validation pipeline that detects fake definitions. We evaluate HarnessAgent on 243 target functions from OSS-Fuzz projects (65 C projects and 178 C++ projects). It improves the three-shot success rate by approximately 20\\% compared to state-of-the-art techniques, reaching 87\\% for C and 81\\% for C++. Our one-hour fuzzing results show that more than 75\\% of the harnesses generated by HarnessAgent increase the target function coverage, surpassing the baselines by over 10\\%. In addition, the hybrid tool-pool system of HarnessAgent achieves a response rate of over 90\\% for source code retrieval, outperforming Fuzz Introspector by more than 30\\%.",
    "arxiv_url": "https://arxiv.org/abs/2512.03420",
    "authors": [
      "Kang Yang",
      "Yunhang Zhang",
      "Zichuan Li",
      "Guanhong Tao",
      "Jun Xu",
      "Xiaojing Liao"
    ],
    "first_author": "Kang Yang",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Fuzzing harness generation",
      "Compilation-error triage and routing",
      "Hybrid symbol/source retrieval",
      "LSP- and AST-based retrieval backend",
      "Agentic tool-augmented pipeline",
      "Detection of fake definitions during validation",
      "Scalable internal-function targeting",
      "Coverage-driven harness evaluation"
    ],
    "summary": "HarnessAgent 提出了一种工具增强的 agent 框架，通过编译错误分流、混合符号/源码检索与加强的验证管道，实现对大型 C/C++ 代码库中内部函数的可扩展自动化模糊测试 harness 生成，并在 243 个目标上显著提升生成成功率与 fuzz 覆盖率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.03420v3",
    "published": "2025-12-03",
    "update_time": "2025-12-11",
    "download_time": "2025-12-16 14:38:19"
  },
  {
    "id": "2512.05073",
    "title": "David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?",
    "abstract": "Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated agentic AI framework on NVIDIA's Comprehensive Verilog Design Problems(CVDP) benchmark. Results show that agentic workflows: through task decomposition, iterative feedback, and correction - not only unlock near-LLM performance at a fraction of the cost but also create learning opportunities for agents, paving the way for efficient, adaptive solutions in complex design tasks.",
    "arxiv_url": "https://arxiv.org/abs/2512.05073",
    "authors": [
      "Shashwat Shankar",
      "Subhranshu Pandey",
      "Innocent Dengkhw Mochahari",
      "Bhabesh Mali",
      "Animesh Basak Chowdhury",
      "Sukanta Bhattacharjee",
      "Chandan Karfa"
    ],
    "first_author": "Shashwat Shankar",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "Agentic task decomposition",
      "SLM-aware prompt engineering",
      "Verilog RTL generation",
      "I/O port usage checking",
      "Iterative validation and rollback",
      "Token-budgeted context management",
      "CocoTB-based functional testing",
      "Energy‑efficiency (intelligence-per-watt) tradeoffs"
    ],
    "summary": "该论文提出了一种面向小模型（SLM）的异构 agentic AI 框架，通过任务分解、结构化提示和迭代校验在 NVIDIA 的 CVDP Verilog 设计基准上实现了接近大型模型的硬件设计生成与理解性能，从而显著降低计算与能耗成本。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.05073v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-16 14:39:06"
  },
  {
    "id": "2512.04680",
    "title": "Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap",
    "abstract": "Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this paper aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI's within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.",
    "arxiv_url": "https://arxiv.org/abs/2512.04680",
    "authors": [
      "Jialong Li",
      "Mingyue Zhang",
      "Nianyu Li",
      "Danny Weyns",
      "Zhi Jin",
      "Kenji Tei"
    ],
    "first_author": "Jialong Li",
    "category": [
      "Survey"
    ],
    "field": "Requirements & Design",
    "task": "Analysis",
    "tags": [
      "MAPE-K enhancement",
      "LLM-assisted analysis",
      "LLM-assisted planning",
      "LLM-assisted monitoring",
      "Human-on-the-loop (HOTL)",
      "Explainability and transparency",
      "Adaptation plan synthesis",
      "Safety and robustness mitigation"
    ],
    "summary": "本文综述了将大型语言模型等生成式AI应用于自适应系统（基于MAPE‑K）的研究现状，归纳其在监测、分析、规划、执行及人机协作方面的潜在增益与挑战，并提出了面向集成与实践的研究路线图与缓解策略。",
    "quality": "High",
    "conference": "ACM Transactions on Autonomous and Adaptive Systems 2024",
    "pdf_url": "https://arxiv.org/pdf/2512.04680v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-16 14:39:58"
  },
  {
    "id": "2512.04702",
    "title": "POLARIS: Is Multi-Agentic Reasoning the Next Wave in Engineering Self-Adaptive Systems?",
    "abstract": "The growing scale, complexity, interconnectivity, and autonomy of modern software ecosystems introduce unprecedented uncertainty, challenging the foundations of traditional self-adaptation. Existing approaches, typically rule-driven controllers or isolated learning components, struggle to generalize to novel contexts or coordinate responses across distributed subsystems, leaving them ill-equipped for emergent unknown unknowns. Recent discussions on Self-Adaptation 2.0 emphasize an equal partnership between AI and adaptive systems, merging learning-driven intelligence with adaptive control for predictive and proactive behavior. Building on this foundation, we introduce POLARIS, a three-layer multi-agentic self-adaptation framework that advances beyond reactive adaptation. POLARIS integrates: (1) a low-latency Adapter layer for monitoring and safe execution, (2) a transparent Reasoning layer that generates and verifies plans using tool-aware, explainable agents, and (3) a Meta layer that records experiences and meta-learns improved adaptation policies over time. Through shared knowledge and predictive models, POLARIS handles uncertainty, learns from past actions, and evolves its strategies, enabling systems that anticipate change and maintain resilient, goal-directed behavior. Preliminary evaluation on two self-adaptive exemplars, SWIM and SWITCH, shows that POLARIS consistently outperforms state-of-the-art baselines. We argue this marks a shift toward Self-Adaptation 3.0, akin to Software 3.0: a paradigm where systems not only learn from their environment but also reason about and evolve their own adaptation processes, continuously improving to meet novel challenges.",
    "arxiv_url": "https://arxiv.org/abs/2512.04702",
    "authors": [
      "Divyansh Pandey",
      "Vyakhya Gupta",
      "Prakhar Singhal",
      "Karthik Vaidhyanathan"
    ],
    "first_author": "Divyansh Pandey",
    "category": [
      "Technical"
    ],
    "field": "Self-Adaptive Systems",
    "task": "Multi-Agentic Reasoning & Meta-Learning for Runtime Adaptation",
    "tags": [
      "Multi-agent orchestration for adaptation",
      "Tool-aware explainable reasoning agents",
      "Meta-learning of adaptation policies",
      "World-model based what-if simulation",
      "Verifier agent for plan validation",
      "Low-latency reactive controller (stabilization)",
      "Episodic knowledge base for experience replay"
    ],
    "summary": "本文提出POLARIS——一个三层多代理自适应框架，通过低延迟执行器、可解释的推理代理与记录并元学习的元层，结合世界模型与验证器实现主动、可演化的运行时自适应，并在SWIM与SWITCH示例上优于现有基线。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.04702v2",
    "published": "2025-12-04",
    "update_time": "2025-12-07",
    "download_time": "2025-12-16 14:40:38"
  },
  {
    "id": "2512.04673",
    "title": "Cross-Task Benchmarking and Evaluation of General-Purpose and Code-Specific Large Language Models",
    "abstract": "Large Language Models (LLMs) have revolutionized both general natural language processing and domain-specific applications such as code synthesis, legal reasoning, and finance. However, while prior studies have explored individual model capabilities, a systematic cross-domain comparison that unifies linguistic, reasoning, and code understanding abilities remains underexplored. In this work, we present a comprehensive evaluation of five general-purpose and three code-specific state-of-the-art LLMs across six diverse benchmarks encompassing linguistic competence, mathematical reasoning, and trustworthiness. Additionally, we analyze model behavior on the CoNaLa dataset for code explanation, comparing natural language and code-specialized LLMs. Our findings reveal that models optimized for code (e.g., CodeLLaMA variants) exhibit strong reasoning and syntactic precision, that even for non-coding tasks can show measurable performance gains, in contrast to general-purpose models like Mistral-7B and Llama-3-8B.",
    "arxiv_url": "https://arxiv.org/abs/2512.04673",
    "authors": [
      "Gunjan Das",
      "Paheli Bhattacharya",
      "Rishabh Gupta"
    ],
    "first_author": "Gunjan Das",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Summarization",
    "tags": [
      "Cross-domain benchmarking",
      "Code vs general-purpose model comparison",
      "Code explanation / intent generation",
      "Trustworthiness evaluation",
      "Commonsense and mathematical reasoning assessment",
      "Consolidation of lexical and embedding-based metrics"
    ],
    "summary": "本文系统比较评估了多款通用与代码专用大型语言模型在多项自然语言与代码解释基准上的表现，发现代码专用模型在推理能力与语法精确性上具有显著优势并在某些非代码任务上也能带来提升。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.04673v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-16 14:41:13"
  },
  {
    "id": "2512.05100",
    "title": "Structured Document Translation via Format Reinforcement Learning",
    "abstract": "Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \\textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.",
    "arxiv_url": "https://arxiv.org/abs/2512.05100",
    "authors": [
      "Haiyue Song",
      "Johannes Eschbach-Dymanus",
      "Hour Kaing",
      "Sumire Honda",
      "Hideki Tanaka",
      "Bianka Buschbeck",
      "Masao Utiyama"
    ],
    "first_author": "Haiyue Song",
    "category": [
      "Technical"
    ],
    "field": "Structured Document Translation",
    "task": "Structured Document Translation",
    "tags": [
      "Format-aware Reinforcement Learning",
      "Group Relative Policy Optimization",
      "Tree edit-distance reward (TreeSim)",
      "Node-level chrF reward",
      "Structure-Aware AUC (StrucAUC)",
      "XML/HTML structure preservation",
      "Synthetic markup injection for data augmentation",
      "Document-level localization"
    ],
    "summary": "本文提出FORMATRL，一种在监督微调基础上使用GRPO并通过TreeSim和Node-chrF等结构感知奖励直接优化XML/HTML文档结构保真度与节点级翻译质量的强化学习方法，并在软件文档数据集上显著提升了结构与翻译评估指标。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.05100v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-16 14:41:56"
  },
  {
    "id": "2512.04785",
    "title": "ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications",
    "abstract": "AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.",
    "arxiv_url": "https://arxiv.org/abs/2512.04785",
    "authors": [
      "Eranga Bandara",
      "Amin Hass",
      "Ross Gore",
      "Sachin Shetty",
      "Ravi Mukkamala",
      "Safdar H. Bouk",
      "Xueping Liang",
      "Ng Wee Keong",
      "Kasun De Zoysa",
      "Aruna Withanage",
      "Nilaan Loganathan"
    ],
    "first_author": "Eranga Bandara",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Requirements & Design",
    "task": "Analysis",
    "tags": [
      "Automated diagram-driven threat modeling",
      "STRIDE extension for AI agents",
      "AI agent-specific threats",
      "Vision-language ensemble for diagram parsing",
      "Prompt injection and instruction-manipulation detection",
      "Reasoning-based threat synthesis",
      "Quantized low-latency fine-tuning for edge inference",
      "Explainable security assessments"
    ],
    "summary": "本文提出ASTRIDE，一种将STRIDE扩展以包含AI代理特有攻击并结合微调视觉—语言模型集群与推理LLM，从架构图（如数据流图）自动生成可解释威胁模型的平台。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.04785v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-16 14:42:40"
  },
  {
    "id": "2512.04611",
    "title": "PBFuzz: Agentic Directed Fuzzing for PoV Generation",
    "abstract": "Proof-of-Vulnerability (PoV) input generation is a critical task in software security and supports downstream applications such as path generation and validation. Generating a PoV input requires solving two sets of constraints: (1) reachability constraints for reaching vulnerable code locations, and (2) triggering constraints for activating the target vulnerability. Existing approaches, including directed greybox fuzzing and LLM-assisted fuzzing, struggle to efficiently satisfy these constraints. This work presents an agentic method that mimics human experts. Human analysts iteratively study code to extract semantic reachability and triggering constraints, form hypotheses about PoV triggering strategies, encode them as test inputs, and refine their understanding using debugging feedback. We automate this process with an agentic directed fuzzing framework called PBFuzz. PBFuzz tackles four challenges in agentic PoV generation: autonomous code reasoning for semantic constraint extraction, custom program-analysis tools for targeted inference, persistent memory to avoid hypothesis drift, and property-based testing for efficient constraint solving while preserving input structure. Experiments on the Magma benchmark show strong results. PBFuzz triggered 57 vulnerabilities, surpassing all baselines, and uniquely triggered 17 vulnerabilities not exposed by existing fuzzers. PBFuzz achieved this within a 30-minute budget per target, while conventional approaches use 24 hours. Median time-to-exposure was 339 seconds for PBFuzz versus 8680 seconds for AFL++ with CmpLog, giving a 25.6x efficiency improvement with an API cost of 1.83 USD per vulnerability.",
    "arxiv_url": "https://arxiv.org/abs/2512.04611",
    "authors": [
      "Haochen Zeng",
      "Andrew Bao",
      "Jiajun Cheng",
      "Chengyu Song"
    ],
    "first_author": "Haochen Zeng",
    "category": [
      "Technical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Agentic reasoning",
      "Directed fuzzing",
      "Proof-of-vulnerability generation",
      "Property-based test generation",
      "Semantic reachability and triggering constraint extraction",
      "Persistent agent memory",
      "On-demand program analysis tooling",
      "Fine-grained execution feedback"
    ],
    "summary": "本文提出PBFuzz，一种将自治式代码推理、按需程序分析与基于属性的测试相结合的定向模糊测试框架，用于高效自动生成Proof‑of‑Vulnerability输入并在Magma基准上显著超越现有方法。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.04611v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-16 14:43:21"
  },
  {
    "id": "2512.04538",
    "title": "Completion by Comprehension: Guiding Code Generation with Multi-Granularity Understanding",
    "abstract": "As code completion task from function-level to repository-level, leveraging contextual information from large-scale codebases becomes a core challenge. However, existing retrieval-augmented generation (RAG) methods typically treat code as plain natural language, relying primarily on shallow semantic matching while overlooking structural semantics and code-specific dependencies. This limits their ability to capture control flow and underlying intent, ultimately constraining the quality of generated code. Therefore, we propose CoCo, a novel framework that enables code Completion by Comprehension of multi-granularity context from large-scale code repositories. CoCo employs static code analysis to extract structured context at the function, file, and project levels, capturing execution logic and semantic dependencies. It then adopts an graph-based multi-granularity context selection mechanism to filter out redundant information and remove noise. Consequently, the information is converted into natural language in a consistent manner, thereby functioning as explicit contextual prompts to guide subsequent code completion. Additionally, a structure-aware code re-ranker mechanism ensures alignment at both semantic and structural levels. Extensive experiments on CrossCodeEval and RepoEval benchmarks demonstrate that CoCo consistently surpasses state-of-the-art baselines, achieving up to 20.2% gains in EM. Moreover, the framework is model-agnostic and can be seamlessly integrated into existing methods, leading to significant performance.",
    "arxiv_url": "https://arxiv.org/abs/2512.04538",
    "authors": [
      "Xinkui Zhao",
      "Rongkai Liu",
      "Yifan Zhang",
      "Chen Zhi",
      "Lufei Zhang",
      "Guanjie Cheng",
      "Yueshen Xu",
      "Shuiguang Deng",
      "Jianwei Yin"
    ],
    "first_author": "Xinkui Zhao",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "Static code analysis for multi-granularity context",
      "Function-file-repository context modeling",
      "Graph-based context selection",
      "Structure-aware code re-ranking",
      "Natural-language prompt synthesis from structural context",
      "Repository-level dependency and control-flow modeling",
      "Retrieval-augmented generation integration"
    ],
    "summary": "本文提出CoCo，通过静态分析提取函数/文件/仓库三级结构化上下文、用图筛选相关信息并将其转化为自然语言提示，结合结构感知的代码重排序，显著改进检索增强的仓库级代码补全性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.04538v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-16 14:44:03"
  },
  {
    "id": "2512.04419",
    "title": "Solving LLM Repetition Problem in Production: A Comprehensive Study of Multiple Solutions",
    "abstract": "The repetition problem, where Large Language Models (LLMs) continuously generate repetitive content without proper termination, poses a critical challenge in production deployments, causing severe performance degradation and system stalling. This paper presents a comprehensive investigation and multiple practical solutions for the repetition problem encountered in real-world batch code interpretation tasks.   We identify three distinct repetition patterns: (1) business rule generation repetition, (2) method call relationship analysis repetition, and (3) PlantUML diagram syntax generation repetition. Through rigorous theoretical analysis based on Markov models, we establish that the root cause lies in greedy decoding's inability to escape repetitive loops, exacerbated by self-reinforcement effects.   Our comprehensive experimental evaluation demonstrates three viable solutions: (1) Beam Search decoding with early_stopping=True serves as a universal post-hoc mechanism that effectively resolves all three repetition patterns; (2) presence_penalty hyperparameter provides an effective solution specifically for BadCase 1; and (3) Direct Preference Optimization (DPO) fine-tuning offers a universal model-level solution for all three BadCases.   The primary value of this work lies in combining first-hand production experience with extensive experimental validation. Our main contributions include systematic theoretical analysis of repetition mechanisms, comprehensive evaluation of multiple solutions with task-specific applicability mapping, identification of early_stopping as the critical parameter for Beam Search effectiveness, and practical production-ready solutions validated in real deployment environments.",
    "arxiv_url": "https://arxiv.org/abs/2512.04419",
    "authors": [
      "Weiwei Wang",
      "Weijie Zou",
      "Jiyong Min"
    ],
    "first_author": "Weiwei Wang",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Understanding",
    "tags": [
      "Generation repetition (repeater)",
      "Greedy decoding loop",
      "Beam search with early_stopping",
      "Presence_penalty tuning",
      "Direct Preference Optimization (DPO) fine‑tuning",
      "Markov-chain theoretical analysis",
      "Production batch code interpretation",
      "Framework parameter integration (FastChat↔vLLM)"
    ],
    "summary": "本文基于批量代码解释的生产实践，利用马尔可夫模型分析LLM重复生成的根因，并通过大规模实验验证三种可行解决方案（启用early_stopping的束搜索、presence_penalty调优与DPO微调），给出任务适配与生产部署建议。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.04419v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-16 14:44:59"
  },
  {
    "id": "2512.05908",
    "title": "Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures",
    "abstract": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.",
    "arxiv_url": "https://arxiv.org/abs/2512.05908",
    "authors": [
      "Amirkia Rafiei Oskooei",
      "S. Selcan Yukcu",
      "Mehmet Cevheri Bozoglan",
      "Mehmet S. Aktas"
    ],
    "first_author": "Amirkia Rafiei Oskooei",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Bug Localization",
    "tags": [
      "Context-aware code summarization",
      "Hierarchical NL knowledge base",
      "Repository routing",
      "Top-down directory→file localization",
      "NL-to-NL retrieval",
      "Explainable repository→directory→file search",
      "Chain-of-Thought prompting",
      "Multi-repository microservice bug localization"
    ],
    "summary": "本文提出将多仓库微服务代码库转换为层次化、上下文感知的自然语言摘要，并通过先路由到候选仓库再自顶向下目录与文件检索的双阶段搜索，将错误定位转化为NL-to-NL推理，从而在工业规模数据上显著优于基于原始代码的检索与RAG方法。",
    "quality": "High",
    "conference": "LLM4Code Workshop, ICSE 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.05908v1",
    "published": "2025-12-05",
    "update_time": "2025-12-05",
    "download_time": "2025-12-16 14:45:35"
  },
  {
    "id": "2512.05887",
    "title": "Bootstrapping Fuzzers for Compilers of Low-Resource Language Dialects Using Language Models",
    "abstract": "Modern extensible compiler frameworks-such as MLIR-enable rapid creation of domain-specific language dialects. This flexibility, however, makes correctness harder to ensure as the same extensibility that accelerates development also complicates maintaining the testing infrastructure. Extensible languages require automated test generation that is both dialect-agnostic (works across dialects without manual adaptation) and dialect-effective (targets dialect-specific features to find bugs). Existing approaches typically sacrifice one of these goals by either requiring manually constructed seed corpora for each dialect, or by failing to be effective. We present a dialect-agnostic and dialect-effective grammar-based and coverage-guided fuzzing approach for extensible compilers that combines two key insights from existing work: (i) the grammars of dialects, which already encode the structural and type constraints, can often be extracted automatically from the dialect specification; and (ii) these grammars can be used in combination with pre-trained large language models to automatically generate representative and diverse seed inputs from the full dialect space without requiring any manual input or training data. These seeds can then be used to bootstrap coverage-guided fuzzers. We built this approach into a tool, Germinator. When evaluated on six MLIR projects spanning 91 dialects, Germinator generated seeds improve line coverage by 10-120% over grammar-based baselines. We compare against grammar-based baselines because they are the only class of existing automatic seed generators that can be applied uniformly across MLIR's heterogeneous dialect ecosystem. Germinator discovers 88 previously unknown bugs (40 confirmed), including 23 in dialects with no prior automated test generators, demonstrating effective and controllable testing of low-resource dialects at scale.",
    "arxiv_url": "https://arxiv.org/abs/2512.05887",
    "authors": [
      "Sairam Vaidya",
      "Marcel Böhme",
      "Loris D'Antoni"
    ],
    "first_author": "Sairam Vaidya",
    "category": [
      "Technical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "TableGen grammar extraction",
      "Grammar-constrained decoding",
      "Seed program generation",
      "Coverage-guided fuzzing integration",
      "Dialect-agnostic fuzzing",
      "Low-resource MLIR dialects",
      "Compiler fuzzing",
      "Bug discovery"
    ],
    "summary": "本文提出 Germinator，通过从 MLIR 的 TableGen 自动提取语法并对预训练语言模型进行语法约束采样生成多样化种子，以引导覆盖驱动的模糊测试，从而在 91 个方言上显著提升覆盖率并发现大量新缺陷。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.05887v1",
    "published": "2025-12-05",
    "update_time": "2025-12-05",
    "download_time": "2025-12-16 14:46:15"
  },
  {
    "id": "2512.07814",
    "title": "Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach",
    "abstract": "Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.",
    "arxiv_url": "https://arxiv.org/abs/2512.07814",
    "authors": [
      "Hua Yang",
      "Alejandro Velasco",
      "Sen Fang",
      "Bowen Xu",
      "Denys Poshyvanyk"
    ],
    "first_author": "Hua Yang",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Privacy & Security",
    "task": "PII Leakage Analysis (causal)",
    "tags": [
      "PII Types",
      "Training Dynamics",
      "Memorization and Leakage",
      "Structural Causal Model",
      "PII Dataset from Code",
      "LLM4Code Fine-tuning",
      "Type-aware Privacy Risk",
      "Leakage Attack Evaluation"
    ],
    "summary": "本文构建多类型PII数据集并在不同规模和架构的代码模型上计算训练动态，基于结构因果模型实证证明不同PII类型的可学性与推理时泄露风险存在因果关联，为类型感知的防护策略提供依据。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.07814v1",
    "published": "2025-12-08",
    "update_time": "2025-12-08",
    "download_time": "2025-12-10 01:51:03"
  },
  {
    "id": "2512.07666",
    "title": "Bridging Code Graphs and Large Language Models for Better Code Understanding",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.",
    "arxiv_url": "https://arxiv.org/abs/2512.07666",
    "authors": [
      "Zeqi Chen",
      "Zhaoyang Chu",
      "Yi Gui",
      "Feng Guo",
      "Yao Wan",
      "Chuan Shi"
    ],
    "first_author": "Zeqi Chen",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Summarization",
    "tags": [
      "Code Property Graph (CPG)",
      "GNN-based code graph encoder",
      "Self-supervised graph pretraining",
      "Cross-modal attention / alignment",
      "Bridge module (plug-and-play)",
      "Structure-informed soft prompts",
      "Graph-text contrastive learning",
      "Code translation"
    ],
    "summary": "本文提出CGBridge——一种可插拔的桥接模块，通过对约27万条异构代码图进行自监督预训练、采用跨模态对齐（图-文本对比与匹配）并生成结构感知提示注入冻结的LLM，从而在代码摘要与翻译等代码理解任务上显著提升性能并提高推理效率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.07666v1",
    "published": "2025-12-08",
    "update_time": "2025-12-08",
    "download_time": "2025-12-10 01:51:28"
  },
  {
    "id": "2512.07631",
    "title": "The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds",
    "abstract": "When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\\Itotal$ bits to identify a solution and gains $\\Istep$ bits per action at cost $\\Cstep$, yielding an effective cost $\\Ceff = (\\Itotal/\\Istep), \\Cstep$ that predicts resource requirements before search. We prove that $\\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \\",
    "arxiv_url": "https://arxiv.org/abs/2512.07631",
    "authors": [
      "Shahar Lutati"
    ],
    "first_author": "Shahar Lutati",
    "category": [
      "Technical"
    ],
    "field": "Agent Decision & Planning",
    "task": "Solvability Prediction / Resource Allocation",
    "tags": [
      "Agent Capability Problem",
      "Solvability Prediction",
      "Information-Theoretic Bounds",
      "Effective Cost (Ceffective)",
      "Mutual Information per Action",
      "Stopping Time Analysis",
      "Lorden's Inequality",
      "Gaussian Process Approximation",
      "Resource Allocation for LLM-based Agents"
    ],
    "summary": "本文提出了代理能力问题（ACP），用信息论视角将解决所需信息量 Itotal 与每步信息增益 Is 及每步代价 Cs 结合成 Ceffective，并给出该量的下界与概率性上界以在资源受限下预测并引导代理求解可行性，实验以高斯过程近似验证了理论预测的有效性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.07631v1",
    "published": "2025-12-08",
    "update_time": "2025-12-08",
    "download_time": "2025-12-10 01:55:01"
  },
  {
    "id": "2512.08867",
    "title": "SimpleDevQA: Benchmarking Large Language Models on Development Knowledge QA",
    "abstract": "The Development Knowledge Question Answering (Dev Knowledge QA) task aims to provide natural language answers to knowledge-seeking questions during software development. To investigate its importance and to what extent it has been explored, we analyze real user-LLM dialogues from WildChat and find that: (1) The Dev Knowledge QA task accounts for 39.6% of interactions(highest among all tasks), revealing broad knowledge needs beyond code generation (32.3%). (2) Only 27.5% of real Dev Knowledge QA dialogues focus on code understanding, leaving out development knowledge-seeking. (3) Only 17.1% of real-world Dev Knowledge QA dialogues can be used for constructing a benchmark. Existing benchmarks have two primary limitations for evaluating the Dev Knowledge QA capability of LLMs. First, existing benchmarks offer a limited development knowledge scope, mainly focusing on code understanding and neglecting broader knowledge during development. Second, some benchmarks are not built from real user queries. To bridge this gap, we design a three-phase pipeline that transforms real-world dialogue into simple development knowledge-seeking QA pairs. Through this pipeline, we introduce SimpleDevQA, a multilingual benchmark derived from real user dialogues. It contains 2,740 QA pairs in three languages (English, Chinese, and Russian), and focuses on questions with unique, short, and verifiable answers for accurate and simple evaluation. Experiments show that: Code LLMs generally outperform general LLMs of similar scale; Knowledge injection with the Retrieval-Augmented Generation (RAG) strategy can boost LLM accuracy by 11.3% on average; LLMs show systematic overconfidence in Dev Knowledge QA, and the answering accuracy of LLMs shows a positive correlation with their stated confidence; Generally, LLMs with stronger code generation performance also exhibit stronger performance in Dev Knowledge QA.",
    "arxiv_url": "https://arxiv.org/abs/2512.08867",
    "authors": [
      "Jing Zhang",
      "Lianghong Guo",
      "Yanlin Wang",
      "Mingwei Liu",
      "Jiachi Chen",
      "Yuchi Ma",
      "Ensheng Shi",
      "Terry Yue Zhuo",
      "Hongyu Zhang",
      "Zibin Zheng"
    ],
    "first_author": "Jing Zhang",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Development Knowledge",
    "task": "Development Knowledge QA",
    "tags": [
      "Benchmark Construction",
      "Dev Knowledge QA",
      "Multilingual Dataset",
      "WildChat Dialogue Mining",
      "Data Pipeline",
      "Reference Retrieval",
      "RAG (Retrieval-Augmented Generation)",
      "LLM Evaluation",
      "Factuality Verification",
      "Human Annotation",
      "Closed-source vs Open-source Comparison",
      "Code LLM vs General LLM Performance",
      "Overconfidence Analysis"
    ],
    "summary": "本文提出SimpleDevQA——一个基于真实用户对话、覆盖英中俄三语的开发知识问答基准及其三阶段构建流水线，并通过对18种主流LLM的评测揭示了RAG可提升性能、闭源/代码模型普遍优于开源/通用模型以及模型过度自信等现象。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.08867v1",
    "published": "2025-12-09",
    "update_time": "2025-12-09",
    "download_time": "2025-12-11 01:52:20"
  },
  {
    "id": "2512.08810",
    "title": "Multicalibration for LLM-based Code Generation",
    "abstract": "As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.",
    "arxiv_url": "https://arxiv.org/abs/2512.08810",
    "authors": [
      "Viola Campos",
      "Robin Kuschnereit",
      "Adrian Ulges"
    ],
    "first_author": "Viola Campos",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Uncertainty Estimation & Calibration",
    "tags": [
      "Multicalibration",
      "Post-hoc calibration",
      "Uncertainty estimation",
      "Group-aware calibration (complexity/code-length/prompt-length/language)",
      "Brier Skill Score",
      "Expected Calibration Error",
      "Token likelihood aggregation",
      "CALIBRI dataset",
      "Function synthesis benchmarks",
      "Code LLM evaluation (Qwen3/GPT-OSS/DeepSeek)"
    ],
    "summary": "本文首次将多组校准(multicalibration)方法应用于代码生成LLM，通过按复杂度、代码/提示长度及编程语言分组的后验校准显著提高置信度估计的可靠性，并发布了包含生成代码、似然与正确性标签的CALIBRI数据集以促进后续研究。",
    "quality": "High",
    "conference": "AI-SQE 2026 (The 1st International Workshop on AI for Software Quality Evaluation: Judgment, Metrics, Benchmarks, and Beyond) 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.08810v1",
    "published": "2025-12-09",
    "update_time": "2025-12-09",
    "download_time": "2025-12-11 01:52:53"
  },
  {
    "id": "2512.08769",
    "title": "A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows",
    "abstract": "Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.",
    "arxiv_url": "https://arxiv.org/abs/2512.08769",
    "authors": [
      "Eranga Bandara",
      "Ross Gore",
      "Peter Foytik",
      "Sachin Shetty",
      "Ravi Mukkamala",
      "Abdul Rahman",
      "Xueping Liang",
      "Safdar H. Bouk",
      "Amin Hass",
      "Sachini Rajapakse",
      "Ng Wee Keong",
      "Kasun De Zoysa",
      "Aruna Withanage",
      "Nilaan Loganathan"
    ],
    "first_author": "Eranga Bandara",
    "category": [
      "Survey",
      "Technical"
    ],
    "field": "Requirements & Design",
    "task": "Management",
    "tags": [
      "Agentic AI",
      "Multi-Agent Workflows",
      "Model Context Protocol (MCP)",
      "Tool Integration",
      "Orchestration",
      "Deterministic Execution",
      "Responsible AI",
      "Deployment & Containerization",
      "Prompt Management",
      "Single-Responsibility Agents",
      "Case Study: Multimodal News Analysis"
    ],
    "summary": "本文提供了一套面向生产的 agentic AI 工作流的端到端工程指南，包含工作流分解、多代理设计模式、Model Context Protocol、工具集成、确定性编排、责任化 AI 考量及部署策略，并通过多模态新闻分析案例演示最佳实践。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.08769v1",
    "published": "2025-12-09",
    "update_time": "2025-12-09",
    "download_time": "2025-12-11 01:57:12"
  },
  {
    "id": "2512.09679",
    "title": "Understanding Chain-of-Thought Effectiveness in Code Generation: An Empirical and Information-Theoretic Analysis",
    "abstract": "Large language models (LLMs) achieve strong performance on code generation, but the mechanisms by which Chain-of-Thought (CoT) prompting helps remain unclear. We present a systematic empirical and information-theoretic study of CoT effectiveness in neural code generation, evaluating five paradigms (Zero-Shot, Zero-Shot CoT, Self-Planning, Structured CoT, Reasoning-CoT) across six Python benchmarks, a multilingual benchmark with 12 programming languages, and six models from 7B to 480B parameters, using conditional mutual information $I(Y;C|X)$ as a conceptual lens. Our results show that externally guided CoT consistently outperforms direct generation, with structured methods improving Pass@1 by 5--12\\% on average while using substantially fewer tokens than reflective reasoning, and that CoT benefits depend on language type systems and model capacity. We further find that reasoning \\emph{quality} is critical: high-quality structured CoT from strong generators yields significantly higher accuracy than lightweight alternatives with the same template, whereas naive Zero-Shot CoT can even degrade performance. These findings provide practical guidance for choosing CoT strategies based on model capacity, language characteristics, and task complexity.",
    "arxiv_url": "https://arxiv.org/abs/2512.09679",
    "authors": [
      "Naizhu Jin",
      "Zhong Li",
      "Guang Yang",
      "Tian Zhang",
      "Qingkai Zeng"
    ],
    "first_author": "Naizhu Jin",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Chain-of-Thought prompting",
      "Structured CoT (SCoT)",
      "Self-planning prompts",
      "Reflective reasoning",
      "Conditional mutual information",
      "Information density metric",
      "Cross-language generalization",
      "Model scale / capacity effects",
      "Token-efficiency vs accuracy",
      "Reasoning quality evaluation"
    ],
    "summary": "本文提出基于条件互信息的信息论框架，并通过跨语言、跨模型的大规模实证实验系统地评估多种 Chain-of-Thought 提示范式在代码生成中对准确性与效率的影响，发现结构化 CoT 在令牌效率和稳定性上优于反思式方法且推理质量与模型容量显著影响效果。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.09679v1",
    "published": "2025-12-10",
    "update_time": "2025-12-10",
    "download_time": "2025-12-12 01:51:32"
  },
  {
    "id": "2512.09627",
    "title": "LogICL: Distilling LLM Reasoning to Bridge the Semantic Gap in Cross-Domain Log Anomaly Detection",
    "abstract": "Effective log anomaly detection is critical to sustaining reliability in large-scale IT infrastructures. Transformer-based models require substantial resources and labeled data, exacerbating the cold-start problem in target domains where logs are scarce. Existing cross-domain methods leverage source logs but struggle with generalization due to reliance on surface lexical similarity, failing to capture latent semantic equivalence amid structural divergences. To address this, we propose LogICL, a framework distilling Large Language Model (LLM) reasoning into a lightweight encoder for cross-domain anomaly detection. During training, LogICL constructs a delta matrix measuring the utility of demonstrations selected via Maximal Marginal Relevance relative to zero-shot inference. The encoder is optimized via a multi-objective loss comprising an ICL-Guided term that aligns representations based on reasoning assistance utility, maximum mean discrepancy for domain alignment, and supervised contrastive loss. At inference, the optimized encoder retrieves reasoning-aware demonstrations using semantic similarity and delta scores, enabling frozen-LLM in-context learning with Chain-of-Thought for accurate and interpretable detection. Experiments on few-shot and zero-shot cross-domain benchmarks confirm LogICL achieves state-of-the-art performance across heterogeneous systems. Further analysis via visualizations and case studies confirms LogICL bridges the semantic gap beyond surface lexical similarity, effectively capturing latent semantic equivalence for rapid deployment.",
    "arxiv_url": "https://arxiv.org/abs/2512.09627",
    "authors": [
      "Jingwei Ye",
      "Zhi Wang",
      "Chenbin Su",
      "Jieshuai Yang",
      "Jiayi Ding",
      "Chunbo Liu",
      "Ge Chu"
    ],
    "first_author": "Jingwei Ye",
    "category": [
      "Technical"
    ],
    "field": "AIOps",
    "task": "Log Anomaly Detection",
    "tags": [
      "In-Context Learning Distillation",
      "Delta Matrix for Demonstration Utility",
      "Maximal Marginal Relevance Demonstration Selection",
      "ICL-guided Contrastive Representation Learning",
      "Maximum Mean Discrepancy Domain Alignment",
      "Supervised Contrastive Loss for Anomaly Discrimination",
      "Reasoning-aware Demonstration Retrieval",
      "Frozen-LLM Chain-of-Thought Inference",
      "Lightweight Log Sequence Encoder",
      "Cross-domain Few-shot/Zero-shot Adaptation"
    ],
    "summary": "本文提出LogICL，通过将大模型的推理能力蒸馏到轻量级日志编码器并结合基于ICL的示例选择、delta矩阵衡量、MMD域对齐和对比损失，实现对异构系统的跨域少样本/零样本日志异常检测与可解释推理。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.09627v1",
    "published": "2025-12-10",
    "update_time": "2025-12-10",
    "download_time": "2025-12-12 01:52:03"
  },
  {
    "id": "2512.09543",
    "title": "SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs",
    "abstract": "Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood.   Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs.   Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration.   Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency.   Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.",
    "arxiv_url": "https://arxiv.org/abs/2512.09543",
    "authors": [
      "Arihant Tripathy",
      "Ch Pavan Harshit",
      "Karthik Vaidhyanathan"
    ],
    "first_author": "Arihant Tripathy",
    "category": [
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Bug Repair",
    "tags": [
      "Energy-aware agentic frameworks",
      "Hardware-level CPU/GPU energy profiling",
      "Small-model multi-turn reasoning limitations",
      "Framework architectural trade-offs",
      "Resource-constrained autonomous bug repair",
      "Wasted inference energy from reasoning loops",
      "Reproducible energy measurement methodology",
      "Failure-mode analysis for agentic workflows"
    ],
    "summary": "本文在固定硬件上用两种小型语言模型对四种代理式问题修复框架进行大规模能耗与性能评估，发现框架架构决定能耗且大部分能耗因SLM推理能力受限导致无效循环被浪费，表明需设计主动管理SLM弱点的新型架构以实现低能耗可行性。",
    "quality": "High",
    "conference": "AGENT (ICSE 2026 workshop) 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.09543v1",
    "published": "2025-12-10",
    "update_time": "2025-12-10",
    "download_time": "2025-12-12 01:58:17"
  },
  {
    "id": "2512.09108",
    "title": "Evolving Excellence: Automated Optimization of LLM-based Agents",
    "abstract": "Agentic AI systems built on large language models (LLMs) offer significant potential for automating complex workflows, from software development to customer support. However, LLM agents often underperform due to suboptimal configurations; poorly tuned prompts, tool descriptions, and parameters that typically require weeks of manual refinement. Existing optimization methods either are too complex for general use or treat components in isolation, missing critical interdependencies.   We present ARTEMIS, a no-code evolutionary optimization platform that jointly optimizes agent configurations through semantically-aware genetic operators. Given only a benchmark script and natural language goals, ARTEMIS automatically discovers configurable components, extracts performance signals from execution logs, and evolves configurations without requiring architectural modifications.   We evaluate ARTEMIS on four representative agent systems: the \\emph{ALE Agent} for competitive programming on AtCoder Heuristic Contest, achieving a \\textbf{$13.6\\%$ improvement} in acceptance rate; the \\emph{Mini-SWE Agent} for code optimization on SWE-Perf, with a statistically significant \\textbf{10.1\\% performance gain}; and the \\emph{CrewAI Agent} for cost and mathematical reasoning on Math Odyssey, achieving a statistically significant \\textbf{$36.9\\%$ reduction} in the number of tokens required for evaluation. We also evaluate the \\emph{MathTales-Teacher Agent} powered by a smaller open-source model (Qwen2.5-7B) on GSM8K primary-level mathematics problems, achieving a \\textbf{22\\% accuracy improvement} and demonstrating that ARTEMIS can optimize agents based on both commercial and local models.",
    "arxiv_url": "https://arxiv.org/abs/2512.09108",
    "authors": [
      "Paul Brookes",
      "Vardan Voskanyan",
      "Rafail Giavrimis",
      "Matthew Truscott",
      "Mina Ilieva",
      "Chrystalla Pavlou",
      "Alexandru Staicu",
      "Manal Adham",
      "Will Evers- Hood",
      "Jingzhi Gong",
      "Kejia Zhang",
      "Matvey Fedoseev",
      "Vishal Sharma",
      "Roman Bauer",
      "Zheng Wang",
      "Hema Nair",
      "Wei Jie",
      "Tianhua Xu",
      "Aurora Constantin",
      "Leslie Kanthan",
      "Michail Basios"
    ],
    "first_author": "Paul Brookes",
    "category": [
      "Technical"
    ],
    "field": "LLM Agents & Deployment",
    "task": "Agent Configuration Optimization",
    "tags": [
      "Black-box agent tuning",
      "Semantically-aware mutation and crossover",
      "No-code evolutionary platform",
      "Execution-log based fitness",
      "LLM-ensemble guided edits",
      "Joint textual and parametric optimization",
      "Prompt + tool description co-optimization",
      "Cost vs. performance trade-off analysis"
    ],
    "summary": "本文提出Artemis——一个无代码的基于进化算法的代理配置自动优化平台，通过语义感知的变异与交叉操作并利用执行日志与基准反馈，自动联合调优提示、工具描述和参数，从而显著提升不同LLM代理在多种任务上的准确性与效率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.09108v1",
    "published": "2025-12-09",
    "update_time": "2025-12-09",
    "download_time": "2025-12-12 02:05:43"
  },
  {
    "id": "2512.04355",
    "title": "Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity",
    "abstract": "Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to \"count without running\" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench",
    "arxiv_url": "https://arxiv.org/abs/2512.04355",
    "authors": [
      "Gregory Bolet",
      "Giorgis Georgakoudis",
      "Konstantinos Parasyris",
      "Harshitha Menon",
      "Niranjan Hasabnis",
      "Kirk W. Cameron",
      "Gal Oren"
    ],
    "first_author": "Gregory Bolet",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Performance Optimization",
    "tags": [
      "Static FLOP counting",
      "CUDA kernel FLOP estimation",
      "Execution-attribute annotations",
      "Implicit/hidden FLOPs (division, intrinsics, templates)",
      "Single vs double-precision FLOP labels",
      "Prompting for structured performance predictions",
      "Hardware/microcode execution effects",
      "Benchmark for static performance reasoning"
    ],
    "summary": "本文提出GPUFLOPBENCH数据集与评测框架，包含577个CUDA内核的单/双精度FLOP计数与执行属性标注，并评估现有封闭式推理模型在静态预测代码FLOP时的表现，揭示其在处理隐式FLOP来源（如除法、内建函数、编译器/运行时行为）时的显著失败模式。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.04355v1",
    "published": "2025-12-04",
    "update_time": "2025-12-04",
    "download_time": "2025-12-12 21:46:22"
  },
  {
    "id": "2512.10713",
    "title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code",
    "abstract": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.",
    "arxiv_url": "https://arxiv.org/abs/2512.10713",
    "authors": [
      "Itay Dreyfuss",
      "Antonio Abu Nassar",
      "Samuel Ackerman",
      "Axel Ben David",
      "Rami Katan",
      "Orna Raz",
      "Marcel Zalmanovici"
    ],
    "first_author": "Itay Dreyfuss",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Automated benchmark generation",
      "Deterministic expected-result checking",
      "Code dry-running simulation",
      "Instruction concatenation pipelines",
      "Contamination-resistant variant generation",
      "Difficulty control via instruction count and output length",
      "Prompt vs chat mode evaluation"
    ],
    "summary": "本文提出了PACIFIC框架，用可控的指令拼接与引用实现自动生成可确定性评估样本，以检测大型模型的代码逐步推理（干运行）与指令遵循能力，并通过可变难度和去污染的基准区分模型表现。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10713v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-13 01:45:41"
  },
  {
    "id": "2512.10493",
    "title": "Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild",
    "abstract": "Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.",
    "arxiv_url": "https://arxiv.org/abs/2512.10493",
    "authors": [
      "Binquan Zhang",
      "Li Zhang",
      "Haoyuan Zhang",
      "Fang Liu",
      "Song Wang",
      "Bo Shen",
      "An Fu",
      "Lin Shi"
    ],
    "first_author": "Binquan Zhang",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "tags": [
      "Multi-turn dialogue patterns",
      "Interaction topology (linear/star/tree)",
      "Instruction-following compliance",
      "User satisfaction trajectory",
      "Task taxonomy for coding interactions",
      "LLM-assisted data disentanglement",
      "Open card sorting annotation",
      "Statistical significance testing of interaction effects"
    ],
    "summary": "本文基于真实多轮对话数据实证分析了编码场景下的人-LLM协作，归纳出五类任务与线性/星状/树状三种交互模式，评估模型指令遵循能力与用户满意度并提出改进建议。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10493v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-13 01:46:15"
  },
  {
    "id": "2512.10789",
    "title": "Natural Language Interface for Firewall Configuration",
    "abstract": "This paper presents the design and prototype implementation of a natural language interface for configuring enterprise firewalls. The framework allows administrators to express access control policies in plain language, which are then translated into vendor specific configurations. A compact schema bound intermediate representation separates human intent from device syntax and in the current prototype compiles to Palo Alto PAN OS command line configuration while remaining extensible to other platforms. Large language models are used only as assistive parsers that generate typed intermediate representation objects, while compilation and enforcement remain deterministic. The prototype integrates three validation layers, namely a static linter that checks structural and vendor specific constraints, a safety gate that blocks overly permissive rules such as any to any allows, and a Batfish based simulator that validates configuration syntax and referential integrity against a synthetic device model. The paper describes the architecture, implementation, and test methodology on synthetic network context datasets and discusses how this approach can evolve into a scalable auditable and human centered workflow for firewall policy management.",
    "arxiv_url": "https://arxiv.org/abs/2512.10789",
    "authors": [
      "F. Taghiyev",
      "A. Aslanbayli"
    ],
    "first_author": "F. Taghiyev",
    "category": [
      "Technical"
    ],
    "field": "Network Security & Configuration",
    "task": "Natural Language to Firewall Configuration",
    "tags": [
      "Schema-bound intermediate representation",
      "Schema-constrained LLM parsing",
      "Resolver agent for entity grounding",
      "Vendor-agnostic compiler (Palo Alto backend prototype)",
      "IR linter (general and vendor-specific)",
      "Safety Gate preventing any-to-any and missing-fields",
      "Batfish-based configuration simulation",
      "Role-conditioned prompting and constrained decoding",
      "Least-privilege policy synthesis",
      "Audit logging and stage-only (side-effect free) compilation"
    ],
    "summary": "本文提出并实现了一个原型系统，利用受约束的LLM将管理员的自然语言访问策略解析为类型化中间表示，并通过静态lint、安全门控和Batfish仿真将其无副作用地编译为供应商特定的防火墙配置以确保安全与可审计性。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10789v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-13 01:50:04"
  },
  {
    "id": "2512.10563",
    "title": "NormCode: A Semi-Formal Language for Context-Isolated AI Planning",
    "abstract": "Multistep workflows that chain large language model (LLM) calls suffer from context pollution: as information accumulates across steps, models hallucinate, confuse intermediate outputs, and lose track of task constraints. We present NormCode, a semiformal language for constructing plans of inferences, structured decompositions where each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design. NormCode enforces a strict separation between semantic operations (LLMdriven reasoning, nondeterministic) and syntactic operations (deterministic data restructuring), enabling precise cost and reliability tracing. The language exists in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting progressive formalization from sketch to production. We validate NormCode through two demonstrations: (1) a base X addition algorithm achieving 100 percent accuracy on arbitrary length inputs, and (2) self hosted execution of NormCode's own five phase compiler pipeline. The working orchestrator provides dependency driven scheduling, SQLite backed checkpointing, and loop management, making AI workflows auditable by design and addressing a critical need for transparency in high stakes domains such as legal reasoning, medical decision making, and financial analysis.",
    "arxiv_url": "https://arxiv.org/abs/2512.10563",
    "authors": [
      "Xin Guan"
    ],
    "first_author": "Xin Guan",
    "category": [
      "Technical"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Context Isolation",
      "Inference Decomposition",
      "Semi-Formal Intermediate Representation",
      "Semantic vs Syntactic Separation",
      "Progressive Formalization (.ncds/.ncd/.ncn)",
      "Auditable AI Workflows",
      "Dependency-Driven Orchestration",
      "Checkpointed Execution and Loop Management"
    ],
    "summary": "NormCode提出一种半形式化语言与执行框架，通过在多步LLM推理中强制显式数据隔离、将语义（非确定性LLM推理）与句法（确定性数据重构）分离，并提供三种同构格式与可检查的编译/运行时支持，从而实现可审计、可靠的AI工作流编排与执行。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10563v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-13 01:53:27"
  },
  {
    "id": "2512.10485",
    "title": "From Lab to Reality: A Practical Evaluation of Deep Learning Models and LLMs for Vulnerability Detection",
    "abstract": "Vulnerability detection methods based on deep learning (DL) have shown strong performance on benchmark datasets, yet their real-world effectiveness remains underexplored. Recent work suggests that both graph neural network (GNN)-based and transformer-based models, including large language models (LLMs), yield promising results when evaluated on curated benchmark datasets. These datasets are typically characterized by consistent data distributions and heuristic or partially noisy labels. In this study, we systematically evaluate two representative DL models-ReVeal and LineVul-across four representative datasets: Juliet, Devign, BigVul, and ICVul. Each model is trained independently on each respective dataset, and their code representations are analyzed using t-SNE to uncover vulnerability related patterns. To assess realistic applicability, we deploy these models along with four pretrained LLMs, Claude 3.5 Sonnet, GPT-o3-mini, GPT-4o, and GPT-5 on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably. These results expose a persistent gap between academic benchmarks and real-world deployment, emphasizing the value of our deployment-oriented evaluation framework and the need for more robust code representations and higher-quality datasets.",
    "arxiv_url": "https://arxiv.org/abs/2512.10485",
    "authors": [
      "Chaomeng Lu",
      "Bert Lagaisse"
    ],
    "first_author": "Chaomeng Lu",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Time-wise out-of-distribution evaluation",
      "Whole-file evaluation mode",
      "Function-pair evaluation mode",
      "Cross-dataset generalization",
      "Code representation clustering (t-SNE & centroid distance)",
      "Graph-based vs token-based code representations",
      "Zero-shot LLM vulnerability probing",
      "Linux CVE fix-based testbed",
      "Label quality and dataset bias analysis"
    ],
    "summary": "本文构建了一个时间分离的真实漏洞测试集并提出部署导向的评估框架，系统性比较图/序列深度模型与LLM在漏洞检测上的表示能力与跨数据集泛化性，结果表明现有模型在真实场景中表现显著下降。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10485v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-14 01:58:04"
  },
  {
    "id": "2512.10452",
    "title": "UniCoR: Modality Collaboration for Robust Cross-Language Hybrid Code Retrieval",
    "abstract": "Effective code retrieval is indispensable and it has become an important paradigm to search code in hybrid mode using both natural language and code snippets. Nevertheless, it remains unclear whether existing approaches can effectively leverage such hybrid queries, particularly in cross-language contexts. We conduct a comprehensive empirical study of representative code models and reveal three challenges: (1) insufficient semantic understanding; (2) inefficient fusion in hybrid code retrieval; and (3) weak generalization in cross-language scenarios. To address these challenges, we propose UniCoR, a novel self-supervised framework that learns Unified Code Representations framework designed to learn unified and robust code representations. Firstly, we design a multi-perspective supervised contrastive learning module to enhance semantic understanding and modality fusion. It aligns representations from multiple perspectives, including code-to-code, natural language-to-code, and natural language-to-natural language, enforcing the model to capture a semantic essence among modalities. Secondly, we introduce a representation distribution consistency learning module to improve cross-language generalization, which explicitly aligns the feature distributions of different programming languages, enabling language-agnostic representation learning. Extensive experiments on both empirical benchmark and large-scale benchmark show that UniCoR outperforms all baseline models, achieving an average improvement of 8.64% in MRR and 11.54% in MAP over the best-performing baseline. Furthermore, UniCoR exhibits stability in hybrid code retrieval and generalization capability in cross-language scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2512.10452",
    "authors": [
      "Yang Yang",
      "Li Kuang",
      "Jiakun Liu",
      "Zhongxin Liu",
      "Yingjie Xia",
      "David Lo"
    ],
    "first_author": "Yang Yang",
    "category": [
      "Technical",
      "Empirical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Retrieval",
    "tags": [
      "Hybrid query fusion (code + natural language)",
      "Cross-language alignment",
      "Multi-perspective supervised contrastive learning",
      "Representation distribution consistency",
      "Maximum Mean Discrepancy (MMD) for language-agnostic alignment",
      "Modality collaboration and fusion",
      "Functionally equivalent code pairing",
      "Automated natural-language query generation",
      "Self-supervised code representation learning",
      "Fusion strategies: input remix / vector concat / score weighting"
    ],
    "summary": "本文提出UniCoR，一种结合多视角监督对比学习与表示分布一致性约束的自监督框架，通过强化模态协同与跨语言对齐，显著提升混合（代码+自然语言）跨语言代码检索的语义理解、融合效果与泛化能力。",
    "quality": "High",
    "conference": "IEEE/ACM International Conference on Software Engineering (ICSE 2026)",
    "pdf_url": "https://arxiv.org/pdf/2512.10452v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-14 01:58:31"
  },
  {
    "id": "2512.10415",
    "title": "How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation",
    "abstract": "The use of Large Language Models (LLMs) as automatic judges for code evaluation is becoming increasingly prevalent in academic environments. But their reliability can be compromised by students who may employ adversarial prompting strategies in order to induce misgrading and secure undeserved academic advantages. In this paper, we present the first large-scale study of jailbreaking LLM-based automated code evaluators in academic context. Our contributions are: (i) We systematically adapt 20+ jailbreaking strategies for jailbreaking AI code evaluators in the academic context, defining a new class of attacks termed academic jailbreaking. (ii) We release a poisoned dataset of 25K adversarial student submissions, specifically designed for the academic code-evaluation setting, sourced from diverse real-world coursework and paired with rubrics and human-graded references, and (iii) In order to capture the multidimensional impact of academic jailbreaking, we systematically adapt and define three jailbreaking metrics (Jailbreak Success Rate, Score Inflation, and Harmfulness). (iv) We comprehensively evalulate the academic jailbreaking attacks using six LLMs. We find that these models exhibit significant vulnerability, particularly to persuasive and role-play-based attacks (up to 97% JSR). Our adversarial dataset and benchmark suite lay the groundwork for next-generation robust LLM-based evaluators in academic code assessment.",
    "arxiv_url": "https://arxiv.org/abs/2512.10415",
    "authors": [
      "Devanshu Sahoo",
      "Vasudev Majhi",
      "Arjun Neekhra",
      "Yash Sinha",
      "Murari Mandal",
      "Dhruv Kumar"
    ],
    "first_author": "Devanshu Sahoo",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Understanding",
    "tags": [
      "Academic Jailbreaking",
      "Adversarial Prompt Injection",
      "Poisoned Student Submissions",
      "Rubric-aware Attack Design",
      "Jailbreak Metrics (JSR/SIR/Harmfulness)",
      "Persona/Role-play Attacks",
      "Token/Emoji-level Manipulation",
      "Cross-model Robustness Benchmark"
    ],
    "summary": "本文首次系统研究了针对LLM自动代码评分器的“学术越狱”攻击，构建了约25K条对抗性学生提交数据集、提出专门的评价指标并在六种模型上进行基准评估，揭示了角色扮演与说服类攻击下的严重脆弱性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10415v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-14 02:06:36"
  },
  {
    "id": "2512.10398",
    "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale",
    "abstract": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.",
    "arxiv_url": "https://arxiv.org/abs/2512.10398",
    "authors": [
      "Zhaodong Wang",
      "Zhenting Qi",
      "Sherman Wong",
      "Nathan Hu",
      "Samuel Lin",
      "Jun Ge",
      "Erwin Gao",
      "Yining Yang",
      "Ben Maurer",
      "Wenlin Chen",
      "David Recordon",
      "Yilun Du",
      "Minlan Yu",
      "Ying Zhang"
    ],
    "first_author": "Zhaodong Wang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Hierarchical working memory",
      "Adaptive context compression",
      "Persistent note-taking",
      "Cross-session continual learning",
      "Meta-agent build–test–improve loop",
      "Modular tool extensions with typed callbacks",
      "Agent orchestration for long-horizon tasks",
      "Developer observability and DX-focused tooling",
      "Industrial-scale repository navigation",
      "Ablation-driven agent evaluation"
    ],
    "summary": "本文提出并开源了Confucius SDK及其实例化的Confucius Code Agent，通过层次化工作内存、自适应上下文压缩、持久化笔记和模块化扩展，并借助元代理的构建-测试-改进循环，实现面向工业规模代码库的长期推理与跨会话记忆，在多种软件工程任务上表现优异。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10398v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-14 02:07:20"
  },
  {
    "id": "2512.10393",
    "title": "Cross-modal Retrieval Models for Stripped Binary Analysis",
    "abstract": "LLM-agent based binary code analysis has demonstrated significant potential across a wide range of software security scenarios, including vulnerability detection, malware analysis, etc. In agent workflow, however, retrieving the positive from thousands of stripped binary functions based on user query remains under-studied and challenging, as the absence of symbolic information distinguishes it from source code retrieval. In this paper, we introduce, BinSeek, the first two-stage cross-modal retrieval framework for stripped binary code analysis. It consists of two models: BinSeekEmbedding is trained on large-scale dataset to learn the semantic relevance of the binary code and the natural language description, furthermore, BinSeek-Reranker learns to carefully judge the relevance of the candidate code to the description with context augmentation. To this end, we built an LLM-based data synthesis pipeline to automate training construction, also deriving a domain benchmark for future research. Our evaluation results show that BinSeek achieved the state-of-the-art performance, surpassing the the same scale models by 31.42% in Rec@3 and 27.17% in MRR@3, as well as leading the advanced general-purpose models that have 16 times larger parameters.",
    "arxiv_url": "https://arxiv.org/abs/2512.10393",
    "authors": [
      "Guoqiang Chen",
      "Lingyun Ying",
      "Ziyang Song",
      "Daguang Liu",
      "Qiang Wang",
      "Zhiqi Wang",
      "Li Hu",
      "Shaoyin Cheng",
      "Weiming Zhang",
      "Nenghai Yu"
    ],
    "first_author": "Guoqiang Chen",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Retrieval",
    "tags": [
      "stripped-binary retrieval",
      "decompiled pseudocode embeddings",
      "two-stage retrieval (embedding + reranker)",
      "context-augmented reranking with calling-context",
      "LLM-driven synthetic label generation",
      "contrastive embedding training (InfoNCE)",
      "domain-specific binary retrieval benchmark",
      "retrieval-augmented binary analysis for security"
    ],
    "summary": "本文提出了BinSeek，一种针对无符号（stripped）二进制函数的两阶段跨模态检索框架（嵌入召回 + 上下文增强重排序），并通过LLM驱动的数据合成构建训练集与首个领域基准，显著提升二进制代码检索性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10393v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-15 01:56:55"
  },
  {
    "id": "2512.10238",
    "title": "Studying and Automating Issue Resolution for Software Quality",
    "abstract": "Effective issue resolution is crucial for maintaining software quality. Yet developers frequently encounter challenges such as low-quality issue reports, limited understanding of real-world workflows, and a lack of automated support. This research aims to address these challenges through three complementary directions. First, we enhance issue report quality by proposing techniques that leverage LLM reasoning and application-specific information. Second, we empirically characterize developer workflows in both traditional and AI-augmented systems. Third, we automate cognitively demanding resolution tasks, including buggy UI localization and solution identification, through ML, DL, and LLM-based approaches. Together, our work delivers empirical insights, practical tools, and automated methods to advance AI-driven issue resolution, supporting more maintainable and high-quality software systems.",
    "arxiv_url": "https://arxiv.org/abs/2512.10238",
    "authors": [
      "Antu Saha"
    ],
    "first_author": "Antu Saha",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Quality Management",
    "task": "Bug Localization",
    "tags": [
      "Issue report quality assessment",
      "Steps-to-Reproduce (S2R) enhancement",
      "LLM reasoning aligned with dynamic app execution",
      "Execution model of screens and interactions",
      "Context-aware issue report generation",
      "Buggy UI localization",
      "Multi-modal text+vision retrieval for UI mapping",
      "Solution identification in issue discussions",
      "Empirical analysis of issue resolution workflows",
      "Cross-project generalizability of classifiers"
    ],
    "summary": "本文结合基于应用执行模型的多模态信息与LLM推理，提升缺陷报告（OB/EB/S2R）质量，实证分析传统与AI/ML集成系统的问题解决流程，并自动化实现Buggy UI定位与解决方案识别以加速问题定位与修复。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10238v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-15 01:57:20"
  },
  {
    "id": "2512.10501",
    "title": "Zero-shot 3D Map Generation with LLM Agents: A Dual-Agent Architecture for Procedural Content Generation",
    "abstract": "Procedural Content Generation (PCG) offers scalable methods for algorithmically creating complex, customizable worlds. However, controlling these pipelines requires the precise configuration of opaque technical parameters. We propose a training-free architecture that utilizes LLM agents for zero-shot PCG parameter configuration. While Large Language Models (LLMs) promise a natural language interface for PCG tools, off-the-shelf models often fail to bridge the semantic gap between abstract user instructions and strict parameter specifications. Our system pairs an Actor agent with a Critic agent, enabling an iterative workflow where the system autonomously reasons over tool parameters and refines configurations to progressively align with human design preferences. We validate this approach on the generation of various 3D maps, establishing a new benchmark for instruction-following in PCG. Experiments demonstrate that our approach outperforms single-agent baselines, producing diverse and structurally valid environments from natural language descriptions. These results demonstrate that off-the-shelf LLMs can be effectively repurposed as generalized agents for arbitrary PCG tools. By shifting the burden from model training to architectural reasoning, our method offers a scalable framework for mastering complex software without task-specific fine-tuning.",
    "arxiv_url": "https://arxiv.org/abs/2512.10501",
    "authors": [
      "Lim Chien Her",
      "Ming Yan",
      "Yunshu Bai",
      "Ruihao Li",
      "Hao Zhang"
    ],
    "first_author": "Lim Chien Her",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Procedural Content Generation",
    "task": "Zero-shot PCG parameter configuration for 3D map generation (dual-agent Actor–Critic)",
    "tags": [
      "Dual-agent Actor–Critic architecture",
      "Zero-shot parameter synthesis",
      "Documentation-grounded verification",
      "Iterative agent dialogic refinement",
      "Training-free LLM tool control",
      "3D map / terrain synthesis",
      "PCG parameter hallucination correction",
      "Instruction-following benchmark for PCG"
    ],
    "summary": "本文提出一种训练免费、双代理（Actor–Critic）的架构，利用离线大模型在零样本情形下通过迭代对话将自然语言提示映射为可执行的 PCG 参数，从而生成多样且结构有效的 3D 地图，并在新建的指令跟随基准上显著优于单代理基线。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10501v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-15 02:05:01"
  },
  {
    "id": "2512.10317",
    "title": "Translating Informal Proofs into Formal Proofs Using a Chain of States",
    "abstract": "We address the problem of translating informal mathematical proofs expressed in natural language into formal proofs in Lean4 under a constrained computational budget. Our approach is grounded in two key insights. First, informal proofs tend to proceed via a sequence of logical transitions - often implications or equivalences - without explicitly specifying intermediate results or auxiliary lemmas. In contrast, formal systems like Lean require an explicit representation of each proof state and the tactics that connect them. Second, each informal reasoning step can be viewed as an abstract transformation between proof states, but identifying the corresponding formal tactics often requires nontrivial domain knowledge and precise control over proof context. To bridge this gap, we propose a two stage framework. Rather than generating formal tactics directly, we first extract a Chain of States (CoS), a sequence of intermediate formal proof states aligned with the logical structure of the informal argument. We then generate tactics to transition between adjacent states in the CoS, thereby constructing the full formal proof. This intermediate representation significantly reduces the complexity of tactic generation and improves alignment with informal reasoning patterns. We build dedicated datasets and benchmarks for training and evaluation, and introduce an interactive framework to support tactic generation from formal states. Empirical results show that our method substantially outperforms existing baselines, achieving higher proof success rates.",
    "arxiv_url": "https://arxiv.org/abs/2512.10317",
    "authors": [
      "Ziyu Wang",
      "Bowen Yang",
      "Shihao Zhou",
      "Chenyi Li",
      "Yuan Zhang",
      "Bin Dong",
      "Zaiwen Wen"
    ],
    "first_author": "Ziyu Wang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Formal Methods & Theorem Proving",
    "task": "Informal-to-Formal Proof Translation",
    "tags": [
      "Chain of States (CoS) intermediate representation",
      "proof-state extraction from informal text",
      "state-aligned tactic synthesis",
      "error-aware tactic regeneration",
      "Lean4-specific proof rewriting heuristics",
      "metaprogramming-based dataset construction",
      "interactive tactic generation framework",
      "reduction of prover invocations / compute-efficient proving"
    ],
    "summary": "本文提出了CoS两阶段框架：先从非正式数学证明抽取对齐的形式化证明状态链，再在相邻状态之间生成tactic以在Lean4中还原完整形式化证明，并通过元编程构建数据集与基准，显著提高了证明成功率与效率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.10317v1",
    "published": "2025-12-11",
    "update_time": "2025-12-11",
    "download_time": "2025-12-15 02:09:36"
  },
  {
    "id": "2512.11589",
    "title": "A Study of Library Usage in Agent-Authored Pull Requests",
    "abstract": "Coding agents are becoming increasingly capable of completing end-to-end software engineering workflows that previously required a human developer, including raising pull requests (PRs) to propose their changes. However, we still know little about how these agents use libraries when generating code, a core part of real-world software development. To fill this gap, we study 26,760 agent-authored PRs from the AIDev dataset to examine three questions: how often do agents import libraries, how often do they introduce new dependencies (and with what versioning), and which specific libraries do they choose? We find that agents often import libraries (29.5% of PRs) but rarely add new dependencies (1.3% of PRs); and when they do, they follow strong versioning practices (75.0% specify a version), an improvement on direct LLM usage where versions are rarely mentioned. Generally, agents draw from a surprisingly diverse set of external libraries, contrasting with the limited \"library preferences\" seen in prior non-agentic LLM studies. Our results offer an early empirical view into how AI coding agents interact with today's software ecosystems.",
    "arxiv_url": "https://arxiv.org/abs/2512.11589",
    "authors": [
      "Lukas Twist"
    ],
    "first_author": "Lukas Twist",
    "category": [
      "Empirical"
    ],
    "field": "Version Control & Collaboration",
    "task": "Git VCS",
    "tags": [
      "Agent-authored pull requests",
      "Library import frequency",
      "New dependency introduction",
      "Dependency version specification (version hygiene)",
      "Library diversity across ecosystems",
      "Multi-language empirical analysis (TypeScript, Python, Go, C#)",
      "Import and manifest regex extraction",
      "Agentic workflow behavior"
    ],
    "summary": "本文通过分析26,760个由代码代理提交的拉取请求，实证研究代理在导入库、引入新依赖及选择库上的行为，发现代理常导入外部库但很少新增依赖且通常会指定版本，同时展现出较高的库多样性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.11589v1",
    "published": "2025-12-12",
    "update_time": "2025-12-12",
    "download_time": "2025-12-16 01:52:27"
  },
  {
    "id": "2512.11482",
    "title": "Towards Privacy-Preserving Code Generation: Differentially Private Code Language Models",
    "abstract": "Large language models specialized for code (CodeLLMs) have demonstrated remarkable capabilities in generating code snippets, documentation, and test cases. However, despite their promising capabilities, CodeLLMs can inadvertently memorize and reproduce snippets from their training data, which poses risks of privacy breaches and intellectual property violations. These risks restrict the deployment of CodeLLMs in sensitive domains and limit their training datasets to publicly available sources. To mitigate the memorization risk without compromising their task performance, we apply Differential Privacy (DP) to CodeLLMs. To the best of our knowledge, this is the first comprehensive study that systematically evaluates the effectiveness of DP in CodeLLMs. DP adds calibrated noise to the training process to protect individual data points while still allowing the model to learn useful patterns. To this end, we first identify and understand the driving reasons of the memorization behaviour of the CodeLLMs during their fine-tuning. Then, to address this issue, we empirically evaluate the effect of DP on mitigating memorization while preserving code generation capabilities. Our findings show that DP substantially reduces memorization in CodeLLMs across all the tested snippet types. The snippet types most prone to memorization are also the most effectively mitigated by DP. Furthermore, we observe that DP slightly increases perplexity but preserves, and can even enhance, the code generation capabilities of CodeLLMs, which makes it feasible to apply DP in practice without significantly compromising model utility. Finally, we analyze the impact of DP on training efficiency and energy consumption, finding that DP does not significantly affect training time or energy usage, making it a practical choice for privacy-preserving CodeLLMs training.",
    "arxiv_url": "https://arxiv.org/abs/2512.11482",
    "authors": [
      "Melih Catal",
      "Pooja Rani",
      "Harald C. Gall"
    ],
    "first_author": "Melih Catal",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Pre-Training",
    "tags": [
      "DP-SGD training",
      "Memorization taxonomy",
      "Data‑extraction and membership‑inference attacks",
      "Privacy‑utility trade-off",
      "Snippet entropy and frequency analysis",
      "Functional correctness evaluation",
      "Training efficiency & energy analysis",
      "Privacy evaluation benchmarks"
    ],
    "summary": "本文系统评估在代码生成模型中应用差分隐私（DP）的可行性，发现DP显著降低对训练片段的记忆风险且在大多数情况下不会显著损害甚至可提升代码生成功能性，并发布了两个用于隐私与效用评估的新基准数据集。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.11482v1",
    "published": "2025-12-12",
    "update_time": "2025-12-12",
    "download_time": "2025-12-16 01:53:12"
  },
  {
    "id": "2512.11783",
    "title": "Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously",
    "abstract": "The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious inputs. In this work, we advance the study of adversarial inputs by introducing Super Suffixes, suffixes capable of overriding multiple alignment objectives across various models with different tokenization schemes. We demonstrate their effectiveness, along with our joint optimization technique, by successfully bypassing the protection mechanisms of Llama Prompt Guard 2 on five different text generation models for malicious text and code generation. To the best of our knowledge, this is the first work to reveal that Llama Prompt Guard 2 can be compromised through joint optimization.   Additionally, by analyzing the changing similarity of a model's internal state to specific concept directions during token sequence processing, we propose an effective and lightweight method to detect Super Suffix attacks. We show that the cosine similarity between the residual stream and certain concept directions serves as a distinctive fingerprint of model intent. Our proposed countermeasure, DeltaGuard, significantly improves the detection of malicious prompts generated through Super Suffixes. It increases the non-benign classification rate to nearly 100%, making DeltaGuard a valuable addition to the guard model stack and enhancing robustness against adversarial prompt attacks.",
    "arxiv_url": "https://arxiv.org/abs/2512.11783",
    "authors": [
      "Andrew Adiletta",
      "Kathryn Adiletta",
      "Kemal Derya",
      "Berk Sunar"
    ],
    "first_author": "Andrew Adiletta",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Super Suffixes",
      "Joint optimization across tokenization schemes",
      "Adversarial suffix attacks",
      "Guard model bypass",
      "Refusal concept direction",
      "Dynamic residual-similarity detection",
      "DeltaGuard",
      "Malicious code-generation dataset"
    ],
    "summary": "本文提出 Super Suffixes —— 通过在不同分词方案下联合优化以同时绕过文本生成模型的对齐与守卫模型，并基于残差流与拒绝概念方向的动态余弦相似度提出 DeltaGuard 检测方法，同时构建恶意代码生成数据集进行评估。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.11783v1",
    "published": "2025-12-12",
    "update_time": "2025-12-12",
    "download_time": "2025-12-16 01:53:54"
  },
  {
    "id": "2512.11402",
    "title": "REMODEL-LLM: Transforming C code to Java using LLMs",
    "abstract": "The automated translation of C code to Java code is a notoriously difficult task, fraught with challenges stemming from fundamental paradigm shifts (procedural vs. Object Oriented), memory models (manual pointers vs. Garbage Collection), and incompatible data types. This paper investigates the efficacy of 19 small, quantized LLMs (under 20 billion parameters) for the C to Java translation task. We use a novel, hybrid pipeline that leverages Abstract Syntax Trees (ASTs) for semantic decomposition and employs a highly constrained, rule based prompting strategy. The results are stark: a clear multi tiered performance divide emerged. The vast majority of models (Tier 3, e.g., llama3.1, gemma3, starcoder2) failed 100\\% of the tests, proving incapable of generating even basic, runnable Java boilerplate. A small middle tier (Tier 2, e.g., mistral-nemo and mistral) produced runnable code but was plagued by dangerous semantic failures and wrong translations. Only three models (Tier 1: phi4, deepseek-coder-v2, codeqwen) proved viable, passing over 50\\% of the test suite. Even these top models failed on the most complex C concepts, such as function pointers, sizeof, and enum logic, revealing a hard ceiling for the reasoning capabilities of current quantized models.",
    "arxiv_url": "https://arxiv.org/abs/2512.11402",
    "authors": [
      "Aryan Gupta",
      "Y. Raghu Reddy"
    ],
    "first_author": "Aryan Gupta",
    "category": [
      "Empirical",
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "C-to-Java migration",
      "AST-guided decomposition",
      "Constrained rule-based prompting",
      "Quantized edge LLM evaluation",
      "Semantic vs. syntactic failure analysis",
      "Translation edge cases (function pointers, sizeof, enums)",
      "20-case C→Java benchmark"
    ],
    "summary": "本文提出基于AST的混合翻译流水线和受限提示策略，评估了19个量化小型LLM在C到Java迁移任务上的表现并构建了20个边界用例基准，结果显示大多数模型在关键语义转换（如函数指针、sizeof、枚举逻辑）上失败，仅三款模型在测试中通过率超过50%。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.11402v1",
    "published": "2025-12-12",
    "update_time": "2025-12-12",
    "download_time": "2025-12-16 01:59:31"
  },
  {
    "id": "2512.12719",
    "title": "Towards AI Agents Supported Research Problem Formulation",
    "abstract": "Poorly formulated research problems can compromise the practical relevance of Software Engineering studies by not reflecting the complexities of industrial practice. This vision paper explores the use of artificial intelligence agents to support SE researchers during the early stage of a research project, the formulation of the research problem. Based on the Lean Research Inception framework and using a published study on code maintainability in machine learning as a reference, we developed a descriptive evaluation of a scenario illustrating how AI agents, integrated into LRI, can support SE researchers by pre filling problem attributes, aligning stakeholder perspectives, refining research questions, simulating multiperspective assessments, and supporting decision making. The descriptive evaluation of the scenario suggests that AI agent support can enrich collaborative discussions and enhance critical reflection on the value, feasibility, and applicability of the research problem. Although the vision of integrating AI agents into LRI was perceived as promising to support the context aware and practice oriented formulation of research problems, empirical validation is needed to confirm and refine the integration of AI agents into problem formulation.",
    "arxiv_url": "https://arxiv.org/abs/2512.12719",
    "authors": [
      "Anrafel Fernandes Pereira",
      "Maria Teresa Baldassarre",
      "Daniel Mendez",
      "Marcos Kalinowski"
    ],
    "first_author": "Anrafel Fernandes Pereira",
    "category": [
      "Technical"
    ],
    "field": "Research Process & Methodology",
    "task": "Research Problem Formulation",
    "tags": [
      "AI-assisted problem framing",
      "Problem Vision board pre-filling",
      "Multi-agent reasoning for research design",
      "Stakeholder-perspective simulation",
      "Semantic-differential assessment augmentation",
      "Literature-and-industry evidence synthesis",
      "Decision-support for go/pivot/abort"
    ],
    "summary": "该愿景论文提出将 AI 多代理体整合入 Lean Research Inception 框架，通过预填问题属性、促进利益相关者对齐、生成多视角评估并支持决策流程以改善软件工程研究问题的表述，但仍需实证验证。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.12719v1",
    "published": "2025-12-14",
    "update_time": "2025-12-14",
    "download_time": "2025-12-16 02:40:02"
  },
  {
    "id": "2512.12706",
    "title": "Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning",
    "abstract": "The widespread adoption of the \"Games as a Service\" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness.",
    "arxiv_url": "https://arxiv.org/abs/2512.12706",
    "authors": [
      "Enhong Mu",
      "Minami Yoda",
      "Yan Zhang",
      "Mingyue Zhang",
      "Yutaka Matsuno",
      "Jialong Li"
    ],
    "first_author": "Enhong Mu",
    "category": [
      "Technical"
    ],
    "field": "Software Testing",
    "task": "Testing automation",
    "tags": [
      "AST-diff semantic interpretation",
      "LLM-driven subgoal decomposition",
      "Hybrid structural-functional reward",
      "Coverage-aware reinforcement learning",
      "Context-aware mapping of subgoals to code anchors",
      "Game-update regression testing"
    ],
    "summary": "本文提出SMART框架，利用LLM解析AST差异将更新意图分解为语义子目标，并通过将这些子目标与结构性代码锚点动态映射并构建混合奖励来引导强化学习代理在游戏更新测试中同时实现高分支覆盖率与高任务完成率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.12706v1",
    "published": "2025-12-14",
    "update_time": "2025-12-14",
    "download_time": "2025-12-16 02:40:31"
  },
  {
    "id": "2512.11296",
    "title": "Few-Shot VLM-Based G-Code and HMI Verification in CNC Machining",
    "abstract": "Manual generation of G-code is important for learning the operation of CNC machines. Prior work in G-code verification uses Large-Language Models (LLMs), which primarily examine errors in the written programming. However, CNC machining requires extensive use and knowledge of the Human-Machine Interface (HMI), which displays machine status and errors. LLMs currently lack the capability to leverage knowledge of HMIs due to their inability to access the vision modality. This paper proposes a few-shot VLM-based verification approach that simultaneously evaluates the G-code and the HMI display for errors and safety status. The input dataset includes paired G-code text and associated HMI screenshots from a 15-slant-PRO lathe, including both correct and error-prone cases. To enable few-shot learning, the VLM is provided with a structured JSON schema based on prior heuristic knowledge. After determining the prompts, instances of G-code and HMI that either contain errors or are error free are used as few-shot examples to guide the VLM. The model was then evaluated in comparison to a zero-shot VLM through multiple scenarios of incorrect G-code and HMI errors with respect to per-slot accuracy. The VLM showed that few-shot prompting led to overall enhancement of detecting HMI errors and discrepancies with the G-code for more comprehensive debugging. Therefore, the proposed framework was demonstrated to be suitable for verification of manually generated G-code that is typically developed in CNC training.",
    "arxiv_url": "https://arxiv.org/abs/2512.11296",
    "authors": [
      "Yasaman Hashem Pour",
      "Nazanin Mahjourian",
      "Vinh Nguyen"
    ],
    "first_author": "Yasaman Hashem Pour",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Understanding",
    "tags": [
      "G-code verification",
      "HMI indicator recognition",
      "Multimodal VLM few-shot prompting",
      "Structured JSON slot-filling",
      "G-code–machine-state alignment",
      "CNC safety/status checking"
    ],
    "summary": "本文提出一种基于视觉-语言模型的少样本联合校验方法，通过将G-code文本与机床HMI截图配对并使用结构化JSON schema进行few-shot提示，从而同时检测G-code语法错误与HMI显示与代码的不一致以提升人工编写G-code的验证与安全性。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.11296v1",
    "published": "2025-12-12",
    "update_time": "2025-12-12",
    "download_time": "2025-12-16 02:48:32"
  },
  {
    "id": "2512.11270",
    "title": "A-LAMP: Agentic LLM-Based Framework for Automated MDP Modeling and Policy Generation",
    "abstract": "Applying reinforcement learning (RL) to real-world tasks requires converting informal descriptions into a formal Markov decision process (MDP), implementing an executable environment, and training a policy agent. Automating this process is challenging due to modeling errors, fragile code, and misaligned objectives, which often impede policy training. We introduce an agentic large language model (LLM)-based framework for automated MDP modeling and policy generation (A-LAMP), that automatically translates free-form natural language task descriptions into an MDP formulation and trained policy. The framework decomposes modeling, coding, and training into verifiable stages, ensuring semantic alignment throughout the pipeline. Across both classic control and custom RL domains, A-LAMP consistently achieves higher policy generation capability than a single state-of-the-art LLM model. Notably, even its lightweight variant, which is built on smaller language models, approaches the performance of much larger models. Failure analysis reveals why these improvements occur. In addition, a case study also demonstrates that A-LAMP generates environments and policies that preserve the task's optimality, confirming its correctness and reliability.",
    "arxiv_url": "https://arxiv.org/abs/2512.11270",
    "authors": [
      "Hong Je-Gal",
      "Chan-Bin Yi",
      "Hyun-Suk Lee"
    ],
    "first_author": "Hong Je-Gal",
    "category": [
      "Technical"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Agentic multi-agent orchestration",
      "Automated MDP synthesis",
      "Natural-language to environment code generation",
      "Reward function specification",
      "Policy training automation",
      "Modular verifiable pipeline",
      "Failure-mode analysis"
    ],
    "summary": "该论文提出A-LAMP，一个基于多智能体LLM的框架，能够将自由文本任务描述自动转为形式化MDP、生成可执行环境与训练代码并产出可验证的策略，且在多域实验中优于单一大模型。",
    "quality": "High",
    "conference": "NeurIPS 2025 Workshop: Multi-Turn Interactions in Large Language Models 2025",
    "pdf_url": "https://arxiv.org/pdf/2512.11270v1",
    "published": "2025-12-12",
    "update_time": "2025-12-12",
    "download_time": "2025-12-16 02:49:40"
  },
  {
    "id": "2512.13515",
    "title": "Fine-tuned LLM-based Code Migration Framework",
    "abstract": "The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2512.13515",
    "authors": [
      "Oleg Grynets",
      "Vasyl Lyashkevych",
      "Dmytro Baran",
      "Maksym Orliansky",
      "Taras Zelenyy",
      "Markiian Leshchyshyn"
    ],
    "first_author": "Oleg Grynets",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "Two-stage fine-tuning",
      "Hybrid Code Feature Profiling Engine (HCFPE)",
      "Retrieval-Augmented Generation for migration",
      "Closed-loop human-in-the-loop error correction",
      "SQL feature detection and annotation",
      "Multi-repository vs unified FAISS knowledge base",
      "Syntactic-description and direct transformation dataset configurator",
      "Semi-supervised error analysis and failure taxonomy"
    ],
    "summary": "本文提出了一个面向Oracle到PostgreSQL的自动化代码迁移框架，结合两阶段微调、特征分析引擎、RAG知识库与人机闭环错误分析以提升迁移的语法与语义准确性。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.13515v1",
    "published": "2025-12-15",
    "update_time": "2025-12-15",
    "download_time": "2025-12-17 01:48:11"
  },
  {
    "id": "2512.13438",
    "title": "From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents",
    "abstract": "While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.",
    "arxiv_url": "https://arxiv.org/abs/2512.13438",
    "authors": [
      "Dezhi Ran",
      "Zhi Gong",
      "Yuzhe Guo",
      "Mengzhou Wu",
      "Yuan Cao",
      "Haochuan Lu",
      "Hengyu Zhang",
      "Xia Zeng",
      "Gang Cao",
      "Liangchao Yao",
      "Yuetang Deng",
      "Wei Yang",
      "Tao Xie"
    ],
    "first_author": "Dezhi Ran",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Performance Optimization",
    "tags": [
      "UI representation optimization",
      "DSL for UI transformations",
      "Program synthesis for UI trees",
      "Iterative LLM-based refinement",
      "Efficiency–completeness co-optimization",
      "Local decomposition and tree merging",
      "Lightweight plugin integration for agents",
      "Constraint-based evaluation rewards"
    ],
    "summary": "本文提出UIFORMER，通过设计面向UI的DSL并结合基于LLM的迭代约束优化与结构化分解，自动合成将复杂UI树转换为低令牌开销且保持语义完备的表示的转换程序，从而显著降低LLM代理的token成本并在多平台基准与实际微信部署中保持或提升代理性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.13438v1",
    "published": "2025-12-15",
    "update_time": "2025-12-15",
    "download_time": "2025-12-17 01:49:07"
  },
  {
    "id": "2512.13607",
    "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models",
    "abstract": "Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.",
    "arxiv_url": "https://arxiv.org/abs/2512.13607",
    "authors": [
      "Boxin Wang",
      "Chankyu Lee",
      "Nayeon Lee",
      "Sheng-Chieh Lin",
      "Wenliang Dai",
      "Yang Chen",
      "Yangyi Chen",
      "Zhuolin Yang",
      "Zihan Liu",
      "Mohammad Shoeybi",
      "Bryan Catanzaro",
      "Wei Ping"
    ],
    "first_author": "Boxin Wang",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Cascaded domain-wise reinforcement learning",
      "RLHF as a pre-step to boost reasoning",
      "Instruction-following RL (IF-RL)",
      "Math-specific RL curriculum",
      "Code RL for competitive programming",
      "Software-engineering RL (SWE RL)",
      "Execution-free reward modeling for SWE",
      "Generation–retrieval for code localization",
      "Test-time scaling / thinking modes",
      "Sequential-domain curriculum to mitigate forgetting"
    ],
    "summary": "本文提出级联领域强化学习（Cascade RL）来训练通用推理模型Nemotron‑Cascade，通过序贯的领域级RL（包含RLHF、IF‑RL、Math RL、Code RL、SWE RL）与工程化技巧提升代码推理与竞赛编程等任务的表现，并公开训练与数据配方。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.13607v1",
    "published": "2025-12-15",
    "update_time": "2025-12-15",
    "download_time": "2025-12-17 01:50:27"
  },
  {
    "id": "2512.13102",
    "title": "Socratic Students: Teaching Language Models to Learn by Asking Questions",
    "abstract": "Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.",
    "arxiv_url": "https://arxiv.org/abs/2512.13102",
    "authors": [
      "Rajeev Bhatt Ambati",
      "Tianyi Niu",
      "Aashu Singh",
      "Shlok Mishra",
      "Shashank Srivastava",
      "Snigdha Chaturvedi"
    ],
    "first_author": "Rajeev Bhatt Ambati",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Student-led interaction",
      "Active question generation",
      "Direct Preference Optimization for question selection",
      "Self- and peer-guidance",
      "Chain-of-Thought guided questioning",
      "Pre- and mid-assessment grounding",
      "Teacher-student dialogue protocol",
      "Pass@k-based preference collection"
    ],
    "summary": "本文提出一种学生主导的交互学习框架，训练小型模型通过主动提问（包含CoT指导、评估插入和基于DPO的自我/同辈指导）与强教师交互，在数学与编程任务上显著提升了Pass@k表现。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.13102v1",
    "published": "2025-12-15",
    "update_time": "2025-12-15",
    "download_time": "2025-12-17 01:57:25"
  },
  {
    "id": "2512.14429",
    "title": "Seismology modeling agent: A smart assistant for geophysical researchers",
    "abstract": "To address the steep learning curve and reliance on complex manual file editing and command-line operations in the traditional workflow of the mainstream open-source seismic wave simulation software SPECFEM, this paper proposes an intelligent, interactive workflow powered by Large Language Models (LLMs). We introduce the first Model Context Protocol (MCP) server suite for SPECFEM (supporting 2D, 3D Cartesian, and 3D Globe versions), which decomposes the entire simulation process into discrete, agent-executable tools spanning from parameter generation and mesh partitioning to solver execution and visualization. This approach enables a paradigm shift from file-driven to intent-driven conversational interactions. The framework supports both fully automated execution and human-in-the-loop collaboration, allowing researchers to guide simulation strategies in real time and retain scientific decision-making authority while significantly reducing tedious low-level operations. Validated through multiple case studies, the workflow operates seamlessly in both autonomous and interactive modes, yielding high-fidelity results consistent with standard baselines. As the first application of MCP technology to computational seismology, this study significantly lowers the entry barrier, enhances reproducibility, and offers a promising avenue for advancing computational geophysics toward AI-assisted and automated scientific research. The complete source code is available at https://github.com/RenYukun1563/specfem-mcp.",
    "arxiv_url": "https://arxiv.org/abs/2512.14429",
    "authors": [
      "Yukun Ren",
      "Siwei Yu",
      "Kai Chen",
      "Jianwei Ma"
    ],
    "first_author": "Yukun Ren",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Agents",
    "tags": [
      "Code Agents",
      "MCP Integration",
      "Agentic Workflow",
      "Seismic Simulation",
      "Human-in-the-loop",
      "Legacy Software Wrapping",
      "Intent-driven Interface",
      "Reproducibility"
    ],
    "summary": "本文为SPECFEM构建了首个Model Context Protocol (MCP) 服务器套件，并结合LLM驱动的代理实现以自然语言为中心的自动化与交互式地震波模拟工作流，从而降低使用门槛并提升可重复性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.14429v1",
    "published": "2025-12-16",
    "update_time": "2025-12-16",
    "download_time": "2025-12-18 01:48:51"
  },
  {
    "id": "2512.14233",
    "title": "PentestEval: Benchmarking LLM-based Penetration Testing with Modular and Stage-Level Design",
    "abstract": "Penetration testing is essential for assessing and strengthening system security against real-world threats, yet traditional workflows remain highly manual, expertise-intensive, and difficult to scale. Although recent advances in Large Language Models (LLMs) offer promising opportunities for automation, existing applications rely on simplistic prompting without task decomposition or domain adaptation, resulting in unreliable black-box behavior and limited insight into model capabilities across penetration testing stages. To address this gap, we introduce PentestEval, the first comprehensive benchmark for evaluating LLMs across six decomposed penetration testing stages: Information Collection, Weakness Gathering and Filtering, Attack Decision-Making, Exploit Generation and Revision. PentestEval integrates expert-annotated ground truth with a fully automated evaluation pipeline across 346 tasks covering all stages in 12 realistic vulnerable scenarios. Our stage-level evaluation of 9 widely used LLMs reveals generally weak performance and distinct limitations across the stages of penetration-testing workflow. End-to-end pipelines reach only 31% success rate, and existing LLM-powered systems such as PentestGPT, PentestAgent, and VulnBot exhibit similar limitations, with autonomous agents failing almost entirely. These findings highlight that autonomous penetration testing demands stronger structured reasoning, where modularization enhances each individual stage and improves overall performance. PentestEval provides the foundational benchmark needed for future research on fine-grained, stage-level evaluation, paving the way toward more reliable LLM-based automation.",
    "arxiv_url": "https://arxiv.org/abs/2512.14233",
    "authors": [
      "Ruozhao Yang",
      "Mingfei Cheng",
      "Gelei Deng",
      "Tianwei Zhang",
      "Junjie Wang",
      "Xiaofei Xie"
    ],
    "first_author": "Ruozhao Yang",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Security Testing",
    "task": "Penetration Testing Evaluation",
    "tags": [
      "Exploit Generation",
      "Exploit Revision",
      "Attack Decision-Making",
      "Weakness Gathering",
      "Weakness Filtering",
      "Stage-level Evaluation",
      "Modular Benchmark",
      "Ground-truth Annotations",
      "Autonomous Agent Failure",
      "Structured Reasoning",
      "End-to-End Evaluation"
    ],
    "summary": "本文提出PentestEval——一个针对渗透测试六个分阶段的模块化基准，配备专家注释与自动化评测管线，用以细粒度评估LLM在信息收集、弱点过滤、攻击决策与利用生成等环节的能力并揭示现有模型与自动化系统的显著局限性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.14233v1",
    "published": "2025-12-16",
    "update_time": "2025-12-16",
    "download_time": "2025-12-18 01:49:18"
  },
  {
    "id": "2512.14481",
    "title": "SASQ: Static Activation Scaling for Quantization-Aware Training in Large Language Models",
    "abstract": "Large language models (LLMs) excel at natural language tasks but face deployment challenges due to their growing size outpacing GPU memory advancements. Model quantization mitigates this issue by lowering weight and activation precision, but existing solutions face fundamental trade-offs: dynamic quantization incurs high computational overhead and poses deployment challenges on edge devices, while static quantization sacrifices accuracy. Existing approaches of quantization-aware training (QAT) further suffer from weight training costs. We propose SASQ: a lightweight QAT framework specifically tailored for activation quantization factors. SASQ exclusively optimizes only the quantization factors (without changing pre-trained weights), enabling static inference with high accuracy while maintaining deployment efficiency. SASQ adaptively truncates some outliers, thereby reducing the difficulty of quantization while preserving the distributional characteristics of the activations. SASQ not only surpasses existing SOTA quantization schemes but also outperforms the corresponding FP16 models. On LLaMA2-7B, it achieves 5.2% lower perplexity than QuaRot and 4.7% lower perplexity than the FP16 model on WikiText2.",
    "arxiv_url": "https://arxiv.org/abs/2512.14481",
    "authors": [
      "Shizhuo Mao",
      "Song Chen",
      "Yi Kang"
    ],
    "first_author": "Shizhuo Mao",
    "category": [
      "Technical"
    ],
    "field": "Model Compression & Deployment",
    "task": "Activation Quantization-Aware Training",
    "tags": [
      "Activation Quantization",
      "Quantization-Aware Training",
      "Static Quantization",
      "Outlier Truncation",
      "Phased Quantization",
      "Static Inference",
      "Inference Efficiency",
      "Per-token Quantization"
    ],
    "summary": "本文提出SASQ，一种仅优化激活量化因子的轻量级量化感知训练方法，通过自适应截断异常值与分阶段量化实现静态推理，从而在不修改预训练权重的情况下提升大模型静态量化精度与部署效率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.14481v1",
    "published": "2025-12-16",
    "update_time": "2025-12-16",
    "download_time": "2025-12-18 01:52:36"
  },
  {
    "id": "2512.14417",
    "title": "PortAgent: LLM-driven Vehicle Dispatching Agent for Port Terminals",
    "abstract": "Vehicle Dispatching Systems (VDSs) are critical to the operational efficiency of Automated Container Terminals (ACTs). However, their widespread commercialization is hindered due to their low transferability across diverse terminals. This transferability challenge stems from three limitations: high reliance on port operational specialists, a high demand for terminal-specific data, and time-consuming manual deployment processes. Leveraging the emergence of Large Language Models (LLMs), this paper proposes PortAgent, an LLM-driven vehicle dispatching agent that fully automates the VDS transferring workflow. It bears three features: (1) no need for port operations specialists; (2) low need of data; and (3) fast deployment. Specifically, specialist dependency is eliminated by the Virtual Expert Team (VET). The VET collaborates with four virtual experts, including a Knowledge Retriever, Modeler, Coder, and Debugger, to emulate a human expert team for the VDS transferring workflow. These experts specialize in the domain of terminal VDS via a few-shot example learning approach. Through this approach, the experts are able to learn VDS-domain knowledge from a few VDS examples. These examples are retrieved via a Retrieval-Augmented Generation (RAG) mechanism, mitigating the high demand for terminal-specific data. Furthermore, an automatic VDS design workflow is established among these experts to avoid extra manual interventions. In this workflow, a self-correction loop inspired by the LLM Reflexion framework is created",
    "arxiv_url": "https://arxiv.org/abs/2512.14417",
    "authors": [
      "Jia Hu",
      "Junqi Li",
      "Weimeng Lin",
      "Peng Jia",
      "Yuxiong Ji",
      "Jintao Lai"
    ],
    "first_author": "Jia Hu",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Agents",
    "tags": [
      "Code Agents",
      "Virtual Expert Team",
      "RAG",
      "Few-shot Adaptation",
      "Self-Correction",
      "Automated Deployment",
      "Vehicle Dispatching",
      "AGV Scheduling",
      "Domain Transferability",
      "Human-Free Validation"
    ],
    "summary": "本文提出PortAgent——一种基于大型语言模型的港口车辆调度迁移代理，通过构建虚拟专家团队、检索增强的少样本学习与自我修正的代码生成-执行-调试闭环，实现低数据、无专家且快速部署的VDS迁移，在未见场景上取得高成功率并显著缩短部署时间。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.14417v1",
    "published": "2025-12-16",
    "update_time": "2025-12-16",
    "download_time": "2025-12-18 01:54:02"
  },
  {
    "id": "2512.15699",
    "title": "FrontierCS: Evolving Challenges for Evolving Intelligence",
    "abstract": "We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.",
    "arxiv_url": "https://arxiv.org/abs/2512.15699",
    "authors": [
      "Qiuyang Mang",
      "Wenhao Chai",
      "Zhifei Li",
      "Huanzhi Mao",
      "Shang Zhou",
      "Alexander Du",
      "Hanchen Li",
      "Shu Liu",
      "Edwin Chen",
      "Yichuan Wang",
      "Xieting Chu",
      "Zerui Cheng",
      "Yuan Xu",
      "Tian Xia",
      "Zirui Wang",
      "Tianneng Shi",
      "Jianzhu Yao",
      "Yilong Zhao",
      "Qizheng Zhang",
      "Charlie Ruan",
      "Zeyu Shen",
      "Kaiyuan Liu",
      "Runyuan He",
      "Dong Xing",
      "Zerui Li",
      "Zirong Zeng",
      "Yige Jiang",
      "Lufeng Cheng",
      "Ziyi Zhao",
      "Youran Sun",
      "Wesley Zheng",
      "Meiyuwang Zhang",
      "Ruyi Ji",
      "Xuechang Tu",
      "Zihan Zheng",
      "Zexing Chen",
      "Kangyang Zhou",
      "Zhaozi Wang",
      "Jingbang Chen",
      "Aleksandra Korolova",
      "Peter Henderson",
      "Pramod Viswanath",
      "Vijay Ganesh",
      "Saining Xie",
      "Zhuang Liu",
      "Dawn Song",
      "Sewon Min",
      "Ion Stoica",
      "Joseph E. Gonzalez",
      "Jingbo Shang",
      "Alvin Cheung"
    ],
    "first_author": "Qiuyang Mang",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Algorithmic Optimization",
      "Open-Ended Benchmark",
      "Executable Solver Evaluation",
      "Automatic Scoring",
      "Parametric Instance Generation",
      "Expert Reference Solutions",
      "Human-Model Gap"
    ],
    "summary": "FrontierCS 提出一个含 156 个多领域、开放式且可量化评分的计算机科学编程基准，要求模型生成可执行求解器并通过自动评估器在无已知最优解的问题上进行定量比较，实验证明当前模型在这些前沿问题上远落后于人类专家。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.15699v1",
    "published": "2025-12-17",
    "update_time": "2025-12-17",
    "download_time": "2025-12-19 01:51:14"
  },
  {
    "id": "2512.15468",
    "title": "How Do Semantically Equivalent Code Transformations Impact Membership Inference on LLMs for Code?",
    "abstract": "The success of large language models for code relies on vast amounts of code data, including public open-source repositories, such as GitHub, and private, confidential code from companies. This raises concerns about intellectual property compliance and the potential unauthorized use of license-restricted code. While membership inference (MI) techniques have been proposed to detect such unauthorized usage, their effectiveness can be undermined by semantically equivalent code transformation techniques, which modify code syntax while preserving semantic.   In this work, we systematically investigate whether semantically equivalent code transformation rules might be leveraged to evade MI detection. The results reveal that model accuracy drops by only 1.5% in the worst case for each rule, demonstrating that transformed datasets can effectively serve as substitutes for fine-tuning. Additionally, we find that one of the rules (RenameVariable) reduces MI success by 10.19%, highlighting its potential to obscure the presence of restricted code. To validate these findings, we conduct a causal analysis confirming that variable renaming has the strongest causal effect in disrupting MI detection. Notably, we find that combining multiple transformations does not further reduce MI effectiveness. Our results expose a critical loophole in license compliance enforcement for training large language models for code, showing that MI detection can be substantially weakened by transformation-based obfuscation techniques.",
    "arxiv_url": "https://arxiv.org/abs/2512.15468",
    "authors": [
      "Hua Yang",
      "Alejandro Velasco",
      "Thanh Le-Cong",
      "Md Nazmul Haque",
      "Bowen Xu",
      "Denys Poshyvanyk"
    ],
    "first_author": "Hua Yang",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Training Data Auditing & Privacy",
    "task": "Membership Inference",
    "tags": [
      "Semantics-Preserving Obfuscation",
      "RenameVariable",
      "Membership Inference",
      "Causal Analysis",
      "Model Memorization",
      "Fine-tuning Robustness",
      "License Compliance"
    ],
    "summary": "本文系统研究了23种语义等价代码变换对代码型大语言模型会员推断（MI）的影响，发现变换后模型性能几乎不降但MI成功率显著下降（尤其是变量重命名），并通过结构因果模型验证了该因果效应，揭示了基于变换的规避许可合规检测的风险。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.15468v1",
    "published": "2025-12-17",
    "update_time": "2025-12-17",
    "download_time": "2025-12-19 01:51:46"
  },
  {
    "id": "2512.15688",
    "title": "BashArena: A Control Setting for Highly Privileged AI Agents",
    "abstract": "Future AI agents might run autonomously with elevated privileges. If these agents are misaligned, they might abuse these privileges to cause serious damage. The field of AI control develops techniques that make it harder for misaligned AIs to cause such damage, while preserving their usefulness. We introduce BashArena, a setting for studying AI control techniques in security-critical environments. BashArena contains 637 Linux system administration and infrastructure engineering tasks in complex, realistic environments, along with four sabotage objectives (execute malware, exfiltrate secrets, escalate privileges, and disable firewall) for a red team to target. We evaluate multiple frontier LLMs on their ability to complete tasks, perform sabotage undetected, and detect sabotage attempts. Claude Sonnet 4.5 successfully executes sabotage while evading monitoring by GPT-4.1 mini 26% of the time, at 4% trajectory-wise FPR. Our findings provide a baseline for designing more effective control protocols in BashArena. We release the dataset as a ControlArena setting and share our task generation pipeline.",
    "arxiv_url": "https://arxiv.org/abs/2512.15688",
    "authors": [
      "Adam Kaufman",
      "James Lucassen",
      "Tyler Tracy",
      "Cody Rushing",
      "Aryan Bhatt"
    ],
    "first_author": "Adam Kaufman",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "AI Control & Security",
    "task": "Privileged-Agent Control Benchmarking",
    "tags": [
      "Red Teaming",
      "Control Protocols",
      "Monitoring Evasion",
      "Privileged Execution",
      "Multi-Step Interaction",
      "Docker Environments",
      "Adversarial Benchmark",
      "Side-Task Detection",
      "Security Testing",
      "Autonomous Agents"
    ],
    "summary": "BashArena 提出一个包含637个复杂 Linux 运维主任务和四类破坏（下载恶意软件、泄露秘密、权限提升、禁用防火墙）的对抗性控制基准，用于评估在高权限环境中 AI 代理的破坏能力与监测防护效果并发布了任务生成管道与数据集。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.15688v1",
    "published": "2025-12-17",
    "update_time": "2025-12-17",
    "download_time": "2025-12-19 01:52:32"
  },
  {
    "id": "2512.15466",
    "title": "On Assessing the Relevance of Code Reviews Authored by Generative Models",
    "abstract": "The use of large language models like ChatGPT in code review offers promising efficiency gains but also raises concerns about correctness and safety. Existing evaluation methods for code review generation either rely on automatic comparisons to a single ground truth, which fails to capture the variability of human perspectives, or on subjective assessments of \"usefulness\", a highly ambiguous concept. We propose a novel evaluation approach based on what we call multi-subjective ranking. Using a dataset of 280 self-contained code review requests and corresponding comments from CodeReview StackExchange, multiple human judges ranked the quality of ChatGPT-generated comments alongside the top human responses from the platform. Results show that ChatGPT's comments were ranked significantly better than human ones, even surpassing StackExchange's accepted answers. Going further, our proposed method motivates and enables more meaningful assessments of generative AI's performance in code review, while also raising awareness of potential risks of unchecked integration into review processes.",
    "arxiv_url": "https://arxiv.org/abs/2512.15466",
    "authors": [
      "Robert Heumüller",
      "Frank Ortmeier"
    ],
    "first_author": "Robert Heumüller",
    "category": [
      "Empirical",
      "Benchmark"
    ],
    "field": "Maintenance",
    "task": "Code Review",
    "tags": [
      "Multi-Subjective Ranking",
      "Human Annotation",
      "Code Review Comment Generation",
      "Evaluation Tooling",
      "Statistical Analysis",
      "Stylistic Bias"
    ],
    "summary": "本文提出并应用多主观排序（multi-subjective ranking）的评估方法，对来自CodeReview StackExchange的280个实例由四名评审者对比排序，结果显示ChatGPT生成的代码审查评论在排名上显著优于人类评论，并发布了数据集与Rankr评估工具，同时讨论了生成评论因语体更为精炼导致的表面偏好风险。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.15466v1",
    "published": "2025-12-17",
    "update_time": "2025-12-17",
    "download_time": "2025-12-19 01:54:45"
  },
  {
    "id": "2512.16816",
    "title": "Toward Systematic Counterfactual Fairness Evaluation of Large Language Models: The CAFFE Framework",
    "abstract": "Nowadays, Large Language Models (LLMs) are foundational components of modern software systems. As their influence grows, concerns about fairness have become increasingly pressing. Prior work has proposed metamorphic testing to detect fairness issues, applying input transformations to uncover inconsistencies in model behavior. This paper introduces an alternative perspective for testing counterfactual fairness in LLMs, proposing a structured and intent-aware framework coined CAFFE (Counterfactual Assessment Framework for Fairness Evaluation). Inspired by traditional non-functional testing, CAFFE (1) formalizes LLM-Fairness test cases through explicitly defined components, including prompt intent, conversational context, input variants, expected fairness thresholds, and test environment configuration, (2) assists testers by automatically generating targeted test data, and (3) evaluates model responses using semantic similarity metrics. Our experiments, conducted on three different architectural families of LLM, demonstrate that CAFFE achieves broader bias coverage and more reliable detection of unfair behavior than existing metamorphic approaches.",
    "arxiv_url": "https://arxiv.org/abs/2512.16816",
    "authors": [
      "Alessandra Parziale",
      "Gianmario Voria",
      "Valeria Pontillo",
      "Gemma Catolino",
      "Andrea De Lucia",
      "Fabio Palomba"
    ],
    "first_author": "Alessandra Parziale",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Counterfactual Fairness",
      "Intent-aware Testing",
      "Structured Test Cases",
      "Semantic Similarity Evaluation",
      "Stereotype-aware Prompt Generation",
      "Fairness Bug Detection",
      "Reproducibility"
    ],
    "summary": "本文提出CAFFE框架，通过结构化的意图感知反事实测试用例生成和基于语义相似度的评估，系统性地检测并量化大语言模型的公平性缺陷，在多种模型上较现有变形测试方法提升了明显的检测覆盖与准确性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.16816v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-20 01:45:28"
  },
  {
    "id": "2512.16790",
    "title": "Inside Out: Uncovering How Comment Internalization Steers LLMs for Better or Worse",
    "abstract": "While comments are non-functional elements of source code, Large Language Models (LLM) frequently rely on them to perform Software Engineering (SE) tasks. Yet, where in the model this reliance resides, and how it affects performance, remains poorly understood. We present the first concept-level interpretability study of LLMs in SE, analyzing three tasks - code completion, translation, and refinement - through the lens of internal comment representation. Using Concept Activation Vectors (CAV), we show that LLMs not only internalize comments as distinct latent concepts but also differentiate between subtypes such as Javadocs, inline, and multiline comments. By systematically activating and deactivating these concepts in the LLMs' embedding space, we observed significant, model-specific, and task-dependent shifts in performance ranging from -90% to +67%. Finally, we conducted a controlled experiment using the same set of code inputs, prompting LLMs to perform 10 distinct SE tasks while measuring the activation of the comment concept within their latent representations. We found that code summarization consistently triggered the strongest activation of comment concepts, whereas code completion elicited the weakest sensitivity. These results open a new direction for building SE tools and models that reason about and manipulate internal concept representations rather than relying solely on surface-level input.",
    "arxiv_url": "https://arxiv.org/abs/2512.16790",
    "authors": [
      "Aaron Imani",
      "Mohammad Moshirpour",
      "Iftekhar Ahmed"
    ],
    "first_author": "Aaron Imani",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Understanding",
    "tags": [
      "Interpretability",
      "Comment Internalization",
      "Concept Activation Vectors",
      "Comment Type Differentiation",
      "Latent Space Intervention",
      "Task-Conditional Sensitivity",
      "Model-Specific Effects"
    ],
    "summary": "本文首次使用概念激活向量（CAV）在LLM的隐藏表示中实证揭示了代码注释作为独立概念的内部化及不同注释类型的区分，并通过激活/抑制这些概念展示了对代码补全、翻译和重构等任务性能可正可负的显著影响。",
    "quality": "High",
    "conference": "IEEE/ACM International Conference on Software Engineering (ICSE 2026)",
    "pdf_url": "https://arxiv.org/pdf/2512.16790v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-20 01:45:49"
  },
  {
    "id": "2512.16814",
    "title": "Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs",
    "abstract": "Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.",
    "arxiv_url": "https://arxiv.org/abs/2512.16814",
    "authors": [
      "William English",
      "Dominic Simon",
      "Sumit Kumar Jha",
      "Rickard Ewetz"
    ],
    "first_author": "William English",
    "category": [
      "Technical"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Grammar Forcing",
      "Grammar-Constrained Decoding",
      "Atomic Predicate Lifting",
      "Masked AP Prediction",
      "Co-reference Handling",
      "Theoretical Learning Analysis",
      "Out-of-Distribution Robustness"
    ],
    "summary": "本文提出GraFT框架，通过对原子命题使用掩码语言模型进行提升并在序列到序列翻译中基于时序逻辑语法动态约束输出词元，从而提高自然语言到时序逻辑的翻译准确性，并给出理论证明与多基准实证评估。",
    "quality": "High",
    "conference": "Proceedings of the 42nd International Conference on Machine Learning (ICML 2025)",
    "pdf_url": "https://arxiv.org/pdf/2512.16814v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-20 01:48:23"
  },
  {
    "id": "2512.16770",
    "title": "GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation",
    "abstract": "Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\\mathcal{P}$. We decompose the grounding task hierarchically -- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\\%$, a $1.4\\times$ improvement over SOTA.",
    "arxiv_url": "https://arxiv.org/abs/2512.16770",
    "authors": [
      "William English",
      "Chase Walker",
      "Dominic Simon",
      "Rickard Ewetz"
    ],
    "first_author": "William English",
    "category": [
      "Technical"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Predicate Grounding",
      "Argument Grounding",
      "System Signatures",
      "Lifted NL Translation",
      "Hierarchical Grounding",
      "Executable LTL",
      "Model Checking",
      "Logical Equivalence"
    ],
    "summary": "本文提出GinSign框架，通过将自然语言中提升的原子命题分层地分类为系统签名中的谓词和类型化参数来实现可执行的时序逻辑（LTL）翻译，从而显著提升了有语义的地面化翻译准确率并支持下游模型检验。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.16770v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-20 01:49:12"
  },
  {
    "id": "2512.16272",
    "title": "Beyond Blind Spots: Analytic Hints for Mitigating LLM-Based Evaluation Pitfalls",
    "abstract": "Large Language Models are increasingly deployed as judges (LaaJ) in code generation pipelines. While attractive for scalability, LaaJs tend to overlook domain specific issues raising concerns about their reliability in critical evaluation tasks. To better understand these limitations in practice, we examine LaaJ behavior in a concrete industrial use case: legacy code modernization via COBOL code generation. In this setting, we find that even production deployed LaaJs can miss domain critical errors, revealing consistent blind spots in their evaluation capabilities.   To better understand these blind spots, we analyze generated COBOL programs and associated LaaJs judgments, drawing on expert knowledge to construct a preliminary taxonomy. Based on this taxonomy, we develop a lightweight analytic checker tool that flags over 30 domain specific issues observed in practice. We use its outputs as analytic hints, dynamically injecting them into the judges prompt to encourage LaaJ to revisit aspects it may have overlooked.   Experiments on a test set of 100 programs using four production level LaaJs show that LaaJ alone detects only about 45% of the errors present in the code (in all judges we tested), while the analytic checker alone lacks explanatory depth. When combined, the LaaJ+Hints configuration achieves up to 94% coverage (for the best performing judge and injection prompt) and produces qualitatively richer, more accurate explanations, demonstrating that analytic-LLM hybrids can substantially enhance evaluation reliability in deployed pipelines. We release the dataset and all used prompts.",
    "arxiv_url": "https://arxiv.org/abs/2512.16272",
    "authors": [
      "Ora Nova Fandina",
      "Eitan Farchi",
      "Shmulik Froimovich",
      "Raviv Gal",
      "Wesam Ibraheem",
      "Rami Katan",
      "Alice Podolsky"
    ],
    "first_author": "Ora Nova Fandina",
    "category": [
      "Technical",
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Analytic Hints",
      "LaaJ Blind Spots",
      "Rule-based Checker",
      "Hint Injection",
      "Evaluation Taxonomy",
      "COBOL Modernization",
      "Prompt Engineering",
      "Hybrid Evaluation",
      "Non-local Reasoning"
    ],
    "summary": "本文在COBOL代码现代化评估中发现并分类了LLM作为评判器（LaaJ）的领域盲点，构建了含30+问题类型的基于规则的分析检查器，并通过将解析性提示注入评判模型的提示中显著提升了错误检测覆盖率，且发布了数据集与提示。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.16272v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-21 01:58:24"
  },
  {
    "id": "2512.16070",
    "title": "LLM4Perf: Large Language Models Are Effective Samplers for Multi-Objective Performance Modeling (Copy)",
    "abstract": "The performance of modern software systems is critically dependent on their complex configuration options. Building accurate performance models to navigate this vast space requires effective sampling strategies, yet existing methods often struggle with multi-objective optimization and cannot leverage semantic information from documentation. The recent success of Large Language Models (LLMs) motivates the central question of this work: Can LLMs serve as effective samplers for multi-objective performance modeling? To explore this, we present a comprehensive empirical study investigating the capabilities and characteristics of LLM-driven sampling. We design and implement LLM4Perf, a feedback-based framework, and use it to systematically evaluate the LLM-guided sampling process across four highly configurable, real-world systems. Our study reveals that the LLM-guided approach outperforms traditional baselines in most cases. Quantitatively, LLM4Perf achieves the best performance in nearly 68.8% (77 out of 112) of all evaluation scenarios, demonstrating its superior effectiveness. We find this effectiveness stems from the LLM's dual capabilities of configuration space pruning and feedback-driven strategy refinement. The effectiveness of this pruning is further validated by the fact that it also improves the performance of the baseline methods in nearly 91.5% (410 out of 448) of cases. Furthermore, we show how the LLM choices for each component and hyperparameters within LLM4Perf affect its effectiveness. Overall, this paper provides strong evidence for the effectiveness of LLMs in performance engineering and offers concrete insights into the mechanisms that drive their success.",
    "arxiv_url": "https://arxiv.org/abs/2512.16070",
    "authors": [
      "Xin Wang",
      "Zhenhao Li",
      "Zishuo Ding"
    ],
    "first_author": "Xin Wang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Performance Engineering",
    "task": "Multi-Objective Configuration Sampling & Performance Modeling",
    "tags": [
      "Configuration Pruning",
      "Feedback-Driven Sampling",
      "Multi-Objective Optimization",
      "Sampling Strategy Design",
      "Configuration Generation",
      "Configuration Voting",
      "Performance Trend Analysis",
      "Hyperparameter Sensitivity"
    ],
    "summary": "本文提出LLM4Perf，一个利用大语言模型通过文档驱动的配置空间剪枝与迭代反馈引导多目标性能建模采样的框架，并构建了一个新的多目标性能数据集；实验证明LLM引导的采样在多数情景下优于传统基线，能提升性能模型的准确性与稳定性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.16070v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-21 01:58:47"
  },
  {
    "id": "2512.16465",
    "title": "cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution",
    "abstract": "Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.",
    "arxiv_url": "https://arxiv.org/abs/2512.16465",
    "authors": [
      "Jinwu Chen",
      "Qidie Wu",
      "Bin Li",
      "Lin Ma",
      "Xin Si",
      "Yang Hu",
      "Shouyi Yin",
      "Jun Yang"
    ],
    "first_author": "Jinwu Chen",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Performance Optimization",
    "tags": [
      "Strategy-Level Crossover",
      "Multi-Agent Framework",
      "Code Agents",
      "Roofline-Guided Prompting",
      "Strategy-Level Initialization",
      "Evolutionary Optimization",
      "CUDA Kernel Optimization",
      "Hardware-Aware Optimization",
      "Code RAG"
    ],
    "summary": "本文提出 cuPilot——一种以“策略”作为中间表示的多智能体进化框架，通过策略级交叉与翻译、roofline 引导的提示和策略级种群初始化，有效提升 CUDA 内核性能，在 100 个 KernelBench 基准上对比 PyTorch 平均提速 3.09×。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.16465v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-21 02:04:47"
  },
  {
    "id": "2512.16262",
    "title": "Learning to Wait: Synchronizing Agents with the Physical World",
    "abstract": "Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \\textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \\textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \\textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.",
    "arxiv_url": "https://arxiv.org/abs/2512.16262",
    "authors": [
      "Yifei She",
      "Ping Zhang",
      "He Liu",
      "Yanmin Jia",
      "Yang Jing",
      "Zijun Liu",
      "Peng Sun",
      "Xiangbin Li",
      "Xiaohe Hu"
    ],
    "first_author": "Yifei She",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Agent Interaction",
    "task": "Temporal Synchronization",
    "tags": [
      "Code-as-Action",
      "Temporal Synchronization",
      "Active Wait",
      "In-Context Learning",
      "Asynchronous Environment",
      "Latency Modeling",
      "Regret Score",
      "Semantic Latency Estimation",
      "Context Efficiency"
    ],
    "summary": "本文提出一种Agent端方法，通过将Code-as-Action扩展到时间域，让LLM代理利用语义先验与In-Context Learning预测time.sleep(t)以同步其认知时间线与物理世界的异步延迟，并在模拟Kubernetes集群中实验证明可在减少查询开销的同时降低执行延迟。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.16262v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-21 02:08:10"
  },
  {
    "id": "2512.15980",
    "title": "Embedding Software Intent: Lightweight Java Module Recovery",
    "abstract": "As an increasing number of software systems reach unprecedented scale, relying solely on code-level abstractions is becoming impractical. While architectural abstractions offer a means to manage these systems, maintaining their consistency with the actual code has been problematic. The Java Platform Module System (JPMS), introduced in Java 9, addresses this limitation by enabling explicit module specification at the language level. JPMS enhances architectural implementation through improved encapsulation and direct specification of ground-truth architectures within Java projects. Although many projects are written in Java, modularizing existing monolithic projects to JPMS modules is an open challenge due to ineffective module recovery by existing architecture recovery techniques. To address this challenge, this paper presents ClassLAR (Class-and Language model-based Architectural Recovery), a novel, lightweight, and efficient approach that recovers Java modules from monolithic Java systems using fully-qualified class names. ClassLAR leverages language models to extract semantic information from package and class names, capturing both structural and functional intent. In evaluations across 20 popular Java projects, ClassLAR outperformed all state-of-the-art techniques in architectural-level similarity metrics while achieving execution times that were 3.99 to 10.50 times faster.",
    "arxiv_url": "https://arxiv.org/abs/2512.15980",
    "authors": [
      "Yirui He",
      "Yuqi Huai",
      "Xingyu Chen",
      "Joshua Garcia"
    ],
    "first_author": "Yirui He",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Architecture Recovery",
    "task": "Module Recovery",
    "tags": [
      "Module Recovery",
      "Class-name Semantics",
      "LM Topic Modeling",
      "JPMS Ground Truth",
      "Encapsulation",
      "Architecture Matching",
      "Lightweight",
      "Efficiency"
    ],
    "summary": "本文提出ClassLAR，一种仅基于类的全限定名并利用语言模型的轻量级Java模块（JPMS）恢复方法，在20个开源项目上相比现有方法在架构相似性和封装性上表现更好且运行更快。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.15980v1",
    "published": "2025-12-17",
    "update_time": "2025-12-17",
    "download_time": "2025-12-22 01:57:54"
  },
  {
    "id": "2512.15979",
    "title": "OLAF: Towards Robust LLM-Based Annotation Framework in Empirical Software Engineering",
    "abstract": "Large Language Models (LLMs) are increasingly used in empirical software engineering (ESE) to automate or assist annotation tasks such as labeling commits, issues, and qualitative artifacts. Yet the reliability and reproducibility of such annotations remain underexplored. Existing studies often lack standardized measures for reliability, calibration, and drift, and frequently omit essential configuration details. We argue that LLM-based annotation should be treated as a measurement process rather than a purely automated activity. In this position paper, we outline the \\textbf{Operationalization for LLM-based Annotation Framework (OLAF)}, a conceptual framework that organizes key constructs: \\textit{reliability, calibration, drift, consensus, aggregation}, and \\textit{transparency}. The paper aims to motivate methodological discussion and future empirical work toward more transparent and reproducible LLM-based annotation in software engineering research.",
    "arxiv_url": "https://arxiv.org/abs/2512.15979",
    "authors": [
      "Mia Mohammad Imran",
      "Tarannum Shaila Zaman"
    ],
    "first_author": "Mia Mohammad Imran",
    "category": [
      "Survey",
      "Technical"
    ],
    "field": "Empirical Methods",
    "task": "LLM-based Annotation & Measurement",
    "tags": [
      "Operationalization",
      "Annotator Reliability",
      "Consensus Measurement",
      "Calibration",
      "Drift Analysis",
      "Probabilistic Aggregation",
      "Prompt Sensitivity",
      "Human-in-the-Loop",
      "Transparency",
      "Annotation Workflows"
    ],
    "summary": "本文提出了OLAF（一套面向软件工程中基于大模型的标注的操作化框架），将标注视为测量过程并定义了可靠性、共识、校准、漂移、聚合与透明性六个维度及相应的配置与报告指南，以提升LLM标注的可重复性与可审计性。",
    "quality": "Middle",
    "conference": "3rd International Workshop on Methodological Issues with Empirical Studies in Software Engineering (WSESE) 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.15979v1",
    "published": "2025-12-17",
    "update_time": "2025-12-17",
    "download_time": "2025-12-22 01:58:12"
  },
  {
    "id": "2512.16855",
    "title": "TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge",
    "abstract": "Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.",
    "arxiv_url": "https://arxiv.org/abs/2512.16855",
    "authors": [
      "Khurram Khalil",
      "Khaza Anuarul Hoque"
    ],
    "first_author": "Khurram Khalil",
    "category": [
      "Technical"
    ],
    "field": "Model Compression & Deployment",
    "task": "LLM Compression with Formal Guarantees",
    "tags": [
      "Temporal Logic",
      "STL Robustness",
      "Robustness-Guided Optimization",
      "Layer-wise Quantization",
      "Layer-wise Pruning",
      "Bayesian Optimization",
      "Formal Guarantees",
      "Edge Deployment",
      "Runtime Adaptability",
      "Coherence Preservation"
    ],
    "summary": "本文提出TOGGLE，一种利用信号时序逻辑（STL）作为形式化约束、通过稳健性驱动的贝叶斯优化在无需重训练的情况下联合搜索层级量化与剪枝配置，从而在保证连贯性、长程依赖、上下文一致性和事实准确性等语言属性的前提下将LLM压缩并部署到边缘设备的框架。",
    "quality": "High",
    "conference": "IEEE ICCAD 2025",
    "pdf_url": "https://arxiv.org/pdf/2512.16855v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-22 01:59:54"
  },
  {
    "id": "2512.16626",
    "title": "Stackelberg Learning from Human Feedback: Preference Optimization as a Sequential Game",
    "abstract": "We introduce Stackelberg Learning from Human Feedback (SLHF), a new framework for preference optimization. SLHF frames the alignment problem as a sequential-move game between two policies: a Leader, which commits to an action, and a Follower, which responds conditionally on the Leader's action. This approach decomposes preference optimization into a refinement problem for the Follower and an optimization problem against an adversary for the Leader. Unlike Reinforcement Learning from Human Feedback (RLHF), which assigns scalar rewards to actions, or Nash Learning from Human Feedback (NLHF), which seeks a simultaneous-move equilibrium, SLHF leverages the asymmetry of sequential play to capture richer preference structures. The sequential design of SLHF naturally enables inference-time refinement, as the Follower learns to improve the Leader's actions, and these refinements can be leveraged through iterative sampling. We compare the solution concepts of SLHF, RLHF, and NLHF, and lay out key advantages in consistency, data sensitivity, and robustness to intransitive preferences. Experiments on large language models demonstrate that SLHF achieves strong alignment across diverse preference datasets, scales from 0.5B to 8B parameters, and yields inference-time refinements that transfer across model families without further fine-tuning.",
    "arxiv_url": "https://arxiv.org/abs/2512.16626",
    "authors": [
      "Barna Pásztor",
      "Thomas Kleine Buening",
      "Andreas Krause"
    ],
    "first_author": "Barna Pásztor",
    "category": [
      "Technical"
    ],
    "field": "LLM Alignment & Preference Learning",
    "task": "Preference Optimization (Sequential Stackelberg Learning from Human Feedback)",
    "tags": [
      "Stackelberg Game",
      "Preference Optimization",
      "Inference-Time Refinement",
      "Follower Refinement",
      "Two-Timescale GDA",
      "Pairwise Preference Modeling",
      "Intransitive Preferences",
      "Transferable Refinements",
      "Human-LLM Interaction",
      "Robustness"
    ],
    "summary": "本文提出了Stackelberg Learning from Human Feedback (SLHF)，将偏好优化建模为Leader-First、Follower-响应的序贯博弈，给出近似求解算法STACKELBERGGDA并展示其在推理时通过Follower迭代采样实现无微调的输出改进与跨模型迁移能力。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.16626v1",
    "published": "2025-12-18",
    "update_time": "2025-12-18",
    "download_time": "2025-12-22 02:11:57"
  },
  {
    "id": "2512.17814",
    "title": "LLM-based Behaviour Driven Development for Hardware Design",
    "abstract": "Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.   Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.",
    "arxiv_url": "https://arxiv.org/abs/2512.17814",
    "authors": [
      "Rolf Drechsler",
      "Qian Liu"
    ],
    "first_author": "Rolf Drechsler",
    "category": [
      "Technical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Behavior-Driven Development",
      "Gherkin Scenario Generation",
      "NL-to-Verilog",
      "Hardware Verification",
      "BDD Automation",
      "Prompt Engineering",
      "Executable Test Scenarios",
      "Simulation"
    ],
    "summary": "本文提出将大型语言模型应用于硬件设计的行为驱动开发（BDD），自动从自然语言规格生成可执行的Gherkin场景并生成对应的Verilog实现，以在ALU案例上进行仿真验证。",
    "quality": "Middle",
    "conference": "2nd International Symposium on Artificial Intelligence and Internet of Things (AIIoT-25) 2025",
    "pdf_url": "https://arxiv.org/pdf/2512.17814v1",
    "published": "2025-12-19",
    "update_time": "2025-12-19",
    "download_time": "2025-12-23 01:52:18"
  },
  {
    "id": "2512.17540",
    "title": "SGCR: A Specification-Grounded Framework for Trustworthy LLM Code Review",
    "abstract": "Automating code review with Large Language Models (LLMs) shows immense promise, yet practical adoption is hampered by their lack of reliability, context-awareness, and control. To address this, we propose Specification-Grounded Code Review (SGCR), a framework that grounds LLMs in human-authored specifications to produce trustworthy and relevant feedback. SGCR features a novel dual-pathway architecture: an explicit path ensures deterministic compliance with predefined rules derived from these specifications, while an implicit path heuristically discovers and verifies issues beyond those rules. Deployed in a live industrial environment at HiThink Research, SGCR's suggestions achieved a 42% developer adoption rate-a 90.9% relative improvement over a baseline LLM (22%). Our work demonstrates that specification-grounding is a powerful paradigm for bridging the gap between the generative power of LLMs and the rigorous reliability demands of software engineering.",
    "arxiv_url": "https://arxiv.org/abs/2512.17540",
    "authors": [
      "Kai Wang",
      "Bingcheng Mao",
      "Shuai Jia",
      "Yujie Ding",
      "Dongming Han",
      "Tianyi Ma",
      "Bin Cao"
    ],
    "first_author": "Kai Wang",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Maintenance",
    "task": "Code Review",
    "tags": [
      "Specification Grounding",
      "Dual-Pathway",
      "Explicit Specification Injection",
      "Implicit Specification Discovery",
      "Code RAG",
      "Ensemble Aggregation",
      "Specification Segmentation",
      "Explainability",
      "Reliability",
      "Production Deployment"
    ],
    "summary": "本文提出SGCR，一种将人工编写规范与双路径（显式规范注入与隐式规范发现）相结合的LLM驱动代码审查框架，结合RAG检索、模型集成与聚合以提高可控性与可信度，并在生产环境中验证了显著的开发者采纳率提升。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.17540v1",
    "published": "2025-12-19",
    "update_time": "2025-12-19",
    "download_time": "2025-12-23 01:52:38"
  },
  {
    "id": "2512.17419",
    "title": "SWE-Bench++: A Framework for the Scalable Generation of Software Engineering Benchmarks from Open-Source Repositories",
    "abstract": "Benchmarks like SWE-bench have standardized the evaluation of Large Language Models (LLMs) on repository-level software engineering tasks. However, these efforts remain limited by manual curation, static datasets, and a focus on Python-based bug fixes. We introduce SWE-Bench++, an automated framework that generates repository-level coding tasks from open-source GitHub projects. Unlike synthetic approaches, our pipeline harvests live pull requests to cover both bug fixes and feature requests across 11 languages. SWE-Bench++ turns GitHub pull requests (PRs) into reproducible, execution-based tasks via four stages: programmatic sourcing, environment synthesis, test oracle extraction, and quality assurance. A final hint-guided trajectory synthesis step converts instances that strong models fail on into training trajectories. Our initial benchmark consists of 11,133 instances from 3,971 repositories across 11 languages. On a subset of 1,782 instances of this benchmark, today's strongest models perform as follows: claude-sonnet-4.5 achieves 36.20% pass@10, gpt-5-2025-08-07 34.57%, gemini/gemini-2.5-pro 24.92%, and gpt-4o 16.89%. We further demonstrate the utility of our dataset by showing that fine-tuning on SWE-Bench++ instances yields measurable improvements on the SWE-bench Multilingual benchmark. SWE-Bench++ provides a scalable, multilingual benchmark for evaluating and improving repository-level code generation.",
    "arxiv_url": "https://arxiv.org/abs/2512.17419",
    "authors": [
      "Lilin Wang",
      "Lucas Ramalho",
      "Alan Celestino",
      "Phuc Anthony Pham",
      "Yu Liu",
      "Umang Kumar Sinha",
      "Andres Portillo",
      "Onassis Osunwa",
      "Gabriel Maduekwe"
    ],
    "first_author": "Lilin Wang",
    "category": [
      "Benchmark",
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Repository-Level Coding",
    "tags": [
      "Repository-Level Coding",
      "Environment Synthesis",
      "Adaptive Log Parsing",
      "State-Differential Oracle",
      "Hint-Guided Trajectories",
      "Multilingual Benchmark",
      "Contamination-Aware Evaluation",
      "Automated Benchmark Generation"
    ],
    "summary": "SWE-Bench++ 提出一个自动化、多语言且可扩展的框架，能够从真实 GitHub pull request 自动合成可复现的容器环境和自适应日志解析器、以三态差分判定区分缺陷与功能请求，并通过提示引导生成可用于训练的轨迹来构建大规模仓库级评测集。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.17419v1",
    "published": "2025-12-19",
    "update_time": "2025-12-19",
    "download_time": "2025-12-23 01:56:12"
  },
  {
    "id": "2512.17259",
    "title": "Verifiability-First Agents: Provable Observability and Lightweight Audit Agents for Controlling Autonomous LLM Systems",
    "abstract": "As LLM-based agents grow more autonomous and multi-modal, ensuring they remain controllable, auditable, and faithful to deployer intent becomes critical. Prior benchmarks measured the propensity for misaligned behavior and showed that agent personalities and tool access significantly influence misalignment. Building on these insights, we propose a Verifiability-First architecture that (1) integrates run-time attestations of agent actions using cryptographic and symbolic methods, (2) embeds lightweight Audit Agents that continuously verify intent versus behavior using constrained reasoning, and (3) enforces challenge-response attestation protocols for high-risk operations. We introduce OPERA (Observability, Provable Execution, Red-team, Attestation), a benchmark suite and evaluation protocol designed to measure (i) detectability of misalignment, (ii) time to detection under stealthy strategies, and (iii) resilience of verifiability mechanisms to adversarial prompt and persona injection. Our approach shifts the evaluation focus from how likely misalignment is to how quickly and reliably misalignment can be detected and remediated.",
    "arxiv_url": "https://arxiv.org/abs/2512.17259",
    "authors": [
      "Abhivansh Gupta"
    ],
    "first_author": "Abhivansh Gupta",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Autonomous Agents & Safety",
    "task": "Agent Verifiability and Audit",
    "tags": [
      "Action Attestation",
      "Intent Specification",
      "Audit Agents",
      "Provenance Log",
      "Challenge–Response Attestation",
      "Signed Receipts",
      "Runtime Verification",
      "Verifiability Benchmark",
      "Adversarial Prompt Robustness",
      "Remediation Controls"
    ],
    "summary": "本文提出可验证性优先的智能体架构，通过意图规范、加密动作证明与轻量级审计代理以及挑战-响应协议实现运行时可观测性与可审计性，并引入评估检测速度与鲁棒性的OPERA基准。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.17259v1",
    "published": "2025-12-19",
    "update_time": "2025-12-19",
    "download_time": "2025-12-23 01:57:34"
  },
  {
    "id": "2512.19644",
    "title": "More code, less validation: Risk factors for over-reliance on AI coding tools among scientists",
    "abstract": "Programming is essential to modern scientific research, yet most scientists report inadequate training for the software development their work demands. Generative AI tools capable of code generation may support scientific programmers, but user studies indicate risks of over-reliance, particularly among inexperienced users. We surveyed 868 scientists who program, examining adoption patterns, tool preferences, and factors associated with perceived productivity. Adoption is highest among students and less experienced programmers, with variation across fields. Scientific programmers overwhelmingly prefer general-purpose conversational interfaces like ChatGPT over developer-specific tools. Both inexperience and limited use of development practices (like testing, code review, and version control) are associated with greater perceived productivity-but these factors interact, suggesting formal practices may partially compensate for inexperience. The strongest predictor of perceived productivity is the number of lines of generated code typically accepted at once. These findings suggest scientific programmers using generative AI may gauge productivity by code generation rather than validation, raising concerns about research code integrity.",
    "arxiv_url": "https://arxiv.org/abs/2512.19644",
    "authors": [
      "Gabrielle O'Brien",
      "Alexis Parker",
      "Nasir Eisty",
      "Jeffrey Carver"
    ],
    "first_author": "Gabrielle O'Brien",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "tags": [
      "Human-LLM Interaction",
      "Over-reliance",
      "Scientific Software",
      "Survey Study",
      "Code Validation",
      "Testing Adoption",
      "Version Control Usage",
      "Perceived Productivity",
      "Experience Effects",
      "Tool Preference"
    ],
    "summary": "本文基于对868名从事编程的科学家的问卷调查，发现学生与经验较少的研究人员更常使用以对话为主的生成式AI（如ChatGPT），且缺乏测试、代码审查与版本控制等实践与更高的自感生产力相关，尤其是一次性接受较多生成代码是感知生产力的最强预测因子，提示科研人员可能以生成量而非验证为依据过度依赖AI，威胁研究代码完整性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.19644v1",
    "published": "2025-12-22",
    "update_time": "2025-12-22",
    "download_time": "2025-12-24 01:50:13"
  },
  {
    "id": "2512.19509",
    "title": "Beyond Language Boundaries: Uncovering Programming Language Families for Code Language Models",
    "abstract": "The rapid proliferation of diverse programming languages presents both opportunities and challenges for developing multilingual code LLMs. While existing techniques often train code LLMs by simply aggregating multilingual code data, few explore the deeper relationships between programming languages(PLs) and how such relationships can be utilized to optimize the training and inference of code LLMs. In this work, we investigate 2 fundamental questions: 1) What are the deep linguistic relationships among PLs? and 2) How can these relationships be leveraged to improve multilingual code LLMs? We propose an embedding-based framework to uncover the latent families of PLs. Our approach begins by defining 21 primary linguistic features of programming languages, such as variable definition, control structures, and method declarations, and then employs LLMs to generate feature-aligned code samples across multiple languages. By embedding these semantically parallel code snippets from 19 languages, we construct a similarity matrix and perform hierarchical clustering to uncover inherent language relationships. Our analysis reveals clear hierarchical structures among programming languages. Closely related languages form well-defined clusters (e.g., C, C++, Java, and Swift group together), while Go exhibits as a central language with the highest cross-language similarity. Building on the uncovered language families, we propose three strategies to enhance multilingual LLM training: transfer learning across linguistically related languages, linguistic proximity-guided curriculum learning, and centroid-based intermediary code translation. Experiments on 4 code intelligence tasks demonstrate that our methods significantly improve multilingual LLM performance. This work offers a universal perspective on programming languages and advances more effective strategies for multilingual code LLM training.",
    "arxiv_url": "https://arxiv.org/abs/2512.19509",
    "authors": [
      "Shangbo Yun",
      "Xiaodong Gu",
      "Jianghong Huang",
      "Beijun Shen"
    ],
    "first_author": "Shangbo Yun",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Representation Learning",
    "tags": [
      "Code Representation Learning",
      "Programming Language Families",
      "Language Similarity",
      "Feature-aligned Generation",
      "Transfer Learning",
      "Curriculum Learning",
      "Intermediary Code Translation",
      "Centroid Language"
    ],
    "summary": "本文通过定义21项编程语言特征并生成跨19种语言的语义对齐代码片段，基于代码模型嵌入发现编程语言族谱，并利用语言相似性提出迁移学习、相似度引导的课程学习和基于质心的中介代码翻译策略以提升多语言代码LLM性能。",
    "quality": "High",
    "conference": "FSE 2026",
    "pdf_url": "https://arxiv.org/pdf/2512.19509v1",
    "published": "2025-12-22",
    "update_time": "2025-12-22",
    "download_time": "2025-12-24 01:50:36"
  },
  {
    "id": "2512.19481",
    "title": "A Dataset and Preliminary Study of Using GPT-5 for Code-change Impact Analysis",
    "abstract": "Understanding source code changes and their impact on other code entities is a crucial skill in software development. However, the analysis of code changes and their impact is often performed manually and therefore is time-consuming. Recent advancements in AI, and in particular large language models (LLMs) show promises to help developers in various code analysis tasks. However, the extent to which this potential can be utilized for understanding code changes and their impact is underexplored. To address this gap, we study the capabilities of GPT-5 and GPT-5-mini to predict the code entities impacted by given source code changes. We construct a dataset containing information about seed-changes, change pairs, and change types for each commit. Existing datasets lack crucial information about seed changes and impacted code entities. Our experiments evaluate the LLMs in two configurations: (1) seed-change information and the parent commit tree and (2) seed-change information, the parent commit tree, and the diff hunk of each seed change. We found that both LLMs perform poorly in the two experiments, whereas GPT-5 outperforms GPT-5-mini. Furthermore, the provision of the diff hunks helps both models to slightly improve their performance.",
    "arxiv_url": "https://arxiv.org/abs/2512.19481",
    "authors": [
      "Katharina Stengg",
      "Christian Macho",
      "Martin Pinzger"
    ],
    "first_author": "Katharina Stengg",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Maintenance",
    "task": "Change Impact Analysis",
    "tags": [
      "Change Impact Prediction",
      "Commit-level Dataset",
      "Seed-change Annotation",
      "Diff-hunk Prompting",
      "Prompt Optimization",
      "Manual Annotation",
      "Java Code Analysis",
      "LLM Evaluation"
    ],
    "summary": "本文构建了一个扩展的代码变更影响数据集（包含种子变更、变更对与diff hunk），并对GPT-5与GPT-5-mini在代码变更影响预测任务上进行了初步评估，结果表明模型总体表现较差但在提供diff hunk时略有改善。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.19481v1",
    "published": "2025-12-22",
    "update_time": "2025-12-22",
    "download_time": "2025-12-24 01:52:55"
  },
  {
    "id": "2512.19396",
    "title": "EchoTrail-GUI: Building Actionable Memory for GUI Agents via Critic-Guided Self-Exploration",
    "abstract": "Contemporary GUI agents, while increasingly capable due to advances in Large Vision-Language Models (VLMs), often operate with a critical limitation: they treat each task in isolation, lacking a mechanism to systematically learn from past successes. This digital ''amnesia'' results in sub-optimal performance, repeated errors, and poor generalization to novel challenges. To bridge this gap, we introduce EchoTrail-GUI, a novel framework designed to mimic human-like experiential learning by equipping agents with a dynamic, accessible memory. Our framework operates in three distinct stages. First, during Experience Exploration, an agent autonomously interacts with GUI environments to build a curated database of successful task trajectories, validated by a reward model. Crucially, the entire knowledge base construction is thus fully automated, requiring no human supervision. Second, in the Memory Injection stage, upon receiving a new task, our system efficiently retrieves the most relevant past trajectories to serve as actionable ''memories''. Finally, during GUI Task Inference, these memories are injected as in-context guidance to inform the agent's reasoning and decision-making process. We demonstrate the efficacy of our approach on benchmarks including Android World and AndroidLab. The results show that EchoTrail-GUI significantly improves the task success rate and operational efficiency of baseline agents, validating the power of structured memory in creating more robust and intelligent GUI automation.",
    "arxiv_url": "https://arxiv.org/abs/2512.19396",
    "authors": [
      "Runze Li",
      "Yuwen Zhai",
      "Bo Xu",
      "LiWu Xu",
      "Nian Shi",
      "Wei Zhang",
      "Ran Lin",
      "Liang Wang"
    ],
    "first_author": "Runze Li",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "GUI Interaction & Agents",
    "task": "Memory-Augmented GUI Automation",
    "tags": [
      "Memory-Augmented Agents",
      "Critic-Guided Exploration",
      "Autonomous Trajectory Collection",
      "Trajectory Curation",
      "Dense-Sparse Retrieval",
      "Memory Injection",
      "Retrieval-Augmented Inference",
      "GUI Automation"
    ],
    "summary": "本文提出EchoTrail-GUI，通过评论家引导的自主探索自动构建高质量的GUI操作轨迹库，并采用稠密-稀疏检索将相关成功轨迹作为记忆注入到代理的推理过程中，从而在Android基准上显著提升任务成功率与效率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2512.19396v1",
    "published": "2025-12-22",
    "update_time": "2025-12-22",
    "download_time": "2025-12-24 01:53:43"
  }
]