[
  {
    "id": "2211.16490",
    "title": "Coder Reviewer Reranking for Code Generation",
    "abstract": "Sampling diverse programs from a code language model and reranking with model likelihood is a popular method for code generation but it is prone to preferring degenerate solutions. Inspired by collaborative programming, we propose Coder-Reviewer reranking. We augment Coder language models from past work, which generate programs given language instructions, with Reviewer models, which evaluate the likelihood of the instruction given the generated programs. We perform an extensive study across six datasets with eight models from three model families. Experimental results show that Coder-Reviewer reranking leads to consistent and significant improvement (up to 17% absolute accuracy gain) over reranking with the Coder model only. When combined with executability filtering, Coder-Reviewer reranking can often outperform the minimum Bayes risk method. Coder-Reviewer reranking is easy to implement by prompting, can generalize to different programming languages, and works well with off-the-shelf hyperparameters.",
    "arxiv_url": "https://arxiv.org/abs/2211.16490",
    "authors": [
      "Tianyi Zhang",
      "Tao Yu",
      "Tatsunori B. Hashimoto",
      "Mike Lewis",
      "Wen-tau Yih",
      "Daniel Fried",
      "Sida I. Wang"
    ],
    "first_author": "Tianyi Zhang",
    "category": [
      "Technical / Method"
    ],
    "field": "Coding Assistant",
    "tag": "Code Completion",
    "summary": "本文提出Coder-Reviewer重排方法：通过prompt将生成的程序放在前并估计p(x|y)，与原始p(y|x)的乘积用于重排候选代码，从而减少退化解并在多数据集、多模型上显著提升代码生成准确率且无需额外训练。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2211.16490v1",
    "published": "2022-11-29",
    "update_time": "2022-11-29",
    "download_time": "2025-12-04 23:23:28"
  },
  {
    "id": "2211.11501",
    "title": "DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation",
    "abstract": "We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from StackOverflow. Second, our automatic evaluation is highly specific (reliable) -- across all Codex-002-predicted solutions that our evaluation accept, only 1.8% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original StackOverflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.",
    "arxiv_url": "https://arxiv.org/abs/2211.11501",
    "authors": [
      "Yuhang Lai",
      "Chengxi Li",
      "Yiming Wang",
      "Tianyi Zhang",
      "Ruiqi Zhong",
      "Luke Zettlemoyer",
      "Scott Wen-tau Yih",
      "Daniel Fried",
      "Sida Wang",
      "Tao Yu"
    ],
    "first_author": "Yuhang Lai",
    "category": [
      "Benchmark / Dataset"
    ],
    "field": "Coding Assistant",
    "tag": "Code Completion",
    "summary": "本文提出并发布了DS-1000——一个包含1000个来自StackOverflow的真实数据科学Python问题的基准，配备可执行的多准则自动评测并通过扰动问题防止模型记忆化，以可靠评估代码生成模型性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2211.11501v1",
    "published": "2022-11-18",
    "update_time": "2022-11-18",
    "download_time": "2025-12-04 23:30:49"
  }
]