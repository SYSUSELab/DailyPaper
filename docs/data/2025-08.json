[
  {
    "id": "2508.09945",
    "title": "VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models",
    "abstract": "Multimodal large language models (MLLMs) have significantly advanced the integration of visual and textual understanding. However, their ability to generate code from multimodal inputs remains limited. In this work, we introduce VisCodex, a unified framework that seamlessly merges vision and coding language models to empower MLLMs with strong multimodal code generation abilities. Leveraging a task vector-based model merging technique, we integrate a state-of-the-art coding LLM into a strong vision-language backbone, while preserving both visual comprehension and advanced coding skills. To support training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a large-scale and diverse collection of 598k samples, including high-quality HTML code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic problems. Furthermore, we propose InfiBench-V, a novel and challenging benchmark specifically designed to assess models on visually-rich, real-world programming questions that demand a nuanced understanding of both textual and visual contexts. Extensive experiments show that VisCodex achieves state-of-the-art performance among open-source MLLMs and approaches proprietary models like GPT-4o, highlighting the effectiveness of our model merging strategy and new datasets.",
    "arxiv_url": "https://arxiv.org/abs/2508.09945",
    "authors": [
      "Lingjie Jiang",
      "Shaohan Huang",
      "Xun Wu",
      "Yixia Li",
      "Dongdong Zhang",
      "Furu Wei"
    ],
    "first_author": "Lingjie Jiang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Instruction-Tuning",
    "tags": [
      "Multimodal code generation",
      "Task-vector model merging",
      "Parameter arithmetic fusion of LMs",
      "Preserve visual encoder while merging LM backbone",
      "Large-scale multimodal instruction tuning data",
      "HTML-from-screenshot synthesis",
      "Chart-image-to-code synthesis",
      "Visually-rich code QA benchmark"
    ],
    "summary": "VisCodex通过基于任务向量的参数合并，将视觉-语言模型与专用代码语言模型融合为统一的多模态代码生成器，并同时发布了一个598k样本的多模态指令微调数据集和一个用于视觉密集型编程问答的挑战性基准以提升与评估多模态代码生成能力。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2508.09945v1",
    "published": "2025-08-13",
    "update_time": "2025-08-13",
    "download_time": "2025-12-12 23:23:19"
  },
  {
    "id": "2508.09101",
    "title": "AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, with code generation emerging as a key area of focus. While numerous benchmarks have been proposed to evaluate their code generation abilities, these benchmarks face several critical limitations. First, they often rely on manual annotations, which are time-consuming and difficult to scale across different programming languages and problem complexities. Second, most existing benchmarks focus primarily on Python, while the few multilingual benchmarks suffer from limited difficulty and uneven language distribution. To address these challenges, we propose AutoCodeGen, an automated method for generating high-difficulty multilingual code generation datasets without manual annotations. AutoCodeGen ensures the correctness and completeness of test cases by generating test inputs with LLMs and obtaining test outputs through a multilingual sandbox, while achieving high data quality through reverse-order problem generation and multiple filtering steps. Using this novel method, we introduce AutoCodeBench, a large-scale code generation benchmark comprising 3,920 problems evenly distributed across 20 programming languages. It is specifically designed to evaluate LLMs on challenging, diverse, and practical multilingual tasks. We evaluate over 30 leading open-source and proprietary LLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The results show that even the most advanced LLMs struggle with the complexity, diversity, and multilingual nature of these tasks. Besides, we introduce AutoCodeBench-Complete, specifically designed for base models to assess their few-shot code generation capabilities. We hope the AutoCodeBench series will serve as a valuable resource and inspire the community to focus on more challenging and practical multilingual code generation scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2508.09101",
    "authors": [
      "Jason Chou",
      "Ao Liu",
      "Yuchi Deng",
      "Zhiying Zeng",
      "Tao Zhang",
      "Haotian Zhu",
      "Jianwei Cai",
      "Yue Mao",
      "Chenchen Zhang",
      "Lingyun Tan",
      "Ziyan Xu",
      "Bohui Zhai",
      "Hengyi Liu",
      "Speed Zhu",
      "Wiggin Zhou",
      "Fengzong Lian"
    ],
    "first_author": "Jason Chou",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "Automated dataset synthesis",
      "LLM-sandbox interaction",
      "Reverse-order problem generation",
      "Multilingual sandbox execution",
      "LLM-as-critic filtering",
      "High-difficulty multilingual problems",
      "Multi-logical problem design",
      "Balanced language distribution"
    ],
    "summary": "本文提出AutoCodeGen自动化工作流并发布AutoCodeBench，一个包含3920个高难度、多语言且无人工注释的代码生成基准数据集，通过LLM与多语言沙箱交互生成并验证测试用例、采用反向问题生成与多重过滤以保证高质量，并评估了30+模型的性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2508.09101v1",
    "published": "2025-08-12",
    "update_time": "2025-08-12",
    "download_time": "2025-12-12 23:46:00"
  },
  {
    "id": "2508.16402",
    "title": "AetherCode: Evaluating LLMs' Ability to Win In Premier Programming Competitions",
    "abstract": "Competitive programming has emerged as a critical benchmark for evaluating the reasoning and coding capabilities of Large Language Models (LLMs). Despite impressive progress on existing benchmarks, we argue that current evaluations overstate model proficiency, masking a substantial gap between LLMs and elite human programmers. This gap arises from two key limitations: insufficient difficulty and scope of benchmark problems, and evaluation bias from low-quality test cases. To address these shortcomings, we present AetherCode, a new benchmark that draws problems from premier programming competitions such as IOI and ICPC, offering broader coverage and higher difficulty. AetherCode further incorporates comprehensive, expert-validated test suites built through a hybrid of automated generation and human curation, ensuring rigorous and reliable assessment. By combining challenging problem design with robust evaluation, AetherCode provides a more faithful measure of LLM capabilities and sets a new standard for future research in code reasoning.",
    "arxiv_url": "https://arxiv.org/abs/2508.16402",
    "authors": [
      "Zihan Wang",
      "Jiaze Chen",
      "Zhicheng Liu",
      "Markus Mak",
      "Yidi Du",
      "Geonsik Moon",
      "Luoqi Xu",
      "Aaron Tua",
      "Kunshuo Peng",
      "Jiayi Lu",
      "Mingfei Xia",
      "Boqian Zou",
      "Chenyang Ran",
      "Guang Tian",
      "Shoutai Zhu",
      "Yeheng Duan",
      "Zhenghui Kang",
      "Zhenxing Lin",
      "Shangshu Li",
      "Qiang Luo",
      "Qingshen Long",
      "Zhiyong Chen",
      "Yihan Xiao",
      "Yurong Wu",
      "Daoguang Zan",
      "Yuyi Fu",
      "Mingxuan Wang",
      "Ming Ding"
    ],
    "first_author": "Zihan Wang",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Competitive programming problems",
      "Olympiad and ICPC curation",
      "Expert-validated test suites",
      "Hybrid automated-and-human test generation",
      "Zero false-positive/negative validation",
      "Solution-corpus-based test validation",
      "PDF-to-Markdown/LaTeX problem conversion"
    ],
    "summary": "AetherCode 提出一个来自 IOI、ICPC 等顶级竞赛、并以自动化生成结合专家审核的高质量测试用例构建的新基准，以更严格地评估大模型在竞赛级编程题上的能力。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2508.16402v1",
    "published": "2025-08-22",
    "update_time": "2025-08-22",
    "download_time": "2025-12-12 23:46:31"
  }
]