[
  {
    "id": "2309.12499",
    "title": "CodePlan: Repository-level Coding using LLMs and Planning",
    "abstract": "Software engineering activities such as package migration, fixing errors reports from static analysis or testing, and adding type annotations or other specifications to a codebase, involve pervasively editing the entire repository of code. We formulate these activities as repository-level coding tasks.   Recent tools like GitHub Copilot, which are powered by Large Language Models (LLMs), have succeeded in offering high-quality solutions to localized coding problems. Repository-level coding tasks are more involved and cannot be solved directly using LLMs, since code within a repository is inter-dependent and the entire repository may be too large to fit into the prompt. We frame repository-level coding as a planning problem and present a task-agnostic framework, called CodePlan to solve it. CodePlan synthesizes a multi-step chain of edits (plan), where each step results in a call to an LLM on a code location with context derived from the entire repository, previous code changes and task-specific instructions. CodePlan is based on a novel combination of an incremental dependency analysis, a change may-impact analysis and an adaptive planning algorithm.   We evaluate the effectiveness of CodePlan on two repository-level tasks: package migration (C#) and temporal code edits (Python). Each task is evaluated on multiple code repositories, each of which requires inter-dependent changes to many files (between 2-97 files). Coding tasks of this level of complexity have not been automated using LLMs before. Our results show that CodePlan has better match with the ground truth compared to baselines. CodePlan is able to get 5/6 repositories to pass the validity checks (e.g., to build without errors and make correct code edits) whereas the baselines (without planning but with the same type of contextual information as CodePlan) cannot get any of the repositories to pass them.",
    "arxiv_url": "https://arxiv.org/abs/2309.12499",
    "authors": [
      "Ramakrishna Bairi",
      "Atharv Sonwane",
      "Aditya Kanade",
      "Vageesh D C",
      "Arun Iyer",
      "Suresh Parthasarathy",
      "Sriram Rajamani",
      "B. Ashok",
      "Shashank Shet"
    ],
    "first_author": "Ramakrishna Bairi",
    "category": [
      "Technical"
    ],
    "field": "Maintenance",
    "task": "Refactoring",
    "tags": [
      "Repository-level planning",
      "Incremental dependency analysis",
      "Change may-impact analysis",
      "Adaptive planning algorithm",
      "Edit-obligation plan graph",
      "Seed vs derived edit specification synthesis",
      "Oracle-driven validation (build/tests/type)",
      "Multi-file API migration"
    ],
    "summary": "本文提出CodePlan，将仓库级代码编辑建模为规划问题，结合增量依赖分析、变更影响分析与自适应规划，生成多步LLM驱动的跨文件编辑以完成包迁移和时间性代码修改等仓库级维护任务并通过构建/验证保证正确性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2309.12499v1",
    "published": "2023-09-21",
    "update_time": "2023-09-21",
    "download_time": "2025-12-11 16:32:15"
  },
  {
    "id": "2309.01940",
    "title": "CodeApex: A Bilingual Programming Evaluation Benchmark for Large Language Models",
    "abstract": "With the emergence of Large Language Models (LLMs), there has been a significant improvement in the programming capabilities of models, attracting growing attention from researchers. Evaluating the programming capabilities of LLMs is crucial as it reflects the multifaceted abilities of LLMs, and it has numerous downstream applications. In this paper, we propose CodeApex, a bilingual benchmark dataset focusing on the programming comprehension, code generation, and code correction abilities of LLMs. Programming comprehension task tests LLMs on multiple-choice exam questions covering conceptual understanding, commonsense reasoning, and multi-hop reasoning. The code generation task evaluates LLMs through completing C++ functions based on provided descriptions and prototypes. The code correction task asks LLMs to fix real-world erroneous code segments with different error messages. We evaluate 12 widely used LLMs, including both general-purpose and specialized models. GPT-4 exhibits the best programming capabilities, achieving approximate accuracy of 69%, 54%, and 66% on the three tasks, respectively. Compared to human performance, there is still significant room for improvement in LLM programming. We hope that CodeApex can serve as a reference for evaluating the coding capabilities of LLMs, further promoting their development and growth.",
    "arxiv_url": "https://arxiv.org/abs/2309.01940",
    "authors": [
      "Lingyue Fu",
      "Huacan Chai",
      "Shuang Luo",
      "Kounianhua Du",
      "Weiming Zhang",
      "Longteng Fan",
      "Jiayi Lei",
      "Renting Rui",
      "Jianghao Lin",
      "Yuchen Fang",
      "Yifan Liu",
      "Jingkuan Wang",
      "Siyuan Qi",
      "Kangning Zhang",
      "Weinan Zhang",
      "Yong Yu"
    ],
    "first_author": "Lingyue Fu",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Understanding",
    "tags": [
      "Bilingual evaluation (English and Chinese)",
      "Programming comprehension multiple-choice",
      "C++ function completion",
      "Real-world code correction (error-message driven)",
      "Fine-grained category annotations",
      "Human-expert validation and human-vs-model comparison",
      "Prompt strategy and bilingual performance analysis",
      "Leader-board style large-scale model evaluation"
    ],
    "summary": "本文提出了CodeApex——一个面向大模型的双语（中英）编程能力评测基准，包含编程理解、C++代码生成与真实代码修复三项任务并在大规模细粒度标注与人工验证下评估多款主流LLM以比较其与人类的差距。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2309.01940v4",
    "published": "2023-09-05",
    "update_time": "2024-03-11",
    "download_time": "2025-12-12 23:28:17"
  }
]