[
  {
    "id": "2510.14509",
    "title": "E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task",
    "abstract": "The rapid advancement in large language models (LLMs) has demonstrated significant potential in End-to-End Software Development (E2ESD). However, existing E2ESD benchmarks are limited by coarse-grained requirement specifications and unreliable evaluation protocols, hindering a true understanding of current framework capabilities. To address these limitations, we present E2EDev, a novel benchmark grounded in the principles of Behavior-Driven Development (BDD), which evaluates the capabilities of E2ESD frameworks by assessing whether the generated software meets user needs through mimicking real user interactions (Figure 1). E2EDev comprises (i) a fine-grained set of user requirements, (ii) multiple BDD test scenarios with corresponding Python step implementations for each requirement, and (iii) a fully automated testing pipeline built on the Behave framework. To ensure its quality while reducing the annotation effort, E2EDev leverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework (HITL-MAA). By evaluating various E2ESD frameworks and LLM backbones with E2EDev, our analysis reveals a persistent struggle to effectively solve these tasks, underscoring the critical need for more effective and cost-efficient E2ESD solutions. Our codebase and benchmark are publicly available at https://github.com/SCUNLP/E2EDev.",
    "arxiv_url": "https://arxiv.org/abs/2510.14509",
    "authors": [
      "Jingyao Liu",
      "Chen Huang",
      "Zhizhao Guan",
      "Wenqiang Lei",
      "Yang Deng"
    ],
    "first_author": "Jingyao Liu",
    "category": [
      "Benchmark",
      "Technical"
    ],
    "field": "Requirements & Design",
    "task": "Specification & Validation",
    "tags": [
      "Behavior-Driven Development",
      "Executable BDD test scenarios",
      "Human-in-the-Loop Multi-Agent Annotation",
      "Behave framework integration",
      "Fine-grained user requirement decomposition",
      "End-to-end software generation evaluation",
      "Multi-agent vs single-agent E2ESD comparison",
      "Error analysis and token-cost evaluation"
    ],
    "summary": "该论文提出了基于BDD的E2EDev基准与人机协同多代理注释框架，通过可执行的BDD测试和自动化管道评估端到端软件生成框架的需求满足情况并分析其性能与开销。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2510.14509v2",
    "published": "2025-10-16",
    "update_time": "2025-10-24",
    "download_time": "2025-12-11 15:54:37"
  },
  {
    "id": "2510.05318",
    "title": "BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions",
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance on single-turn text-to-SQL tasks, but real-world database applications predominantly require multi-turn interactions to handle ambiguous queries, execution errors, and evolving user requirements. Existing multi-turn benchmarks fall short by treating conversation histories as static context or limiting evaluation to read-only operations, failing to reflect production-grade database assistant challenges. We introduce BIRD-INTERACT, a benchmark that restores this realism through: (1) a comprehensive interaction environment coupling each database with a hierarchical knowledge base, metadata files, and a function-driven user simulator, enabling models to solicit clarifications, retrieve knowledge, and recover from errors without human supervision; (2) two evaluation settings consisting of a pre-defined conversational protocol (c-Interact) and an open-ended agentic setting (a-Interact) where models autonomously decide when to query the user simulator or explore the environment; (3) a challenging task suite covering the full CRUD spectrum for business-intelligence and operational use cases, guarded by executable test cases. Each task features ambiguous and follow-up sub-tasks requiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600 tasks, up to 11,796 interactions) for comprehensive performance assessment, and BIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed behavioral analysis and rapid method development. Our empirical results highlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in c-Interact and 17.00% in a-Interact. Analysis via memory grafting and Interaction Test-time Scaling validates the importance of effective interaction for complex, dynamic text-to-SQL tasks.",
    "arxiv_url": "https://arxiv.org/abs/2510.05318",
    "authors": [
      "Nan Huo",
      "Xiaohan Xu",
      "Jinyang Li",
      "Per Jacobsson",
      "Shipei Lin",
      "Bowen Qin",
      "Binyuan Hui",
      "Xiaolong Li",
      "Ge Qu",
      "Shuzheng Si",
      "Linheng Han",
      "Edward Alexander",
      "Xintong Zhu",
      "Rui Qin",
      "Ruihan Yu",
      "Yiyao Jin",
      "Feige Zhou",
      "Weihao Zhong",
      "Yun Chen",
      "Hongyu Liu",
      "Chenhao Ma",
      "Fatma Ozcan",
      "Yannis Papakonstantinou",
      "Reynold Cheng"
    ],
    "first_author": "Nan Huo",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Natural Language Interfaces to Databases (NLIDB)",
    "task": "Interactive Text-to-SQL Evaluation",
    "tags": [
      "Function-driven user simulator",
      "Hierarchical knowledge base and metadata",
      "Protocol-guided (c-Interact) vs agentic (a-Interact) settings",
      "Executable test-case based correctness",
      "Full CRUD and schema/modification operations",
      "Controlled simulator to prevent ground-truth leakage",
      "Interaction Test-time Scaling (ITS)",
      "Memory grafting behavioral analysis"
    ],
    "summary": "BIRD-INTERACT 提出一个面向动态多轮交互的文本到 SQL 基准，包含函数驱动的用户模拟器、层级知识库与可执行测试，并通过协议式与自治式两种评估设置衡量 LLM 在覆盖 CRUD 的交互式数据库任务中的表现与挑战。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2510.05318v2",
    "published": "2025-10-06",
    "update_time": "2025-10-08",
    "download_time": "2025-12-12 22:49:08"
  },
  {
    "id": "2510.24762",
    "title": "Falcon: A Comprehensive Chinese Text-to-SQL Benchmark for Enterprise-Grade Evaluation",
    "abstract": "We introduce Falcon, a cross-domain Chinese text-to-SQL benchmark grounded in an enterprise-compatible dialect (MaxCompute/Hive). It contains 600 Chinese questions over 28 databases; 77% require multi-table reasoning and over half touch more than four tables. Each example is annotated along SQL-computation features and Chinese semantics. For evaluation, we release a robust execution comparator and an automated evaluation pipeline, under which all current state-of-the-art large-scale models (including Deepseek) achieve accuracies of at most 50%. Major errors originate from two sources: (1) schema linking in large enterprise landscapes - hundreds of tables, denormalized fields, ambiguous column names, implicit foreign-key relations and domain-specific synonyms that make correct join/column selection difficult; and (2) mapping concise, colloquial Chinese into the exact operators and predicates required for analytics - e.g., choosing the correct aggregation and group-by keys, expressing time windows and granularities, applying unit conversions, handling NULLs and data-quality rules, and formulating nested or windowed subqueries. Falcon therefore targets Chinese-specific semantics and enterprise dialects (abbreviations, business jargon, fuzzy entity references) and provides a reproducible middle ground before full production deployment by using realistic enterprise schemas, query templates, an execution comparator, and an automated evaluation pipeline for end-to-end validation.",
    "arxiv_url": "https://arxiv.org/abs/2510.24762",
    "authors": [
      "Wenzhen Luo",
      "Wei Guan",
      "Yifan Yao",
      "Yimin Pan",
      "Feng Wang",
      "Zhipeng Yu",
      "Zhe Wen",
      "Liang Chen",
      "Yihong Zhuang"
    ],
    "first_author": "Wenzhen Luo",
    "category": [
      "Benchmark"
    ],
    "field": "Natural Language Interfaces to Databases",
    "task": "Text-to-SQL (Chinese, enterprise dialects)",
    "tags": [
      "MaxCompute/Hive dialect support",
      "Enterprise-scale wide/denormalized schemas",
      "Multi-table join reasoning",
      "Schema linking and ambiguous column resolution",
      "Chinese ellipsis and business-jargon mapping",
      "Execution-based schema-aware SQL comparator",
      "Automated dialect-aware evaluation pipeline",
      "Synthetic enterprise case generation and annotation"
    ],
    "summary": "该论文构建并公开了 Falcon——一个针对企业级中文文本到 SQL 的基准与评测工具，覆盖 MaxCompute/Hive 方言、复杂多表推理与中文商业术语并提供精细注释与可执行性比较器以衡量模型在真实企业场景下的性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2510.24762v1",
    "published": "2025-10-23",
    "update_time": "2025-10-23",
    "download_time": "2025-12-12 22:49:40"
  },
  {
    "id": "2510.09595",
    "title": "LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?",
    "abstract": "Competitive programming problems increasingly serve as valuable benchmarks to evaluate the coding capabilities of large language models (LLMs) due to their complexity and ease of verification. Yet, current coding benchmarks face limitations such as lack of exceptionally challenging problems, insufficient test case coverage, reliance on online platform APIs that limit accessibility. To address these issues, we introduce LiveOIBench, a comprehensive benchmark featuring 403 expert-curated Olympiad-level competitive programming problems, each with an average of 60 expert-designed test cases. The problems are sourced directly from 72 official Informatics Olympiads in different regions conducted between 2023 and 2025. LiveOIBench distinguishes itself through four key features: (1) meticulously curated high-quality tasks with detailed subtask rubrics and extensive private test cases; (2) direct integration of elite contestant performance data to enable informative comparison against top-performing humans; (3) planned continuous, contamination-free updates from newly released Olympiad problems; and (4) a self-contained evaluation system facilitating offline and easy-to-reproduce assessments. Benchmarking 32 popular general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable 81.76th percentile, a strong result that nonetheless falls short of top human contestant performance, who usually place above 90th. In contrast, among open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile, underscoring significant capability disparities from frontier closed models. Detailed analyses indicate that robust reasoning models prioritize precise problem analysis over excessive exploration, suggesting future models should emphasize structured analysis and minimize unnecessary exploration. All data, code, and leaderboard results will be made publicly available on our website.",
    "arxiv_url": "https://arxiv.org/abs/2510.09595",
    "authors": [
      "Kaijian Zou",
      "Aaron Xiong",
      "Yunxiang Zhang",
      "Frederick Zhang",
      "Yueqi Ren",
      "Jirong Yang",
      "Ayoung Lee",
      "Shitanshu Bhushan",
      "Lu Wang"
    ],
    "first_author": "Kaijian Zou",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Olympiad-level competitive programming",
      "Expert-curated private test cases",
      "Fine-grained subtask rubrics",
      "Offline reproducible judge",
      "Human-contestant percentile comparison",
      "Continuous contamination-free updates",
      "Reasoning-trace / token-efficiency analysis",
      "Algorithm-specific failure analysis (e.g., dynamic programming)"
    ],
    "summary": "本文提出LiveOIBench——一个包含403道信息学奥赛题、平均60个专家私有测试用例、细粒度子任务评分与离线评测系统的持续更新竞赛编程基准，并用该基准对32个主流模型进行了评测与深入分析，揭示模型在动态规划等算法与推理策略上的不足。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2510.09595v1",
    "published": "2025-10-10",
    "update_time": "2025-10-10",
    "download_time": "2025-12-12 23:47:36"
  },
  {
    "id": "2510.17868",
    "title": "UniCode: A Framework for Generating High Quality Competitive Coding Problems",
    "abstract": "The reliance of competitive coding benchmarks on static, human-authored problems creates significant challenges, including data contamination and limited scalability. To address these issues, we introduce UniCode, a novel framework that automatically generates high-quality algorithmic problems alongside robust, contamination-resistant test cases. Inspired by biological evolution that creates better and diverse offspring, our framework leverages Large Language Models (LLMs) to systematically diversify problems through three strategies: single problem extension, same-type fusion, and cross-type fusion. A key innovation is our stress-driven test case synthesis pipeline, which generates reliable test suites without requiring a canonical ground-truth solution. This pipeline combines brute-force grounding for small-scale inputs with a consensus-based validation mechanism for large-scale inputs to ensure high correctness and coverage. We demonstrate effectiveness of our framework by curating a benchmark of 492 problems and evaluating 19 state-of-the-art LLMs. The results reveal that UniCode is highly challenging and discriminative, with the top-performing model, o4-mini, achieving a pass rate of only 70.3%. Our framework provides a scalable and reliable solution for generating dynamic evaluation datasets in coding domain.",
    "arxiv_url": "https://arxiv.org/abs/2510.17868",
    "authors": [
      "Xinyue Zheng",
      "Haowei Lin",
      "Shaofei Cai",
      "Zilong Zheng",
      "Yitao Liang"
    ],
    "first_author": "Xinyue Zheng",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Benchmarking & Dataset Generation",
    "task": "Problem Generation and Stress-driven Testcase Synthesis",
    "tags": [
      "Generative evaluation",
      "Problem evolution strategies",
      "Single-problem extension",
      "Same-type fusion",
      "Cross-type fusion",
      "Stress-driven test-case synthesis",
      "Brute-force grounding for small inputs",
      "Consensus-based validation with LLM adjudication",
      "Hybrid input sampling (random/adversarial/LLM)",
      "Contamination-resistant competitive coding benchmark"
    ],
    "summary": "本文提出UniCode框架，通过单题扩展、同类/跨类融合等进化策略自动生成竞赛级编程题，并结合穷举验证与多数共识（由LLM仲裁）合成高质量抗污染测试用例，构建了492道题的可扩展评测基准并评估了19个模型。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2510.17868v1",
    "published": "2025-10-16",
    "update_time": "2025-10-16",
    "download_time": "2025-12-12 23:49:41"
  },
  {
    "id": "2510.26130",
    "title": "Beyond Synthetic Benchmarks: Evaluating LLM Performance on Real-World Class-Level Code Generation",
    "abstract": "Large language models (LLMs) have demonstrated strong performance on function-level code generation benchmarks, yet real-world software development increasingly demands class-level implementations that integrate multiple methods, attributes, and dependencies within authentic project contexts. This gap between benchmark performance and practical utility raises critical questions about LLMs' readiness for production code assistance, particularly regarding their ability to generalize across familiar and novel codebases.   We introduce a benchmark derived from real-world open-source repositories, comprising classes divided into seen and unseen partitions to evaluate generalization under practical conditions. We systematically examine how input specification completeness and retrieval-augmented generation affect class-level correctness across multiple state-of-the-art LLMs.   Our evaluation reveals a substantial performance gap: while LLMs achieve 84 to 89% correctness on synthetic benchmarks, they attain only 25 to 34% on real-world class tasks, with minimal distinction between familiar and novel codebases. Comprehensive documentation provides marginal improvements (1 to 3%), whereas retrieval augmentation yields greater gains (4 to 7%) by supplying concrete implementation patterns. Error analysis identifies AttributeError, TypeError, and AssertionError as dominant failure modes, with distinct patterns between synthetic and real-world scenarios.   These findings provide actionable insights for enhancing context modelling, documentation strategies, and retrieval integration in production code assistance tools.",
    "arxiv_url": "https://arxiv.org/abs/2510.26130",
    "authors": [
      "Musfiqur Rahman",
      "SayedHassan Khatoonabadi",
      "Emad Shihab"
    ],
    "first_author": "Musfiqur Rahman",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "Real-world class-level benchmark",
      "Seen-vs-unseen repository split",
      "Retrieval-augmented generation evaluation",
      "Docstring completeness analysis",
      "Error taxonomy (AttributeError/TypeError/AssertionError)",
      "Functional correctness measurement",
      "Repository-level dependency modeling"
    ],
    "summary": "本文构建并公开了来自真实开源项目的类级代码基准（包含seen/unseen划分），系统评估LLM在真实类实现上的生成表现，分析文档完整性与检索增强对函数正确性与错误类型的影响，并发现真实场景下的性能显著低于合成基准且RAG能带来小幅提升。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2510.26130v2",
    "published": "2025-10-30",
    "update_time": "2025-11-04",
    "download_time": "2025-12-12 23:50:47"
  }
]