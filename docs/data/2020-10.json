[
  {
    "id": "2010.01544",
    "title": "Review4Repair: Code Review Aided Automatic Program Repairing",
    "abstract": "Context: Learning-based automatic program repair techniques are showing promise to provide quality fix suggestions for detected bugs in the source code of the software. These tools mostly exploit historical data of buggy and fixed code changes and are heavily dependent on bug localizers while applying to a new piece of code. With the increasing popularity of code review, dependency on bug localizers can be reduced. Besides, the code review-based bug localization is more trustworthy since reviewers' expertise and experience are reflected in these suggestions.   Objective: The natural language instructions scripted on the review comments are enormous sources of information about the bug's nature and expected solutions. However, none of the learning-based tools has utilized the review comments to fix programming bugs to the best of our knowledge. In this study, we investigate the performance improvement of repair techniques using code review comments.   Method: We train a sequence-to-sequence model on 55,060 code reviews and associated code changes. We also introduce new tokenization and preprocessing approaches that help to achieve significant improvement over state-of-the-art learning-based repair techniques.   Results: We boost the top-1 accuracy by 20.33% and top-10 accuracy by 34.82%. We could provide a suggestion for stylistics and non-code errors unaddressed by prior techniques.   Conclusion: We believe that the automatic fix suggestions along with code review generated by our approach would help developers address the review comment quickly and correctly and thus save their time and effort.",
    "arxiv_url": "https://arxiv.org/abs/2010.01544",
    "authors": [
      "Faria Huq",
      "Masum Hasan",
      "Mahim Anzum Haque Pantho",
      "Sazan Mahbub",
      "Anindya Iqbal",
      "Toufique Ahmed"
    ],
    "first_author": "Faria Huq",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Bug Repair",
    "tags": [
      "Review-to-Patch Generation",
      "Code Review Comment Utilization",
      "Pointer-Generator Seq2Seq",
      "Hard and Soft Tokenization",
      "Joint Natural Language and Code Modeling",
      "Stylistic and Non-functional Fixes",
      "Bug Localization via Reviews"
    ],
    "summary": "本文提出Review4Repair，利用代码审查评论与代码变更联合训练指针生成序列到序列模型并采用新的硬/软分词预处理，在55,060条审查-修复对上显著提高了自动程序修复的top-1和top-10准确率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2010.01544v2",
    "published": "2020-10-04",
    "update_time": "2020-10-06",
    "download_time": "2025-12-11 16:59:33"
  },
  {
    "id": "2010.12773",
    "title": "Structure-Grounded Pretraining for Text-to-SQL",
    "abstract": "Learning to capture text-table alignment is essential for tasks like text-to-SQL. A model needs to correctly recognize natural language references to columns and values and to ground them in the given database schema. In this paper, we present a novel weakly supervised Structure-Grounded pretraining framework (StruG) for text-to-SQL that can effectively learn to capture text-table alignment based on a parallel text-table corpus. We identify a set of novel prediction tasks: column grounding, value grounding and column-value mapping, and leverage them to pretrain a text-table encoder. Additionally, to evaluate different methods under more realistic text-table alignment settings, we create a new evaluation set Spider-Realistic based on Spider dev set with explicit mentions of column names removed, and adopt eight existing text-to-SQL datasets for cross-database evaluation. STRUG brings significant improvement over BERT-LARGE in all settings. Compared with existing pretraining methods such as GRAPPA, STRUG achieves similar performance on Spider, and outperforms all baselines on more realistic sets. The Spider-Realistic dataset is available at https://doi.org/10.5281/zenodo.5205322.",
    "arxiv_url": "https://arxiv.org/abs/2010.12773",
    "authors": [
      "Xiang Deng",
      "Ahmed Hassan Awadallah",
      "Christopher Meek",
      "Oleksandr Polozov",
      "Huan Sun",
      "Matthew Richardson"
    ],
    "first_author": "Xiang Deng",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "Structure-grounded pretraining",
      "Text-table alignment",
      "Column grounding",
      "Value grounding",
      "Column-value mapping",
      "Weakly-supervised alignment labeling",
      "Schema linking for SQL generation",
      "Cross-database generalization"
    ],
    "summary": "本文提出STRUG，一种基于并行文本-表格语料通过列对齐、值对齐和列-值映射等弱监督预训练任务来增强文本到SQL的结构对齐能力，并构建更现实的评估设置以验证其跨库泛化效果。",
    "quality": "High",
    "conference": "NAACL 2021",
    "pdf_url": "https://arxiv.org/pdf/2010.12773v3",
    "published": "2020-10-24",
    "update_time": "2022-08-31",
    "download_time": "2025-12-12 21:52:10"
  }
]