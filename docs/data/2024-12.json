[
  {
    "id": "2412.05210",
    "title": "Evaluating and Aligning CodeLLMs on Human Preference",
    "abstract": "Code large language models (codeLLMs) have made significant strides in code generation. Most previous code-related benchmarks, which consist of various programming exercises along with the corresponding test cases, are used as a common measure to evaluate the performance and capabilities of code LLMs. However, the current code LLMs focus on synthesizing the correct code snippet, ignoring the alignment with human preferences, where the query should be sampled from the practical application scenarios and the model-generated responses should satisfy the human preference. To bridge the gap between the model-generated response and human preference, we present a rigorous human-curated benchmark CodeArena to emulate the complexity and diversity of real-world coding tasks, where 397 high-quality samples spanning 40 categories and 44 programming languages, carefully curated from user queries. Further, we propose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B tokens) by scaling instructions from the website to verify the effectiveness of the large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder totally trained on synthetic instruction data can achieve top-tier performance of open-source code LLMs. The results find performance differences between execution-based benchmarks and CodeArena. Our systematic experiments of CodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code LLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring the importance of the human preference alignment.\\footnote{\\url{https://codearenaeval.github.io/ }}",
    "arxiv_url": "https://arxiv.org/abs/2412.05210",
    "authors": [
      "Jian Yang",
      "Jiaxi Yang",
      "Ke Jin",
      "Yibo Miao",
      "Lei Zhang",
      "Liqun Yang",
      "Zeyu Cui",
      "Yichang Zhang",
      "Binyuan Hui",
      "Junyang Lin"
    ],
    "first_author": "Jian Yang",
    "category": [
      "Benchmark",
      "Technical",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Alignment",
    "summary": "本文提出了面向人类偏好的代码评估基准 CodeArena 与大规模合成指令语料 SynCode-Instruct，并基于此训练与评测多款 codeLLM，揭示开放模型与闭源模型在人类偏好对齐上的差距。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2412.05210v1",
    "published": "2024-12-06",
    "update_time": "2024-12-06",
    "download_time": "2025-12-04 23:24:39"
  },
  {
    "id": "2412.18843",
    "title": "Improving the Readability of Automatically Generated Tests using Large Language Models",
    "abstract": "Search-based test generators are effective at producing unit tests with high coverage. However, such automatically generated tests have no meaningful test and variable names, making them hard to understand and interpret by developers. On the other hand, large language models (LLMs) can generate highly readable test cases, but they are not able to match the effectiveness of search-based generators, in terms of achieved code coverage.   In this paper, we propose to combine the effectiveness of search-based generators with the readability of LLM generated tests. Our approach focuses on improving test and variable names produced by search-based tools, while keeping their semantics (i.e., their coverage) unchanged.   Our evaluation on nine industrial and open source LLMs show that our readability improvement transformations are overall semantically-preserving and stable across multiple repetitions. Moreover, a human study with ten professional developers, show that our LLM-improved tests are as readable as developer-written tests, regardless of the LLM employed.",
    "arxiv_url": "https://arxiv.org/abs/2412.18843",
    "authors": [
      "Matteo Biagiola",
      "Gianluca Ghislotti",
      "Paolo Tonella"
    ],
    "first_author": "Matteo Biagiola",
    "category": [
      "Technical",
      "Empirical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Automated unit-test readability improvement",
      "Identifier and test-name renaming",
      "Semantic-preserving transformations",
      "Multi-step prompting to limit context",
      "Coverage-preservation validation",
      "Stability analysis across LLM runs",
      "Human developer readability study",
      "Integration with search-based generators (Evosuite)"
    ],
    "summary": "本文提出一种基于多步提示的技术，利用大型语言模型对搜索驱动（如Evosuite）自动生成的单元测试进行语义保留的标识符与测试名重命名以提升可读性，并通过九个LLM的实验与人工评估验证了其在保持覆盖率的同时可读性和稳定性。",
    "quality": "High",
    "conference": "IEEE Conference on Software Testing, Verification and Validation (ICST) 2025",
    "pdf_url": "https://arxiv.org/pdf/2412.18843v1",
    "published": "2024-12-25",
    "update_time": "2024-12-25",
    "download_time": "2025-12-11 15:40:05"
  },
  {
    "id": "2501.00217",
    "title": "The Potential of LLMs in Automating Software Testing: From Generation to Reporting",
    "abstract": "Having a high quality software is essential in software engineering, which requires robust validation and verification processes during testing activities. Manual testing, while effective, can be time consuming and costly, leading to an increased demand for automated methods. Recent advancements in Large Language Models (LLMs) have significantly influenced software engineering, particularly in areas like requirements analysis, test automation, and debugging. This paper explores an agent-oriented approach to automated software testing, using LLMs to reduce human intervention and enhance testing efficiency. The proposed framework integrates LLMs to generate unit tests, visualize call graphs, and automate test execution and reporting. Evaluations across multiple applications in Python and Java demonstrate the system's high test coverage and efficient operation. This research underscores the potential of LLM-powered agents to streamline software testing workflows while addressing challenges in scalability and accuracy.",
    "arxiv_url": "https://arxiv.org/abs/2501.00217",
    "authors": [
      "Betim Sherifi",
      "Khaled Slhoub",
      "Fitzroy Nembhard"
    ],
    "first_author": "Betim Sherifi",
    "category": [
      "Technical"
    ],
    "field": "Software Testing",
    "task": "Test Generation",
    "tags": [
      "Multi-agent testing framework",
      "Automated unit test generation",
      "Natural-language prompt-driven testing",
      "Call-graph DOT visualization",
      "Automated test execution and PDF reporting",
      "Code file locator and extraction",
      "Cross-language evaluation (Python, Java)",
      "Test rationale generation"
    ],
    "summary": "本文提出了一个由多智能体与大型语言模型驱动的自动化软件测试框架，能够基于自然语言输入自动生成单元测试、可视化调用图并执行测试与生成报告，并在多个 Python 与 Java 应用上进行案例评估。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2501.00217v1",
    "published": "2024-12-31",
    "update_time": "2024-12-31",
    "download_time": "2025-12-11 15:40:40"
  },
  {
    "id": "2412.01769",
    "title": "Commit0: Library Generation from Scratch",
    "abstract": "With the goal of benchmarking generative systems beyond expert software development ability, we introduce Commit0, a benchmark that challenges AI agents to write libraries from scratch. Agents are provided with a specification document outlining the library's API as well as a suite of interactive unit tests, with the goal of producing an implementation of this API accordingly. The implementation is validated through running these unit tests. As a benchmark, Commit0 is designed to move beyond static one-shot code generation towards agents that must process long-form natural language specifications, adapt to multi-stage feedback, and generate code with complex dependencies. Commit0 also offers an interactive environment where models receive static analysis and execution feedback on the code they generate. Our experiments demonstrate that while current agents can pass some unit tests, none can yet fully reproduce full libraries. Results also show that interactive feedback is quite useful for models to generate code that passes more unit tests, validating the benchmarks that facilitate its use.",
    "arxiv_url": "https://arxiv.org/abs/2412.01769",
    "authors": [
      "Wenting Zhao",
      "Nan Jiang",
      "Celine Lee",
      "Justin T Chiu",
      "Claire Cardie",
      "Matthias Gallé",
      "Alexander M Rush"
    ],
    "first_author": "Wenting Zhao",
    "category": [
      "Benchmark",
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Repository-level code generation",
      "Interactive test-driven synthesis",
      "Long-form specification processing",
      "Dependency-aware implementation ordering",
      "Execution & static-analysis feedback loop",
      "Starter-repo hole-filling",
      "Automated unit-test evaluation"
    ],
    "summary": "COMMIT0 提出一个从零生成完整软件库的基准，提供长篇规范、起始仓库与交互式单元测试以评估和促进模型在多轮反馈、依赖管理与长上下文下的代码生成能力。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2412.01769v1",
    "published": "2024-12-02",
    "update_time": "2024-12-02",
    "download_time": "2025-12-11 16:41:31"
  },
  {
    "id": "2412.11990",
    "title": "ExecRepoBench: Multi-level Executable Code Completion Evaluation",
    "abstract": "Code completion has become an essential tool for daily software development. Existing evaluation benchmarks often employ static methods that do not fully capture the dynamic nature of real-world coding environments and face significant challenges, including limited context length, reliance on superficial evaluation metrics, and potential overfitting to training datasets. In this work, we introduce a novel framework for enhancing code completion in software development through the creation of a repository-level benchmark ExecRepoBench and the instruction corpora Repo-Instruct, aim at improving the functionality of open-source large language models (LLMs) in real-world coding scenarios that involve complex interdependencies across multiple files. ExecRepoBench includes 1.2K samples from active Python repositories. Plus, we present a multi-level grammar-based completion methodology conditioned on the abstract syntax tree to mask code fragments at various logical units (e.g. statements, expressions, and functions). Then, we fine-tune the open-source LLM with 7B parameters on Repo-Instruct to produce a strong code completion baseline model Qwen2.5-Coder-Instruct-C based on the open-source model. Qwen2.5-Coder-Instruct-C is rigorously evaluated against existing benchmarks, including MultiPL-E and ExecRepoBench, which consistently outperforms prior baselines across all programming languages. The deployment of \\ourmethod{} can be used as a high-performance, local service for programming development\\footnote{\\url{https://execrepobench.github.io/}}.",
    "arxiv_url": "https://arxiv.org/abs/2412.11990",
    "authors": [
      "Jian Yang",
      "Jiajun Zhang",
      "Jiaxi Yang",
      "Ke Jin",
      "Lei Zhang",
      "Qiyao Peng",
      "Ken Deng",
      "Yibo Miao",
      "Tianyu Liu",
      "Zeyu Cui",
      "Binyuan Hui",
      "Junyang Lin"
    ],
    "first_author": "Jian Yang",
    "category": [
      "Benchmark",
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "Executable repository benchmark",
      "Unit-test-based evaluation",
      "AST-based multi-level masking",
      "Repository-level (cross-file) completion",
      "Instruction corpus for repo completion",
      "Data decontamination (leakage removal)",
      "Supervised fine-tuning for completion"
    ],
    "summary": "本文提出了可执行的仓库级代码补全基准 EXECREPOBENCH 与基于抽象语法树的多级语法掩码指令语料 REPO-INSTRUCT，并在此基础上微调开源模型以实现基于单元测试的跨文件代码补全评测与改进。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2412.11990v1",
    "published": "2024-12-16",
    "update_time": "2024-12-16",
    "download_time": "2025-12-11 16:42:03"
  },
  {
    "id": "2412.17744",
    "title": "RepoTransBench: A Real-World Benchmark for Repository-Level Code Translation",
    "abstract": "Repository-level code translation refers to translating an entire code repository from one programming language to another while preserving the functionality of the source repository. Many benchmarks have been proposed to evaluate the performance of such code translators. However, previous benchmarks mostly provide fine-grained samples, focusing at either code snippet, function, or file-level code translation. Such benchmarks do not accurately reflect real-world demands, where entire repositories often need to be translated, involving longer code length and more complex functionalities. To address this gap, we propose a new benchmark, named RepoTransBench, which is a real-world repository-level code translation benchmark with an automatically executable test suite. We conduct experiments on RepoTransBench to evaluate the translation performance of 11 advanced LLMs. We find that the Success@1 score (test success in one attempt) of the best-performing LLM is only 7.33%. To further explore the potential of LLMs for repository-level code translation, we provide LLMs with error-related feedback to perform iterative debugging and observe an average 7.09% improvement on Success@1. However, even with this improvement, the Success@1 score of the best-performing LLM is only 21%, which may not meet the need for reliable automatic repository-level code translation. Finally, we conduct a detailed error analysis and highlight current LLMs' deficiencies in repository-level code translation, which could provide a reference for further improvements.",
    "arxiv_url": "https://arxiv.org/abs/2412.17744",
    "authors": [
      "Yanli Wang",
      "Yanlin Wang",
      "Suiquan Wang",
      "Daya Guo",
      "Jiachi Chen",
      "John Grundy",
      "Xilin Liu",
      "Yuchi Ma",
      "Mingzhi Mao",
      "Hongyu Zhang",
      "Zibin Zheng"
    ],
    "first_author": "Yanli Wang",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "Repository-level translation",
      "Execution-based test suites",
      "Iterative debugging with error feedback",
      "Resource and configuration file migration",
      "Compilability and functional correctness evaluation",
      "Real-world repository selection and filtering",
      "Error taxonomy for translation failures",
      "Success@k evaluation metric"
    ],
    "summary": "本文提出了RepoTransBench——一个包含100个真实可执行仓库并附自动化测试套件的仓库级代码翻译基准，评估了多款大型模型并通过迭代调试与错误分析揭示了当前模型在仓库级翻译中的不足。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2412.17744v1",
    "published": "2024-12-23",
    "update_time": "2024-12-23",
    "download_time": "2025-12-11 16:54:25"
  },
  {
    "id": "2412.14764",
    "title": "CodeRepoQA: A Large-scale Benchmark for Software Engineering Question Answering",
    "abstract": "In this work, we introduce CodeRepoQA, a large-scale benchmark specifically designed for evaluating repository-level question-answering capabilities in the field of software engineering. CodeRepoQA encompasses five programming languages and covers a wide range of scenarios, enabling comprehensive evaluation of language models. To construct this dataset, we crawl data from 30 well-known repositories in GitHub, the largest platform for hosting and collaborating on code, and carefully filter raw data. In total, CodeRepoQA is a multi-turn question-answering benchmark with 585,687 entries, covering a diverse array of software engineering scenarios, with an average of 6.62 dialogue turns per entry.   We evaluate ten popular large language models on our dataset and provide in-depth analysis. We find that LLMs still have limitations in question-answering capabilities in the field of software engineering, and medium-length contexts are more conducive to LLMs' performance. The entire benchmark is publicly available at https://github.com/kinesiatricssxilm14/CodeRepoQA.",
    "arxiv_url": "https://arxiv.org/abs/2412.14764",
    "authors": [
      "Ruida Hu",
      "Chao Peng",
      "Jingyi Ren",
      "Bo Jiang",
      "Xiangxin Meng",
      "Qinyun Wu",
      "Pengfei Gao",
      "Xinchen Wang",
      "Cuiyun Gao"
    ],
    "first_author": "Ruida Hu",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Understanding",
    "tags": [
      "Repository-level question answering",
      "Multi-turn developer dialogues",
      "GitHub issue mining",
      "Cross-language coverage (Python/JavaScript/TypeScript/Java/Go)",
      "Automatic data filtering heuristics",
      "Context-length impact analysis",
      "Evaluation with text-similarity metrics"
    ],
    "summary": "本文提出CodeRepoQA——一个从30个热门GitHub仓库爬取并过滤得到的包含585,687条多轮（平均6.62轮）、跨五种语言的仓库级软件工程问答大规模基准，并用BLEU/ROUGE/编辑相似度评估多款大语言模型，发现模型在真实仓库QA场景仍存在明显不足且中等长度上下文更有利于性能。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2412.14764v1",
    "published": "2024-12-19",
    "update_time": "2024-12-19",
    "download_time": "2025-12-12 23:44:07"
  },
  {
    "id": "2412.04626",
    "title": "BigDocs: An Open Dataset for Training Multimodal Models on Document and Code Tasks",
    "abstract": "Multimodal AI has the potential to significantly enhance document-understanding tasks, such as processing receipts, understanding workflows, extracting data from documents, and summarizing reports. Code generation tasks that require long-structured outputs can also be enhanced by multimodality. Despite this, their use in commercial applications is often limited due to limited access to training data and restrictive licensing, which hinders open access. To address these limitations, we introduce BigDocs-7.5M, a high-quality, open-access dataset comprising 7.5 million multimodal documents across 30 tasks. We use an efficient data curation process to ensure our data is high-quality and license-permissive. Our process emphasizes accountability, responsibility, and transparency through filtering rules, traceable metadata, and careful content analysis. Additionally, we introduce BigDocs-Bench, a benchmark suite with 10 novel tasks where we create datasets that reflect real-world use cases involving reasoning over Graphical User Interfaces (GUI) and code generation from images. Our experiments show that training with BigDocs-Bench improves average performance up to 25.8% over closed-source GPT-4o in document reasoning and structured output tasks such as Screenshot2HTML or Image2Latex generation. Finally, human evaluations showed a preference for outputs from models trained on BigDocs over GPT-4o. This suggests that BigDocs can help both academics and the open-source community utilize and improve AI tools to enhance multimodal capabilities and document reasoning. The project is hosted at https://bigdocs.github.io .",
    "arxiv_url": "https://arxiv.org/abs/2412.04626",
    "authors": [
      "Juan Rodriguez",
      "Xiangru Jian",
      "Siba Smarak Panigrahi",
      "Tianyu Zhang",
      "Aarash Feizi",
      "Abhay Puri",
      "Akshay Kalkunte",
      "François Savard",
      "Ahmed Masry",
      "Shravan Nayak",
      "Rabiul Awal",
      "Mahsa Massoud",
      "Amirhossein Abaskohi",
      "Zichao Li",
      "Suyuchen Wang",
      "Pierre-André Noël",
      "Mats Leon Richter",
      "Saverio Vadacchino",
      "Shubham Agarwal",
      "Sanket Biswas",
      "Sara Shanian",
      "Ying Zhang",
      "Noah Bolger",
      "Kurt MacDonald",
      "Simon Fauvel",
      "Sathwik Tejaswi",
      "Srinivas Sunkara",
      "Joao Monteiro",
      "Krishnamurthy DJ Dvijotham",
      "Torsten Scholak",
      "Nicolas Chapados",
      "Sepideh Kharagani",
      "Sean Hughes",
      "M. Özsu",
      "Siva Reddy",
      "Marco Pedersoli",
      "Yoshua Bengio",
      "Christopher Pal",
      "Issam Laradji",
      "Spandana Gella",
      "Perouz Taslakian",
      "David Vazquez",
      "Sai Rajeswar"
    ],
    "first_author": "Juan Rodriguez",
    "category": [
      "Benchmark",
      "Technical"
    ],
    "field": "Multimodal Document Understanding",
    "task": "Image-to-Structured-Code Generation (HTML/LaTeX/JSON/SVG)",
    "tags": [
      "License-permissive dataset curation",
      "Continual pretraining dataset for multimodal models",
      "Image-to-markup/code generation",
      "GUI / Screenshot-to-HTML tasks",
      "Unified metadata and dataset traceability",
      "Data contamination analysis",
      "Benchmark suite for long-structured outputs",
      "Data preprocessing & curation toolkit",
      "Human preference evaluation against closed-source models"
    ],
    "summary": "该论文提出并开源BigDocs-7.5M大规模、许可友好的多模态文档数据集及BigDocs-Bench基准、工具包和模型，专注于图像到长结构化输出（如HTML/LaTeX/JSON/SVG）的生成与文档理解，并通过自动与人工评估展示了显著性能提升。",
    "quality": "High",
    "conference": "ICLR 2025",
    "pdf_url": "https://arxiv.org/pdf/2412.04626v2",
    "published": "2024-12-05",
    "update_time": "2025-03-17",
    "download_time": "2025-12-13 00:09:54"
  },
  {
    "id": "2412.17315",
    "title": "CodeV: Issue Resolving with Visual Data",
    "abstract": "Large Language Models (LLMs) have advanced rapidly in recent years, with their applications in software engineering expanding to more complex repository-level tasks. GitHub issue resolving is a key challenge among these tasks. While recent approaches have made progress on this task, they focus on textual data within issues, neglecting visual data. However, this visual data is crucial for resolving issues as it conveys additional knowledge that text alone cannot. We propose CodeV, the first approach to leveraging visual data to enhance the issue-resolving capabilities of LLMs. CodeV resolves each issue by following a two-phase process: data processing and patch generation. To evaluate CodeV, we construct a benchmark for visual issue resolving, namely Visual SWE-bench. Through extensive experiments, we demonstrate the effectiveness of CodeV, as well as provide valuable insights into leveraging visual data to resolve GitHub issues.",
    "arxiv_url": "https://arxiv.org/abs/2412.17315",
    "authors": [
      "Linhao Zhang",
      "Daoguang Zan",
      "Quanshun Yang",
      "Zhirong Huang",
      "Dong Chen",
      "Bo Shen",
      "Tianyu Liu",
      "Yongshun Gong",
      "Pengjie Huang",
      "Xudong Lu",
      "Guangtai Liang",
      "Lizhen Cui",
      "Qianxiang Wang"
    ],
    "first_author": "Linhao Zhang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Bug Repair",
    "tags": [
      "Visual GitHub issue resolving",
      "Vision–language fine-grained descriptions",
      "Structured issue summarization",
      "Multimodal prompt enrichment for patch generation",
      "Repository-level patch generation",
      "Low-cost multimodal pipeline",
      "Integration with agent and agentless resolvers"
    ],
    "summary": "本文提出CODEV，通过视觉-语言模型对Issue中的截图/视频生成细粒度描述与结构化摘要，并将这些信息拼接到Issue中以辅助大模型生成修复补丁，同时构建了用于评估视觉问题修复的基准并验证了显著性能提升。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2412.17315v1",
    "published": "2024-12-23",
    "update_time": "2024-12-23",
    "download_time": "2025-12-13 00:12:22"
  }
]