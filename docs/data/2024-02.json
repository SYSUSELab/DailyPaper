[
  {
    "id": "2402.19173",
    "title": "StarCoder 2 and The Stack v2: The Next Generation",
    "abstract": "The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.",
    "arxiv_url": "https://arxiv.org/abs/2402.19173",
    "authors": [
      "Anton Lozhkov",
      "Raymond Li",
      "Loubna Ben Allal",
      "Federico Cassano",
      "Joel Lamy-Poirier",
      "Nouamane Tazi",
      "Ao Tang",
      "Dmytro Pykhtar",
      "Jiawei Liu",
      "Yuxiang Wei",
      "Tianyang Liu",
      "Max Tian",
      "Denis Kocetkov",
      "Arthur Zucker",
      "Younes Belkada",
      "Zijian Wang",
      "Qian Liu",
      "Dmitry Abulkhanov",
      "Indraneil Paul",
      "Zhuang Li",
      "Wen-Ding Li",
      "Megan Risdal",
      "Jia Li",
      "Jian Zhu",
      "Terry Yue Zhuo",
      "Evgenii Zheltonozhskii",
      "Nii Osae Osae Dade",
      "Wenhao Yu",
      "Lucas Krauß",
      "Naman Jain",
      "Yixuan Su",
      "Xuanli He",
      "Manan Dey",
      "Edoardo Abati",
      "Yekun Chai",
      "Niklas Muennighoff",
      "Xiangru Tang",
      "Muhtasham Oblokulov",
      "Christopher Akiki",
      "Marc Marone",
      "Chenghao Mou",
      "Mayank Mishra",
      "Alex Gu",
      "Binyuan Hui",
      "Tri Dao",
      "Armel Zebaze",
      "Olivier Dehaene",
      "Nicolas Patry",
      "Canwen Xu",
      "Julian McAuley",
      "Han Hu",
      "Torsten Scholak",
      "Sebastien Paquet",
      "Jennifer Robinson",
      "Carolyn Jane Anderson",
      "Nicolas Chapados",
      "Mostofa Patwary",
      "Nima Tajbakhsh",
      "Yacine Jernite",
      "Carlos Muñoz Ferrandis",
      "Lingming Zhang",
      "Sean Hughes",
      "Thomas Wolf",
      "Arjun Guha",
      "Leandro von Werra",
      "Harm de Vries"
    ],
    "first_author": "Anton Lozhkov",
    "category": [
      "Technical / Method",
      "Benchmark / Dataset"
    ],
    "field": "Coding Assistant",
    "tag": "Code Pre-Training",
    "summary": "本文介绍了基于 Software Heritage 构建的 The Stack v2 数据集并在其上训练并公开了 StarCoder2（3B/7B/15B）系列代码大模型，同时发布训练数据标识与权重并在多项代码基准上进行全面评估以展示其性能优势。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2402.19173v1",
    "published": "2024-02-29",
    "update_time": "2024-02-29",
    "download_time": "2025-12-04 23:12:22"
  },
  {
    "id": "2402.16906",
    "title": "Debug like a Human: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step",
    "abstract": "Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifically, LDB segments the programs into basic blocks and tracks the values of intermediate variables after each block throughout the runtime execution. This allows LLMs to concentrate on simpler code units within the overall execution flow, verify their correctness against the task description block by block, and efficiently pinpoint any potential errors. Experiments demonstrate that LDB consistently enhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and TransCoder benchmarks, archiving new state-of-the-art performance in code debugging for various LLM selections.",
    "arxiv_url": "https://arxiv.org/abs/2402.16906",
    "authors": [
      "Li Zhong",
      "Zilong Wang",
      "Jingbo Shang"
    ],
    "first_author": "Li Zhong",
    "category": [
      "Technical / Method"
    ],
    "field": "Coding Assistant",
    "tag": "Code Editing",
    "summary": "本文提出LDB，一种通过将程序分解为基本块并采集每块的运行时中间变量，让LLM逐步验证每个代码块并定位错误以迭代修复生成代码的方法，在HumanEval、MBPP和TransCoder上显著提升了调试与代码生成性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2402.16906v6",
    "published": "2024-02-25",
    "update_time": "2024-06-06",
    "download_time": "2025-12-04 23:21:15"
  }
]