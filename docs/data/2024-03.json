[
  {
    "id": "2403.07974",
    "title": "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code",
    "abstract": "Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and May 2024. We have evaluated 18 base LLMs and 34 instruction-tuned LLMs on LiveCodeBench. We present empirical findings on contamination, holistic performance comparisons, potential overfitting in existing benchmarks as well as individual model comparisons. We will release all prompts and model completions for further community analysis, along with a general toolkit for adding new scenarios and model",
    "arxiv_url": "https://arxiv.org/abs/2403.07974",
    "authors": [
      "Naman Jain",
      "King Han",
      "Alex Gu",
      "Wen-Ding Li",
      "Fanjia Yan",
      "Tianjun Zhang",
      "Sida Wang",
      "Armando Solar-Lezama",
      "Koushik Sen",
      "Ion Stoica"
    ],
    "first_author": "Naman Jain",
    "category": [
      "Experience / Empirical",
      "Benchmark / Dataset"
    ],
    "field": "Coding Assistant",
    "tag": "Holistic Benchmarking & Contamination-free Evaluation",
    "summary": "本文提出 LiveCodeBench，一个持续更新且防止训练数据污染的综合性代码评测基准，收集 2023–2024 年竞赛题并在代码生成、自我修复、代码执行与测试输出预测等多种场景上对多款 LLM 进行广泛评估与污染分析。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2403.07974v2",
    "published": "2024-03-12",
    "update_time": "2024-06-06",
    "download_time": "2025-12-04 23:26:44"
  },
  {
    "id": "2403.08604",
    "title": "Prompting Large Language Models to Tackle the Full Software Development Lifecycle: A Case Study",
    "abstract": "Recent advancements in large language models (LLMs) have significantly enhanced their coding capabilities. However, existing benchmarks predominantly focused on simplified or isolated aspects of coding, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. In this case study, we explore the performance of LLMs across the entire software development lifecycle with DevEval, encompassing stages including software design, environment setup, implementation, acceptance testing, and unit testing. DevEval features four programming languages, multiple domains, high-quality data collection, and carefully designed and verified metrics for each task. Empirical studies show that current LLMs, including GPT-4, fail to solve the challenges presented within DevEval. Our findings offer actionable insights for the future development of LLMs toward real-world programming applications.",
    "arxiv_url": "https://arxiv.org/abs/2403.08604",
    "authors": [
      "Bowen Li",
      "Wenhan Wu",
      "Ziwei Tang",
      "Lin Shi",
      "John Yang",
      "Jinyang Li",
      "Shunyu Yao",
      "Chen Qian",
      "Binyuan Hui",
      "Qicheng Zhang",
      "Zhiyin Yu",
      "He Du",
      "Ping Yang",
      "Dahua Lin",
      "Chao Peng",
      "Kai Chen"
    ],
    "first_author": "Bowen Li",
    "category": [
      "Benchmark / Dataset",
      "Experience / Empirical"
    ],
    "field": "Coding Assistant",
    "tag": "Code Reasoning",
    "summary": "本文提出DevEval，一个覆盖软件设计、环境搭建、实现与验收/单元测试等软件全生命周期任务的基准，并通过对多种LLM（含GPT‑4系列）的评估发现其在仓库级实现与测试上仍存在显著不足。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2403.08604v3",
    "published": "2024-03-13",
    "update_time": "2024-12-14",
    "download_time": "2025-12-04 23:27:14"
  }
]