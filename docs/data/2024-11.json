[
  {
    "id": "2411.18019",
    "title": "A Real-World Benchmark for Evaluating Fine-Grained Issue Solving Capabilities of Large Language Models",
    "abstract": "Automatically resolving software issues is crucial for software development in practice, impacting the software quality and user experience. The process of resolving real-world issues encompasses tasks such as question-answering (QA), fault localization, and code editing. Existing benchmarks such as HumanEval fall short in their ability to assess LLMs' proficiency in solving issues within a codebase. Although benchmarks like SWE-Bench are designed to evaluate the LLMs' capability to handle real-world GitHub issues, the end-to-end evaluation method cannot provide granular insights on the performance of subtasks involved in issue solving. To address existing deficiencies in benchmarking LLMs for practical software engineering tasks, we introduce FAUN-Eval, a benchmark specifically designed to evaluate the Fine-grAined issUe solviNg capabilities of LLMs. FAUN-Eval systematically assesses LLMs across three distinct tasks: QA, fault localization, and code editing. This benchmark is constructed using a dataset curated from 30 well-known GitHub repositories. For each entry, issue and pull request (PR) pairs are meticulously compiled and validated using cross-referencing and keyword verification methods. FAUN-Eval includes 300 entries and employs both LLM and manual checks to ensure data quality. We evaluate ten LLMs with FAUN-Eval, including four closed-source and six open-source models. Our experimental results reveal several key findings. We find that the top-performing LLMs differ across the different tasks. Additionally, features in issues may lead LLMs to generate incorrect information. Moreover, models may vary in their proficiency with texts of different lengths.",
    "arxiv_url": "https://arxiv.org/abs/2411.18019",
    "authors": [
      "Ruida Hu",
      "Chao Peng",
      "Jingyi Ren",
      "Bo Jiang",
      "Xiangxin Meng",
      "Qinyun Wu",
      "Pengfei Gao",
      "Xinchen Wang",
      "Cuiyun Gao"
    ],
    "first_author": "Ruida Hu",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Fine-grained issue decomposition",
      "Real-world GitHub issue–PR pairing",
      "Fault localization evaluation",
      "Code edit/patch generation assessment",
      "Question-answering for issue triage",
      "Data quality verification (LLM + manual checks)",
      "Multi-task performance metrics (EM, CodeBLEU, Edit Similarity)",
      "Sensitivity to issue text features and length"
    ],
    "summary": "本文提出FAUN-Eval，一个从真实GitHub仓库构建的细粒度基准与评估框架，用于分别评测大模型在问题问答、故障定位与代码编辑三项子任务上的能力并对多款模型进行对比分析。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2411.18019v1",
    "published": "2024-11-27",
    "update_time": "2024-11-27",
    "download_time": "2025-12-11 16:40:55"
  },
  {
    "id": "2411.06145",
    "title": "ClassEval-T: Evaluating Large Language Models in Class-Level Code Translation",
    "abstract": "In recent years, Large Language Models (LLMs) have dramatically advanced the performance of automated code translation, making their computational accuracy score reach up to over 80% on many previous benchmarks. However, most code samples in these benchmarks are short, standalone, statement/method-level, and algorithmic, which is not aligned with practical coding tasks. Therefore, it is still unknown the actual capability of LLMs in translating code samples written for daily development. To achieve this, we construct a class-level code translation benchmark, ClassEval-T, and make the first attempt to extensively assess recent LLMs' performance on class-level code translation. ClassEval-T is extended from ClassEval, a well-known class-level Python code generation benchmark consisting of multiple practical coding topics, such as database operation and game design, and diverse contextual dependencies (e.g., fields, methods, and libraries). It cost us 360 person-hours to accomplish the manual migration to Java and C++ with complete code samples and associated test suites. Subsequently, we design three translation strategies (i.e., holistic, min-dependency, and standalone) for class-level code translations and evaluate eight recent LLMs of commercial, general, and code kinds in diverse families and sizes on ClassEval-T. Experimental results demonstrate a remarkable performance drop compared with the most widely studied method-level code translation benchmark, and obvious discrepancies among LLMs appear, showing the effectiveness of ClassEval-T in measuring recent LLMs. Afterwards, we further discuss the usage scenarios for diverse translation strategies and LLMs' ability to dependency awareness when translating class samples. Finally, 1,243 failure cases made by the best-performing LLM under test are analyzed and categorized in this paper for practical guidance and future enlightenment.",
    "arxiv_url": "https://arxiv.org/abs/2411.06145",
    "authors": [
      "Pengyu Xue",
      "Linhao Wu",
      "Zhen Yang",
      "Chengyi Wang",
      "Xiang Li",
      "Yuxiang Zhang",
      "Jia Li",
      "Ruikai Jin",
      "Yifei Pei",
      "Zhaoyan Shen",
      "Xiran Lyu",
      "Jacky Wai Keung"
    ],
    "first_author": "Pengyu Xue",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "Class-level code translation",
      "Cross-language migration (Python, Java, C++)",
      "Holistic / min-dependency / standalone translation strategies",
      "Dependency-awareness evaluation",
      "High-coverage test suites for correctness & compilation",
      "Manual failure-case taxonomy"
    ],
    "summary": "本文构建并公开了面向类级别的代码翻译基准ClassEval-T（含高覆盖测试用例），并用三种翻译策略评估多款LLM的跨语言翻译能力、依赖感知与失败模式，给出实践性分析与建议。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2411.06145v4",
    "published": "2024-11-09",
    "update_time": "2025-04-14",
    "download_time": "2025-12-11 16:53:16"
  },
  {
    "id": "2411.13990",
    "title": "RustRepoTrans: Repository-level Code Translation Benchmark Targeting Rust",
    "abstract": "Recent advancements in large language models (LLMs) have demonstrated impressive capabilities in code translation, typically evaluated using benchmarks like CodeTransOcean and RepoTransBench. However, dependency-free benchmarks fail to capture real-world complexities by focusing primarily on simple function-level translations and overlooking repository-level context (e.g., dependencies). Full-repository translation benchmarks significantly exceed the current capabilities of existing models, resulting in performance bottlenecks that fail to provide actionable insights for guiding model development. Furthermore, existing benchmarks do not account for the scenario of incrementally translating new or modified modules from the source to the target language, which demands careful handling of repository-level contexts such as dependencies, cross-module references, and architectural divergence. Moreover, LLMs' effectiveness in translating to newer, low-resource languages like Rust remains largely underexplored. To address these gaps, we introduce RustRepoTrans, the first repository-level context code translation benchmark targeting incremental translation, comprising 375 tasks translating into Rust from C, Java, and Python. Using this benchmark, we evaluate seven representative LLMs, analyzing their errors to assess limitations in complex translation scenarios. Among them, DeepSeek-R1 performs best with 51.5% Pass@1, excelling in both basic functionality and additional translation abilities, such as noise robustness and syntactical difference identification. However, even DeepSeek-R1 experiences a 22.2% performance drop (Pass@1 from 73.7% to 51.5%) when handling repository-level context compared to previous benchmarks without such context.",
    "arxiv_url": "https://arxiv.org/abs/2411.13990",
    "authors": [
      "Guangsheng Ou",
      "Mingwei Liu",
      "Yuxuan Chen",
      "Yanlin Wang",
      "Xin Peng",
      "Zibin Zheng"
    ],
    "first_author": "Guangsheng Ou",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "Repository-level context",
      "Incremental code migration",
      "Rust migration challenges",
      "Dependency extraction",
      "Function-pair mining",
      "Test-case-driven verification",
      "Error taxonomy (dependency failures)",
      "Fine-grained translation metrics"
    ],
    "summary": "本文提出RustRepoTrans——一个包含375个增量仓库级上下文代码翻译任务、面向从C/Java/Python到Rust迁移的基准，并评估七款LLM、归类错误并引入更细粒度的评估框架。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2411.13990v6",
    "published": "2024-11-21",
    "update_time": "2025-10-17",
    "download_time": "2025-12-11 16:53:49"
  },
  {
    "id": "2411.02310",
    "title": "MdEval: Massively Multilingual Code Debugging",
    "abstract": "Code large language models (LLMs) have made significant progress in code debugging by directly generating the correct code based on the buggy code snippet. Programming benchmarks, typically consisting of buggy code snippet and their associated test cases, are used to assess the debugging capabilities of LLMs. However, many existing benchmarks primarily focus on Python and are often limited in terms of language diversity (e.g., DebugBench and DebugEval). To advance the field of multilingual debugging with LLMs, we propose the first massively multilingual debugging benchmark, which includes 3.6K test samples of 18 programming languages and covers the automated program repair (APR) task, the code review (CR) task, and the bug identification (BI) task. Further, we introduce the debugging instruction corpora MDEVAL-INSTRUCT by injecting bugs into the correct multilingual queries and solutions (xDebugGen). Further, a multilingual debugger xDebugCoder trained on MDEVAL-INSTRUCT as a strong baseline specifically to handle the bugs of a wide range of programming languages (e.g. \"Missing Mut\" in language Rust and \"Misused Macro Definition\" in language C). Our extensive experiments on MDEVAL reveal a notable performance gap between open-source models and closed-source LLMs (e.g., GPT and Claude series), highlighting huge room for improvement in multilingual code debugging scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2411.02310",
    "authors": [
      "Shukai Liu",
      "Linzheng Chai",
      "Jian Yang",
      "Jiajun Shi",
      "He Zhu",
      "Liran Wang",
      "Ke Jin",
      "Wei Zhang",
      "Hualei Zhu",
      "Shuyue Guo",
      "Tao Sun",
      "Jiaheng Liu",
      "Yunlong Duan",
      "Yu Hao",
      "Liqun Yang",
      "Guanglin Niu",
      "Ge Zhang",
      "Zhoujun Li"
    ],
    "first_author": "Shukai Liu",
    "category": [
      "Benchmark",
      "Technical"
    ],
    "field": "Quality Management",
    "task": "Bug Repair",
    "tags": [
      "Multilingual debugging benchmark",
      "Automated program repair",
      "Bug localization",
      "Bug identification",
      "Bug-injection data augmentation",
      "Instruction-tuning for debugging",
      "Round-trip code translation for bug generation",
      "Language-specific error taxonomy",
      "Human-annotated multilingual dataset",
      "Multitask evaluation and leaderboard"
    ],
    "summary": "本文提出了首个覆盖20种编程语言的多语言代码调试基准与指令语料，通过三种注入错误策略生成调试训练对并构建多语言调试基线模型，对40个模型进行大规模评测以揭示多语言调试能力差距。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2411.02310v2",
    "published": "2024-11-04",
    "update_time": "2025-02-24",
    "download_time": "2025-12-11 17:02:09"
  },
  {
    "id": "2411.17274",
    "title": "CleanVul: Automatic Function-Level Vulnerability Detection in Code Commits Using LLM Heuristics",
    "abstract": "Accurate identification of software vulnerabilities is crucial for system integrity. Vulnerability datasets, often derived from the National Vulnerability Database (NVD) or directly from GitHub, are essential for training machine learning models to detect these security flaws. However, these datasets frequently suffer from significant noise, typically 40% to 75%, due primarily to the automatic and indiscriminate labeling of all changes in vulnerability-fixing commits (VFCs) as vulnerability-related. This misclassification occurs because not all changes in a commit aimed at fixing vulnerabilities pertain to security threats; many are routine updates like bug fixes or test improvements.   This paper introduces the first methodology that uses the Large Language Model (LLM) with a heuristic enhancement to automatically identify vulnerability-fixing changes from VFCs, achieving an F1-score of 0.82. VulSifter was applied to a large-scale study, where we conducted a crawl of 127,063 repositories on GitHub, resulting in the acquisition of 5,352,105 commits. VulSifter involves utilizing an LLM to comprehend code semantics and contextual information, while applying heuristics to filter out unrelated changes. We then developed CleanVul, a high-quality dataset comprising 8,198 functions using our LLM heuristic enhancement approach, demonstrating Correctness (90.6%) comparable to established datasets such as SVEN and PrimeVul.   To evaluate the CleanVul dataset, we conducted experiments focusing on fine-tuning various LLMs on CleanVul and other high-quality datasets. Evaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit enhanced accuracy but also superior generalization capabilities compared to those trained on uncleaned datasets. Specifically, models trained on CleanVul and tested on PrimeVul achieve accuracy higher than those trained and tested exclusively on PrimeVul.",
    "arxiv_url": "https://arxiv.org/abs/2411.17274",
    "authors": [
      "Yikun Li",
      "Ting Zhang",
      "Ratnadira Widyasari",
      "Yan Naing Tun",
      "Huu Hung Nguyen",
      "Tan Bui",
      "Ivana Clairine Irsan",
      "Yiran Cheng",
      "Xiang Lan",
      "Han Wei Ang",
      "Frank Liauw",
      "Martin Weyssow",
      "Hong Jin Kang",
      "Eng Lieh Ouh",
      "Lwin Khin Shar",
      "David Lo"
    ],
    "first_author": "Yikun Li",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "LLM-based noise reduction",
      "Vulnerability-fixing commit analysis",
      "Function-level vulnerability labeling",
      "Heuristic filtering of code changes",
      "Commit diff and message contextualization",
      "Automated dataset curation pipeline",
      "Cross-dataset generalization evaluation"
    ],
    "summary": "本文提出将大模型与启发式规则结合的VulSifter方法，自动识别并过滤漏洞修复提交中与漏洞无关的改动，构建高质量的函数级漏洞数据集CleanVul并验证其能提升漏洞检测模型的泛化能力。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2411.17274v7",
    "published": "2024-11-26",
    "update_time": "2025-09-11",
    "download_time": "2025-12-11 17:19:29"
  },
  {
    "id": "2411.07763",
    "title": "Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows",
    "abstract": "Real-world enterprise text-to-SQL workflows often involve complex cloud or local data across various database systems, multiple SQL queries in various dialects, and diverse operations from data transformation to analytics. We introduce Spider 2.0, an evaluation framework comprising 632 real-world text-to-SQL workflow problems derived from enterprise-level database use cases. The databases in Spider 2.0 are sourced from real data applications, often containing over 1,000 columns and stored in local or cloud database systems such as BigQuery and Snowflake. We show that solving problems in Spider 2.0 frequently requires understanding and searching through database metadata, dialect documentation, and even project-level codebases. This challenge calls for models to interact with complex SQL workflow environments, process extremely long contexts, perform intricate reasoning, and generate multiple SQL queries with diverse operations, often exceeding 100 lines, which goes far beyond traditional text-to-SQL challenges. Our evaluations indicate that based on o1-preview, our code agent framework successfully solves only 21.3% of the tasks, compared with 91.2% on Spider 1.0 and 73.0% on BIRD. Our results on Spider 2.0 show that while language models have demonstrated remarkable performance in code generation -- especially in prior text-to-SQL benchmarks -- they require significant improvement in order to achieve adequate performance for real-world enterprise usage. Progress on Spider 2.0 represents crucial steps towards developing intelligent, autonomous, code agents for real-world enterprise settings. Our code, baseline models, and data are available at https://spider2-sql.github.io",
    "arxiv_url": "https://arxiv.org/abs/2411.07763",
    "authors": [
      "Fangyu Lei",
      "Jixuan Chen",
      "Yuxiao Ye",
      "Ruisheng Cao",
      "Dongchan Shin",
      "Hongjin Su",
      "Zhaoqing Suo",
      "Hongcheng Gao",
      "Wenjing Hu",
      "Pengcheng Yin",
      "Victor Zhong",
      "Caiming Xiong",
      "Ruoxi Sun",
      "Qian Liu",
      "Sida Wang",
      "Tao Yu"
    ],
    "first_author": "Fangyu Lei",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "Enterprise text-to-SQL benchmark",
      "Agentic code execution",
      "Multi-step SQL workflows",
      "Large-scale schemas and schema linking",
      "SQL dialect handling (BigQuery/Snowflake/etc.)",
      "Execution-feedback and environment interaction",
      "Data transformation / DBT pipelines",
      "Project codebase grounding"
    ],
    "summary": "Spider 2.0 提出一个面向真实企业场景的文本到 SQL 基准（含632个复杂工作流），要求模型在大规模模式、多方言和多步代理交互下生成并执行复杂 SQL 与数据转换，评估显示当前 LLMs 在此现实任务上性能仍显不足。",
    "quality": "High",
    "conference": "ICLR 2025",
    "pdf_url": "https://arxiv.org/pdf/2411.07763v2",
    "published": "2024-11-12",
    "update_time": "2025-03-17",
    "download_time": "2025-12-12 22:27:52"
  },
  {
    "id": "2411.03012",
    "title": "Leveraging Large Language Models in Code Question Answering: Baselines and Issues",
    "abstract": "Question answering over source code provides software engineers and project managers with helpful information about the implemented features of a software product. This paper presents a work devoted to using large language models for question answering over source code in Python. The proposed method for implementing a source code question answering system involves fine-tuning a large language model on a unified dataset of questions and answers for Python code. To achieve the highest quality answers, we tested various models trained on datasets preprocessed in different ways: a dataset without grammar correction, a dataset with grammar correction, and a dataset augmented with the generated summaries. The model answers were also analyzed for errors manually. We report BLEU-4, BERTScore F1, BLEURT, and Exact Match metric values, along with the conclusions from the manual error analysis. The obtained experimental results highlight the current problems of the research area, such as poor quality of the public genuine question-answering datasets. In addition, the findings include the positive effect of the grammar correction of the training data on the testing metric values. The addressed findings and issues could be important for other researchers who attempt to improve the quality of source code question answering solutions. The training and evaluation code is publicly available at https://github.com/IU-AES-AI4Code/CodeQuestionAnswering.",
    "arxiv_url": "https://arxiv.org/abs/2411.03012",
    "authors": [
      "Georgy Andryushchenko",
      "Vladimir Ivanov",
      "Vladimir Makharev",
      "Elizaveta Tukhtina",
      "Aidar Valeev"
    ],
    "first_author": "Georgy Andryushchenko",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Understanding",
    "tags": [
      "Code question answering baselines",
      "Fine-tuning for code QA",
      "Grammar correction of QA data",
      "Summary-based data augmentation",
      "Manual error analysis of answers",
      "Evaluation with BLEU/BERTScore/BLEURT/ExactMatch",
      "Quality issues in public QA datasets",
      "File-level vs function-level context"
    ],
    "summary": "本文通过对不同预处理（语法修正、摘要增强）下的开源代码大模型进行微调并以自动指标与人工错误分析评估，构建了Python代码问答的基线并揭示了公共问答数据质量等问题。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2411.03012v1",
    "published": "2024-11-05",
    "update_time": "2024-11-05",
    "download_time": "2025-12-12 23:41:50"
  },
  {
    "id": "2411.18932",
    "title": "ScratchEval: Are GPT-4o Smarter than My Child? Evaluating Large Multimodal Models with Visual Programming Challenges",
    "abstract": "Recent advancements in large multimodal models (LMMs) have showcased impressive code generation capabilities, primarily evaluated through image-to-code benchmarks. However, these benchmarks are limited to specific visual programming scenarios where the logic reasoning and the multimodal understanding capacities are split apart. To fill this gap, we propose ScratchEval, a novel benchmark designed to evaluate the visual programming reasoning ability of LMMs. ScratchEval is based on Scratch, a block-based visual programming language widely used in children's programming education. By integrating visual elements and embedded programming logic, ScratchEval requires the model to process both visual information and code structure, thereby comprehensively evaluating its programming intent understanding ability. Our evaluation approach goes beyond the traditional image-to-code mapping and focuses on unified logical thinking and problem-solving abilities, providing a more comprehensive and challenging framework for evaluating the visual programming ability of LMMs. ScratchEval not only fills the gap in existing evaluation methods, but also provides new insights for the future development of LMMs in the field of visual programming. Our benchmark can be accessed at https://github.com/HKBUNLP/ScratchEval .",
    "arxiv_url": "https://arxiv.org/abs/2411.18932",
    "authors": [
      "Rao Fu",
      "Ziyang Luo",
      "Hongzhan Lin",
      "Zhen Ye",
      "Jing Ma"
    ],
    "first_author": "Rao Fu",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Reasoning",
    "tags": [
      "Scratch (block-based programming)",
      "Visual program reasoning",
      "Multimodal multiple-choice QA",
      "Cross-lingual evaluation (English/Chinese)",
      "Chain-of-Thought and eCoT prompting analysis",
      "Spatial and graphic perception tasks",
      "Mathematical and logical reasoning in visual code"
    ],
    "summary": "本文提出了基于儿童图形化编程语言Scratch的ScratchEval基准，通过305道中英双语的可视化编程多项选择题评估大规模多模态模型在图像与嵌入式程序逻辑理解与推理上的能力，并分析了不同提示策略的影响。",
    "quality": "Middle",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2411.18932v1",
    "published": "2024-11-28",
    "update_time": "2024-11-28",
    "download_time": "2025-12-12 23:43:36"
  }
]