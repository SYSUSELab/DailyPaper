[
  {
    "id": "2411.04905",
    "title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models",
    "abstract": "Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems. While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scientific investigation, particularly those with reproducible data processing pipelines and transparent training protocols, remain limited. The scarcity is due to various challenges, including resource constraints, ethical considerations, and the competitive advantages of keeping models advanced. To address the gap, we introduce OpenCoder, a top-tier code LLM that not only achieves performance comparable to leading models but also serves as an \"open cookbook\" for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Through this comprehensive release, we identify the key ingredients for building a top-tier code LLM: (1) code optimized heuristic rules for data cleaning and methods for data deduplication, (2) recall of text corpus related to code and (3) high-quality synthetic data in both annealing and supervised fine-tuning stages. By offering this level of openness, we aim to broaden access to all aspects of a top-tier code LLM, with OpenCoder serving as both a powerful model and an open foundation to accelerate research, and enable reproducible advancements in code AI.",
    "arxiv_url": "https://arxiv.org/abs/2411.04905",
    "authors": [
      "Siming Huang",
      "Tianhao Cheng",
      "J. K. Liu",
      "Jiaran Hao",
      "Liuyihan Song",
      "Yang Xu",
      "J. Yang",
      "Jiaheng Liu",
      "Chenchen Zhang",
      "Linzheng Chai",
      "Ruifeng Yuan",
      "Zhaoxiang Zhang",
      "Jie Fu",
      "Qian Liu",
      "Ge Zhang",
      "Zili Wang",
      "Yuan Qi",
      "Yinghui Xu",
      "Wei Chu"
    ],
    "first_author": "Siming Huang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Pre-Training",
    "summary": "OpenCoder提出并开源了一个顶级代码LLM，发布模型权重、可复现的数据与训练流水线，并通过数据清洗、去重与高质量合成数据等策略揭示构建高性能代码模型的关键要素。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2411.04905v3",
    "published": "2024-11-07",
    "update_time": "2025-03-20",
    "download_time": "2025-12-04 21:51:44"
  },
  {
    "id": "2411.12882",
    "title": "ProSec: Fortifying Code LLMs with Proactive Security Alignment",
    "abstract": "While recent code-specific large language models (LLMs) have greatly enhanced their code generation capabilities, the safety of these models remains under-explored, posing potential risks as insecure code generated by these models may introduce vulnerabilities into real-world systems. Existing methods collect security-focused datasets from real-world vulnerabilities for instruction tuning in order to mitigate such issues. However, they are largely constrained by the data sparsity of vulnerable code, and have limited applicability in the multi-stage post-training workflows of modern LLMs. In this paper, we propose ProSec, a novel proactive security alignment approach designed to align code LLMs with secure coding practices. ProSec systematically exposes the vulnerabilities in a code LLM by synthesizing vulnerability-inducing coding scenarios from Common Weakness Enumerations (CWEs) and generates fixes to vulnerable code snippets, allowing the model to learn secure practices through preference learning objectives. The scenarios synthesized by ProSec trigger 25x more vulnerable code than a normal instruction-tuning dataset, resulting in a security-focused alignment dataset 7x larger than the previous work. Experiments show that models trained with ProSec are 25.2% to 35.4% more secure compared to previous work without degrading models' utility.",
    "arxiv_url": "https://arxiv.org/abs/2411.12882",
    "authors": [
      "Xiangzhe Xu",
      "Zian Su",
      "Jinyao Guo",
      "Kaiyuan Zhang",
      "Zhenting Wang",
      "Xiangyu Zhang"
    ],
    "first_author": "Xiangzhe Xu",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Alignment",
    "summary": "本文提出PROSEC，一种在后训练阶段通过基于CWE合成易触发漏洞的指令、从目标模型采样易受攻击与修复代码并构建偏好学习数据来对代码LLM进行主动安全对齐的方法，从而显著减少生成不安全代码而不损害模型实用性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2411.12882v3",
    "published": "2024-11-19",
    "update_time": "2025-06-06",
    "download_time": "2025-12-04 23:17:39"
  },
  {
    "id": "2412.00535",
    "title": "FullStack Bench: Evaluating LLMs as Full Stack Coders",
    "abstract": "As the capabilities of code large language models (LLMs) continue to expand, their applications across diverse code intelligence domains are rapidly increasing. However, most existing datasets only evaluate limited application domains. To address this gap, we have developed a comprehensive code evaluation dataset FullStack Bench focusing on full-stack programming, which encompasses a wide range of application domains (e.g., basic programming, data analysis, software engineering, mathematics, and machine learning). Besides, to assess multilingual programming capabilities, in FullStack Bench, we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations. Moreover, we also release an effective code sandbox execution tool (i.e., SandboxFusion) supporting various programming languages and packages to evaluate the performance of our FullStack Bench efficiently. Comprehensive experimental results on our FullStack Bench demonstrate the necessity and effectiveness of our FullStack Bench and SandboxFusion.",
    "arxiv_url": "https://arxiv.org/abs/2412.00535",
    "authors": [
      "Bytedance-Seed-Foundation-Code-Team",
      ":",
      "Yao Cheng",
      "Jianfeng Chen",
      "Jie Chen",
      "Li Chen",
      "Liyu Chen",
      "Wentao Chen",
      "Zhengyu Chen",
      "Shijie Geng",
      "Aoyan Li",
      "Bo Li",
      "Bowen Li",
      "Linyi Li",
      "Boyi Liu",
      "Jiaheng Liu",
      "Kaibo Liu",
      "Qi Liu",
      "Shukai Liu",
      "Siyao Liu",
      "Tianyi Liu",
      "Tingkai Liu",
      "Yongfei Liu",
      "Rui Long",
      "Jing Mai",
      "Guanghan Ning",
      "Z. Y. Peng",
      "Kai Shen",
      "Jiahao Su",
      "Jing Su",
      "Tao Sun",
      "Yifan Sun",
      "Yunzhe Tao",
      "Guoyin Wang",
      "Siwei Wang",
      "Xuwu Wang",
      "Yite Wang",
      "Zihan Wang",
      "Jinxiang Xia",
      "Liang Xiang",
      "Xia Xiao",
      "Yongsheng Xiao",
      "Chenguang Xi",
      "Shulin Xin",
      "Jingjing Xu",
      "Shikun Xu",
      "Hongxia Yang",
      "Jack Yang",
      "Yingxiang Yang",
      "Jianbo Yuan",
      "Jun Zhang",
      "Yufeng Zhang",
      "Yuyu Zhang",
      "Shen Zheng",
      "He Zhu",
      "Ming Zhu"
    ],
    "first_author": "Bytedance-Seed-Foundation-Code-Team",
    "category": [
      "Benchmark",
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "summary": "本文提出了FullStack Bench——一个覆盖多领域、16种编程语言且基于真实使用场景与单元测试的全栈代码评测基准，并发布了支持多语言与依赖管理的执行沙箱SandboxFusion以自动化评估LLM的全栈编程能力。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2412.00535v6",
    "published": "2024-11-30",
    "update_time": "2025-05-12",
    "download_time": "2025-12-04 23:25:08"
  },
  {
    "id": "2411.05830",
    "title": "GitChameleon: Unmasking the Version-Switching Capabilities of Code Generation Models",
    "abstract": "The rapid evolution of software libraries presents a significant challenge for code generation models, which must adapt to frequent version updates while maintaining compatibility with previous versions. Existing code completion benchmarks often overlook this dynamic aspect, and the one that does consider it relies on static code prediction tasks without execution-based evaluation, offering a limited perspective on a model's practical usability. To address this gap, we introduce \\textbf{\\GitChameleon{}}, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. \\GitChameleon{} is designed to rigorously assess the ability of modern large language models (LLMs) to generate version-specific code that is not only syntactically correct but also functionally accurate upon execution. Our comprehensive evaluations reveal that state-of-the-art LLMs struggle with this task; for instance, \\textbf{GPT-4o} achieves a pass@10 of only 39.9\\% (43.7\\% when provided with error feedback), highlighting the complexity of the problem and the limitations of current models. By providing an execution-based benchmark that emphasizes the dynamic nature of code libraries, \\GitChameleon{} serves as a critical tool to advance the development of more adaptable and reliable code generation models. For facilitation for further exploration of version-conditioned code generation, we make our code repository publicly accessible at \\url{https://github.com/NizarIslah/GitChameleon}.",
    "arxiv_url": "https://arxiv.org/abs/2411.05830",
    "authors": [
      "Nizar Islah",
      "Justine Gehring",
      "Diganta Misra",
      "Eilif Muller",
      "Irina Rish",
      "Terry Yue Zhuo",
      "Massimo Caccia"
    ],
    "first_author": "Nizar Islah",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "summary": "本文提出了GitChameleon，一个包含116个带可执行单元测试的Python库版本条件代码补全基准，用于评估LLM在库版本变化下生成语法与功能正确代码的能力，并展示了当前模型在该任务上的显著不足。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2411.05830v1",
    "published": "2024-11-05",
    "update_time": "2024-11-05",
    "download_time": "2025-12-04 23:25:36"
  },
  {
    "id": "2411.18019",
    "title": "A Real-World Benchmark for Evaluating Fine-Grained Issue Solving Capabilities of Large Language Models",
    "abstract": "Automatically resolving software issues is crucial for software development in practice, impacting the software quality and user experience. The process of resolving real-world issues encompasses tasks such as question-answering (QA), fault localization, and code editing. Existing benchmarks such as HumanEval fall short in their ability to assess LLMs' proficiency in solving issues within a codebase. Although benchmarks like SWE-Bench are designed to evaluate the LLMs' capability to handle real-world GitHub issues, the end-to-end evaluation method cannot provide granular insights on the performance of subtasks involved in issue solving. To address existing deficiencies in benchmarking LLMs for practical software engineering tasks, we introduce FAUN-Eval, a benchmark specifically designed to evaluate the Fine-grAined issUe solviNg capabilities of LLMs. FAUN-Eval systematically assesses LLMs across three distinct tasks: QA, fault localization, and code editing. This benchmark is constructed using a dataset curated from 30 well-known GitHub repositories. For each entry, issue and pull request (PR) pairs are meticulously compiled and validated using cross-referencing and keyword verification methods. FAUN-Eval includes 300 entries and employs both LLM and manual checks to ensure data quality. We evaluate ten LLMs with FAUN-Eval, including four closed-source and six open-source models. Our experimental results reveal several key findings. We find that the top-performing LLMs differ across the different tasks. Additionally, features in issues may lead LLMs to generate incorrect information. Moreover, models may vary in their proficiency with texts of different lengths.",
    "arxiv_url": "https://arxiv.org/abs/2411.18019",
    "authors": [
      "Ruida Hu",
      "Chao Peng",
      "Jingyi Ren",
      "Bo Jiang",
      "Xiangxin Meng",
      "Qinyun Wu",
      "Pengfei Gao",
      "Xinchen Wang",
      "Cuiyun Gao"
    ],
    "first_author": "Ruida Hu",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "tags": [
      "Fine-grained issue decomposition",
      "Real-world GitHub issue–PR pairing",
      "Fault localization evaluation",
      "Code edit/patch generation assessment",
      "Question-answering for issue triage",
      "Data quality verification (LLM + manual checks)",
      "Multi-task performance metrics (EM, CodeBLEU, Edit Similarity)",
      "Sensitivity to issue text features and length"
    ],
    "summary": "本文提出FAUN-Eval，一个从真实GitHub仓库构建的细粒度基准与评估框架，用于分别评测大模型在问题问答、故障定位与代码编辑三项子任务上的能力并对多款模型进行对比分析。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2411.18019v1",
    "published": "2024-11-27",
    "update_time": "2024-11-27",
    "download_time": "2025-12-11 16:40:55"
  },
  {
    "id": "2411.06145",
    "title": "ClassEval-T: Evaluating Large Language Models in Class-Level Code Translation",
    "abstract": "In recent years, Large Language Models (LLMs) have dramatically advanced the performance of automated code translation, making their computational accuracy score reach up to over 80% on many previous benchmarks. However, most code samples in these benchmarks are short, standalone, statement/method-level, and algorithmic, which is not aligned with practical coding tasks. Therefore, it is still unknown the actual capability of LLMs in translating code samples written for daily development. To achieve this, we construct a class-level code translation benchmark, ClassEval-T, and make the first attempt to extensively assess recent LLMs' performance on class-level code translation. ClassEval-T is extended from ClassEval, a well-known class-level Python code generation benchmark consisting of multiple practical coding topics, such as database operation and game design, and diverse contextual dependencies (e.g., fields, methods, and libraries). It cost us 360 person-hours to accomplish the manual migration to Java and C++ with complete code samples and associated test suites. Subsequently, we design three translation strategies (i.e., holistic, min-dependency, and standalone) for class-level code translations and evaluate eight recent LLMs of commercial, general, and code kinds in diverse families and sizes on ClassEval-T. Experimental results demonstrate a remarkable performance drop compared with the most widely studied method-level code translation benchmark, and obvious discrepancies among LLMs appear, showing the effectiveness of ClassEval-T in measuring recent LLMs. Afterwards, we further discuss the usage scenarios for diverse translation strategies and LLMs' ability to dependency awareness when translating class samples. Finally, 1,243 failure cases made by the best-performing LLM under test are analyzed and categorized in this paper for practical guidance and future enlightenment.",
    "arxiv_url": "https://arxiv.org/abs/2411.06145",
    "authors": [
      "Pengyu Xue",
      "Linhao Wu",
      "Zhen Yang",
      "Chengyi Wang",
      "Xiang Li",
      "Yuxiang Zhang",
      "Jia Li",
      "Ruikai Jin",
      "Yifei Pei",
      "Zhaoyan Shen",
      "Xiran Lyu",
      "Jacky Wai Keung"
    ],
    "first_author": "Pengyu Xue",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "Class-level code translation",
      "Cross-language migration (Python, Java, C++)",
      "Holistic / min-dependency / standalone translation strategies",
      "Dependency-awareness evaluation",
      "High-coverage test suites for correctness & compilation",
      "Manual failure-case taxonomy"
    ],
    "summary": "本文构建并公开了面向类级别的代码翻译基准ClassEval-T（含高覆盖测试用例），并用三种翻译策略评估多款LLM的跨语言翻译能力、依赖感知与失败模式，给出实践性分析与建议。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2411.06145v4",
    "published": "2024-11-09",
    "update_time": "2025-04-14",
    "download_time": "2025-12-11 16:53:16"
  },
  {
    "id": "2411.13990",
    "title": "RustRepoTrans: Repository-level Code Translation Benchmark Targeting Rust",
    "abstract": "Recent advancements in large language models (LLMs) have demonstrated impressive capabilities in code translation, typically evaluated using benchmarks like CodeTransOcean and RepoTransBench. However, dependency-free benchmarks fail to capture real-world complexities by focusing primarily on simple function-level translations and overlooking repository-level context (e.g., dependencies). Full-repository translation benchmarks significantly exceed the current capabilities of existing models, resulting in performance bottlenecks that fail to provide actionable insights for guiding model development. Furthermore, existing benchmarks do not account for the scenario of incrementally translating new or modified modules from the source to the target language, which demands careful handling of repository-level contexts such as dependencies, cross-module references, and architectural divergence. Moreover, LLMs' effectiveness in translating to newer, low-resource languages like Rust remains largely underexplored. To address these gaps, we introduce RustRepoTrans, the first repository-level context code translation benchmark targeting incremental translation, comprising 375 tasks translating into Rust from C, Java, and Python. Using this benchmark, we evaluate seven representative LLMs, analyzing their errors to assess limitations in complex translation scenarios. Among them, DeepSeek-R1 performs best with 51.5% Pass@1, excelling in both basic functionality and additional translation abilities, such as noise robustness and syntactical difference identification. However, even DeepSeek-R1 experiences a 22.2% performance drop (Pass@1 from 73.7% to 51.5%) when handling repository-level context compared to previous benchmarks without such context.",
    "arxiv_url": "https://arxiv.org/abs/2411.13990",
    "authors": [
      "Guangsheng Ou",
      "Mingwei Liu",
      "Yuxuan Chen",
      "Yanlin Wang",
      "Xin Peng",
      "Zibin Zheng"
    ],
    "first_author": "Guangsheng Ou",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "Repository-level context",
      "Incremental code migration",
      "Rust migration challenges",
      "Dependency extraction",
      "Function-pair mining",
      "Test-case-driven verification",
      "Error taxonomy (dependency failures)",
      "Fine-grained translation metrics"
    ],
    "summary": "本文提出RustRepoTrans——一个包含375个增量仓库级上下文代码翻译任务、面向从C/Java/Python到Rust迁移的基准，并评估七款LLM、归类错误并引入更细粒度的评估框架。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2411.13990v6",
    "published": "2024-11-21",
    "update_time": "2025-10-17",
    "download_time": "2025-12-11 16:53:49"
  },
  {
    "id": "2411.02310",
    "title": "MdEval: Massively Multilingual Code Debugging",
    "abstract": "Code large language models (LLMs) have made significant progress in code debugging by directly generating the correct code based on the buggy code snippet. Programming benchmarks, typically consisting of buggy code snippet and their associated test cases, are used to assess the debugging capabilities of LLMs. However, many existing benchmarks primarily focus on Python and are often limited in terms of language diversity (e.g., DebugBench and DebugEval). To advance the field of multilingual debugging with LLMs, we propose the first massively multilingual debugging benchmark, which includes 3.6K test samples of 18 programming languages and covers the automated program repair (APR) task, the code review (CR) task, and the bug identification (BI) task. Further, we introduce the debugging instruction corpora MDEVAL-INSTRUCT by injecting bugs into the correct multilingual queries and solutions (xDebugGen). Further, a multilingual debugger xDebugCoder trained on MDEVAL-INSTRUCT as a strong baseline specifically to handle the bugs of a wide range of programming languages (e.g. \"Missing Mut\" in language Rust and \"Misused Macro Definition\" in language C). Our extensive experiments on MDEVAL reveal a notable performance gap between open-source models and closed-source LLMs (e.g., GPT and Claude series), highlighting huge room for improvement in multilingual code debugging scenarios.",
    "arxiv_url": "https://arxiv.org/abs/2411.02310",
    "authors": [
      "Shukai Liu",
      "Linzheng Chai",
      "Jian Yang",
      "Jiajun Shi",
      "He Zhu",
      "Liran Wang",
      "Ke Jin",
      "Wei Zhang",
      "Hualei Zhu",
      "Shuyue Guo",
      "Tao Sun",
      "Jiaheng Liu",
      "Yunlong Duan",
      "Yu Hao",
      "Liqun Yang",
      "Guanglin Niu",
      "Ge Zhang",
      "Zhoujun Li"
    ],
    "first_author": "Shukai Liu",
    "category": [
      "Benchmark",
      "Technical"
    ],
    "field": "Quality Management",
    "task": "Bug Repair",
    "tags": [
      "Multilingual debugging benchmark",
      "Automated program repair",
      "Bug localization",
      "Bug identification",
      "Bug-injection data augmentation",
      "Instruction-tuning for debugging",
      "Round-trip code translation for bug generation",
      "Language-specific error taxonomy",
      "Human-annotated multilingual dataset",
      "Multitask evaluation and leaderboard"
    ],
    "summary": "本文提出了首个覆盖20种编程语言的多语言代码调试基准与指令语料，通过三种注入错误策略生成调试训练对并构建多语言调试基线模型，对40个模型进行大规模评测以揭示多语言调试能力差距。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2411.02310v2",
    "published": "2024-11-04",
    "update_time": "2025-02-24",
    "download_time": "2025-12-11 17:02:09"
  },
  {
    "id": "2411.17274",
    "title": "CleanVul: Automatic Function-Level Vulnerability Detection in Code Commits Using LLM Heuristics",
    "abstract": "Accurate identification of software vulnerabilities is crucial for system integrity. Vulnerability datasets, often derived from the National Vulnerability Database (NVD) or directly from GitHub, are essential for training machine learning models to detect these security flaws. However, these datasets frequently suffer from significant noise, typically 40% to 75%, due primarily to the automatic and indiscriminate labeling of all changes in vulnerability-fixing commits (VFCs) as vulnerability-related. This misclassification occurs because not all changes in a commit aimed at fixing vulnerabilities pertain to security threats; many are routine updates like bug fixes or test improvements.   This paper introduces the first methodology that uses the Large Language Model (LLM) with a heuristic enhancement to automatically identify vulnerability-fixing changes from VFCs, achieving an F1-score of 0.82. VulSifter was applied to a large-scale study, where we conducted a crawl of 127,063 repositories on GitHub, resulting in the acquisition of 5,352,105 commits. VulSifter involves utilizing an LLM to comprehend code semantics and contextual information, while applying heuristics to filter out unrelated changes. We then developed CleanVul, a high-quality dataset comprising 8,198 functions using our LLM heuristic enhancement approach, demonstrating Correctness (90.6%) comparable to established datasets such as SVEN and PrimeVul.   To evaluate the CleanVul dataset, we conducted experiments focusing on fine-tuning various LLMs on CleanVul and other high-quality datasets. Evaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit enhanced accuracy but also superior generalization capabilities compared to those trained on uncleaned datasets. Specifically, models trained on CleanVul and tested on PrimeVul achieve accuracy higher than those trained and tested exclusively on PrimeVul.",
    "arxiv_url": "https://arxiv.org/abs/2411.17274",
    "authors": [
      "Yikun Li",
      "Ting Zhang",
      "Ratnadira Widyasari",
      "Yan Naing Tun",
      "Huu Hung Nguyen",
      "Tan Bui",
      "Ivana Clairine Irsan",
      "Yiran Cheng",
      "Xiang Lan",
      "Han Wei Ang",
      "Frank Liauw",
      "Martin Weyssow",
      "Hong Jin Kang",
      "Eng Lieh Ouh",
      "Lwin Khin Shar",
      "David Lo"
    ],
    "first_author": "Yikun Li",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "LLM-based noise reduction",
      "Vulnerability-fixing commit analysis",
      "Function-level vulnerability labeling",
      "Heuristic filtering of code changes",
      "Commit diff and message contextualization",
      "Automated dataset curation pipeline",
      "Cross-dataset generalization evaluation"
    ],
    "summary": "本文提出将大模型与启发式规则结合的VulSifter方法，自动识别并过滤漏洞修复提交中与漏洞无关的改动，构建高质量的函数级漏洞数据集CleanVul并验证其能提升漏洞检测模型的泛化能力。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2411.17274v7",
    "published": "2024-11-26",
    "update_time": "2025-09-11",
    "download_time": "2025-12-11 17:19:29"
  },
  {
    "id": "2411.07763",
    "title": "Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows",
    "abstract": "Real-world enterprise text-to-SQL workflows often involve complex cloud or local data across various database systems, multiple SQL queries in various dialects, and diverse operations from data transformation to analytics. We introduce Spider 2.0, an evaluation framework comprising 632 real-world text-to-SQL workflow problems derived from enterprise-level database use cases. The databases in Spider 2.0 are sourced from real data applications, often containing over 1,000 columns and stored in local or cloud database systems such as BigQuery and Snowflake. We show that solving problems in Spider 2.0 frequently requires understanding and searching through database metadata, dialect documentation, and even project-level codebases. This challenge calls for models to interact with complex SQL workflow environments, process extremely long contexts, perform intricate reasoning, and generate multiple SQL queries with diverse operations, often exceeding 100 lines, which goes far beyond traditional text-to-SQL challenges. Our evaluations indicate that based on o1-preview, our code agent framework successfully solves only 21.3% of the tasks, compared with 91.2% on Spider 1.0 and 73.0% on BIRD. Our results on Spider 2.0 show that while language models have demonstrated remarkable performance in code generation -- especially in prior text-to-SQL benchmarks -- they require significant improvement in order to achieve adequate performance for real-world enterprise usage. Progress on Spider 2.0 represents crucial steps towards developing intelligent, autonomous, code agents for real-world enterprise settings. Our code, baseline models, and data are available at https://spider2-sql.github.io",
    "arxiv_url": "https://arxiv.org/abs/2411.07763",
    "authors": [
      "Fangyu Lei",
      "Jixuan Chen",
      "Yuxiao Ye",
      "Ruisheng Cao",
      "Dongchan Shin",
      "Hongjin Su",
      "Zhaoqing Suo",
      "Hongcheng Gao",
      "Wenjing Hu",
      "Pengcheng Yin",
      "Victor Zhong",
      "Caiming Xiong",
      "Ruoxi Sun",
      "Qian Liu",
      "Sida Wang",
      "Tao Yu"
    ],
    "first_author": "Fangyu Lei",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "Enterprise text-to-SQL benchmark",
      "Agentic code execution",
      "Multi-step SQL workflows",
      "Large-scale schemas and schema linking",
      "SQL dialect handling (BigQuery/Snowflake/etc.)",
      "Execution-feedback and environment interaction",
      "Data transformation / DBT pipelines",
      "Project codebase grounding"
    ],
    "summary": "Spider 2.0 提出一个面向真实企业场景的文本到 SQL 基准（含632个复杂工作流），要求模型在大规模模式、多方言和多步代理交互下生成并执行复杂 SQL 与数据转换，评估显示当前 LLMs 在此现实任务上性能仍显不足。",
    "quality": "High",
    "conference": "ICLR 2025",
    "pdf_url": "https://arxiv.org/pdf/2411.07763v2",
    "published": "2024-11-12",
    "update_time": "2025-03-17",
    "download_time": "2025-12-12 22:27:52"
  }
]