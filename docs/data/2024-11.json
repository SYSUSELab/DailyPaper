[
  {
    "id": "2411.04905",
    "title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models",
    "abstract": "Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems. While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scientific investigation, particularly those with reproducible data processing pipelines and transparent training protocols, remain limited. The scarcity is due to various challenges, including resource constraints, ethical considerations, and the competitive advantages of keeping models advanced. To address the gap, we introduce OpenCoder, a top-tier code LLM that not only achieves performance comparable to leading models but also serves as an \"open cookbook\" for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Through this comprehensive release, we identify the key ingredients for building a top-tier code LLM: (1) code optimized heuristic rules for data cleaning and methods for data deduplication, (2) recall of text corpus related to code and (3) high-quality synthetic data in both annealing and supervised fine-tuning stages. By offering this level of openness, we aim to broaden access to all aspects of a top-tier code LLM, with OpenCoder serving as both a powerful model and an open foundation to accelerate research, and enable reproducible advancements in code AI.",
    "arxiv_url": "https://arxiv.org/abs/2411.04905",
    "authors": [
      "Siming Huang",
      "Tianhao Cheng",
      "J. K. Liu",
      "Jiaran Hao",
      "Liuyihan Song",
      "Yang Xu",
      "J. Yang",
      "Jiaheng Liu",
      "Chenchen Zhang",
      "Linzheng Chai",
      "Ruifeng Yuan",
      "Zhaoxiang Zhang",
      "Jie Fu",
      "Qian Liu",
      "Ge Zhang",
      "Zili Wang",
      "Yuan Qi",
      "Yinghui Xu",
      "Wei Chu"
    ],
    "first_author": "Siming Huang",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "tag": "Code Pre-Training",
    "summary": "OpenCoder提出并开源了一个顶级代码LLM，发布模型权重、可复现的数据与训练流水线，并通过数据清洗、去重与高质量合成数据等策略揭示构建高性能代码模型的关键要素。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2411.04905v3",
    "published": "2024-11-07",
    "update_time": "2025-03-20",
    "download_time": "2025-12-04 21:51:44"
  },
  {
    "id": "2411.12882",
    "title": "ProSec: Fortifying Code LLMs with Proactive Security Alignment",
    "abstract": "While recent code-specific large language models (LLMs) have greatly enhanced their code generation capabilities, the safety of these models remains under-explored, posing potential risks as insecure code generated by these models may introduce vulnerabilities into real-world systems. Existing methods collect security-focused datasets from real-world vulnerabilities for instruction tuning in order to mitigate such issues. However, they are largely constrained by the data sparsity of vulnerable code, and have limited applicability in the multi-stage post-training workflows of modern LLMs. In this paper, we propose ProSec, a novel proactive security alignment approach designed to align code LLMs with secure coding practices. ProSec systematically exposes the vulnerabilities in a code LLM by synthesizing vulnerability-inducing coding scenarios from Common Weakness Enumerations (CWEs) and generates fixes to vulnerable code snippets, allowing the model to learn secure practices through preference learning objectives. The scenarios synthesized by ProSec trigger 25x more vulnerable code than a normal instruction-tuning dataset, resulting in a security-focused alignment dataset 7x larger than the previous work. Experiments show that models trained with ProSec are 25.2% to 35.4% more secure compared to previous work without degrading models' utility.",
    "arxiv_url": "https://arxiv.org/abs/2411.12882",
    "authors": [
      "Xiangzhe Xu",
      "Zian Su",
      "Jinyao Guo",
      "Kaiyuan Zhang",
      "Zhenting Wang",
      "Xiangyu Zhang"
    ],
    "first_author": "Xiangzhe Xu",
    "category": [
      "Technical / Method",
      "Benchmark / Dataset"
    ],
    "field": "Coding Assistant",
    "tag": "Code Alignment",
    "summary": "本文提出PROSEC，一种在后训练阶段通过基于CWE合成易触发漏洞的指令、从目标模型采样易受攻击与修复代码并构建偏好学习数据来对代码LLM进行主动安全对齐的方法，从而显著减少生成不安全代码而不损害模型实用性。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2411.12882v3",
    "published": "2024-11-19",
    "update_time": "2025-06-06",
    "download_time": "2025-12-04 23:17:39"
  },
  {
    "id": "2412.00535",
    "title": "FullStack Bench: Evaluating LLMs as Full Stack Coders",
    "abstract": "As the capabilities of code large language models (LLMs) continue to expand, their applications across diverse code intelligence domains are rapidly increasing. However, most existing datasets only evaluate limited application domains. To address this gap, we have developed a comprehensive code evaluation dataset FullStack Bench focusing on full-stack programming, which encompasses a wide range of application domains (e.g., basic programming, data analysis, software engineering, mathematics, and machine learning). Besides, to assess multilingual programming capabilities, in FullStack Bench, we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations. Moreover, we also release an effective code sandbox execution tool (i.e., SandboxFusion) supporting various programming languages and packages to evaluate the performance of our FullStack Bench efficiently. Comprehensive experimental results on our FullStack Bench demonstrate the necessity and effectiveness of our FullStack Bench and SandboxFusion.",
    "arxiv_url": "https://arxiv.org/abs/2412.00535",
    "authors": [
      "Bytedance-Seed-Foundation-Code-Team",
      ":",
      "Yao Cheng",
      "Jianfeng Chen",
      "Jie Chen",
      "Li Chen",
      "Liyu Chen",
      "Wentao Chen",
      "Zhengyu Chen",
      "Shijie Geng",
      "Aoyan Li",
      "Bo Li",
      "Bowen Li",
      "Linyi Li",
      "Boyi Liu",
      "Jiaheng Liu",
      "Kaibo Liu",
      "Qi Liu",
      "Shukai Liu",
      "Siyao Liu",
      "Tianyi Liu",
      "Tingkai Liu",
      "Yongfei Liu",
      "Rui Long",
      "Jing Mai",
      "Guanghan Ning",
      "Z. Y. Peng",
      "Kai Shen",
      "Jiahao Su",
      "Jing Su",
      "Tao Sun",
      "Yifan Sun",
      "Yunzhe Tao",
      "Guoyin Wang",
      "Siwei Wang",
      "Xuwu Wang",
      "Yite Wang",
      "Zihan Wang",
      "Jinxiang Xia",
      "Liang Xiang",
      "Xia Xiao",
      "Yongsheng Xiao",
      "Chenguang Xi",
      "Shulin Xin",
      "Jingjing Xu",
      "Shikun Xu",
      "Hongxia Yang",
      "Jack Yang",
      "Yingxiang Yang",
      "Jianbo Yuan",
      "Jun Zhang",
      "Yufeng Zhang",
      "Yuyu Zhang",
      "Shen Zheng",
      "He Zhu",
      "Ming Zhu"
    ],
    "first_author": "Bytedance-Seed-Foundation-Code-Team",
    "category": [
      "Benchmark / Dataset",
      "Technical / Method"
    ],
    "field": "Coding Assistant",
    "tag": "Code Completion",
    "summary": "本文提出了FullStack Bench——一个覆盖多领域、16种编程语言且基于真实使用场景与单元测试的全栈代码评测基准，并发布了支持多语言与依赖管理的执行沙箱SandboxFusion以自动化评估LLM的全栈编程能力。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2412.00535v6",
    "published": "2024-11-30",
    "update_time": "2025-05-12",
    "download_time": "2025-12-04 23:25:08"
  },
  {
    "id": "2411.05830",
    "title": "GitChameleon: Unmasking the Version-Switching Capabilities of Code Generation Models",
    "abstract": "The rapid evolution of software libraries presents a significant challenge for code generation models, which must adapt to frequent version updates while maintaining compatibility with previous versions. Existing code completion benchmarks often overlook this dynamic aspect, and the one that does consider it relies on static code prediction tasks without execution-based evaluation, offering a limited perspective on a model's practical usability. To address this gap, we introduce \\textbf{\\GitChameleon{}}, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. \\GitChameleon{} is designed to rigorously assess the ability of modern large language models (LLMs) to generate version-specific code that is not only syntactically correct but also functionally accurate upon execution. Our comprehensive evaluations reveal that state-of-the-art LLMs struggle with this task; for instance, \\textbf{GPT-4o} achieves a pass@10 of only 39.9\\% (43.7\\% when provided with error feedback), highlighting the complexity of the problem and the limitations of current models. By providing an execution-based benchmark that emphasizes the dynamic nature of code libraries, \\GitChameleon{} serves as a critical tool to advance the development of more adaptable and reliable code generation models. For facilitation for further exploration of version-conditioned code generation, we make our code repository publicly accessible at \\url{https://github.com/NizarIslah/GitChameleon}.",
    "arxiv_url": "https://arxiv.org/abs/2411.05830",
    "authors": [
      "Nizar Islah",
      "Justine Gehring",
      "Diganta Misra",
      "Eilif Muller",
      "Irina Rish",
      "Terry Yue Zhuo",
      "Massimo Caccia"
    ],
    "first_author": "Nizar Islah",
    "category": [
      "Benchmark / Dataset",
      "Experience / Empirical"
    ],
    "field": "Coding Assistant",
    "tag": "Code Completion",
    "summary": "本文提出了GitChameleon，一个包含116个带可执行单元测试的Python库版本条件代码补全基准，用于评估LLM在库版本变化下生成语法与功能正确代码的能力，并展示了当前模型在该任务上的显著不足。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2411.05830v1",
    "published": "2024-11-05",
    "update_time": "2024-11-05",
    "download_time": "2025-12-04 23:25:36"
  }
]