[
  {
    "id": "2506.04418",
    "title": "Characterizing Multi-Hunk Patches: Divergence, Proximity, and LLM Repair Challenges",
    "abstract": "Multi-hunk bugs, where fixes span disjoint regions of code, are common in practice, yet remain underrepresented in automated repair. Existing techniques and benchmarks pre-dominantly target single-hunk scenarios, overlooking the added complexity of coordinating semantically related changes across the codebase. In this work, we characterize HUNK4J, a dataset of multi-hunk patches derived from 372 real-world defects. We propose hunk divergence, a metric that quantifies the variation among edits in a patch by capturing lexical, structural, and file-level differences, while incorporating the number of hunks involved. We further define spatial proximity, a classification that models how hunks are spatially distributed across the program hierarchy. Our empirical study spanning six LLMs reveals that model success rates decline with increased divergence and spatial dispersion. Notably, when using the LLM alone, no model succeeds in the most dispersed Fragment class. These findings highlight a critical gap in LLM capabilities and motivate divergence-aware repair strategies.",
    "arxiv_url": "https://arxiv.org/abs/2506.04418",
    "authors": [
      "Noor Nashid",
      "Daniel Ding",
      "Keheliya Gallaba",
      "Ahmed E. Hassan",
      "Ali Mesbah"
    ],
    "first_author": "Noor Nashid",
    "category": [
      "Benchmark",
      "Empirical",
      "Technical"
    ],
    "field": "Quality Management",
    "task": "Bug Repair",
    "tags": [
      "Multi-hunk patch characterization",
      "Hunk divergence metric",
      "Spatial proximity classification",
      "Lexical/AST/file-level pairwise distance",
      "Divergence-aware repair analysis",
      "LLM-based program repair evaluation",
      "Context retrieval and scope effects"
    ],
    "summary": "本文提出了衡量多块补丁内部差异的 hunk divergence 指标与表示补丁空间分布的 spatial proximity 分类，并基于372个真实多块缺陷构建基准和评估平台，对六种LLM的多块修复能力进行系统实证，揭示随着散度和分散程度增加模型修复成功率显著下降。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2506.04418v2",
    "published": "2025-06-04",
    "update_time": "2025-11-17",
    "download_time": "2025-12-11 17:03:20"
  },
  {
    "id": "2506.02073",
    "title": "Flow2Code: Evaluating Large Language Models for Flowchart-based Code Generation Capability",
    "abstract": "While large language models (LLMs) show promise in code generation, existing benchmarks neglect the flowchart-based code generation. To promote further research on flowchart-based code generation, this work presents Flow2Code, a novel benchmark for flowchart-based code generation evaluation. The evaluation dataset spans 15 programming languages and includes 5,622 code segments paired with 16,866 flowcharts of three types: code, UML, and pseudocode. Extensive experiments with 13 multimodal LLMs reveal that current LLMs can not generate code based on flowcharts perfectly. Besides, experiment results show that the supervised fine-tuning technique contributes greatly to the models' performance. We publicly release our code and datasets at https://github.com/hml-github/Flow2Code.",
    "arxiv_url": "https://arxiv.org/abs/2506.02073",
    "authors": [
      "Mengliang He",
      "Jiayi Zeng",
      "Yankai Jiang",
      "Wei Zhang",
      "Zeming Liu",
      "Xiaoming Shi",
      "Aimin Zhou"
    ],
    "first_author": "Mengliang He",
    "category": [
      "Benchmark",
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Translation",
    "tags": [
      "Flowchart-to-code benchmark",
      "Multimodal code generation",
      "Code/UML/Pseudocode flowcharts",
      "Multilingual coverage (15 languages)",
      "DOT flowchart representation",
      "Human-in-the-loop verification",
      "Zero-shot vs supervised fine-tuning comparison",
      "Evaluation of 13 multimodal models",
      "Flowchart construction and transformation pipeline"
    ],
    "summary": "本文提出了 Flow2Code，一个包含代码、UML 与伪代码三类流程图并覆盖15种编程语言的大规模多模态基准，并在13个多模态模型上评测，发现当前模型在流程图驱动的代码生成上仍存在明显不足且有监督微调能显著提升性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2506.02073v1",
    "published": "2025-06-02",
    "update_time": "2025-06-02",
    "download_time": "2025-12-13 00:13:38"
  }
]