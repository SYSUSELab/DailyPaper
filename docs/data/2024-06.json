[
  {
    "id": "2406.11931",
    "title": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence",
    "abstract": "We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K. In standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.",
    "arxiv_url": "https://arxiv.org/abs/2406.11931",
    "authors": [
      "DeepSeek-AI",
      "Qihao Zhu",
      "Daya Guo",
      "Zhihong Shao",
      "Dejian Yang",
      "Peiyi Wang",
      "Runxin Xu",
      "Y. Wu",
      "Yukun Li",
      "Huazuo Gao",
      "Shirong Ma",
      "Wangding Zeng",
      "Xiao Bi",
      "Zihui Gu",
      "Hanwei Xu",
      "Damai Dai",
      "Kai Dong",
      "Liyue Zhang",
      "Yishi Piao",
      "Zhibin Gou",
      "Zhenda Xie",
      "Zhewen Hao",
      "Bingxuan Wang",
      "Junxiao Song",
      "Deli Chen",
      "Xin Xie",
      "Kang Guan",
      "Yuxiang You",
      "Aixin Liu",
      "Qiushi Du",
      "Wenjun Gao",
      "Xuan Lu",
      "Qinyu Chen",
      "Yaohui Wang",
      "Chengqi Deng",
      "Jiashi Li",
      "Chenggang Zhao",
      "Chong Ruan",
      "Fuli Luo",
      "Wenfeng Liang"
    ],
    "first_author": "DeepSeek-AI",
    "category": [
      "Technical / Method"
    ],
    "field": "Coding Assistant",
    "tag": "Code Pre-Training",
    "summary": "本文提出了开源的DeepSeek-Coder-V2——基于Mixture-of-Experts的大规模代码模型，通过在DeepSeek-V2上继续使用额外6万亿tokens的预训练并结合指令微调与GRPO对齐，使得236B参数模型在代码与数学推理基准上达到并在若干任务上超越GPT-4-Turbo等闭源模型，支持338种编程语言和128K上下文长度。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2406.11931v1",
    "published": "2024-06-17",
    "update_time": "2024-06-17",
    "download_time": "2025-12-04 23:11:56"
  },
  {
    "id": "2406.06887",
    "title": "$\\textbf{PLUM}$: Improving Code LMs with Execution-Guided On-Policy Preference Learning Driven By Synthetic Test Cases",
    "abstract": "Preference learning provides a promising solution to address the limitations of supervised fine-tuning (SFT) for code language models, where the model is not explicitly trained to differentiate between correct and incorrect code. Recent findings demonstrate that on-policy data is the key to successful preference learning, where the preference data is collected using the same policy LM being trained. Inspired by this, we propose PLUM, an on-policy $\\textbf{P}$reference $\\textbf{L}$earning framework A$\\textbf{u}$gmented with test cases for code L$\\textbf{M}$ s. The framework operates in three key stages: (1) automatic generation of test cases from natural language instructions, (2) creation of a preference data by evaluating candidate code solutions sampled from the policy, which can then be used to (3) train the policy LM. PLUM levitates the need to train reward models, allowing for large scale on-policy and online preference data collation. PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench), delivering substantial improvements over original SFT'ed models and other execution-feedback-driven approaches. We show PLUM's benefits are consistent across various widely-used code LMs even they have been well-trained with SFT. For example, PLUM increases pass rates by up to 4.8% on average on standard benchmarks and 11.8% on LiveCodeBench, demonstrating its effectiveness and generalizability. We also demonstrate the benefits of on-policy and online preference learning by comprehensive experimentation.",
    "arxiv_url": "https://arxiv.org/abs/2406.06887",
    "authors": [
      "Dylan Zhang",
      "Shizhe Diao",
      "Xueyan Zou",
      "Hao Peng"
    ],
    "first_author": "Dylan Zhang",
    "category": [
      "Technical / Method"
    ],
    "field": "Coding Assistant",
    "tag": "Code Instruction-Tuning",
    "summary": "本文提出PLUM，一种通过自动从自然语言题目生成测试用例并对模型自采样候选解进行执行反馈的在线在策略偏好学习框架，以无需训练奖励模型的方式显著提升代码语言模型的正确率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2406.06887v4",
    "published": "2024-06-11",
    "update_time": "2024-10-12",
    "download_time": "2025-12-04 23:18:09"
  },
  {
    "id": "2406.18294",
    "title": "Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs",
    "abstract": "Some recently developed code large language models (Code LLMs) have been pre-trained on repository-level code data (Repo-Code LLMs), enabling these models to recognize repository structures and utilize cross-file information for code completion. However, in real-world development scenarios, simply concatenating the entire code repository often exceeds the context window limits of these Repo-Code LLMs, leading to significant performance degradation. In this study, we conducted extensive preliminary experiments and analyses on six Repo-Code LLMs. The results indicate that maintaining the topological dependencies of files and increasing the code file content in the completion prompts can improve completion accuracy; pruning the specific implementations of functions in all dependent files does not significantly reduce the accuracy of completions. Based on these findings, we proposed a strategy named Hierarchical Context Pruning (HCP) to construct completion prompts with high informational code content. The HCP models the code repository at the function level, maintaining the topological dependencies between code files while removing a large amount of irrelevant code content, significantly reduces the input length for repository-level code completion. We applied the HCP strategy in experiments with six Repo-Code LLMs, and the results demonstrate that our proposed method can significantly enhance completion accuracy while substantially reducing the length of input. Our code and data are available at https://github.com/Hambaobao/HCP-Coder.",
    "arxiv_url": "https://arxiv.org/abs/2406.18294",
    "authors": [
      "Lei Zhang",
      "Yunshui Li",
      "Jiaming Li",
      "Xiaobo Xia",
      "Jiaxi Yang",
      "Run Luo",
      "Minzheng Wang",
      "Longze Chen",
      "Junhao Liu",
      "Min Yang"
    ],
    "first_author": "Lei Zhang",
    "category": [
      "Technical / Method",
      "Experience / Empirical"
    ],
    "field": "Coding Assistant",
    "tag": "Code Completion",
    "summary": "本文提出了分层上下文剪枝（HCP）策略，在函数级别保留文件间拓扑依赖并裁剪无关实现，从而大幅减少仓库级预训练代码LLM的输入长度并显著提升代码补全准确率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2406.18294v2",
    "published": "2024-06-26",
    "update_time": "2024-06-27",
    "download_time": "2025-12-04 23:20:47"
  }
]