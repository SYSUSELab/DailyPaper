[
  {
    "id": "2306.08568",
    "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
    "abstract": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM",
    "arxiv_url": "https://arxiv.org/abs/2306.08568",
    "authors": [
      "Ziyang Luo",
      "Can Xu",
      "Pu Zhao",
      "Qingfeng Sun",
      "Xiubo Geng",
      "Wenxiang Hu",
      "Chongyang Tao",
      "Jing Ma",
      "Qingwei Lin",
      "Daxin Jiang"
    ],
    "first_author": "Ziyang Luo",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Instruction-Tuning",
    "summary": "本文提出了面向代码领域的Code Evol-Instruct指令微调方法，并基于此训练出WizardCoder模型，在多项代码生成基准（HumanEval、MBPP等）上显著优于现有开源模型，部分规模也超越了若干闭源模型。",
    "quality": "High",
    "conference": "ICLR 2024",
    "pdf_url": "https://arxiv.org/pdf/2306.08568v2",
    "published": "2023-06-14",
    "update_time": "2025-05-27",
    "download_time": "2025-12-04 23:10:59"
  },
  {
    "id": "2306.11644",
    "title": "Textbooks Are All You Need",
    "abstract": "We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality\" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.",
    "arxiv_url": "https://arxiv.org/abs/2306.11644",
    "authors": [
      "Suriya Gunasekar",
      "Yi Zhang",
      "Jyoti Aneja",
      "Caio César Teodoro Mendes",
      "Allie Del Giorno",
      "Sivakanth Gopi",
      "Mojan Javaheripi",
      "Piero Kauffmann",
      "Gustavo de Rosa",
      "Olli Saarikivi",
      "Adil Salim",
      "Shital Shah",
      "Harkirat Singh Behl",
      "Xin Wang",
      "Sébastien Bubeck",
      "Ronen Eldan",
      "Adam Tauman Kalai",
      "Yin Tat Lee",
      "Yuanzhi Li"
    ],
    "first_author": "Suriya Gunasekar",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Pre-Training",
    "summary": "本文提出 phi-1：一个仅1.3B参数、以精选“教科书质量”文本与合成编程练习训练的小型代码生成模型，能在HumanEval和MBPP上以极小规模取得接近或超越更大模型的高性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2306.11644v2",
    "published": "2023-06-20",
    "update_time": "2023-10-02",
    "download_time": "2025-12-04 23:13:17"
  },
  {
    "id": "2306.02907",
    "title": "SelfEvolve: A Code Evolution Framework via Large Language Models",
    "abstract": "Large language models (LLMs) have already revolutionized code generation, after being pretrained on publicly available code data. However, while various methods have been proposed to augment LLMs with retrieved knowledge and enhance the quality of code generation, the performance of these retrieval-based methods is limited by the strength of the retrievers used. In addition, while LLMs show great emergent ability, they still struggle to produce the correct code in one turn. To address these challenges, we propose a novel two-step pipeline, called \\autoknow, that leverages LLMs as both knowledge providers and self-reflective programmers. Unlike retrieval-based methods, \\autoknow~obtains the knowledge from input prompts and generates intermediate code based on the generated knowledge. After that, \\autoknow~asks LLM to act as an expert programmer to perform debugging for the generated code. This is achieved by receiving the error message from the interpreter, without requiring special test cases for correctness verification. We evaluate \\autoknow~on three code generation datasets, including DS-1000 for data science code, HumanEval for software engineering code, and TransCoder for C++-to-Python translation. Our empirical experiments show that \\autoknow~outperforms strong baselines by a significant margin on all datasets. We also conduct exhaustive analytical experiments to validate the effectiveness of the two stages of \\autoknow, and find that both are superior to other prompting-based methods. Further scalability analysis demonstrates that \\autoknow~can be adapted to other more advanced models, such as GPT-4, and bring consistent efficacy improvement.",
    "arxiv_url": "https://arxiv.org/abs/2306.02907",
    "authors": [
      "Shuyang Jiang",
      "Yuhao Wang",
      "Yu Wang"
    ],
    "first_author": "Shuyang Jiang",
    "category": [
      "Technical"
    ],
    "field": "Coding Assistant",
    "task": "Code Prompting",
    "summary": "本文提出SELFEVOLVE，一种两阶段的LLM驱动代码演化框架：先让模型自生成与题目相关的知识再生成代码，随后基于解释器错误信息对代码进行自我调试与迭代，从而在多套数据集上显著提升执行正确率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2306.02907v1",
    "published": "2023-06-05",
    "update_time": "2023-06-05",
    "download_time": "2025-12-04 23:21:45"
  },
  {
    "id": "2306.09896",
    "title": "Is Self-Repair a Silver Bullet for Code Generation?",
    "abstract": "Large language models have shown remarkable aptitude in code generation, but still struggle to perform complex tasks. Self-repair -- in which the model debugs and repairs its own code -- has recently become a popular way to boost performance in these settings. However, despite its increasing popularity, existing studies of self-repair have been limited in scope; in many settings, its efficacy thus remains poorly understood. In this paper, we analyze Code Llama, GPT-3.5 and GPT-4's ability to perform self-repair on problems taken from HumanEval and APPS. We find that when the cost of carrying out repair is taken into account, performance gains are often modest, vary a lot between subsets of the data, and are sometimes not present at all. We hypothesize that this is because self-repair is bottlenecked by the model's ability to provide feedback on its own code; using a stronger model to artificially boost the quality of the feedback, we observe substantially larger performance gains. Similarly, a small-scale study in which we provide GPT-4 with feedback from human participants suggests that even for the strongest models, self-repair still lags far behind what can be achieved with human-level debugging.",
    "arxiv_url": "https://arxiv.org/abs/2306.09896",
    "authors": [
      "Theo X. Olausson",
      "Jeevana Priya Inala",
      "Chenglong Wang",
      "Jianfeng Gao",
      "Armando Solar-Lezama"
    ],
    "first_author": "Theo X. Olausson",
    "category": [
      "Empirical"
    ],
    "field": "Coding Assistant",
    "task": "Code Editing",
    "summary": "本文在HumanEval和APPS数据集上系统评估了Code Llama、GPT-3.5和GPT-4的自我修复能力，发现考虑修复计算成本后收益常常有限且随数据子集波动较大，而提升反馈质量（由更强模型或人工提供）能显著提高修复效果。",
    "quality": "High",
    "conference": "ICLR 2024",
    "pdf_url": "https://arxiv.org/pdf/2306.09896v5",
    "published": "2023-06-16",
    "update_time": "2024-02-02",
    "download_time": "2025-12-04 23:22:09"
  },
  {
    "id": "2306.03091",
    "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems",
    "abstract": "Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems. RepoBench is publicly available at https://github.com/Leolty/repobench.",
    "arxiv_url": "https://arxiv.org/abs/2306.03091",
    "authors": [
      "Tianyang Liu",
      "Canwen Xu",
      "Julian McAuley"
    ],
    "first_author": "Tianyang Liu",
    "category": [
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "summary": "本文提出RepoBench，一个面向仓库级别的代码自动补全基准（含检索、下一行补全与端到端流水线三项任务），并提供覆盖Python/Java的数据集与评测分析。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2306.03091v2",
    "published": "2023-06-05",
    "update_time": "2023-10-04",
    "download_time": "2025-12-04 23:28:06"
  },
  {
    "id": "2306.14893",
    "title": "LongCoder: A Long-Range Pre-trained Language Model for Code Completion",
    "abstract": "In this paper, we introduce a new task for code completion that focuses on handling long code input and propose a sparse Transformer model, called LongCoder, to address this task. LongCoder employs a sliding window mechanism for self-attention and introduces two types of globally accessible tokens - bridge tokens and memory tokens - to improve performance and efficiency. Bridge tokens are inserted throughout the input sequence to aggregate local information and facilitate global interaction, while memory tokens are included to highlight important statements that may be invoked later and need to be memorized, such as package imports and definitions of classes, functions, or structures. We conduct experiments on a newly constructed dataset that contains longer code context and the publicly available CodeXGLUE benchmark. Experimental results demonstrate that LongCoder achieves superior performance on code completion tasks compared to previous models while maintaining comparable efficiency in terms of computational resources during inference. All the codes and data are available at https://github.com/microsoft/CodeBERT.",
    "arxiv_url": "https://arxiv.org/abs/2306.14893",
    "authors": [
      "Daya Guo",
      "Canwen Xu",
      "Nan Duan",
      "Jian Yin",
      "Julian McAuley"
    ],
    "first_author": "Daya Guo",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "summary": "本文提出LongCoder——一种用于长代码上下文的稀疏预训练Transformer，采用滑动窗口注意力并引入桥接（bridge）与记忆（memory）令牌以实现全局信息访问，同时构建并发布长代码补全数据集LCC，实验证明在长/常规代码补全上性能优于既有模型且推理效率相当。",
    "quality": "High",
    "conference": "ICML 2023",
    "pdf_url": "https://arxiv.org/pdf/2306.14893v1",
    "published": "2023-06-26",
    "update_time": "2023-06-26",
    "download_time": "2025-12-04 23:28:30"
  },
  {
    "id": "2306.10763",
    "title": "Guiding Language Models of Code with Global Context using Monitors",
    "abstract": "Language models of code (LMs) work well when the surrounding code provides sufficient context. This is not true when it becomes necessary to use types, functionality or APIs defined elsewhere in the repository or a linked library, especially those not seen during training. LMs suffer from limited awareness of such global context and end up hallucinating.   Integrated development environments (IDEs) assist developers in understanding repository context using static analysis. We extend this assistance, enjoyed by developers, to LMs. We propose monitor-guided decoding (MGD) where a monitor uses static analysis to guide the decoding. We construct a repository-level dataset PragmaticCode for method-completion in Java and evaluate MGD on it. On models of varying parameter scale, by monitoring for type-consistent object dereferences, MGD consistently improves compilation rates and agreement with ground truth. Further, LMs with fewer parameters, when augmented with MGD, can outperform larger LMs. With MGD, SantaCoder-1.1B achieves better compilation rate and next-identifier match than the much larger text-davinci-003 model.   We also conduct a generalizability study to evaluate the ability of MGD to generalize to multiple programming languages (Java, C# and Rust), coding scenarios (e.g., correct number of arguments to method calls), and to enforce richer semantic constraints (e.g., stateful API protocols). Our data and implementation are available at https://github.com/microsoft/monitors4codegen .",
    "arxiv_url": "https://arxiv.org/abs/2306.10763",
    "authors": [
      "Lakshya A Agrawal",
      "Aditya Kanade",
      "Navin Goyal",
      "Shuvendu K. Lahiri",
      "Sriram K. Rajamani"
    ],
    "first_author": "Lakshya A Agrawal",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "Monitor-guided decoding",
      "Static-analysis-driven token masking",
      "Language Server Protocol integration",
      "Type-aware identifier resolution",
      "Decoding-time semantic constraints",
      "Repository-level context",
      "Stateful API protocol enforcement",
      "Compilation-focused evaluation"
    ],
    "summary": "本文提出监视器引导解码（MGD），在解码阶段调用静态分析（基于LSP）对模型生成的令牌进行约束，从而利用仓库级全局上下文提升类型一致性、编译率和与真实代码的匹配，并公开了相应的仓库级数据集与工具实现。",
    "quality": "High",
    "conference": "NeurIPS 2023",
    "pdf_url": "https://arxiv.org/pdf/2306.10763v2",
    "published": "2023-06-19",
    "update_time": "2023-11-03",
    "download_time": "2025-12-11 16:30:53"
  },
  {
    "id": "2306.10998",
    "title": "RepoFusion: Training Code Models to Understand Your Repository",
    "abstract": "Despite the huge success of Large Language Models (LLMs) in coding assistants like GitHub Copilot, these models struggle to understand the context present in the repository (e.g., imports, parent classes, files with similar names, etc.), thereby producing inaccurate code completions. This effect is more pronounced when using these assistants for repositories that the model has not seen during training, such as proprietary software or work-in-progress code projects. Recent work has shown the promise of using context from the repository during inference. In this work, we extend this idea and propose RepoFusion, a framework to train models to incorporate relevant repository context. Experiments on single-line code completion show that our models trained with repository context significantly outperform much larger code models as CodeGen-16B-multi ($\\sim73\\times$ larger) and closely match the performance of the $\\sim 70\\times$ larger StarCoderBase model that was trained with the Fill-in-the-Middle objective. We find these results to be a novel and compelling demonstration of the gains that training with repository context can bring. We carry out extensive ablation studies to investigate the impact of design choices such as context type, number of contexts, context length, and initialization within our framework. Lastly, we release Stack-Repo, a dataset of 200 Java repositories with permissive licenses and near-deduplicated files that are augmented with three types of repository contexts. Additionally, we are making available the code and trained checkpoints for our work. Our released resources can be found at \\url{https://huggingface.co/RepoFusion}.",
    "arxiv_url": "https://arxiv.org/abs/2306.10998",
    "authors": [
      "Disha Shrivastava",
      "Denis Kocetkov",
      "Harm de Vries",
      "Dzmitry Bahdanau",
      "Torsten Scholak"
    ],
    "first_author": "Disha Shrivastava",
    "category": [
      "Technical",
      "Benchmark"
    ],
    "field": "Coding Assistant",
    "task": "Code Completion",
    "tags": [
      "Repository-level context integration",
      "Fusion-in-Decoder adaptation",
      "Repo-level prompt proposals",
      "Context retrieval (BM25 and embedding-based)",
      "Surrounding-context augmentation",
      "Single-line code completion",
      "Ablation studies on context design",
      "Java repository corpus release"
    ],
    "summary": "本文提出RepoFusion，通过将多个仓库级上下文用Fusion-in-Decoder训练融入代码模型以提升单行代码补全性能，并发布了带上下文增强的Java仓库语料与训练检查点，且小模型在多项评测中显著优于更大的基线模型。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2306.10998v1",
    "published": "2023-06-19",
    "update_time": "2023-06-19",
    "download_time": "2025-12-11 16:31:32"
  },
  {
    "id": "2306.17193",
    "title": "Uncovering the Limits of Machine Learning for Automatic Vulnerability Detection",
    "abstract": "Recent results of machine learning for automatic vulnerability detection (ML4VD) have been very promising. Given only the source code of a function $f$, ML4VD techniques can decide if $f$ contains a security flaw with up to 70% accuracy. However, as evident in our own experiments, the same top-performing models are unable to distinguish between functions that contain a vulnerability and functions where the vulnerability is patched. So, how can we explain this contradiction and how can we improve the way we evaluate ML4VD techniques to get a better picture of their actual capabilities?   In this paper, we identify overfitting to unrelated features and out-of-distribution generalization as two problems, which are not captured by the traditional approach of evaluating ML4VD techniques. As a remedy, we propose a novel benchmarking methodology to help researchers better evaluate the true capabilities and limits of ML4VD techniques. Specifically, we propose (i) to augment the training and validation dataset according to our cross-validation algorithm, where a semantic preserving transformation is applied during the augmentation of either the training set or the testing set, and (ii) to augment the testing set with code snippets where the vulnerabilities are patched.   Using six ML4VD techniques and two datasets, we find (a) that state-of-the-art models severely overfit to unrelated features for predicting the vulnerabilities in the testing data, (b) that the performance gained by data augmentation does not generalize beyond the specific augmentations applied during training, and (c) that state-of-the-art ML4VD techniques are unable to distinguish vulnerable functions from their patches.",
    "arxiv_url": "https://arxiv.org/abs/2306.17193",
    "authors": [
      "Niklas Risse",
      "Marcel Böhme"
    ],
    "first_author": "Niklas Risse",
    "category": [
      "Empirical",
      "Benchmark",
      "Technical"
    ],
    "field": "Quality Management",
    "task": "Vulnerability Detection",
    "tags": [
      "Semantic-preserving code transformations",
      "Overfitting to superficial features",
      "Cross-transformation generalization",
      "Patch-pair evaluation",
      "Data augmentation robustness",
      "Vuln–patch paired dataset",
      "Evaluation algorithms for ML4VD",
      "Out-of-distribution generalization"
    ],
    "summary": "本文提出两种用于评估自动化漏洞检测（ML4VD）的算法并发布包含漏洞与对应补丁对的新数据集，实验证明现有基于令牌的模型严重依赖与漏洞无关的特征且无法区分漏洞与其补丁。",
    "quality": "High",
    "conference": "Proceedings of the 33rd USENIX Security Symposium (USENIX Security 2024) 2024",
    "pdf_url": "https://arxiv.org/pdf/2306.17193v2",
    "published": "2023-06-28",
    "update_time": "2024-06-06",
    "download_time": "2025-12-11 17:14:01"
  }
]