[
  {
    "id": "2306.08568",
    "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct",
    "abstract": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM",
    "arxiv_url": "https://arxiv.org/abs/2306.08568",
    "authors": [
      "Ziyang Luo",
      "Can Xu",
      "Pu Zhao",
      "Qingfeng Sun",
      "Xiubo Geng",
      "Wenxiang Hu",
      "Chongyang Tao",
      "Jing Ma",
      "Qingwei Lin",
      "Daxin Jiang"
    ],
    "first_author": "Ziyang Luo",
    "category": [
      "Technical / Method"
    ],
    "field": "Coding Assistant",
    "tag": "Code Instruction-Tuning",
    "summary": "本文提出了面向代码领域的Code Evol-Instruct指令微调方法，并基于此训练出WizardCoder模型，在多项代码生成基准（HumanEval、MBPP等）上显著优于现有开源模型，部分规模也超越了若干闭源模型。",
    "quality": "High",
    "conference": "ICLR 2024",
    "pdf_url": "https://arxiv.org/pdf/2306.08568v2",
    "published": "2023-06-14",
    "update_time": "2025-05-27",
    "download_time": "2025-12-04 23:10:59"
  },
  {
    "id": "2306.11644",
    "title": "Textbooks Are All You Need",
    "abstract": "We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality\" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.",
    "arxiv_url": "https://arxiv.org/abs/2306.11644",
    "authors": [
      "Suriya Gunasekar",
      "Yi Zhang",
      "Jyoti Aneja",
      "Caio César Teodoro Mendes",
      "Allie Del Giorno",
      "Sivakanth Gopi",
      "Mojan Javaheripi",
      "Piero Kauffmann",
      "Gustavo de Rosa",
      "Olli Saarikivi",
      "Adil Salim",
      "Shital Shah",
      "Harkirat Singh Behl",
      "Xin Wang",
      "Sébastien Bubeck",
      "Ronen Eldan",
      "Adam Tauman Kalai",
      "Yin Tat Lee",
      "Yuanzhi Li"
    ],
    "first_author": "Suriya Gunasekar",
    "category": [
      "Technical / Method",
      "Benchmark / Dataset"
    ],
    "field": "Coding Assistant",
    "tag": "Code Pre-Training",
    "summary": "本文提出 phi-1：一个仅1.3B参数、以精选“教科书质量”文本与合成编程练习训练的小型代码生成模型，能在HumanEval和MBPP上以极小规模取得接近或超越更大模型的高性能。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2306.11644v2",
    "published": "2023-06-20",
    "update_time": "2023-10-02",
    "download_time": "2025-12-04 23:13:17"
  },
  {
    "id": "2306.02907",
    "title": "SelfEvolve: A Code Evolution Framework via Large Language Models",
    "abstract": "Large language models (LLMs) have already revolutionized code generation, after being pretrained on publicly available code data. However, while various methods have been proposed to augment LLMs with retrieved knowledge and enhance the quality of code generation, the performance of these retrieval-based methods is limited by the strength of the retrievers used. In addition, while LLMs show great emergent ability, they still struggle to produce the correct code in one turn. To address these challenges, we propose a novel two-step pipeline, called \\autoknow, that leverages LLMs as both knowledge providers and self-reflective programmers. Unlike retrieval-based methods, \\autoknow~obtains the knowledge from input prompts and generates intermediate code based on the generated knowledge. After that, \\autoknow~asks LLM to act as an expert programmer to perform debugging for the generated code. This is achieved by receiving the error message from the interpreter, without requiring special test cases for correctness verification. We evaluate \\autoknow~on three code generation datasets, including DS-1000 for data science code, HumanEval for software engineering code, and TransCoder for C++-to-Python translation. Our empirical experiments show that \\autoknow~outperforms strong baselines by a significant margin on all datasets. We also conduct exhaustive analytical experiments to validate the effectiveness of the two stages of \\autoknow, and find that both are superior to other prompting-based methods. Further scalability analysis demonstrates that \\autoknow~can be adapted to other more advanced models, such as GPT-4, and bring consistent efficacy improvement.",
    "arxiv_url": "https://arxiv.org/abs/2306.02907",
    "authors": [
      "Shuyang Jiang",
      "Yuhao Wang",
      "Yu Wang"
    ],
    "first_author": "Shuyang Jiang",
    "category": [
      "Technical / Method"
    ],
    "field": "Coding Assistant",
    "tag": "Code Prompting",
    "summary": "本文提出SELFEVOLVE，一种两阶段的LLM驱动代码演化框架：先让模型自生成与题目相关的知识再生成代码，随后基于解释器错误信息对代码进行自我调试与迭代，从而在多套数据集上显著提升执行正确率。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2306.02907v1",
    "published": "2023-06-05",
    "update_time": "2023-06-05",
    "download_time": "2025-12-04 23:21:45"
  },
  {
    "id": "2306.09896",
    "title": "Is Self-Repair a Silver Bullet for Code Generation?",
    "abstract": "Large language models have shown remarkable aptitude in code generation, but still struggle to perform complex tasks. Self-repair -- in which the model debugs and repairs its own code -- has recently become a popular way to boost performance in these settings. However, despite its increasing popularity, existing studies of self-repair have been limited in scope; in many settings, its efficacy thus remains poorly understood. In this paper, we analyze Code Llama, GPT-3.5 and GPT-4's ability to perform self-repair on problems taken from HumanEval and APPS. We find that when the cost of carrying out repair is taken into account, performance gains are often modest, vary a lot between subsets of the data, and are sometimes not present at all. We hypothesize that this is because self-repair is bottlenecked by the model's ability to provide feedback on its own code; using a stronger model to artificially boost the quality of the feedback, we observe substantially larger performance gains. Similarly, a small-scale study in which we provide GPT-4 with feedback from human participants suggests that even for the strongest models, self-repair still lags far behind what can be achieved with human-level debugging.",
    "arxiv_url": "https://arxiv.org/abs/2306.09896",
    "authors": [
      "Theo X. Olausson",
      "Jeevana Priya Inala",
      "Chenglong Wang",
      "Jianfeng Gao",
      "Armando Solar-Lezama"
    ],
    "first_author": "Theo X. Olausson",
    "category": [
      "Experience / Empirical"
    ],
    "field": "Coding Assistant",
    "tag": "Code Editing",
    "summary": "本文在HumanEval和APPS数据集上系统评估了Code Llama、GPT-3.5和GPT-4的自我修复能力，发现考虑修复计算成本后收益常常有限且随数据子集波动较大，而提升反馈质量（由更强模型或人工提供）能显著提高修复效果。",
    "quality": "High",
    "conference": "ICLR 2024",
    "pdf_url": "https://arxiv.org/pdf/2306.09896v5",
    "published": "2023-06-16",
    "update_time": "2024-02-02",
    "download_time": "2025-12-04 23:22:09"
  },
  {
    "id": "2306.03091",
    "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems",
    "abstract": "Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems. RepoBench is publicly available at https://github.com/Leolty/repobench.",
    "arxiv_url": "https://arxiv.org/abs/2306.03091",
    "authors": [
      "Tianyang Liu",
      "Canwen Xu",
      "Julian McAuley"
    ],
    "first_author": "Tianyang Liu",
    "category": [
      "Benchmark / Dataset"
    ],
    "field": "Coding Assistant",
    "tag": "Code Completion",
    "summary": "本文提出RepoBench，一个面向仓库级别的代码自动补全基准（含检索、下一行补全与端到端流水线三项任务），并提供覆盖Python/Java的数据集与评测分析。",
    "quality": "High",
    "conference": null,
    "pdf_url": "https://arxiv.org/pdf/2306.03091v2",
    "published": "2023-06-05",
    "update_time": "2023-10-04",
    "download_time": "2025-12-04 23:28:06"
  },
  {
    "id": "2306.14893",
    "title": "LongCoder: A Long-Range Pre-trained Language Model for Code Completion",
    "abstract": "In this paper, we introduce a new task for code completion that focuses on handling long code input and propose a sparse Transformer model, called LongCoder, to address this task. LongCoder employs a sliding window mechanism for self-attention and introduces two types of globally accessible tokens - bridge tokens and memory tokens - to improve performance and efficiency. Bridge tokens are inserted throughout the input sequence to aggregate local information and facilitate global interaction, while memory tokens are included to highlight important statements that may be invoked later and need to be memorized, such as package imports and definitions of classes, functions, or structures. We conduct experiments on a newly constructed dataset that contains longer code context and the publicly available CodeXGLUE benchmark. Experimental results demonstrate that LongCoder achieves superior performance on code completion tasks compared to previous models while maintaining comparable efficiency in terms of computational resources during inference. All the codes and data are available at https://github.com/microsoft/CodeBERT.",
    "arxiv_url": "https://arxiv.org/abs/2306.14893",
    "authors": [
      "Daya Guo",
      "Canwen Xu",
      "Nan Duan",
      "Jian Yin",
      "Julian McAuley"
    ],
    "first_author": "Daya Guo",
    "category": [
      "Technical / Method",
      "Benchmark / Dataset"
    ],
    "field": "Coding Assistant",
    "tag": "Code Completion",
    "summary": "本文提出LongCoder——一种用于长代码上下文的稀疏预训练Transformer，采用滑动窗口注意力并引入桥接（bridge）与记忆（memory）令牌以实现全局信息访问，同时构建并发布长代码补全数据集LCC，实验证明在长/常规代码补全上性能优于既有模型且推理效率相当。",
    "quality": "High",
    "conference": "ICML 2023",
    "pdf_url": "https://arxiv.org/pdf/2306.14893v1",
    "published": "2023-06-26",
    "update_time": "2023-06-26",
    "download_time": "2025-12-04 23:28:30"
  }
]