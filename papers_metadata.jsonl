{"id": "2411.04905", "title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models", "abstract": "Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems. While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scientific investigation, particularly those with reproducible data processing pipelines and transparent training protocols, remain limited. The scarcity is due to various challenges, including resource constraints, ethical considerations, and the competitive advantages of keeping models advanced. To address the gap, we introduce OpenCoder, a top-tier code LLM that not only achieves performance comparable to leading models but also serves as an \"open cookbook\" for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Through this comprehensive release, we identify the key ingredients for building a top-tier code LLM: (1) code optimized heuristic rules for data cleaning and methods for data deduplication, (2) recall of text corpus related to code and (3) high-quality synthetic data in both annealing and supervised fine-tuning stages. By offering this level of openness, we aim to broaden access to all aspects of a top-tier code LLM, with OpenCoder serving as both a powerful model and an open foundation to accelerate research, and enable reproducible advancements in code AI.", "arxiv_url": "https://arxiv.org/abs/2411.04905", "authors": ["Siming Huang", "Tianhao Cheng", "J. K. Liu", "Jiaran Hao", "Liuyihan Song", "Yang Xu", "J. Yang", "Jiaheng Liu", "Chenchen Zhang", "Linzheng Chai", "Ruifeng Yuan", "Zhaoxiang Zhang", "Jie Fu", "Qian Liu", "Ge Zhang", "Zili Wang", "Yuan Qi", "Yinghui Xu", "Wei Chu"], "first_author": "Siming Huang", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "OpenCoder提出并开源了一个顶级代码LLM，发布模型权重、可复现的数据与训练流水线，并通过数据清洗、去重与高质量合成数据等策略揭示构建高性能代码模型的关键要素。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.04905v3", "published": "2024-11-07", "update_time": "2025-03-20", "download_time": "2025-12-04 21:51:44"}
{"id": "2505.08503", "title": "ICVul: A Well-labeled C/C++ Vulnerability Dataset with Comprehensive Metadata and VCCs", "abstract": "Machine learning-based software vulnerability detection requires high-quality datasets, which is essential for training effective models. To address challenges related to data label quality, diversity, and comprehensiveness, we constructed ICVul, a dataset emphasizing data quality and enriched with comprehensive metadata, including Vulnerability-Contributing Commits (VCCs). We began by filtering Common Vulnerabilities and Exposures from the NVD, retaining only those linked to GitHub fix commits. Then we extracted functions and files along with relevant metadata from these commits and used the SZZ algorithm to trace VCCs. To further enhance label reliability, we developed the ESC (Eliminate Suspicious Commit) technique, ensuring credible data labels. The dataset is stored in a relational-like database for improved usability and data integrity. Both ICVul and its construction framework are publicly accessible on GitHub, supporting research in related field.", "arxiv_url": "https://arxiv.org/abs/2505.08503", "authors": ["Chaomeng Lu", "Tianyu Li", "Toon Dehaene", "Bert Lagaisse"], "first_author": "Chaomeng Lu", "category": ["Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "summary": "ICVul 提供了一个面向 C/C++ 的高质量漏洞数据集，包含详细的元数据、Vulnerability-Contributing-Commits (VCCs) 追溯以及用于去除噪声提交的 ESC 筛选技术，并公开了可复现的构建框架。", "quality": "High", "conference": "MSR 2025", "pdf_url": "https://arxiv.org/pdf/2505.08503v1", "published": "2025-05-13", "update_time": "2025-05-13", "download_time": "2025-12-04 23:08:14"}
{"id": "2409.12186", "title": "Qwen2.5-Coder Technical Report", "abstract": "In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes six models: Qwen2.5-Coder-(0.5B/1.5B/3B/7B/14B/32B). As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general and math skills. These models have been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will advance research in code intelligence and, with its permissive licensing, support wider adoption by developers in real-world applications.", "arxiv_url": "https://arxiv.org/abs/2409.12186", "authors": ["Binyuan Hui", "Jian Yang", "Zeyu Cui", "Jiaxi Yang", "Dayiheng Liu", "Lei Zhang", "Tianyu Liu", "Jiajun Zhang", "Bowen Yu", "Keming Lu", "Kai Dang", "Yang Fan", "Yichang Zhang", "An Yang", "Rui Men", "Fei Huang", "Bo Zheng", "Yibo Miao", "Shanghaoran Quan", "Yunlong Feng", "Xingzhang Ren", "Xuancheng Ren", "Jingren Zhou", "Junyang Lin"], "first_author": "Binyuan Hui", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "本文介绍了Qwen2.5-Coder系列（0.5B–32B），在继承Qwen2.5架构的基础上用超过5.5万亿token的代码与相关文本进行大规模预训练并通过精心设计的指令微调与数据清洗策略显著提升代码生成、补全与推理任务的性能并开源发布。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2409.12186v3", "published": "2024-09-18", "update_time": "2024-11-12", "download_time": "2025-12-04 23:08:44"}
{"id": "2308.12950", "title": "Code Llama: Open Foundation Models for Code", "abstract": "We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.", "arxiv_url": "https://arxiv.org/abs/2308.12950", "authors": ["Baptiste Rozière", "Jonas Gehring", "Fabian Gloeckle", "Sten Sootla", "Itai Gat", "Xiaoqing Ellen Tan", "Yossi Adi", "Jingyu Liu", "Romain Sauvestre", "Tal Remez", "Jérémy Rapin", "Artyom Kozhevnikov", "Ivan Evtimov", "Joanna Bitton", "Manish Bhatt", "Cristian Canton Ferrer", "Aaron Grattafiori", "Wenhan Xiong", "Alexandre Défossez", "Jade Copet", "Faisal Azhar", "Hugo Touvron", "Louis Martin", "Nicolas Usunier", "Thomas Scialom", "Gabriel Synnaeve"], "first_author": "Baptiste Rozière", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "本文发布并开源了Code Llama——一系列基于Llama 2 的代码专用大模型（多种规模与变体），支持中间填充、超长上下文与指令微调，并在多项代码基准上达到了开源模型的最先进表现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2308.12950v3", "published": "2023-08-24", "update_time": "2024-01-31", "download_time": "2025-12-04 23:09:20"}
{"id": "2305.06161", "title": "StarCoder: may the source be with you!", "abstract": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.", "arxiv_url": "https://arxiv.org/abs/2305.06161", "authors": ["Raymond Li", "Loubna Ben Allal", "Yangtian Zi", "Niklas Muennighoff", "Denis Kocetkov", "Chenghao Mou", "Marc Marone", "Christopher Akiki", "Jia Li", "Jenny Chim", "Qian Liu", "Evgenii Zheltonozhskii", "Terry Yue Zhuo", "Thomas Wang", "Olivier Dehaene", "Mishig Davaadorj", "Joel Lamy-Poirier", "João Monteiro", "Oleh Shliazhko", "Nicolas Gontier", "Nicholas Meade", "Armel Zebaze", "Ming-Ho Yee", "Logesh Kumar Umapathi", "Jian Zhu", "Benjamin Lipkin", "Muhtasham Oblokulov", "Zhiruo Wang", "Rudra Murthy", "Jason Stillerman", "Siva Sankalp Patel", "Dmitry Abulkhanov", "Marco Zocca", "Manan Dey", "Zhihan Zhang", "Nour Fahmy", "Urvashi Bhattacharyya", "Wenhao Yu", "Swayam Singh", "Sasha Luccioni", "Paulo Villegas", "Maxim Kunakov", "Fedor Zhdanov", "Manuel Romero", "Tony Lee", "Nadav Timor", "Jennifer Ding", "Claire Schlesinger", "Hailey Schoelkopf", "Jan Ebert", "Tri Dao", "Mayank Mishra", "Alex Gu", "Jennifer Robinson", "Carolyn Jane Anderson", "Brendan Dolan-Gavitt", "Danish Contractor", "Siva Reddy", "Daniel Fried", "Dzmitry Bahdanau", "Yacine Jernite", "Carlos Muñoz Ferrandis", "Sean Hughes", "Thomas Wolf", "Arjun Guha", "Leandro von Werra", "Harm de Vries"], "first_author": "Raymond Li", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "本文发布并评估了开源代码语言模型StarCoder与StarCoderBase（15.5B参数、8K上下文、填空能力与多查询注意力），在大规模许可代码语料上预训练并对Python进行微调，同时提供PII脱敏与归属追踪工具，且在多项基准上优于其他开源代码模型。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2305.06161v2", "published": "2023-05-09", "update_time": "2023-12-13", "download_time": "2025-12-04 23:09:51"}
{"id": "2307.14936", "title": "PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback", "abstract": "Large Language Models for Code (Code LLM) are flourishing. New and powerful models are released on a weekly basis, demonstrating remarkable performance on the code generation task. Various approaches have been proposed to boost the code generation performance of pre-trained Code LLMs, such as supervised fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we propose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework, which can effectively and efficiently boost pre-trained large language models for code generation. Under this framework, we present PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through an extensive evaluation on CoderEval and LeetCode benchmarks, we show that PanGu-Coder2 consistently outperforms all previous Code LLMs.", "arxiv_url": "https://arxiv.org/abs/2307.14936", "authors": ["Bo Shen", "Jiaxin Zhang", "Taihong Chen", "Daoguang Zan", "Bing Geng", "An Fu", "Muhan Zeng", "Ailun Yu", "Jichuan Ji", "Jingyang Zhao", "Yuenan Guo", "Qianxiang Wang"], "first_author": "Bo Shen", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Instruction-Tuning", "summary": "本文提出了RRTF（Rank Responses to align Test&Teacher Feedback）框架，并基于该框架对StarCoder进行排序反馈式指令微调得到PanGu-Coder2，从而在HumanEval、CoderEval和LeetCode等基准上显著提升代码生成性能并达成新的SOTA。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2307.14936v1", "published": "2023-07-27", "update_time": "2023-07-27", "download_time": "2025-12-04 23:10:29"}
{"id": "2306.08568", "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct", "abstract": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM", "arxiv_url": "https://arxiv.org/abs/2306.08568", "authors": ["Ziyang Luo", "Can Xu", "Pu Zhao", "Qingfeng Sun", "Xiubo Geng", "Wenxiang Hu", "Chongyang Tao", "Jing Ma", "Qingwei Lin", "Daxin Jiang"], "first_author": "Ziyang Luo", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Instruction-Tuning", "summary": "本文提出了面向代码领域的Code Evol-Instruct指令微调方法，并基于此训练出WizardCoder模型，在多项代码生成基准（HumanEval、MBPP等）上显著优于现有开源模型，部分规模也超越了若干闭源模型。", "quality": "High", "conference": "ICLR 2024", "pdf_url": "https://arxiv.org/pdf/2306.08568v2", "published": "2023-06-14", "update_time": "2025-05-27", "download_time": "2025-12-04 23:10:59"}
{"id": "2107.03374", "title": "Evaluating Large Language Models Trained on Code", "abstract": "We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.", "arxiv_url": "https://arxiv.org/abs/2107.03374", "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde de Oliveira Pinto", "Jared Kaplan", "Harri Edwards", "Yuri Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "Scott Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "Felipe Petroski Such", "Dave Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William Hebgen Guss", "Alex Nichol", "Alex Paino", "Nikolas Tezak", "Jie Tang", "Igor Babuschkin", "Suchir Balaji", "Shantanu Jain", "William Saunders", "Christopher Hesse", "Andrew N. Carr", "Jan Leike", "Josh Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "Matthew Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "Peter Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "first_author": "Mark Chen", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文介绍并评估了在公开代码上微调的GPT模型Codex，提出并发布了HumanEval数据集与pass@k评测以衡量从docstring合成Python函数的功能正确性，展示了模型性能、局限性及潜在影响。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2107.03374v2", "published": "2021-07-07", "update_time": "2021-07-14", "download_time": "2025-12-04 23:11:30"}
{"id": "2406.11931", "title": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence", "abstract": "We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K. In standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.", "arxiv_url": "https://arxiv.org/abs/2406.11931", "authors": ["DeepSeek-AI", "Qihao Zhu", "Daya Guo", "Zhihong Shao", "Dejian Yang", "Peiyi Wang", "Runxin Xu", "Y. Wu", "Yukun Li", "Huazuo Gao", "Shirong Ma", "Wangding Zeng", "Xiao Bi", "Zihui Gu", "Hanwei Xu", "Damai Dai", "Kai Dong", "Liyue Zhang", "Yishi Piao", "Zhibin Gou", "Zhenda Xie", "Zhewen Hao", "Bingxuan Wang", "Junxiao Song", "Deli Chen", "Xin Xie", "Kang Guan", "Yuxiang You", "Aixin Liu", "Qiushi Du", "Wenjun Gao", "Xuan Lu", "Qinyu Chen", "Yaohui Wang", "Chengqi Deng", "Jiashi Li", "Chenggang Zhao", "Chong Ruan", "Fuli Luo", "Wenfeng Liang"], "first_author": "DeepSeek-AI", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "本文提出了开源的DeepSeek-Coder-V2——基于Mixture-of-Experts的大规模代码模型，通过在DeepSeek-V2上继续使用额外6万亿tokens的预训练并结合指令微调与GRPO对齐，使得236B参数模型在代码与数学推理基准上达到并在若干任务上超越GPT-4-Turbo等闭源模型，支持338种编程语言和128K上下文长度。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2406.11931v1", "published": "2024-06-17", "update_time": "2024-06-17", "download_time": "2025-12-04 23:11:56"}
{"id": "2402.19173", "title": "StarCoder 2 and The Stack v2: The Next Generation", "abstract": "The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.", "arxiv_url": "https://arxiv.org/abs/2402.19173", "authors": ["Anton Lozhkov", "Raymond Li", "Loubna Ben Allal", "Federico Cassano", "Joel Lamy-Poirier", "Nouamane Tazi", "Ao Tang", "Dmytro Pykhtar", "Jiawei Liu", "Yuxiang Wei", "Tianyang Liu", "Max Tian", "Denis Kocetkov", "Arthur Zucker", "Younes Belkada", "Zijian Wang", "Qian Liu", "Dmitry Abulkhanov", "Indraneil Paul", "Zhuang Li", "Wen-Ding Li", "Megan Risdal", "Jia Li", "Jian Zhu", "Terry Yue Zhuo", "Evgenii Zheltonozhskii", "Nii Osae Osae Dade", "Wenhao Yu", "Lucas Krauß", "Naman Jain", "Yixuan Su", "Xuanli He", "Manan Dey", "Edoardo Abati", "Yekun Chai", "Niklas Muennighoff", "Xiangru Tang", "Muhtasham Oblokulov", "Christopher Akiki", "Marc Marone", "Chenghao Mou", "Mayank Mishra", "Alex Gu", "Binyuan Hui", "Tri Dao", "Armel Zebaze", "Olivier Dehaene", "Nicolas Patry", "Canwen Xu", "Julian McAuley", "Han Hu", "Torsten Scholak", "Sebastien Paquet", "Jennifer Robinson", "Carolyn Jane Anderson", "Nicolas Chapados", "Mostofa Patwary", "Nima Tajbakhsh", "Yacine Jernite", "Carlos Muñoz Ferrandis", "Lingming Zhang", "Sean Hughes", "Thomas Wolf", "Arjun Guha", "Leandro von Werra", "Harm de Vries"], "first_author": "Anton Lozhkov", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "本文介绍了基于 Software Heritage 构建的 The Stack v2 数据集并在其上训练并公开了 StarCoder2（3B/7B/15B）系列代码大模型，同时发布训练数据标识与权重并在多项代码基准上进行全面评估以展示其性能优势。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2402.19173v1", "published": "2024-02-29", "update_time": "2024-02-29", "download_time": "2025-12-04 23:12:22"}
{"id": "2401.14196", "title": "DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence", "abstract": "The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.", "arxiv_url": "https://arxiv.org/abs/2401.14196", "authors": ["Daya Guo", "Qihao Zhu", "Dejian Yang", "Zhenda Xie", "Kai Dong", "Wentao Zhang", "Guanting Chen", "Xiao Bi", "Y. Wu", "Y. K. Li", "Fuli Luo", "Yingfei Xiong", "Wenfeng Liang"], "first_author": "Daya Guo", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "本文提出并开源DeepSeek-Coder系列（1.3B–33B），在包含仓库级语料的2万亿tokens上从零预训练并采用Fill-In-Middle训练与16K上下文，显著提升开源代码模型性能并在多项基准上超越现有开源及部分闭源模型。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2401.14196v2", "published": "2024-01-25", "update_time": "2024-01-26", "download_time": "2025-12-04 23:12:47"}
{"id": "2306.11644", "title": "Textbooks Are All You Need", "abstract": "We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality\" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.", "arxiv_url": "https://arxiv.org/abs/2306.11644", "authors": ["Suriya Gunasekar", "Yi Zhang", "Jyoti Aneja", "Caio César Teodoro Mendes", "Allie Del Giorno", "Sivakanth Gopi", "Mojan Javaheripi", "Piero Kauffmann", "Gustavo de Rosa", "Olli Saarikivi", "Adil Salim", "Shital Shah", "Harkirat Singh Behl", "Xin Wang", "Sébastien Bubeck", "Ronen Eldan", "Adam Tauman Kalai", "Yin Tat Lee", "Yuanzhi Li"], "first_author": "Suriya Gunasekar", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "本文提出 phi-1：一个仅1.3B参数、以精选“教科书质量”文本与合成编程练习训练的小型代码生成模型，能在HumanEval和MBPP上以极小规模取得接近或超越更大模型的高性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2306.11644v2", "published": "2023-06-20", "update_time": "2023-10-02", "download_time": "2025-12-04 23:13:17"}
{"id": "2305.07922", "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation", "abstract": "Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.", "arxiv_url": "https://arxiv.org/abs/2305.07922", "authors": ["Yue Wang", "Hung Le", "Akhilesh Deepak Gotmare", "Nghi D. Q. Bui", "Junnan Li", "Steven C. H. Hoi"], "first_author": "Yue Wang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "本文提出CodeT5+，一种可灵活组合组件的encoder-decoder代码大模型族，通过混合预训练目标（span denoising、对比学习、文本-代码匹配与因果语言建模）、冻结大解码器并仅训练浅编码器与交叉注意力以高效扩展，并结合instruction-tuning，在多项代码理解与生成基准上取得显著提升并在HumanEval上达成新SOTA。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2305.07922v2", "published": "2023-05-13", "update_time": "2023-05-20", "download_time": "2025-12-04 23:13:45"}
{"id": "2305.02309", "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages", "abstract": "Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly.   In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a \"free lunch\" hypothesis. For data distributions, the effect of a mixture distribution and multi-epoch training of programming and natural languages on model performance is explored.   We conduct a comprehensive series of empirical experiments on 1B LLMs, for which failures and successes of this exploration are distilled into five lessons. We will provide a final recipe for training and release CodeGen2 models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training framework as open-source: https://github.com/salesforce/CodeGen.", "arxiv_url": "https://arxiv.org/abs/2305.02309", "authors": ["Erik Nijkamp", "Hiroaki Hayashi", "Caiming Xiong", "Silvio Savarese", "Yingbo Zhou"], "first_author": "Erik Nijkamp", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "该工作通过对模型架构、学习目标、infill 采样和数据分布的大规模消融实验，提出并验证了用于代码与自然语言统一预训练的简洁混合目标与训练配方，并开源了 CodeGen2 模型与训练框架。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2305.02309v2", "published": "2023-05-03", "update_time": "2023-07-11", "download_time": "2025-12-04 23:14:12"}
{"id": "2303.17568", "title": "CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X", "abstract": "Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency for 83.4% of its users. Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX.", "arxiv_url": "https://arxiv.org/abs/2303.17568", "authors": ["Qinkai Zheng", "Xiao Xia", "Xu Zou", "Yuxiao Dong", "Shan Wang", "Yufei Xue", "Zihan Wang", "Lei Shen", "Andi Wang", "Yang Li", "Teng Su", "Zhilin Yang", "Jie Tang"], "first_author": "Qinkai Zheng", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "本文提出并开源了一个13B参数的多语言代码生成预训练模型CodeGeeX，同时构建了用于多语言函数正确性评估的HumanEval-X基准，并展示了模型在生成与翻译任务上的优势与实际IDE部署带来的用户增效。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2303.17568v2", "published": "2023-03-30", "update_time": "2024-07-10", "download_time": "2025-12-04 23:15:01"}
{"id": "2301.03988", "title": "SantaCoder: don't reach for the stars!", "abstract": "The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.", "arxiv_url": "https://arxiv.org/abs/2301.03988", "authors": ["Loubna Ben Allal", "Raymond Li", "Denis Kocetkov", "Chenghao Mou", "Christopher Akiki", "Carlos Munoz Ferrandis", "Niklas Muennighoff", "Mayank Mishra", "Alex Gu", "Manan Dey", "Logesh Kumar Umapathi", "Carolyn Jane Anderson", "Yangtian Zi", "Joel Lamy Poirier", "Hailey Schoelkopf", "Sergey Troshin", "Dmitry Abulkhanov", "Manuel Romero", "Michael Lappert", "Francesco De Toni", "Bernardo García del Río", "Qian Liu", "Shamik Bose", "Urvashi Bhattacharyya", "Terry Yue Zhuo", "Ian Yu", "Paulo Villegas", "Marco Zocca", "Sourab Mangrulkar", "David Lansky", "Huu Nguyen", "Danish Contractor", "Luis Villa", "Jia Li", "Dzmitry Bahdanau", "Yacine Jernite", "Sean Hughes", "Daniel Fried", "Arjun Guha", "Harm de Vries", "Leandro von Werra"], "first_author": "Loubna Ben Allal", "category": ["Technical", "Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "该技术报告介绍了BigCode社区训练的1.1B参数代码模型SantaCoder，包含PII删减管道、架构（MQA、FIM）与数据预处理的消融实验，并在MultiPL-E上优于之前开源多语言模型。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2301.03988v2", "published": "2023-01-09", "update_time": "2023-02-24", "download_time": "2025-12-04 23:15:33"}
{"id": "2203.13474", "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis", "abstract": "Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.", "arxiv_url": "https://arxiv.org/abs/2203.13474", "authors": ["Erik Nijkamp", "Bo Pang", "Hiroaki Hayashi", "Lifu Tu", "Huan Wang", "Yingbo Zhou", "Silvio Savarese", "Caiming Xiong"], "first_author": "Erik Nijkamp", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "该论文训练并开源了多种规模的CODEGEN代码语言模型和训练库JAXFORMER，提出并评估了多轮程序合成范式并构建了Multi-Turn Programming Benchmark以验证多轮提示能显著提升代码生成性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2203.13474v5", "published": "2022-03-25", "update_time": "2023-02-27", "download_time": "2025-12-04 23:16:13"}
{"id": "2312.02120", "title": "Magicoder: Empowering Code Generation with OSS-Instruct", "abstract": "We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open-source references for the production of more realistic and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a new direction for crafting diverse synthetic instruction data for code using abundant open-source references.", "arxiv_url": "https://arxiv.org/abs/2312.02120", "authors": ["Yuxiang Wei", "Zhe Wang", "Jiawei Liu", "Yifeng Ding", "Lingming Zhang"], "first_author": "Yuxiang Wei", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Instruction-Tuning", "summary": "本文提出OSS-INSTRUCT方法从开源代码片段自动生成多样且可控的指令微调数据，并基于此训练并开源了Magicoder系列7B级代码生成模型，在多项代码生成基准上显著提升表现且部分模型超越ChatGPT。", "quality": "High", "conference": "ICML 2024", "pdf_url": "https://arxiv.org/pdf/2312.02120v2", "published": "2023-12-04", "update_time": "2024-06-07", "download_time": "2025-12-04 23:16:40"}
{"id": "2308.07124", "title": "OctoPack: Instruction Tuning Code Large Language Models", "abstract": "Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.", "arxiv_url": "https://arxiv.org/abs/2308.07124", "authors": ["Niklas Muennighoff", "Qian Liu", "Armel Zebaze", "Qinkai Zheng", "Binyuan Hui", "Terry Yue Zhuo", "Swayam Singh", "Xiangru Tang", "Leandro von Werra", "Shayne Longpre"], "first_author": "Niklas Muennighoff", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Instruction-Tuning", "summary": "本文发布了用于指令微调的COMMITPACK（4TB Git 提交数据）及其高质量子集COMMITPACKFT，构建了多语言的HUMANEVALPACK基准，并基于这些数据训练出OCTOCODER/OCTOGEEX，在多语言的代码合成、修复与解释任务上在可许可模型中取得最优表现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2308.07124v2", "published": "2023-08-14", "update_time": "2024-02-18", "download_time": "2025-12-04 23:17:10"}
{"id": "2411.12882", "title": "ProSec: Fortifying Code LLMs with Proactive Security Alignment", "abstract": "While recent code-specific large language models (LLMs) have greatly enhanced their code generation capabilities, the safety of these models remains under-explored, posing potential risks as insecure code generated by these models may introduce vulnerabilities into real-world systems. Existing methods collect security-focused datasets from real-world vulnerabilities for instruction tuning in order to mitigate such issues. However, they are largely constrained by the data sparsity of vulnerable code, and have limited applicability in the multi-stage post-training workflows of modern LLMs. In this paper, we propose ProSec, a novel proactive security alignment approach designed to align code LLMs with secure coding practices. ProSec systematically exposes the vulnerabilities in a code LLM by synthesizing vulnerability-inducing coding scenarios from Common Weakness Enumerations (CWEs) and generates fixes to vulnerable code snippets, allowing the model to learn secure practices through preference learning objectives. The scenarios synthesized by ProSec trigger 25x more vulnerable code than a normal instruction-tuning dataset, resulting in a security-focused alignment dataset 7x larger than the previous work. Experiments show that models trained with ProSec are 25.2% to 35.4% more secure compared to previous work without degrading models' utility.", "arxiv_url": "https://arxiv.org/abs/2411.12882", "authors": ["Xiangzhe Xu", "Zian Su", "Jinyao Guo", "Kaiyuan Zhang", "Zhenting Wang", "Xiangyu Zhang"], "first_author": "Xiangzhe Xu", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Alignment", "summary": "本文提出PROSEC，一种在后训练阶段通过基于CWE合成易触发漏洞的指令、从目标模型采样易受攻击与修复代码并构建偏好学习数据来对代码LLM进行主动安全对齐的方法，从而显著减少生成不安全代码而不损害模型实用性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.12882v3", "published": "2024-11-19", "update_time": "2025-06-06", "download_time": "2025-12-04 23:17:39"}
{"id": "2406.06887", "title": "$\\textbf{PLUM}$: Improving Code LMs with Execution-Guided On-Policy Preference Learning Driven By Synthetic Test Cases", "abstract": "Preference learning provides a promising solution to address the limitations of supervised fine-tuning (SFT) for code language models, where the model is not explicitly trained to differentiate between correct and incorrect code. Recent findings demonstrate that on-policy data is the key to successful preference learning, where the preference data is collected using the same policy LM being trained. Inspired by this, we propose PLUM, an on-policy $\\textbf{P}$reference $\\textbf{L}$earning framework A$\\textbf{u}$gmented with test cases for code L$\\textbf{M}$ s. The framework operates in three key stages: (1) automatic generation of test cases from natural language instructions, (2) creation of a preference data by evaluating candidate code solutions sampled from the policy, which can then be used to (3) train the policy LM. PLUM levitates the need to train reward models, allowing for large scale on-policy and online preference data collation. PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench), delivering substantial improvements over original SFT'ed models and other execution-feedback-driven approaches. We show PLUM's benefits are consistent across various widely-used code LMs even they have been well-trained with SFT. For example, PLUM increases pass rates by up to 4.8% on average on standard benchmarks and 11.8% on LiveCodeBench, demonstrating its effectiveness and generalizability. We also demonstrate the benefits of on-policy and online preference learning by comprehensive experimentation.", "arxiv_url": "https://arxiv.org/abs/2406.06887", "authors": ["Dylan Zhang", "Shizhe Diao", "Xueyan Zou", "Hao Peng"], "first_author": "Dylan Zhang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Instruction-Tuning", "summary": "本文提出PLUM，一种通过自动从自然语言题目生成测试用例并对模型自采样候选解进行执行反馈的在线在策略偏好学习框架，以无需训练奖励模型的方式显著提升代码语言模型的正确率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2406.06887v4", "published": "2024-06-11", "update_time": "2024-10-12", "download_time": "2025-12-04 23:18:09"}
{"id": "2307.04349", "title": "RLTF: Reinforcement Learning from Unit Test Feedback", "abstract": "The goal of program synthesis, or code generation, is to generate executable code based on given descriptions. Recently, there has been an increasing number of studies employing reinforcement learning (RL) to improve the performance of large language models (LLMs) for code. However, current representative works either rely solely on offline frameworks, limiting the exploration of new sample spaces, or fall short in the utilization of unit test signals, not accounting for specific error locations within the code. To address these issues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs. Our approach generates data in real-time during training and simultaneously utilizes fine-grained feedback signals to guide the model towards producing higher-quality code. Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our code is available at: https://github.com/Zyq-scut/RLTF.", "arxiv_url": "https://arxiv.org/abs/2307.04349", "authors": ["Jiate Liu", "Yiqin Zhu", "Kaiwen Xiao", "Qiang Fu", "Xiao Han", "Wei Yang", "Deheng Ye"], "first_author": "Jiate Liu", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "summary": "本文提出 RLTF，一种基于在线强化学习并利用多粒度单元测试反馈（细粒度与自适应反馈）的框架，用以细化代码大模型并在 APPS 与 MBPP 基准上取得显著提升。", "quality": "High", "conference": "TMLR 2023", "pdf_url": "https://arxiv.org/pdf/2307.04349v2", "published": "2023-07-10", "update_time": "2023-11-13", "download_time": "2025-12-04 23:18:42"}
{"id": "2301.13816", "title": "Execution-based Code Generation using Deep Reinforcement Learning", "abstract": "The utilization of programming language (PL) models, pre-trained on large-scale code corpora, as a means of automating software engineering processes has demonstrated considerable potential in streamlining various code generation tasks such as code completion, code translation, and program synthesis. However, current approaches mainly rely on supervised fine-tuning objectives borrowed from text generation, neglecting unique sequence-level characteristics of code, including but not limited to compilability as well as syntactic and functional correctness. To address this limitation, we propose PPOCoder, a new framework for code generation that synergistically combines pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely used deep reinforcement learning technique. By utilizing non-differentiable feedback from code execution and structure alignment, PPOCoder seamlessly integrates external code-specific knowledge into the model optimization process. It's important to note that PPOCoder is a task-agnostic and model-agnostic framework that can be used across different code generation tasks and PLs. Extensive experiments on three code generation tasks demonstrate the effectiveness of our proposed approach compared to SOTA methods, achieving significant improvements in compilation success rates and functional correctness across different PLs.", "arxiv_url": "https://arxiv.org/abs/2301.13816", "authors": ["Parshin Shojaee", "Aneesh Jain", "Sindhu Tipirneni", "Chandan K. Reddy"], "first_author": "Parshin Shojaee", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "该论文提出PPOCoder——一种将预训练代码模型与PPO强化学习结合、利用编译器执行和代码结构对齐作为不可微奖励来优化代码生成以提高可编译性和功能正确性的通用框架，并在多种编程语言与任务上显著优于现有方法。", "quality": "High", "conference": "TMLR 2023", "pdf_url": "https://arxiv.org/pdf/2301.13816v4", "published": "2023-01-31", "update_time": "2023-07-19", "download_time": "2025-12-04 23:19:17"}
{"id": "2207.01780", "title": "CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning", "abstract": "Program synthesis or code generation aims to generate a program that satisfies a problem specification. Recent approaches using large-scale pretrained language models (LMs) have shown promising results, yet they have some critical limitations. In particular, they often follow a standard supervised fine-tuning procedure to train a code generation model only from the pairs of natural-language problem descriptions and ground-truth programs. Such paradigm largely ignores some important but potentially useful signals in the problem specification such as unit tests, which thus often results in poor performance when solving complex unseen coding tasks. To address the limitations, we propose \"CodeRL\", a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL). Specifically, during training, we treat the code-generating LM as an actor network, and introduce a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor. During inference, we introduce a new generation procedure with a critical sampling strategy that allows a model to automatically regenerate programs based on feedback from example unit tests and critic scores. For the model backbones, we extended the encoder-decoder architecture of CodeT5 with enhanced learning objectives, larger model sizes, and better pretraining data. Our method not only achieves new SOTA results on the challenging APPS benchmark, but also shows strong zero-shot transfer capability with new SOTA results on the simpler MBPP benchmark.", "arxiv_url": "https://arxiv.org/abs/2207.01780", "authors": ["Hung Le", "Yue Wang", "Akhilesh Deepak Gotmare", "Silvio Savarese", "Steven C. H. Hoi"], "first_author": "Hung Le", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "summary": "本文提出CodeRL框架，将预训练代码语言模型作为actor并引入critic与基于单元测试的奖励和关键采样策略，通过深度强化学习优化代码生成与自动修复，在APPS和MBPP上取得新的SOTA表现。", "quality": "High", "conference": "NeurIPS 2022", "pdf_url": "https://arxiv.org/pdf/2207.01780v3", "published": "2022-07-05", "update_time": "2022-11-03", "download_time": "2025-12-04 23:19:47"}
{"id": "2410.01215", "title": "From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging", "abstract": "While large language models have made significant strides in code generation, the pass rate of the generated code is bottlenecked on subtle errors, often requiring human intervention to pass tests, especially for complex problems. Existing LLM-based debugging systems treat generated programs as monolithic units, failing to address bugs at multiple levels of granularity, from low-level syntax errors to high-level algorithmic flaws. In this paper, we introduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger by isolating, identifying, and resolving bugs at various levels of granularity. MGDebugger decomposes problematic code into a hierarchical tree structure of subfunctions, with each level representing a particular granularity of error. During debugging, it analyzes each subfunction and iteratively resolves bugs in a bottom-up manner. To effectively test each subfunction, we propose an LLM-simulated Python executor, which traces code execution and tracks important variable states to pinpoint errors accurately. Extensive experiments demonstrate that MGDebugger outperforms existing debugging systems, achieving an 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6% repair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes bugs across different categories and difficulty levels, demonstrating its robustness and effectiveness.", "arxiv_url": "https://arxiv.org/abs/2410.01215", "authors": ["Yuling Shi", "Songsong Wang", "Chengcheng Wan", "Min Wang", "Xiaodong Gu"], "first_author": "Yuling Shi", "category": ["Technical"], "field": "Quality Management", "task": "Bug Repair", "summary": "提出MGDebugger，一种将代码分解为子函数层次、为子函数生成测试并利用LLM模拟执行自底向上定位与修复错误的分层调试方法，显著提升对LLM生成代码与真实软件缺陷的修复率。", "quality": "High", "conference": "ICSE 2026", "pdf_url": "https://arxiv.org/pdf/2410.01215v4", "published": "2024-10-02", "update_time": "2025-11-22", "download_time": "2025-12-04 23:20:17"}
{"id": "2406.18294", "title": "Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs", "abstract": "Some recently developed code large language models (Code LLMs) have been pre-trained on repository-level code data (Repo-Code LLMs), enabling these models to recognize repository structures and utilize cross-file information for code completion. However, in real-world development scenarios, simply concatenating the entire code repository often exceeds the context window limits of these Repo-Code LLMs, leading to significant performance degradation. In this study, we conducted extensive preliminary experiments and analyses on six Repo-Code LLMs. The results indicate that maintaining the topological dependencies of files and increasing the code file content in the completion prompts can improve completion accuracy; pruning the specific implementations of functions in all dependent files does not significantly reduce the accuracy of completions. Based on these findings, we proposed a strategy named Hierarchical Context Pruning (HCP) to construct completion prompts with high informational code content. The HCP models the code repository at the function level, maintaining the topological dependencies between code files while removing a large amount of irrelevant code content, significantly reduces the input length for repository-level code completion. We applied the HCP strategy in experiments with six Repo-Code LLMs, and the results demonstrate that our proposed method can significantly enhance completion accuracy while substantially reducing the length of input. Our code and data are available at https://github.com/Hambaobao/HCP-Coder.", "arxiv_url": "https://arxiv.org/abs/2406.18294", "authors": ["Lei Zhang", "Yunshui Li", "Jiaming Li", "Xiaobo Xia", "Jiaxi Yang", "Run Luo", "Minzheng Wang", "Longze Chen", "Junhao Liu", "Min Yang"], "first_author": "Lei Zhang", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出了分层上下文剪枝（HCP）策略，在函数级别保留文件间拓扑依赖并裁剪无关实现，从而大幅减少仓库级预训练代码LLM的输入长度并显著提升代码补全准确率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2406.18294v2", "published": "2024-06-26", "update_time": "2024-06-27", "download_time": "2025-12-04 23:20:47"}
{"id": "2402.16906", "title": "Debug like a Human: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step", "abstract": "Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifically, LDB segments the programs into basic blocks and tracks the values of intermediate variables after each block throughout the runtime execution. This allows LLMs to concentrate on simpler code units within the overall execution flow, verify their correctness against the task description block by block, and efficiently pinpoint any potential errors. Experiments demonstrate that LDB consistently enhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and TransCoder benchmarks, archiving new state-of-the-art performance in code debugging for various LLM selections.", "arxiv_url": "https://arxiv.org/abs/2402.16906", "authors": ["Li Zhong", "Zilong Wang", "Jingbo Shang"], "first_author": "Li Zhong", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Editing", "summary": "本文提出LDB，一种通过将程序分解为基本块并采集每块的运行时中间变量，让LLM逐步验证每个代码块并定位错误以迭代修复生成代码的方法，在HumanEval、MBPP和TransCoder上显著提升了调试与代码生成性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2402.16906v6", "published": "2024-02-25", "update_time": "2024-06-06", "download_time": "2025-12-04 23:21:15"}
{"id": "2306.02907", "title": "SelfEvolve: A Code Evolution Framework via Large Language Models", "abstract": "Large language models (LLMs) have already revolutionized code generation, after being pretrained on publicly available code data. However, while various methods have been proposed to augment LLMs with retrieved knowledge and enhance the quality of code generation, the performance of these retrieval-based methods is limited by the strength of the retrievers used. In addition, while LLMs show great emergent ability, they still struggle to produce the correct code in one turn. To address these challenges, we propose a novel two-step pipeline, called \\autoknow, that leverages LLMs as both knowledge providers and self-reflective programmers. Unlike retrieval-based methods, \\autoknow~obtains the knowledge from input prompts and generates intermediate code based on the generated knowledge. After that, \\autoknow~asks LLM to act as an expert programmer to perform debugging for the generated code. This is achieved by receiving the error message from the interpreter, without requiring special test cases for correctness verification. We evaluate \\autoknow~on three code generation datasets, including DS-1000 for data science code, HumanEval for software engineering code, and TransCoder for C++-to-Python translation. Our empirical experiments show that \\autoknow~outperforms strong baselines by a significant margin on all datasets. We also conduct exhaustive analytical experiments to validate the effectiveness of the two stages of \\autoknow, and find that both are superior to other prompting-based methods. Further scalability analysis demonstrates that \\autoknow~can be adapted to other more advanced models, such as GPT-4, and bring consistent efficacy improvement.", "arxiv_url": "https://arxiv.org/abs/2306.02907", "authors": ["Shuyang Jiang", "Yuhao Wang", "Yu Wang"], "first_author": "Shuyang Jiang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Prompting", "summary": "本文提出SELFEVOLVE，一种两阶段的LLM驱动代码演化框架：先让模型自生成与题目相关的知识再生成代码，随后基于解释器错误信息对代码进行自我调试与迭代，从而在多套数据集上显著提升执行正确率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2306.02907v1", "published": "2023-06-05", "update_time": "2023-06-05", "download_time": "2025-12-04 23:21:45"}
{"id": "2306.09896", "title": "Is Self-Repair a Silver Bullet for Code Generation?", "abstract": "Large language models have shown remarkable aptitude in code generation, but still struggle to perform complex tasks. Self-repair -- in which the model debugs and repairs its own code -- has recently become a popular way to boost performance in these settings. However, despite its increasing popularity, existing studies of self-repair have been limited in scope; in many settings, its efficacy thus remains poorly understood. In this paper, we analyze Code Llama, GPT-3.5 and GPT-4's ability to perform self-repair on problems taken from HumanEval and APPS. We find that when the cost of carrying out repair is taken into account, performance gains are often modest, vary a lot between subsets of the data, and are sometimes not present at all. We hypothesize that this is because self-repair is bottlenecked by the model's ability to provide feedback on its own code; using a stronger model to artificially boost the quality of the feedback, we observe substantially larger performance gains. Similarly, a small-scale study in which we provide GPT-4 with feedback from human participants suggests that even for the strongest models, self-repair still lags far behind what can be achieved with human-level debugging.", "arxiv_url": "https://arxiv.org/abs/2306.09896", "authors": ["Theo X. Olausson", "Jeevana Priya Inala", "Chenglong Wang", "Jianfeng Gao", "Armando Solar-Lezama"], "first_author": "Theo X. Olausson", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Editing", "summary": "本文在HumanEval和APPS数据集上系统评估了Code Llama、GPT-3.5和GPT-4的自我修复能力，发现考虑修复计算成本后收益常常有限且随数据子集波动较大，而提升反馈质量（由更强模型或人工提供）能显著提高修复效果。", "quality": "High", "conference": "ICLR 2024", "pdf_url": "https://arxiv.org/pdf/2306.09896v5", "published": "2023-06-16", "update_time": "2024-02-02", "download_time": "2025-12-04 23:22:09"}
{"id": "2304.05128", "title": "Teaching Large Language Models to Self-Debug", "abstract": "Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.", "arxiv_url": "https://arxiv.org/abs/2304.05128", "authors": ["Xinyun Chen", "Maxwell Lin", "Nathanael Schärli", "Denny Zhou"], "first_author": "Xinyun Chen", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Editing", "summary": "本文提出 SELF-DEBUGGING，一种通过少量示例提示让大语言模型执行其生成的代码、生成自然语言解释并基于执行结果迭代自我修复程序的调试方法，在多项代码生成基准上显著提升了准确率与样本效率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2304.05128v2", "published": "2023-04-11", "update_time": "2023-10-05", "download_time": "2025-12-04 23:22:35"}
{"id": "2302.08468", "title": "LEVER: Learning to Verify Language-to-Code Generation with Execution", "abstract": "The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generation probability, and marginalizing over programs with the same execution results. On four datasets across the domains of table QA, math QA and basic Python programming, LEVER consistently improves over the base code LLMs(4.6% to 10.9% with code-davinci-002) and achieves new state-of-the-art results on all of them.", "arxiv_url": "https://arxiv.org/abs/2302.08468", "authors": ["Ansong Ni", "Srini Iyer", "Dragomir Radev", "Ves Stoyanov", "Wen-tau Yih", "Sida I. Wang", "Xi Victoria Lin"], "first_author": "Ansong Ni", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "summary": "本文提出LEVER，一种通过对代码LLM生成的候选程序结合其执行结果训练判别器并据此重排序的方法，以提高语言到代码生成的执行准确率，在四个基准上显著优于基线并达成SOTA。", "quality": "High", "conference": "ICML 2023", "pdf_url": "https://arxiv.org/pdf/2302.08468v3", "published": "2023-02-16", "update_time": "2023-09-01", "download_time": "2025-12-04 23:22:58"}
{"id": "2211.16490", "title": "Coder Reviewer Reranking for Code Generation", "abstract": "Sampling diverse programs from a code language model and reranking with model likelihood is a popular method for code generation but it is prone to preferring degenerate solutions. Inspired by collaborative programming, we propose Coder-Reviewer reranking. We augment Coder language models from past work, which generate programs given language instructions, with Reviewer models, which evaluate the likelihood of the instruction given the generated programs. We perform an extensive study across six datasets with eight models from three model families. Experimental results show that Coder-Reviewer reranking leads to consistent and significant improvement (up to 17% absolute accuracy gain) over reranking with the Coder model only. When combined with executability filtering, Coder-Reviewer reranking can often outperform the minimum Bayes risk method. Coder-Reviewer reranking is easy to implement by prompting, can generalize to different programming languages, and works well with off-the-shelf hyperparameters.", "arxiv_url": "https://arxiv.org/abs/2211.16490", "authors": ["Tianyi Zhang", "Tao Yu", "Tatsunori B. Hashimoto", "Mike Lewis", "Wen-tau Yih", "Daniel Fried", "Sida I. Wang"], "first_author": "Tianyi Zhang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出Coder-Reviewer重排方法：通过prompt将生成的程序放在前并估计p(x|y)，与原始p(y|x)的乘积用于重排候选代码，从而减少退化解并在多数据集、多模型上显著提升代码生成准确率且无需额外训练。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2211.16490v1", "published": "2022-11-29", "update_time": "2022-11-29", "download_time": "2025-12-04 23:23:28"}
{"id": "2207.10397", "title": "CodeT: Code Generation with Generated Tests", "abstract": "The task of generating code solutions for a given programming problem can benefit from the use of pre-trained language models such as Codex, which can produce multiple diverse samples. However, a major challenge for this task is to select the most appropriate solution from the multiple samples generated by the pre-trained language models. A natural way to evaluate the quality and correctness of a code solution is to run it against a set of test cases, but the manual creation of such test cases is often costly and time-consuming. In this paper, we propose a novel method, CodeT, that leverages the same pre-trained language models to automatically generate test cases for the code samples, thus reducing the human effort and increasing the coverage of the test scenarios. CodeT then executes the code samples using the generated test cases, and performs a dual execution agreement, which considers both the consistency of the outputs against the generated test cases and the agreement of the outputs with other code samples. We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS and CodeContests, using five different pre-trained language models with varying sizes and capabilities. Our results show that CodeT can significantly improve the performance of code solution selection over previous methods, achieving remarkable and consistent gains across different models and benchmarks. For instance, CodeT improves the pass@1 metric on HumanEval to 65.8%, which represents an absolute improvement of 18.8% over the code-davinci-002 model, and an absolute improvement of more than 20% over the previous state-of-the-art results.", "arxiv_url": "https://arxiv.org/abs/2207.10397", "authors": ["Bei Chen", "Fengji Zhang", "Anh Nguyen", "Daoguang Zan", "Zeqi Lin", "Jian-Guang Lou", "Weizhu Chen"], "first_author": "Bei Chen", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "论文提出CODET方法，利用同一预训练模型自动生成测试用例并通过双重执行一致性（基于生成测试的输出匹配和候选程序间的一致性）从多个候选解中选出最佳代码，从而在HumanEval、MBPP、APPS等基准上显著提升了pass@1性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2207.10397v2", "published": "2022-07-21", "update_time": "2022-11-23", "download_time": "2025-12-04 23:24:02"}
{"id": "2412.05210", "title": "Evaluating and Aligning CodeLLMs on Human Preference", "abstract": "Code large language models (codeLLMs) have made significant strides in code generation. Most previous code-related benchmarks, which consist of various programming exercises along with the corresponding test cases, are used as a common measure to evaluate the performance and capabilities of code LLMs. However, the current code LLMs focus on synthesizing the correct code snippet, ignoring the alignment with human preferences, where the query should be sampled from the practical application scenarios and the model-generated responses should satisfy the human preference. To bridge the gap between the model-generated response and human preference, we present a rigorous human-curated benchmark CodeArena to emulate the complexity and diversity of real-world coding tasks, where 397 high-quality samples spanning 40 categories and 44 programming languages, carefully curated from user queries. Further, we propose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B tokens) by scaling instructions from the website to verify the effectiveness of the large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder totally trained on synthetic instruction data can achieve top-tier performance of open-source code LLMs. The results find performance differences between execution-based benchmarks and CodeArena. Our systematic experiments of CodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code LLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring the importance of the human preference alignment.\\footnote{\\url{https://codearenaeval.github.io/ }}", "arxiv_url": "https://arxiv.org/abs/2412.05210", "authors": ["Jian Yang", "Jiaxi Yang", "Ke Jin", "Yibo Miao", "Lei Zhang", "Liqun Yang", "Zeyu Cui", "Yichang Zhang", "Binyuan Hui", "Junyang Lin"], "first_author": "Jian Yang", "category": ["Benchmark", "Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Alignment", "summary": "本文提出了面向人类偏好的代码评估基准 CodeArena 与大规模合成指令语料 SynCode-Instruct，并基于此训练与评测多款 codeLLM，揭示开放模型与闭源模型在人类偏好对齐上的差距。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2412.05210v1", "published": "2024-12-06", "update_time": "2024-12-06", "download_time": "2025-12-04 23:24:39"}
{"id": "2412.00535", "title": "FullStack Bench: Evaluating LLMs as Full Stack Coders", "abstract": "As the capabilities of code large language models (LLMs) continue to expand, their applications across diverse code intelligence domains are rapidly increasing. However, most existing datasets only evaluate limited application domains. To address this gap, we have developed a comprehensive code evaluation dataset FullStack Bench focusing on full-stack programming, which encompasses a wide range of application domains (e.g., basic programming, data analysis, software engineering, mathematics, and machine learning). Besides, to assess multilingual programming capabilities, in FullStack Bench, we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations. Moreover, we also release an effective code sandbox execution tool (i.e., SandboxFusion) supporting various programming languages and packages to evaluate the performance of our FullStack Bench efficiently. Comprehensive experimental results on our FullStack Bench demonstrate the necessity and effectiveness of our FullStack Bench and SandboxFusion.", "arxiv_url": "https://arxiv.org/abs/2412.00535", "authors": ["Bytedance-Seed-Foundation-Code-Team", ":", "Yao Cheng", "Jianfeng Chen", "Jie Chen", "Li Chen", "Liyu Chen", "Wentao Chen", "Zhengyu Chen", "Shijie Geng", "Aoyan Li", "Bo Li", "Bowen Li", "Linyi Li", "Boyi Liu", "Jiaheng Liu", "Kaibo Liu", "Qi Liu", "Shukai Liu", "Siyao Liu", "Tianyi Liu", "Tingkai Liu", "Yongfei Liu", "Rui Long", "Jing Mai", "Guanghan Ning", "Z. Y. Peng", "Kai Shen", "Jiahao Su", "Jing Su", "Tao Sun", "Yifan Sun", "Yunzhe Tao", "Guoyin Wang", "Siwei Wang", "Xuwu Wang", "Yite Wang", "Zihan Wang", "Jinxiang Xia", "Liang Xiang", "Xia Xiao", "Yongsheng Xiao", "Chenguang Xi", "Shulin Xin", "Jingjing Xu", "Shikun Xu", "Hongxia Yang", "Jack Yang", "Yingxiang Yang", "Jianbo Yuan", "Jun Zhang", "Yufeng Zhang", "Yuyu Zhang", "Shen Zheng", "He Zhu", "Ming Zhu"], "first_author": "Bytedance-Seed-Foundation-Code-Team", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出了FullStack Bench——一个覆盖多领域、16种编程语言且基于真实使用场景与单元测试的全栈代码评测基准，并发布了支持多语言与依赖管理的执行沙箱SandboxFusion以自动化评估LLM的全栈编程能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2412.00535v6", "published": "2024-11-30", "update_time": "2025-05-12", "download_time": "2025-12-04 23:25:08"}
{"id": "2411.05830", "title": "GitChameleon: Unmasking the Version-Switching Capabilities of Code Generation Models", "abstract": "The rapid evolution of software libraries presents a significant challenge for code generation models, which must adapt to frequent version updates while maintaining compatibility with previous versions. Existing code completion benchmarks often overlook this dynamic aspect, and the one that does consider it relies on static code prediction tasks without execution-based evaluation, offering a limited perspective on a model's practical usability. To address this gap, we introduce \\textbf{\\GitChameleon{}}, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. \\GitChameleon{} is designed to rigorously assess the ability of modern large language models (LLMs) to generate version-specific code that is not only syntactically correct but also functionally accurate upon execution. Our comprehensive evaluations reveal that state-of-the-art LLMs struggle with this task; for instance, \\textbf{GPT-4o} achieves a pass@10 of only 39.9\\% (43.7\\% when provided with error feedback), highlighting the complexity of the problem and the limitations of current models. By providing an execution-based benchmark that emphasizes the dynamic nature of code libraries, \\GitChameleon{} serves as a critical tool to advance the development of more adaptable and reliable code generation models. For facilitation for further exploration of version-conditioned code generation, we make our code repository publicly accessible at \\url{https://github.com/NizarIslah/GitChameleon}.", "arxiv_url": "https://arxiv.org/abs/2411.05830", "authors": ["Nizar Islah", "Justine Gehring", "Diganta Misra", "Eilif Muller", "Irina Rish", "Terry Yue Zhuo", "Massimo Caccia"], "first_author": "Nizar Islah", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出了GitChameleon，一个包含116个带可执行单元测试的Python库版本条件代码补全基准，用于评估LLM在库版本变化下生成语法与功能正确代码的能力，并展示了当前模型在该任务上的显著不足。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.05830v1", "published": "2024-11-05", "update_time": "2024-11-05", "download_time": "2025-12-04 23:25:36"}
{"id": "2408.06450", "title": "Evaluating Language Models for Efficient Code Generation", "abstract": "We introduce Differential Performance Evaluation (DPE), a framework designed to reliably evaluate Large Language Models (LLMs) for efficient code generation. Traditional coding benchmarks often fail to provide reliable insights into code efficiency, due to their reliance on simplistic test inputs and the absence of effective compound metrics. DPE addresses these issues by focusing on efficiency-demanding programming tasks and establishing an insightful compound metric for performance evaluation. DPE operates in two phases: To curate efficiency datasets, it selects efficiency-demanding tasks from existing coding benchmarks and generates computationally expensive inputs to stress the efficiency of LLM solutions. To assess the code efficiency, DPE profiles the new solution and compares it globally against a set of reference solutions that exhibit distinct efficiency levels, where the matched level defines its efficiency score. As a proof of concept, we use DPE to create EvalPerf, a benchmark with 121 performance-challenging coding tasks. Our comprehensive evaluation draws interesting findings on the efficiency impact of model sizes, instruction tuning, and prompting. For example, while the scaling law fails to account for code efficiency, general instruction tuning benefits both code correctness and efficiency. We also evaluate the evaluation by examining the effectiveness of DPE, showing that EvalPerf is reliable and convenient to use even across platforms.", "arxiv_url": "https://arxiv.org/abs/2408.06450", "authors": ["Jiawei Liu", "Songrun Xie", "Junhao Wang", "Yuxiang Wei", "Yifeng Ding", "Lingming Zhang"], "first_author": "Jiawei Liu", "category": ["Empirical", "Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出差异化性能评估（DPE）框架并构建了 EVALPERF 基准，通过自动合成性能测试输入、性能聚类与参考解比较，可靠地评估和量化大语言模型生成代码的运行效率，并基于此对模型规模、指令微调和提示进行了实证分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2408.06450v1", "published": "2024-08-12", "update_time": "2024-08-12", "download_time": "2025-12-04 23:26:06"}
{"id": "2403.07974", "title": "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code", "abstract": "Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and May 2024. We have evaluated 18 base LLMs and 34 instruction-tuned LLMs on LiveCodeBench. We present empirical findings on contamination, holistic performance comparisons, potential overfitting in existing benchmarks as well as individual model comparisons. We will release all prompts and model completions for further community analysis, along with a general toolkit for adding new scenarios and model", "arxiv_url": "https://arxiv.org/abs/2403.07974", "authors": ["Naman Jain", "King Han", "Alex Gu", "Wen-Ding Li", "Fanjia Yan", "Tianjun Zhang", "Sida Wang", "Armando Solar-Lezama", "Koushik Sen", "Ion Stoica"], "first_author": "Naman Jain", "category": ["Empirical", "Benchmark"], "field": "Coding Assistant", "task": "Holistic Benchmarking & Contamination-free Evaluation", "summary": "本文提出 LiveCodeBench，一个持续更新且防止训练数据污染的综合性代码评测基准，收集 2023–2024 年竞赛题并在代码生成、自我修复、代码执行与测试输出预测等多种场景上对多款 LLM 进行广泛评估与污染分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2403.07974v2", "published": "2024-03-12", "update_time": "2024-06-06", "download_time": "2025-12-04 23:26:44"}
{"id": "2403.08604", "title": "Prompting Large Language Models to Tackle the Full Software Development Lifecycle: A Case Study", "abstract": "Recent advancements in large language models (LLMs) have significantly enhanced their coding capabilities. However, existing benchmarks predominantly focused on simplified or isolated aspects of coding, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. In this case study, we explore the performance of LLMs across the entire software development lifecycle with DevEval, encompassing stages including software design, environment setup, implementation, acceptance testing, and unit testing. DevEval features four programming languages, multiple domains, high-quality data collection, and carefully designed and verified metrics for each task. Empirical studies show that current LLMs, including GPT-4, fail to solve the challenges presented within DevEval. Our findings offer actionable insights for the future development of LLMs toward real-world programming applications.", "arxiv_url": "https://arxiv.org/abs/2403.08604", "authors": ["Bowen Li", "Wenhan Wu", "Ziwei Tang", "Lin Shi", "John Yang", "Jinyang Li", "Shunyu Yao", "Chen Qian", "Binyuan Hui", "Qicheng Zhang", "Zhiyin Yu", "He Du", "Ping Yang", "Dahua Lin", "Chao Peng", "Kai Chen"], "first_author": "Bowen Li", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "summary": "本文提出DevEval，一个覆盖软件设计、环境搭建、实现与验收/单元测试等软件全生命周期任务的基准，并通过对多种LLM（含GPT‑4系列）的评估发现其在仓库级实现与测试上仍存在显著不足。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2403.08604v3", "published": "2024-03-13", "update_time": "2024-12-14", "download_time": "2025-12-04 23:27:14"}
{"id": "2310.06770", "title": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?", "abstract": "Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.", "arxiv_url": "https://arxiv.org/abs/2310.06770", "authors": ["Carlos E. Jimenez", "John Yang", "Alexander Wettig", "Shunyu Yao", "Kexin Pei", "Ofir Press", "Karthik Narasimhan"], "first_author": "Carlos E. Jimenez", "category": ["Benchmark", "Empirical", "Technical"], "field": "Coding Assistant", "task": "Code Editing", "summary": "本文提出SWE-bench——一个由真实GitHub issue及其修复PR构成的库级代码编辑基准（2,294个实例），并评估多种大型语言模型、发布训练集SWE-bench-train及微调模型SWE-Llama，结果显示现有模型仅能解决极少数实际问题。", "quality": "High", "conference": "ICLR 2024", "pdf_url": "https://arxiv.org/pdf/2310.06770v3", "published": "2023-10-10", "update_time": "2024-11-11", "download_time": "2025-12-04 23:27:45"}
{"id": "2306.03091", "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems", "abstract": "Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems. RepoBench is publicly available at https://github.com/Leolty/repobench.", "arxiv_url": "https://arxiv.org/abs/2306.03091", "authors": ["Tianyang Liu", "Canwen Xu", "Julian McAuley"], "first_author": "Tianyang Liu", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出RepoBench，一个面向仓库级别的代码自动补全基准（含检索、下一行补全与端到端流水线三项任务），并提供覆盖Python/Java的数据集与评测分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2306.03091v2", "published": "2023-06-05", "update_time": "2023-10-04", "download_time": "2025-12-04 23:28:06"}
{"id": "2306.14893", "title": "LongCoder: A Long-Range Pre-trained Language Model for Code Completion", "abstract": "In this paper, we introduce a new task for code completion that focuses on handling long code input and propose a sparse Transformer model, called LongCoder, to address this task. LongCoder employs a sliding window mechanism for self-attention and introduces two types of globally accessible tokens - bridge tokens and memory tokens - to improve performance and efficiency. Bridge tokens are inserted throughout the input sequence to aggregate local information and facilitate global interaction, while memory tokens are included to highlight important statements that may be invoked later and need to be memorized, such as package imports and definitions of classes, functions, or structures. We conduct experiments on a newly constructed dataset that contains longer code context and the publicly available CodeXGLUE benchmark. Experimental results demonstrate that LongCoder achieves superior performance on code completion tasks compared to previous models while maintaining comparable efficiency in terms of computational resources during inference. All the codes and data are available at https://github.com/microsoft/CodeBERT.", "arxiv_url": "https://arxiv.org/abs/2306.14893", "authors": ["Daya Guo", "Canwen Xu", "Nan Duan", "Jian Yin", "Julian McAuley"], "first_author": "Daya Guo", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出LongCoder——一种用于长代码上下文的稀疏预训练Transformer，采用滑动窗口注意力并引入桥接（bridge）与记忆（memory）令牌以实现全局信息访问，同时构建并发布长代码补全数据集LCC，实验证明在长/常规代码补全上性能优于既有模型且推理效率相当。", "quality": "High", "conference": "ICML 2023", "pdf_url": "https://arxiv.org/pdf/2306.14893v1", "published": "2023-06-26", "update_time": "2023-06-26", "download_time": "2025-12-04 23:28:30"}
{"id": "2308.10335", "title": "Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability of Large Language Model Code Generation", "abstract": "Recently, the large language models (LLMs) have shown extraordinary ability in understanding natural language and generating programming code. It has been a common practice of software engineers to consult LLMs when encountering coding questions. Although efforts have been made to avoid syntax errors and align the code with the intended semantics, the reliability and robustness of the code generationfrom LLMs have not yet been thoroughly studied. The executable code is not equivalent to the reliable and robust code, especially in the context of real-world software development. The misuse of APIs in the generated code could lead to severe problem, such as resource leaks, program crashes. To make things worse, the users of LLM code generation services are actually the developers that are most vulnerable to these code that seems right -- They are always novice developers that are not familiar with the APIs that LLMs generate code for them. Therefore, they could hardly tell the misuse in the code generated by LLMs, which further facilitates the incorrect code applied in real-world software. Existing code evaluation benchmark and datasets focus on crafting small tasks such as programming questions in coding interviews, which however deviates from the problem that developers would ask LLM for real-world coding help. To fill the missing piece, in this work, we propose a dataset RobustAPI for evaluating the reliability and robustness of code generated by LLMs. We collect 1208 coding questions from StackOverflow on 24 representative Java APIs. We summarize thecommon misuse patterns of these APIs and evaluate them oncurrent popular LLMs. The evaluation results show that evenfor GPT-4, 62% of the generated code contains API misuses,which would cause unexpected consequences if the code isintroduced into real-world software.", "arxiv_url": "https://arxiv.org/abs/2308.10335", "authors": ["Li Zhong", "Zilong Wang"], "first_author": "Li Zhong", "category": ["Benchmark", "Empirical", "Technical"], "field": "Coding Assistant", "task": "Code Prompting", "summary": "本文提出了面向Java API误用的基准数据集ROBUSTAPI及基于AST的API使用检测器，收集了1208个Stack Overflow问题并评估多款主流LLM，发现即使GPT-4也存在大量API误用（约62%）。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2308.10335v5", "published": "2023-08-20", "update_time": "2024-01-27", "download_time": "2025-12-04 23:28:56"}
{"id": "2308.16458", "title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Large Language Models", "abstract": "Pre-trained large language models (LLMs) have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks and to be appropriately specialized to particular domains. Here, we target bioinformatics due to the amount of domain knowledge, algorithms, and data operations this discipline requires. We present BioCoder, a benchmark developed to evaluate LLMs in generating bioinformatics-specific code. BioCoder spans much of the field, covering cross-file dependencies, class declarations, and global variables. It incorporates 1,026 Python functions and 1,243 Java methods extracted from GitHub, along with 253 examples from the Rosalind Project, all pertaining to bioinformatics. Using topic modeling, we show that the overall coverage of the included code is representative of the full spectrum of bioinformatics calculations. BioCoder incorporates a fuzz-testing framework for evaluation. We have applied it to evaluate various models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+, GPT-3.5, and GPT- 4. Furthermore, we fine-tuned one model (StarCoder), demonstrating that our training dataset can enhance the performance on our testing benchmark (by >15% in terms of Pass@K under certain prompt configurations and always >3%). The results highlight two key aspects of successful models: (1) Successful models accommodate a long prompt (> 2,600 tokens) with full context, including functional dependencies. (2) They contain domain-specific knowledge of bioinformatics, beyond just general coding capability. This is evident from the performance gain of GPT-3.5/4 compared to the smaller models on our benchmark (50% vs. up to 25%). Availability and implementation: Code is available at: https://github.com/gersteinlab/biocoder and https://biocoder-benchmark. github.io/.", "arxiv_url": "https://arxiv.org/abs/2308.16458", "authors": ["Xiangru Tang", "Bill Qian", "Rick Gao", "Jiakang Chen", "Xinyun Chen", "Mark Gerstein"], "first_author": "Xiangru Tang", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "BioCoder 提出一个面向生物信息学的大规模代码生成基准（共 2,269 个问题），从 GitHub 与 Rosalind 提取并清洗函数，提供多文件上下文、依赖解析、执行式模糊测试与 Docker 化评测工具，用以评估并分析不同 LLM 在生物信息学代码生成中的性能和领域知识需求。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2308.16458v5", "published": "2023-08-31", "update_time": "2024-05-20", "download_time": "2025-12-04 23:29:24"}
{"id": "2305.01210", "title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation", "abstract": "Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code. Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis. However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies. While EvalPlus is general, we extend the test-cases of the popular HumanEval benchmark by 80x to build HumanEval+. Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that test insufficiency can lead to mis-ranking. For example, both WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+, while none of them could on HumanEval. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis, but also opens up a new direction to improve such programming benchmarks through automated testing. We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.", "arxiv_url": "https://arxiv.org/abs/2305.01210", "authors": ["Jiawei Liu", "Chunqiu Steven Xia", "Yuyao Wang", "Lingming Zhang"], "first_author": "Jiawei Liu", "category": ["Technical", "Benchmark"], "field": "Software Testing", "task": "Test Generation", "summary": "本文提出EvalPlus，通过结合LLM驱动的种子输入生成与类型感知变异的大规模测试输入扩充并将HUMANEVAL扩展为HUMANEVAL+，从而更严格地评估LLM生成代码的功能正确性并揭示大量先前未被检测的错误与模型排名变化。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2305.01210v3", "published": "2023-05-02", "update_time": "2023-10-30", "download_time": "2025-12-04 23:29:49"}
{"id": "2305.18584", "title": "Coeditor: Leveraging Contextual Changes for Multi-round Code Auto-editing", "abstract": "Developers often dedicate significant time to maintaining and refactoring existing code. However, most prior work on generative models for code focuses solely on creating new code, overlooking the distinctive needs of editing existing code. In this work, we explore a multi-round code auto-editing setting, aiming to predict edits to a code region based on recent changes within the same codebase. Our model, Coeditor, is a fine-tuned language model specifically designed for code editing tasks. We represent code changes using a line diff format and employ static analysis to form large customized model contexts, ensuring the availability of appropriate information for prediction. We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation. In a simplified single-round, single-edit task, Coeditor significantly outperforms GPT-3.5 and SOTA open-source code completion models (bringing exact-match accuracy from 34.7 up to 60.4), demonstrating the benefits of incorporating editing history for code completion. In a multi-round, multi-edit setting, we observe substantial gains by iteratively conditioning on additional user edits. We have open-sourced our code, data, and model weights to encourage future research and have released a VSCode extension powered by our model for interactive IDE usage.", "arxiv_url": "https://arxiv.org/abs/2305.18584", "authors": ["Jiayi Wei", "Greg Durrett", "Isil Dillig"], "first_author": "Jiayi Wei", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Editing", "summary": "该论文提出Coeditor——一种基于repo级行diff编码、扩展CodeT5并采用块稀疏注意力的多轮代码自动编辑模型，同时构建并开源了PYCOMMITS数据集与评测框架，并在多轮编辑任务上显著优于现有基线。", "quality": "High", "conference": "The Twelfth International Conference on Learning Representations 2024", "pdf_url": "https://arxiv.org/pdf/2305.18584v2", "published": "2023-05-29", "update_time": "2024-04-28", "download_time": "2025-12-04 23:30:18"}
{"id": "2211.11501", "title": "DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation", "abstract": "We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from StackOverflow. Second, our automatic evaluation is highly specific (reliable) -- across all Codex-002-predicted solutions that our evaluation accept, only 1.8% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original StackOverflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.", "arxiv_url": "https://arxiv.org/abs/2211.11501", "authors": ["Yuhang Lai", "Chengxi Li", "Yiming Wang", "Tianyi Zhang", "Ruiqi Zhong", "Luke Zettlemoyer", "Scott Wen-tau Yih", "Daniel Fried", "Sida Wang", "Tao Yu"], "first_author": "Yuhang Lai", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出并发布了DS-1000——一个包含1000个来自StackOverflow的真实数据科学Python问题的基准，配备可执行的多准则自动评测并通过扰动问题防止模型记忆化，以可靠评估代码生成模型性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2211.11501v1", "published": "2022-11-18", "update_time": "2022-11-18", "download_time": "2025-12-04 23:30:49"}
{"id": "2208.08227", "title": "MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation", "abstract": "Large language models have demonstrated the ability to generate both natural language and programming language text. Such models open up the possibility of multi-language code generation: could code generation models generalize knowledge from one language to another? Although contemporary code generation models can generate semantically correct Python code, little is known about their abilities with other languages. We propose MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the first massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages.   We use MultiPL-E to extend the HumanEval benchmark and MBPP benchmark to 18 languages that encompass a range of programming paradigms and popularity. Using these new parallel benchmarks, we evaluate the multi-language performance of three state-of-the-art code generation models: Codex, CodeGen, and InCoder. We find that Codex matches or even exceeds its performance on Python for several other languages. The range of programming languages represented in MultiPL-E allow us to explore the impact of language frequency and language features on model performance. Finally, the MultiPL-E approach of compiling code generation benchmarks to new programming languages is both scalable and extensible, making it straightforward to evaluate new models, benchmarks, and languages.", "arxiv_url": "https://arxiv.org/abs/2208.08227", "authors": ["Federico Cassano", "John Gouwar", "Daniel Nguyen", "Sydney Nguyen", "Luna Phipps-Costin", "Donald Pinckney", "Ming-Ho Yee", "Yangtian Zi", "Carolyn Jane Anderson", "Molly Q Feldman", "Arjun Guha", "Michael Greenberg", "Abhinav Jangda"], "first_author": "Federico Cassano", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出 MultiPL-E，将 HumanEval 和 MBPP 翻译并扩展到 19 种编程语言，构建首个大规模多语言并行代码生成基准并用其评估多款模型以分析语言特性、流行度、类型系统和提示敏感性对生成性能的影响。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2208.08227v4", "published": "2022-08-17", "update_time": "2022-12-19", "download_time": "2025-12-04 23:31:19"}
{"id": "2108.07732", "title": "Program Synthesis with Large Language Models", "abstract": "This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.", "arxiv_url": "https://arxiv.org/abs/2108.07732", "authors": ["Jacob Austin", "Augustus Odena", "Maxwell Nye", "Maarten Bosma", "Henryk Michalewski", "David Dohan", "Ellen Jiang", "Carrie Cai", "Michael Terry", "Quoc Le", "Charles Sutton"], "first_author": "Jacob Austin", "category": ["Empirical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文引入了两个用于Python程序合成的新数据集（MBPP和MathQA-Python），并实证评估了不同规模的大型语言模型在few-shot与微调情形下从自然语言描述生成短Python程序的性能、与人类交互以修复代码的能力及模型的语义归属限制。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2108.07732v1", "published": "2021-08-16", "update_time": "2021-08-16", "download_time": "2025-12-04 23:31:49"}
{"id": "2105.09938", "title": "Measuring Coding Challenge Competence With APPS", "abstract": "While programming is one of the most broadly applicable skills in modern society, modern machine learning models still cannot code solutions to basic problems. Despite its importance, there has been surprisingly little work on evaluating code generation, and it can be difficult to accurately assess code generation performance rigorously. To meet this challenge, we introduce APPS, a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code. Similar to how companies assess candidate software developers, we then evaluate models by checking their generated code on test cases. Our benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges. We fine-tune large language models on both GitHub and our training set, and we find that the prevalence of syntax errors is decreasing exponentially as models improve. Recent models such as GPT-Neo can pass approximately 20% of the test cases of introductory problems, so we find that machine learning models are now beginning to learn how to code. As the social significance of automatic code generation increases over the coming years, our benchmark can provide an important measure for tracking advancements.", "arxiv_url": "https://arxiv.org/abs/2105.09938", "authors": ["Dan Hendrycks", "Steven Basart", "Saurav Kadavath", "Mantas Mazeika", "Akul Arora", "Ethan Guo", "Collin Burns", "Samir Puranik", "Horace He", "Dawn Song", "Jacob Steinhardt"], "first_author": "Dan Hendrycks", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "summary": "本文提出并发布了APPS基准——一个包含10000道自然语言编程题及超过130,000条测试用例的Python代码生成数据集，并通过测试展示和分析了多种大规模语言模型在不同难度题目上的生成性能与错误模式。", "quality": "High", "conference": "NeurIPS 2021", "pdf_url": "https://arxiv.org/pdf/2105.09938v3", "published": "2021-05-20", "update_time": "2021-11-08", "download_time": "2025-12-04 23:32:17"}
{"id": "2511.17368", "title": "Exploring Scientific Debt: Harnessing AI for SATD Identification in Scientific Software", "abstract": "Developers often leave behind clues in their code, admitting where it falls short, known as Self-Admitted Technical Debt (SATD). In the world of Scientific Software (SSW), where innovation moves fast and collaboration is key, such debt is not just common but deeply impactful. As research relies on accurate and reproducible results, accumulating SATD can threaten the very foundations of scientific discovery. Yet, despite its significance, the relationship between SATD and SSW remains largely unexplored, leaving a crucial gap in understanding how to manage SATD in this critical domain. This study explores SATD in SSW repositories, comparing SATD in scientific versus general-purpose open-source software and evaluating transformer-based models for SATD identification. We analyzed SATD in 27 scientific and general-purpose repositories across multiple domains and languages. We fine-tuned and compared 10 transformer-based models (100M-7B parameters) on 67,066 labeled code comments. SSW contains 9.25x more Scientific Debt and 4.93x more SATD than general-purpose software due to complex computations, domain constraints, and evolving research needs. Furthermore, our best model outperforms existing ones. This study uncovers how SATD in SSW differs from general software, revealing its impact on quality and scientific validity. By recognizing these challenges, developers and researchers can adopt smarter strategies to manage debt and safeguard the integrity of scientific discovery.", "arxiv_url": "https://arxiv.org/abs/2511.17368", "authors": ["Eric L. Melin", "Ahmed Musa Awon", "Nasir U. Eisty", "Neil A. Ernst", "Shurui Zhou"], "first_author": "Eric L. Melin", "category": ["Technical", "Benchmark", "Experience"], "field": "Technical Debt", "task": "SATD Identification and Classification (including Scientific Debt) in Scientific Software", "summary": "本文构建并扩充了面向科学软件的SATD数据集、微调并比较了10种Transformer/大型语言模型用于自动识别与分类包括新定义的“Scientific Debt”在内的自认技术债务，并发现科学软件中的SATD显著多于通用软件且所提模型优于现有基线。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.17368v1", "published": "2025-11-21", "update_time": "2025-11-21", "download_time": "2025-12-04 23:58:50"}
{"id": "2511.17330", "title": "Agentic Program Verification", "abstract": "Automatically generated code is gaining traction recently, owing to the prevalence of Large Language Models (LLMs). Further, the AlphaProof initiative has demonstrated the possibility of using AI for general mathematical reasoning. Reasoning about computer programs (software) can be accomplished via general mathematical reasoning; however, it tends to be more structured and richer in contexts. This forms an attractive proposition, since then AI agents can be used to reason about voluminous code that gets generated by AI.   In this work, we present a first LLM agent, AutoRocq, for conducting program verification. Unlike past works, which rely on extensive training of LLMs on proof examples, our agent learns on-the-fly and improves the proof via an iterative refinement loop. The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback. The final result of the iteration is a proof derivation checked by the Rocq theorem prover. In this way, our proof construction involves autonomous collaboration between the proof agent and the theorem prover. This autonomy facilitates the search for proofs and decision-making in deciding on the structure of the proof tree.   Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification. As automation in code generation becomes more widespread, we posit that our proof agent can be potentially integrated with AI coding agents to achieve a generate and validate loop, thus moving closer to the vision of trusted automatic programming.", "arxiv_url": "https://arxiv.org/abs/2511.17330", "authors": ["Haoxin Tu", "Huan Zhao", "Yahui Song", "Mehtab Zafar", "Ruijie Meng", "Abhik Roychoudhury"], "first_author": "Haoxin Tu", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "summary": "提出AutoRocq——一个与Rocq/Coq交互的agent式LLM证明代理，通过迭代地从定理证明器获取上下文与反馈并精炼策略，自动生成可被证明器检验的程序证明，在SV-COMP基准与Linux内核模块上显著提高了自动化程序验证效果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.17330v1", "published": "2025-11-21", "update_time": "2025-11-21", "download_time": "2025-12-04 23:59:18"}
{"id": "2511.17262", "title": "SlsReuse: LLM-Powered Serverless Function Reuse", "abstract": "Serverless computing has rapidly emerged as a popular cloud computing paradigm. It enables developers to implement function-level tasks, i.e., serverless functions, without managing infrastructure. While reducing operational overhead, it poses challenges, especially for novice developers. Developing functions from scratch requires adapting to heterogeneous, platform-specific programming styles, making the process time-consuming and error-prone. Function reuse offers a promising solution to address these challenges. However, research on serverless computing lacks a dedicated approach for function recommendation. Existing techniques from traditional contexts remain insufficient due to the semantic gap between task descriptions and heterogeneous function implementations. Advances in large language models (LLMs), pre-trained on large-scale corpora, create opportunities to bridge this gap by aligning developer requirements with function semantics.   This paper presents SlsReuse, the first LLM-powered framework for serverless function reuse. Specifically, SlsReuse first constructs a reusable function repository serving as a foundational knowledge base. Then, it learns unified semantic-enhanced representations of heterogeneous functions through effective prompt engineering with few-shot prompting, capturing implicit code intent, target platforms, programming languages, and cloud services. Finally, given a natural language task query, SlsReuse performs intent-aware discovery combined with a multi-level pruning strategy and similarity matching. We evaluate SlsReuse on a curated dataset of 110 task queries. Built on ChatGPT-4o, one of the most representative LLMs, SlsReuse achieves Recall@10 of 91.20%, exceeding the state-of-the-art baseline by 24.53 percentage points.", "arxiv_url": "https://arxiv.org/abs/2511.17262", "authors": ["Jinfeng Wen", "Yuehan Sun"], "first_author": "Jinfeng Wen", "category": ["Technical", "Benchmark"], "field": "Serverless Computing", "task": "Function Recommendation / Reuse", "summary": "本文提出SlsReuse——一个基于大型语言模型的无服务器函数重用框架，利用少样本提示工程构建语义增强表示并通过多级剪枝与相似度匹配实现意图感知的函数发现与推荐，在自建的500个函数与110个查询数据集上显著优于现有基线。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.17262v1", "published": "2025-11-21", "update_time": "2025-11-21", "download_time": "2025-12-04 23:59:50"}
{"id": "2511.17027", "title": "ReVul-CoT: Towards Effective Software Vulnerability Assessment with Retrieval-Augmented Generation and Chain-of-Thought Prompting", "abstract": "Context: Software Vulnerability Assessment (SVA) plays a vital role in evaluating and ranking vulnerabilities in software systems to ensure their security and reliability. Objective: Although Large Language Models (LLMs) have recently shown remarkable potential in SVA, they still face two major limitations. First, most LLMs are trained on general-purpose corpora and thus lack domain-specific knowledge essential for effective SVA. Second, they tend to rely on shallow pattern matching instead of deep contextual reasoning, making it challenging to fully comprehend complex code semantics and their security implications. Method: To alleviate these limitations, we propose a novel framework ReVul-CoT that integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (COT) prompting. In ReVul-CoT, the RAG module dynamically retrieves contextually relevant information from a constructed local knowledge base that consolidates vulnerability data from authoritative sources (such as NVD and CWE), along with corresponding code snippets and descriptive information. Building on DeepSeek-V3.1, CoT prompting guides the LLM to perform step-by-step reasoning over exploitability, impact scope, and related factors Results: We evaluate ReVul-CoT on a dataset of 12,070 vulnerabilities. Experimental results show that ReVul-CoT outperforms state-of-the-art SVA baselines by 16.50%-42.26% in terms of MCC, and outperforms the best baseline by 10.43%, 15.86%, and 16.50% in Accuracy, F1-score, and MCC, respectively. Our ablation studies further validate the contributions of considering dynamic retrieval, knowledge integration, and CoT-based reasoning. Conclusion: Our results demonstrate that combining RAG with CoT prompting significantly enhances LLM-based SVA and points out promising directions for future research.", "arxiv_url": "https://arxiv.org/abs/2511.17027", "authors": ["Zhijie Chen", "Xiang Chen", "Ziming Li", "Jiacheng Xue", "Chaoyang Gao"], "first_author": "Zhijie Chen", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Assessment (Severity Classification)", "summary": "本文提出ReVul-CoT框架，将检索增强生成(RAG)与链式思维(CoT)提示结合，利用从NVD/CWE构建的本地知识库对12,070条漏洞进行动态检索与逐步推理，从而显著提升基于LLM的CVSS v3漏洞严重性评估效果并公开数据与代码。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.17027v1", "published": "2025-11-21", "update_time": "2025-11-21", "download_time": "2025-12-05 00:00:20"}
{"id": "2511.16858", "title": "Is the Cure Still Worse Than the Disease? Test Overfitting by LLMs in Automated Program Repair", "abstract": "Automated program repair has been shown to be susceptible to generating repaired code that passes on seen tests but fails on a hold-out set of hidden tests. This problem, dubbed test overfitting, has been identified and studied before the rise of large language models. We experimentally study how much test overfitting is still a problem today, using repository-level SWE-bench tasks.", "arxiv_url": "https://arxiv.org/abs/2511.16858", "authors": ["Toufique Ahmed", "Jatin Ganhotra", "Avraham Shinnar", "Martin Hirzel"], "first_author": "Toufique Ahmed", "category": ["Empirical"], "field": "Software Testing", "task": "Test Generation", "summary": "本文在仓库级别的自动程序修复任务上，使用Agentless与e-Otter++并基于Claude-3.7-Sonnet与GPT-4o实证量化了LLM生成补丁对白盒测试的过拟合程度，评估了基于白盒测试的补丁改进对过拟合的影响并探索了缓解手段。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16858v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-05 00:00:46"}
{"id": "2511.16787", "title": "NALA_MAINZ at BLP-2025 Task 2: A Multi-agent Approach for Bangla Instruction to Python Code Generation", "abstract": "This paper presents JGU Mainz's winning system for the BLP-2025 Shared Task on Code Generation from Bangla Instructions. We propose a multi-agent-based pipeline. First, a code-generation agent produces an initial solution from the input instruction. The candidate program is then executed against the provided unit tests (pytest-style, assert-based). Only the failing cases are forwarded to a debugger agent, which reruns the tests, extracts error traces, and, conditioning on the error messages, the current program, and the relevant test cases, generates a revised solution. Using this approach, our submission achieved first place in the shared task with a $Pass@1$ score of 95.4. We also make our code public.", "arxiv_url": "https://arxiv.org/abs/2511.16787", "authors": ["Hossain Shaikh Saadi", "Faria Alam", "Mario Sanz-Guerrero", "Minh Duc Bui", "Manuel Mager", "Katharina von der Wense"], "first_author": "Hossain Shaikh Saadi", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Prompting", "summary": "本文提出了一种用于孟加拉语指令到Python代码生成的多智能体流水线：先由生成代理产生初始程序，运行pytest单元测试仅将失败样例连同错误追踪交给调试代理进行最小化修复，最终在BLP-2025共享任务中以95.4% Pass@1获胜并开源代码。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16787v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-05 00:01:29"}
{"id": "2511.16395", "title": "CorrectHDL: Agentic HDL Design with LLMs Leveraging High-Level Synthesis as Reference", "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in hardware front-end design using hardware description languages (HDLs). However, their inherent tendency toward hallucination often introduces functional errors into the generated HDL designs. To address this issue, we propose the framework CorrectHDL that leverages high-level synthesis (HLS) results as functional references to correct potential errors in LLM-generated HDL designs.The input to the proposed framework is a C/C++ program that specifies the target circuit's functionality. The program is provided to an LLM to directly generate an HDL design, whose syntax errors are repaired using a Retrieval-Augmented Generation (RAG) mechanism. The functional correctness of the LLM-generated circuit is iteratively improved by comparing its simulated behavior with an HLS reference design produced by conventional HLS tools, which ensures the functional correctness of the result but can lead to suboptimal area and power efficiency. Experimental results demonstrate that circuits generated by the proposed framework achieve significantly better area and power efficiency than conventional HLS designs and approach the quality of human-engineered circuits. Meanwhile, the correctness of the resulting HDL implementation is maintained, highlighting the effectiveness and potential of agentic HDL design leveraging the generative capabilities of LLMs and the rigor of traditional correctness-driven IC design flows.", "arxiv_url": "https://arxiv.org/abs/2511.16395", "authors": ["Kangwei Xu", "Grace Li Zhang", "Ulf Schlichtmann", "Bing Li"], "first_author": "Kangwei Xu", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Translation", "summary": "提出CorrectHDL框架，利用HLS生成的HDL作为功能参考，结合LLM、RAG与差分验证迭代修复LLM生成的HDL，从而实现功能正确且在面积与功耗上显著优于传统HLS的硬件设计。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16395v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-05 09:51:26"}
{"id": "2511.16383", "title": "An Agent-Based Framework for the Automatic Validation of Mathematical Optimization Models", "abstract": "Recently, using Large Language Models (LLMs) to generate optimization models from natural language descriptions has became increasingly popular. However, a major open question is how to validate that the generated models are correct and satisfy the requirements defined in the natural language description. In this work, we propose a novel agent-based method for automatic validation of optimization models that builds upon and extends methods from software testing to address optimization modeling . This method consists of several agents that initially generate a problem-level testing API, then generate tests utilizing this API, and, lastly, generate mutations specific to the optimization model (a well-known software testing technique assessing the fault detection power of the test suite). In this work, we detail this validation framework and show, through experiments, the high quality of validation provided by this agent ensemble in terms of the well-known software testing measure called mutation coverage.", "arxiv_url": "https://arxiv.org/abs/2511.16383", "authors": ["Alexander Zadorojniy", "Segev Wasserkrug", "Eitan Farchi"], "first_author": "Alexander Zadorojniy", "category": ["Technical"], "field": "Software Testing", "task": "Test Generation", "summary": "本文提出一种基于多代理的自动化验证框架，利用LLM生成问题级测试接口、测试用例和针对优化模型的变异体，并通过变异覆盖率评估来自自然语言描述的线性规划模型的正确性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16383v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-05 09:51:51"}
{"id": "2511.16224", "title": "Beyond Code Similarity: Benchmarking the Plausibility, Efficiency, and Complexity of LLM-Generated Smart Contracts", "abstract": "Smart Contracts are critical components of blockchain ecosystems, with Solidity as the dominant programming language. While LLMs excel at general-purpose code generation, the unique constraints of Smart Contracts, such as gas consumption, security, and determinism, raise open questions about the reliability of LLM-generated Solidity code. Existing studies lack a comprehensive evaluation of these critical functional and non-functional properties. We benchmark four state-of-the-art models under zero-shot and retrieval-augmented generation settings across 500 real-world functions. Our multi-faceted assessment employs code similarity metrics, semantic embeddings, automated test execution, gas profiling, and cognitive and cyclomatic complexity analysis. Results show that while LLMs produce code with high semantic similarity to real contracts, their functional correctness is low: only 20% to 26% of zero-shot generations behave identically to ground-truth implementations under testing. The generated code is consistently simpler, with significantly lower complexity and gas consumption, often due to omitted validation logic. Retrieval-Augmented Generation markedly improves performance, boosting functional correctness by up to 45% and yielding more concise and efficient code. Our findings reveal a significant gap between semantic similarity and functional plausibility in LLM-generated Smart Contracts. We conclude that while RAG is a powerful enhancer, achieving robust, production-ready code generation remains a substantial challenge, necessitating careful expert validation.", "arxiv_url": "https://arxiv.org/abs/2511.16224", "authors": ["Francesco Salzano", "Simone Scalabrino", "Rocco Oliveto", "Remo Pareschi"], "first_author": "Francesco Salzano", "category": ["Empirical", "Benchmark"], "field": "Coding Assistant", "task": "Code Prompting", "summary": "本文系统评估了四款大型模型在从自然语言描述生成Solidity智能合约函数时的语义相似度、功能可行性、gas效率与代码复杂度，发现尽管语义相似但功能正确率较低，而检索增强生成（RAG）能显著提升表现并降低gas消耗。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16224v2", "published": "2025-11-20", "update_time": "2025-11-21", "download_time": "2025-12-05 09:56:09"}
{"id": "2511.16123", "title": "Domain-constrained Synthesis of Inconsistent Key Aspects in Textual Vulnerability Descriptions", "abstract": "Textual Vulnerability Descriptions (TVDs) are crucial for security analysts to understand and address software vulnerabilities. However, the key aspect inconsistencies in TVDs from different repositories pose challenges for achieving a comprehensive understanding of vulnerabilities. Existing approaches aim to mitigate inconsistencies by aligning TVDs with external knowledge bases, but they often discard valuable information and fail to synthesize comprehensive representations. In this paper, we propose a domain-constrained LLM-based synthesis framework for unifying key aspects of TVDs. Our framework consists of three stages: 1) Extraction, guided by rule-based templates to ensure all critical details are captured; 2) Self-evaluation, using domain-specific anchor words to assess semantic variability across sources; and 3) Fusion, leveraging information entropy to reconcile inconsistencies and prioritize relevant details. This framework improves synthesis performance, increasing the F1 score for key aspect augmentation from 0.82 to 0.87, while enhancing comprehension and efficiency by over 30\\%. We further develop Digest Labels, a practical tool for visualizing TVDs, which human evaluations show significantly boosts usability.", "arxiv_url": "https://arxiv.org/abs/2511.16123", "authors": ["Linyi Han", "Shidong Pan", "Zhenchang Xing", "Sofonias Yitagesu", "Xiaowang Zhang", "Zhiyong Feng", "Jiamou Sun", "Qing Huang"], "first_author": "Linyi Han", "category": ["Technical"], "field": "Vulnerability Analysis & Documentation", "task": "Textual Vulnerability Description Synthesis", "summary": "本文提出一种域约束的大模型合成框架（包含提取、基于领域锚词的自评估和基于信息熵的融合）及 Digest Labels 可视化工具，用以统一并保全来自不同漏洞库中不一致的关键要素，从而提高漏洞描述的完整性与分析效率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16123v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-05 09:56:31"}
{"id": "2511.16092", "title": "The Future of Development Environments with AI Foundation Models: NII Shonan Meeting 222 Report", "abstract": "Generative Artificial Intelligence (GenAI) models are achieving remarkable performance in various tasks, including code generation, testing, code review, and program repair. The ability to increase the level of abstraction away from writing code has the potential to change the Human-AI interaction within the integrated development environment (IDE). To explore the impact of GenAI on IDEs, 33 experts from the Software Engineering, Artificial Intelligence, and Human-Computer Interaction domains gathered to discuss challenges and opportunities at Shonan Meeting 222. This is the report", "arxiv_url": "https://arxiv.org/abs/2511.16092", "authors": ["Xing Hu", "Raula Gaikovina Kula", "Christoph Treude"], "first_author": "Xing Hu", "category": ["Survey"], "field": "Requirements & Design", "task": "Analysis", "summary": "本文为一次Shonan会议报告，综述并讨论了AI基础模型对软件开发环境（IDE）的影响、可能带来的挑战与风险，并提出若干未来研究问题与方向。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16092v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-05 09:56:54"}
{"id": "2511.16708", "title": "Multi-Agent Code Verification via Information Theory", "abstract": "LLMs generate buggy code: 29.6% of SWE-bench solved patches fail, 62% of BaxBench solutions have vulnerabilities, and existing   tools only catch 65% of bugs with 35% false positives. We built CodeX-Verify, a multi-agent system that uses four specialized   agents to detect different types of bugs. We prove mathematically that combining agents with different detection patterns   finds more bugs than any single agent when the agents look for different problems, using submodularity of mutual information   under conditional independence. Measuring agent correlation of rho = 0.05 to 0.25 confirms they detect different bugs. Testing   on 99 code samples with verified labels shows our system catches 76.1% of bugs, matching the best existing method (Meta   Prompt Testing: 75%) while running faster and without test execution. We tested all 15 agent combinations and found that using   multiple agents improves accuracy by 39.7 percentage points (from 32.8% to 72.4%) compared to single agents, with diminishing   returns of +14.9pp, +13.5pp, and +11.2pp for agents 2, 3, and 4, validating our theoretical model. The best two-agent   combination (Correctness + Performance) reaches 79.3% accuracy. Testing on 300 real patches from Claude Sonnet 4.5 runs in   under 200ms per sample, making this practical for production use.", "arxiv_url": "https://arxiv.org/abs/2511.16708", "authors": ["Shreshth Rajan"], "first_author": "Shreshth Rajan", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Defect Prediction", "summary": "本文提出CodeX-Verify，一种由正确性、 安全、 性能与风格四个专用智能体组成的多智能体静态代码验证系统，并基于条件独立下互信息的次模性质证明组合智能体可提高缺陷检测率，实验在99个有验证标签样本上达到76.1%检出率并开源数据集。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16708v2", "published": "2025-11-20", "update_time": "2025-12-02", "download_time": "2025-12-05 09:57:21"}
{"id": "2511.16005", "title": "InfCode-C++: Intent-Guided Semantic Retrieval and AST-Structured Search for C++ Issue Resolution", "abstract": "Large language model (LLM) agents have recently shown strong performance on repository-level issue resolution, but existing systems are almost exclusively designed for Python and rely heavily on lexical retrieval and shallow code navigation. These approaches transfer poorly to C++ projects, where overloaded identifiers, nested namespaces, template instantiations, and deep control-flow structures make context retrieval and fault localization substantially more difficult. As a result, state-of-the-art Python-oriented agents show a drastic performance drop on the C++ subset of MultiSWE-bench. We introduce INFCODE-C++, the first C++-aware autonomous system for end-to-end issue resolution. The system combines two complementary retrieval mechanisms -- semantic code-intent retrieval and deterministic AST-structured querying -- to construct accurate, language-aware context for repair.These components enable precise localization and robust patch synthesis in large, statically typed C++ repositories. Evaluated on the \\texttt{MultiSWE-bench-CPP} benchmark, INFCODE-C++ achieves a resolution rate of 25.58\\%, outperforming the strongest prior agent by 10.85 percentage points and more than doubling the performance of MSWE-agent. Ablation and behavioral studies further demonstrate the critical role of semantic retrieval, structural analysis, and accurate reproduction in C++ issue resolution. INFCODE-C++ highlights the need for language-aware reasoning in multi-language software agents and establishes a foundation for future research on scalable, LLM-driven repair for complex, statically typed ecosystems.", "arxiv_url": "https://arxiv.org/abs/2511.16005", "authors": ["Qingao Dong", "Mengfei Wang", "Hengzhi Zhang", "Zhichao Li", "Yuan Yuan", "Mu Li", "Xiang Gao", "Hailong Sun", "Chunming Hu", "Weifeng Lv"], "first_author": "Qingao Dong", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Bug Repair", "summary": "本文提出InfCode-C++，结合意图引导的语义检索与基于AST的结构化查询，为复杂C++代码库构建语言感知的上下文检索与修复流程，在MultiSWE-bench-CPP上将问题解决率提升至25.58%，显著优于现有代理。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16005v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-05 09:57:47"}
{"id": "2511.16004", "title": "InfCode: Adversarial Iterative Refinement of Tests and Patches for Reliable Software Issue Resolution", "abstract": "Large language models have advanced software engineering automation, yet resolving real-world software issues remains difficult because it requires repository-level reasoning, accurate diagnostics, and strong verification signals. Existing agent-based and pipeline-based methods often rely on insufficient tests, which can lead to patches that satisfy verification but fail to fix the underlying defect. We present InfCode, an adversarial multi-agent framework for automated repository-level issue resolution. InfCode iteratively refines both tests and patches through adversarial interaction between a Test Patch Generator and a Code Patch Generator, while a Selector agent identifies the most reliable fix. The framework runs inside a containerized environment that supports realistic repository inspection, modification, and validation. Experiments on SWE-bench Lite and SWE-bench Verified using models such as DeepSeek-V3 and Claude 4.5 Sonnet show that InfCode consistently outperforms strong baselines. It achieves 79.4% performance on SWE-bench Verified, establishing a new state-of-the-art. We have released InfCode as an open-source project at https://github.com/Tokfinity/InfCode.", "arxiv_url": "https://arxiv.org/abs/2511.16004", "authors": ["KeFan Li", "Mengfei Wang", "Hengzhi Zhang", "Zhichao Li", "Yuan Yuan", "Mu Li", "Xiang Gao", "Hailong Sun", "Chunming Hu", "Weifeng Lv"], "first_author": "KeFan Li", "category": ["Technical"], "field": "Quality Management", "task": "Bug Repair", "summary": "本文提出 InfCode —— 一个在容器化环境中通过测试生成器与代码补丁生成器的对抗迭代、多代理协作来强化仓库级别缺陷定位与修复的框架，并在 SWE-bench 验证集上取得了新的 SOTA（79.4%）。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16004v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-05 09:58:13"}
{"id": "2511.15817", "title": "A Causal Perspective on Measuring, Explaining and Mitigating Smells in LLM-Generated Code", "abstract": "Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.   This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code.", "arxiv_url": "https://arxiv.org/abs/2511.15817", "authors": ["Alejandro Velasco", "Daniel Rodriguez-Cardenas", "Dipin Khati", "David N. Palacio", "Luftar Rahman Alif", "Denys Poshyvanyk"], "first_author": "Alejandro Velasco", "category": ["Empirical", "Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Prompting", "summary": "本文从因果视角验证并应用PSC指标，系统测量、解释并缓解LLM生成代码中的code smells，量化解码策略、模型架构与提示设计的影响并发布基准与数据集。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.15817v2", "published": "2025-11-19", "update_time": "2025-11-21", "download_time": "2025-12-05 09:58:39"}
{"id": "2511.15293", "title": "A Viable Paradigm of Software Automation: Iterative End-to-End Automated Software Development", "abstract": "Software development automation is a long-term goal in software engineering. With the development of artificial intelligence (AI), more and more researchers are exploring approaches to software automation. They view AI systems as tools or assistants in software development, still requiring significant human involvement. Another initiative is ``vibe coding'', where AI systems write and repeatedly revise most (or even all) of the code. We foresee these two development paths will converge towards the same destination: AI systems participate in throughout the software development lifecycle, expanding boundaries of full-stack software development. In this paper, we present a vision of an iterative end-to-end automated software development paradigm AutoSW. It operates in an analyze-plan-implement-deliver loop, where AI systems as human partners become first-class actors, translating human intentions expressed in natural language into executable software. We explore a lightweight prototype across the paradigm and initially execute various representative cases. The results indicate that AutoSW can successfully deliver executable software, providing a feasible direction for truly end-to-end automated software development.", "arxiv_url": "https://arxiv.org/abs/2511.15293", "authors": ["Jia Li", "Zhi Jin", "Huangzhao Zhang", "Kechi Zhang", "Jiaru Qian", "Tiankuo Zhao"], "first_author": "Jia Li", "category": ["Technical"], "field": "Requirements & Design", "task": "Elicitation", "summary": "本文提出了名为AutoSW的迭代端到端自动化软件开发范式，基于协同代理在分析-规划-实现-交付的循环中将AI作为第一类参与者，从自然语言意图出发自动化产生可测试、可部署的软件并支持可追溯的演化。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.15293v2", "published": "2025-11-19", "update_time": "2025-11-23", "download_time": "2025-12-05 09:59:31"}
{"id": "2511.15757", "title": "Rethinking Kernel Program Repair: Benchmarking and Enhancing LLMs with RGym", "abstract": "Large Language Models (LLMs) have revolutionized automated program repair (APR) but current benchmarks like SWE-Bench predominantly focus on userspace applications and overlook the complexities of kernel-space debugging and repair. The Linux kernel poses unique challenges due to its monolithic structure, concurrency, and low-level hardware interactions. Prior efforts such as KGym and CrashFixer have highlighted the difficulty of APR in this domain, reporting low success rates or relying on costly and complex pipelines and pricey cloud infrastructure. In this work, we introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel designed to operate on local commodity hardware. Built on RGym, we propose a simple yet effective APR pipeline leveraging specialized localization techniques (e.g., call stacks and blamed commits) to overcome the unrealistic usage of oracles in KGym. We test on a filtered and verified dataset of 143 bugs. Our method achieves up to a 43.36% pass rate with GPT-5 Thinking while maintaining a cost of under $0.20 per bug. We further conduct an ablation study to analyze contributions from our proposed localization strategy, prompt structure, and model choice, and demonstrate that feedback-based retries can significantly enhance success rates.", "arxiv_url": "https://arxiv.org/abs/2511.15757", "authors": ["Kareem Shehada", "Yifan Wu", "Wyatt D. Feng", "Adithya Iyer", "Gryphon Kumfert", "Yangruibo Ding", "Zhiyun Qian"], "first_author": "Kareem Shehada", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Bug Repair", "summary": "本文提出 RGym——一个可在本地运行的轻量级内核APR评估框架，发布并验证了包含143个KASAN内核漏洞的数据集，并设计了基于bug-inducing commit与调用栈的现实定位策略与简单高效的LLM补丁流水线，实现在低成本下显著提升修复通过率并进行了消融分析。", "quality": "High", "conference": "NeurIPS 2025 Workshop: Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent Abilities, and Scaling 2025", "pdf_url": "https://arxiv.org/pdf/2511.15757v1", "published": "2025-11-19", "update_time": "2025-11-19", "download_time": "2025-12-05 09:59:57"}
{"id": "2511.15168", "title": "Finetuning LLMs for Automatic Form Interaction on Web-Browser in Selenium Testing Framework", "abstract": "Automated web application testing is a critical component of modern software development, with frameworks like Selenium widely adopted for validating functionality through browser automation. Among the essential aspects of such testing is the ability to interact with and validate web forms, a task that requires syntactically correct, executable scripts with high coverage of input fields. Despite its importance, this task remains underexplored in the context of large language models (LLMs), and no public benchmark or dataset exists to evaluate LLMs on form interaction generation systematically. This paper introduces a novel method for training LLMs to generate high-quality test cases in Selenium, specifically targeting form interaction testing. We curate both synthetic and human-annotated datasets for training and evaluation, covering diverse real-world forms and testing scenarios. We define clear metrics for syntax correctness, script executability, and input field coverage. Our empirical study demonstrates that our approach significantly outperforms strong baselines, including GPT-4o and other popular LLMs, across all evaluation metrics. Our work lays the groundwork for future research on LLM-based web testing and provides resources to support ongoing progress in this area.", "arxiv_url": "https://arxiv.org/abs/2511.15168", "authors": ["Nguyen-Khang Le", "Hiep Nguyen", "Ngoc-Minh Nguyen", "Son T. Luu", "Trung Vo", "Quan Minh Bui", "Shoshin Nomura", "Le-Minh Nguyen"], "first_author": "Nguyen-Khang Le", "category": ["Technical", "Benchmark"], "field": "Software Testing", "task": "Test Generation", "summary": "本文提出一种针对Selenium表单交互的LLM微调方法并构建并发布首个表单交互测试数据集，通过合成与人工标注数据训练模型生成语法正确、可执行且覆盖率高的Selenium测试脚本，实验证明在语法正确性、脚本可执行性和输入字段覆盖率上优于多种强基线（含GPT‑4o）。", "quality": "High", "conference": "Proceedings of KSE 2025", "pdf_url": "https://arxiv.org/pdf/2511.15168v2", "published": "2025-11-19", "update_time": "2025-11-20", "download_time": "2025-12-05 10:00:56"}
{"id": "2511.15755", "title": "Multi-Agent LLM Orchestration Achieves Deterministic, High-Quality Decision Support for Incident Response", "abstract": "Large language models (LLMs) promise to accelerate incident response in production systems, yet single-agent approaches generate vague, unusable recommendations. We present MyAntFarm.ai, a reproducible containerized framework demonstrating that multi-agent orchestration fundamentally transforms LLM-based incident response quality. Through 348 controlled trials comparing single-agent copilot versus multi-agent systems on identical incident scenarios, we find that multi-agent orchestration achieves 100% actionable recommendation rate versus 1.7% for single-agent approaches, an 80 times improvement in action specificity and 140 times improvement in solution correctness. Critically, multi-agent systems exhibit zero quality variance across all trials, enabling production SLA commitments impossible with inconsistent single-agent outputs. Both architectures achieve similar comprehension latency (approx.40s), establishing that the architectural value lies in deterministic quality, not speed. We introduce Decision Quality (DQ), a novel metric capturing validity, specificity, and correctness properties essential for operational deployment that existing LLM metrics do not address. These findings reframe multi-agent orchestration from a performance optimization to a production-readiness requirement for LLM-based incident response. All code, Docker configurations, and trial data are publicly available for reproduction.", "arxiv_url": "https://arxiv.org/abs/2511.15755", "authors": ["Philip Drammeh"], "first_author": "Philip Drammeh", "category": ["Technical", "Empirical"], "field": "AIOps", "task": "Incident Response / Decision Support", "summary": "本文通过可复现的容器化框架 MyAntFarm.ai，在348次受控试验中比较单体 LLM 与分工多智能体编排，提出多维决策质量（Decision Quality）指标并证明多智能体在可执行性、具体性与正确性上显著优于单体方案，使其在事故响应场景中实现确定性、高质量的决策支持。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.15755v1", "published": "2025-11-19", "update_time": "2025-11-19", "download_time": "2025-12-05 10:01:17"}
{"id": "2511.19427", "title": "Prompt Less, Smile More: MTP with Semantic Engineering in Lieu of Prompt Engineering", "abstract": "AI-Integrated programming is emerging as a foundational paradigm for building intelligent systems with large language models (LLMs). Recent approaches such as Meaning Typed Programming (MTP) automate prompt generation by leveraging the semantics already present in code. However, many real-world applications depend on contextual cues, developer intent, and domain-specific reasoning that extend beyond what static code semantics alone can express. To address this limitation, we introduce Semantic Engineering, a lightweight method for enriching program semantics so that LLM-based systems can more accurately reflect developer intent without requiring full manual prompt design. We present Semantic Context Annotations (SemTexts), a language-level mechanism that allows developers to embed natural-language context directly into program constructs. Integrated into the Jac programming language, Semantic Engineering extends MTP to incorporate these enriched semantics during prompt generation. We further introduce a benchmark suite designed to reflect realistic AI-Integrated application scenarios. Our evaluation shows that Semantic Engineering substantially improves prompt fidelity, achieving performance comparable to Prompt Engineering while requiring significantly less developer effort.", "arxiv_url": "https://arxiv.org/abs/2511.19427", "authors": ["Jayanaka L. Dantanarayana", "Savini Kashmira", "Thakee Nathees", "Zichen Zhang", "Krisztian Flautner", "Lingjia Tang", "Jason Mars"], "first_author": "Jayanaka L. Dantanarayana", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Prompting", "summary": "本文提出了语义工程并引入语言级的语义上下文注释SemText，将自然语言意图绑定到程序构造中以扩展MTP的中间表示，从而在显著减少开发者手工提示成本的同时将自动生成的提示性能提升到接近或优于传统Prompt Engineering，并通过在Jac中实现及一套更贴近实际场景的基准评测验证其有效性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.19427v1", "published": "2025-11-24", "update_time": "2025-11-24", "download_time": "2025-12-05 10:01:39"}
{"id": "2511.19132", "title": "LLMs-Powered Real-Time Fault Injection: An Approach Toward Intelligent Fault Test Cases Generation", "abstract": "A well-known testing method for the safety evaluation and real-time validation of automotive software systems (ASSs) is Fault Injection (FI). In accordance with the ISO 26262 standard, the faults are introduced artificially for the purpose of analyzing the safety properties and verifying the safety mechanisms during the development phase. However, the current FI method and tools have a significant limitation in that they require manual identification of FI attributes, including fault type, location and time. The more complex the system, the more expensive, time-consuming and labour-intensive the process. To address the aforementioned challenge, a novel Large Language Models (LLMs)-assisted fault test cases (TCs) generation approach for utilization during real-time FI tests is proposed in this paper. To this end, considering the representativeness and coverage criteria, the applicability of various LLMs to create fault TCs from the functional safety requirements (FSRs) has been investigated. Through the validation results of LLMs, the superiority of the proposed approach utilizing gpt-4o in comparison to other state-of-the-art models has been demonstrated. Specifically, the proposed approach exhibits high performance in terms of FSRs classification and fault TCs generation with F1-score of 88% and 97.5%, respectively. To illustrate the proposed approach, the generated fault TCs were executed in real time on a hardware-in-the-loop system, where a high-fidelity automotive system model served as a case study. This novel approach offers a means of optimizing the real-time testing process, thereby reducing costs while simultaneously enhancing the safety properties of complex safety-critical ASSs.", "arxiv_url": "https://arxiv.org/abs/2511.19132", "authors": ["Mohammad Abboush", "Ahmad Hatahet", "Andreas Rausch"], "first_author": "Mohammad Abboush", "category": ["Technical", "Empirical"], "field": "Software Testing", "task": "Test Generation", "summary": "本文提出一种基于大型语言模型（如GPT‑4o）的实时故障注入测试用例生成方法，能够从功能安全需求自动进行传感器/执行器分类并生成单点及并发故障测试用例，并在硬件在环（HIL）仿真中验证，取得分类F1=88%与测试用例生成F1=97.5%的优异结果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.19132v1", "published": "2025-11-24", "update_time": "2025-11-24", "download_time": "2025-12-05 10:02:07"}
{"id": "2511.19422", "title": "SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning", "abstract": "Recent advancements in large language models (LLMs) have shown very impressive capabilities in code generation across many programming languages. However, even state-of-the-art LLMs generate programs that contains syntactic errors and fail to complete the given tasks, especially for low-resource programming languages (LRPLs). In addition, high training cost makes finetuning LLMs unaffordable with constrained computational resources, further undermining the effectiveness of LLMs for code generation. In this work, we propose SLMFix, a novel code generation pipeline that leverages a small language model (SLM) finetuned using reinforcement learning (RL) techniques to fix syntactic errors in LLM-generated programs to improve the quality of LLM-generated programs for domain-specific languages (DSLs). In specific, we applied RL on the SLM for the program repair task using a reward calculated using both a static validator and a static semantic similarity metric. Our experimental results demonstrate the effectiveness and generalizability of our approach across multiple DSLs, achieving more than 95% pass rate on the static validator. Notably, SLMFix brings substantial improvement to the base model and outperforms supervised finetuning approach even for 7B models on a LRPL, showing the potential of our approach as an alternative to traditional finetuning approaches.", "arxiv_url": "https://arxiv.org/abs/2511.19422", "authors": ["David Jiahao Fu", "Aryan Gupta", "Aaron Councilman", "David Grove", "Yu-Xiong Wang", "Vikram Adve"], "first_author": "David Jiahao Fu", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Editing", "summary": "本文提出SLMFix：在不微调大型模型的前提下，利用经强化学习微调的小型语言模型修复LLM生成的DSL代码中的静态错误，并构建了Ansible数据集以验证方法在低资源语言上的有效性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.19422v1", "published": "2025-11-24", "update_time": "2025-11-24", "download_time": "2025-12-05 10:02:30"}
{"id": "2511.19130", "title": "Can LLMs Recover Program Semantics? A Systematic Evaluation with Symbolic Execution", "abstract": "Obfuscation poses a persistent challenge for software engineering tasks such as program comprehension, maintenance, testing, and vulnerability detection. While compiler optimizations and third-party code often introduce transformations that obscure program intent, existing analysis tools and large language models (LLMs) struggle to recover the original semantics. In this work, we investigate whether LLMs, when fine-tuned with symbolic execution artifacts, can effectively deobfuscate programs and restore analyzability. We construct a benchmark by applying four widely studied transformations-control-flow flattening, opaque predicates, arithmetic encoding, and branch encoding-across diverse C programs from TUM Obfuscation Benchmarks, the LLVM test suite, and algorithmic repositories. We then compare three state-of-the-art LLMs under two training configurations: baseline fine-tuning on obfuscated/original code pairs, and enhanced fine-tuning with additional KLEE artifacts such as SMT constraints, path statistics, and test cases. Our evaluation examines syntactic correctness (compilation success), semantic fidelity (behavioral equivalence under symbolic execution), and code quality (readability and structure). Results show that GPT-4.1-mini achieves the strongest deobfuscation overall, and that incorporating KLEE artifacts consistently improves semantic preservation and compilation success across models. These findings highlight deobfuscation as a broader software engineering concern, demonstrating that combining LLMs with symbolic execution can strengthen automated testing, static analysis, and program comprehension in the presence of obfuscation.", "arxiv_url": "https://arxiv.org/abs/2511.19130", "authors": ["Rong Feng", "Suman Saha"], "first_author": "Rong Feng", "category": ["Technical", "Benchmark", "Empirical"], "field": "Program Analysis & Reverse Engineering", "task": "Deobfuscation / Program Semantics Recovery", "summary": "本文构建包含四种常见混淆变换的数据集，并提出将 KLEE 符号执行产生的约束、路径统计和测试用例等工件与 LLM 微调相结合的混合方法以恢复被混淆程序的可编译性与语义一致性，实验证明该方法显著提升了语法正确性、语义保真度和代码可读性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.19130v1", "published": "2025-11-24", "update_time": "2025-11-24", "download_time": "2025-12-05 10:02:56"}
{"id": "2511.20403", "title": "LLMs for Automated Unit Test Generation and Assessment in Java: The AgoneTest Framework", "abstract": "Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly. This paper introduces AgoneTest, an automated evaluation framework for Large Language Model-generated (LLM) unit tests in Java. AgoneTest does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions. We introduce the Classes2Test dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment. Experimental results show that, for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection. Our findings also demonstrate that enhanced prompting strategies contribute to test quality. AgoneTest clarifies the potential of LLMs in software testing and offers insights for future improvements in model design, prompt engineering, and testing practices.", "arxiv_url": "https://arxiv.org/abs/2511.20403", "authors": ["Andrea Lops", "Fedelucio Narducci", "Azzurra Ragone", "Michelantonio Trizio", "Claudio Bartolini"], "first_author": "Andrea Lops", "category": ["Technical", "Benchmark", "Empirical"], "field": "Software Testing", "task": "Test Generation", "summary": "本文提出AGONETEST——一个用于端到端自动化评估LLM生成的Java类级别单元测试的框架，并发布了映射类到测试类的CLASSES2TEST数据集，结合覆盖率、变异分数和测试味等指标对生成测试进行综合评估，实验证明在可编译测试子集上LLM生成的测试在覆盖率和缺陷检测能力上可与人工测试媲美。", "quality": "High", "conference": "ASE 2025", "pdf_url": "https://arxiv.org/pdf/2511.20403v2", "published": "2025-11-25", "update_time": "2025-11-26", "download_time": "2025-12-05 10:03:31"}
{"id": "2511.21382", "title": "Large Language Models for Unit Test Generation: Achievements, Challenges, and the Road Ahead", "abstract": "Unit testing is an essential yet laborious technique for verifying software and mitigating regression risks. Although classic automated methods effectively explore program structures, they often lack the semantic information required to produce realistic inputs and assertions. Large Language Models (LLMs) address this limitation by utilizing by leveraging their data-driven knowledge of code semantics and programming patterns. To analyze the state of the art in this domain, we conducted a systematic literature review of 115 publications published between May 2021 and August 2025. We propose a unified taxonomy based on the unit test generation lifecycle that treats LLMs as stochastic generators requiring systematic engineering constraints. This framework analyzes the literature regarding core generative strategies and a set of enhancement techniques ranging from pre-generation context enrichment to post-generation quality assurance. Our analysis reveals that prompt engineering has emerged as the dominant utilization strategy and accounts for 89% of the studies due to its flexibility. We find that iterative validation and repair loops have become the standard mechanism to ensure robust usability and lead to significant improvements in compilation and execution pass rates. However, critical challenges remain regarding the weak fault detection capabilities of generated tests and the lack of standardized evaluation benchmarks. We conclude with a roadmap for future research that emphasizes the progression towards autonomous testing agents and hybrid systems combining LLMs with traditional software engineering tools. This survey provides researchers and practitioners with a comprehensive perspective on converting the potential of LLMs into industrial-grade testing solutions.", "arxiv_url": "https://arxiv.org/abs/2511.21382", "authors": ["Bei Chu", "Yang Feng", "Kui Liu", "Zifan Nan", "Zhaoqiang Guo", "Baowen Xu"], "first_author": "Bei Chu", "category": ["Survey"], "field": "Software Testing", "task": "Test Generation", "summary": "本文对2021–2025年间115篇关于利用大语言模型生成单元测试的研究进行了系统综述，提出基于生成生命周期的统一分类，梳理了核心策略与增强技术，指出了评估基准不足与缺陷检测能力薄弱等关键挑战并给出未来路线图。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.21382v1", "published": "2025-11-26", "update_time": "2025-11-26", "download_time": "2025-12-05 10:03:51"}
{"id": "2511.21380", "title": "Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions", "abstract": "Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research.", "arxiv_url": "https://arxiv.org/abs/2511.21380", "authors": ["Jingyi Chen", "Xiaoyan Guo", "Songqiang Chen", "Shing-Chi Cheung", "Jiasi Shen"], "first_author": "Jingyi Chen", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Editing", "summary": "本文首次对基于大型语言模型的多智能体系统（以GitHub Copilot agent为代表）在软件工程研究工件的数据集适配任务上进行了系统实证评估，提出五阶段评估流程并发现当前系统能识别关键文件并生成部分适配代码但很少产生功能正确的实现，且提供执行错误信息与参考代码等提示能显著提升生成代码的结构相似度。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.21380v1", "published": "2025-11-26", "update_time": "2025-11-26", "download_time": "2025-12-05 10:04:12"}
{"id": "2511.21022", "title": "Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations", "abstract": "Pre-trained or fine-tuned on large code corpora, Large Language Models (LLMs) have demonstrated strong performance in code completion tasks. However, their embedded knowledge is constrained by the timeliness of training data, which often includes code using deprecated APIs. Consequently, LLMs frequently generate deprecated APIs that will no longer be supported in future versions of third-party libraries. While retraining LLMs on updated codebases could refresh their API knowledge, this approach is computationally expensive. Recently, lightweight model editing methods have emerged to efficiently correct specific knowledge in LLMs. However, it remains unclear whether these methods can effectively update deprecated API knowledge and enable edited models to generate up-to-date APIs. To address this gap, we conduct the first systematic study applying 10 state-of-the-art model editing techniques to update deprecated API knowledge in three LLMs: Qwen2.5-Coder, StarCoder2, and DeepSeek-Coder. We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances. Our results show that the parameter-efficient fine-tuning method AdaLoRA achieves the best performance in enabling edited models to generate correct, up-to-date APIs, but falls short in Specificity (i.e., the editing influences untargeted knowledge). To resolve this, we propose AdaLoRA-L, which defines \"Common API Layers\" (layers within the LLMs with high importance across all APIs, storing general knowledge and excluded from editing) and restricts edits exclusively to \"Specific API Layers\" (layers with high importance only for the target API, storing the API-specific knowledge). Experimental results demonstrate that AdaLoRA-L significantly improves Specificity while maintaining comparable performance across other evaluation metrics.", "arxiv_url": "https://arxiv.org/abs/2511.21022", "authors": ["Guancheng Lin", "Xiao Yu", "Jacky Keung", "Xing Hu", "Xin Xia", "Alex X. Liu"], "first_author": "Guancheng Lin", "category": ["Empirical", "Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文构建了用于评估过时API知识编辑的基准EDAPIBench、比较了10种轻量级模型编辑方法在三款代码模型上的表现，并提出AdaLoRA-L通过区分“通用API层”和“特定API层”显著提高编辑的特异性同时保持效果与泛化能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.21022v1", "published": "2025-11-26", "update_time": "2025-11-26", "download_time": "2025-12-05 10:04:39"}
{"id": "2511.20933", "title": "Hierarchical Evaluation of Software Design Capabilities of Large Language Models of Code", "abstract": "Large language models (LLMs) are being increasingly adopted in the software engineering domain, yet the robustness of their grasp on core software design concepts remains unclear. We conduct an empirical study to systematically evaluate their understanding of cohesion (intra-module) and coupling (inter-module). We programmatically generate poorly designed code fragments and test the DeepSeek-R1 model family ($14$B, $32$B, $70$B) under varying levels of guidance, from simple \\textit{Verification} to \\textit{Guided} and \\textit{Open-ended Generation}, while varying contextual noise by injecting distractor elements. While models exhibit a solid baseline understanding of both concepts in ideal conditions, their practical knowledge is fragile and highly asymmetrical. Reasoning about coupling proves brittle; performance collapses in noisy, open-ended scenarios, with F1 scores dropping by over $50\\%$. In contrast, the models' analysis of cohesion is remarkably robust to internal noise in guided tasks, showing little performance degradation. However, this resilience also fails when all guidance is removed. Reasoning-trace analysis confirms these failure modes, revealing \\textit{cognitive shortcutting} for coupling versus a more exhaustive (yet still failing) analysis for cohesion. To summarize, while LLMs can provide reliable assistance for recognizing design flaws, their ability to reason autonomously in noisy, realistic contexts is limited, highlighting the critical need for more scalable and robust program understanding capabilities.", "arxiv_url": "https://arxiv.org/abs/2511.20933", "authors": ["Mootez Saad", "Boqi Chen", "José Antonio Hernández López", "Dániel Varró", "Tushar Sharma"], "first_author": "Mootez Saad", "category": ["Benchmark", "Empirical"], "field": "Requirements & Design", "task": "Analysis", "summary": "本文提出了一个可控的分层评估基准与方法，系统生成带有内聚与耦合缺陷的代码片段并在不同引导级别与噪声条件下评测DeepSeek‑R1系列模型，发现在噪声与开放式任务下耦合推理高度脆弱而内聚分析相对稳健。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.20933v1", "published": "2025-11-25", "update_time": "2025-11-25", "download_time": "2025-12-05 10:05:01"}
{"id": "2511.21197", "title": "Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools", "abstract": "AI-assisted tools support developers in performing cognitively demanding tasks such as bug detection and code readability assessment. Despite the advancements in the technical characteristics of these tools, little is known about how developers mentally model them and how mismatches affect trust, control, and adoption. We conducted six co-design workshops with 58 developers to elicit their mental models about AI-assisted bug detection and readability features. It emerged that developers conceive bug detection tools as \\textit{bug detectives}, which warn users only in case of critical issues, guaranteeing transparency, actionable feedback, and confidence cues. Readability assessment tools, on the other hand, are envisioned as \\textit{quality coaches}, which provide contextual, personalized, and progressive guidance. Trust, in both tasks, depends on the clarity of explanations, timing, and user control. A set of design principles for Human-Centered AI in IDEs has been distilled, aiming to balance disruption with support, conciseness with depth, and automation with human agency.", "arxiv_url": "https://arxiv.org/abs/2511.21197", "authors": ["Paolo Buono", "Mary Cerullo", "Stefano Cirillo", "Giuseppe Desolda", "Francesco Greco", "Emanuela Guglielmi", "Grazia Margarella", "Giuseppe Polese", "Simone Scalabrino", "Cesare Tucci"], "first_author": "Paolo Buono", "category": ["Empirical"], "field": "Quality Management", "task": "Defect Prediction", "summary": "本文通过六场涵盖58名开发者的共创工作坊，探讨了开发者对IDE中AI辅助缺陷检测与代码可读性评估工具的心智模型，提出“Bug Detective”和“Quality Coach”两种隐喻并归纳出以解释清晰、时机与可控性为核心的设计原则以增强信任与协作。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.21197v1", "published": "2025-11-26", "update_time": "2025-11-26", "download_time": "2025-12-05 10:05:26"}
{"id": "2511.19875", "title": "CodeFuse-CommitEval: Towards Benchmarking LLM's Power on Commit Message and Code Change Inconsistency Detection", "abstract": "Version control relies on commit messages to convey the rationale for code changes, but these messages are often low quality and, more critically, inconsistent with their diffs-known as message-code inconsistency (MCI). MCIs mislead reviewers, hinder maintenance, contaminate research datasets, and may obscure security patches. Yet, no dedicated benchmark exists to evaluate models for MCI detection. We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models (LLMs). Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits and apply two-fold validation to verify both positive and negative samples. Using this labeled dataset of message-diff pairs, we evaluate six state-of-the-art open-source LLMs under a vanilla setting and with three augmentation strategies: few-shot prompting, chain-of-thought, and extended context. Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%); gpt-oss-20B performs best overall but uses over twice the tokens of others. Augmentation effects vary: adjacent context helps larger models but adds noise for smaller ones; few-shot improves accuracy and reduces token use, yet increases universally incorrect predictions; chain-of-thought boosts precision and specificity at the cost of recall and higher token consumption. Type-wise analysis reveals higher detectability for component, file-path, and operation inconsistencies, but lower accuracy and higher token cost for intent-level \"purpose\" inconsistencies. CODEFUSE-COMMITEVAL provides a rigorous foundation for measuring, comparing, and advancing MCI detection, highlighting the need for richer context and balanced data to capture high-level semantic gaps.", "arxiv_url": "https://arxiv.org/abs/2511.19875", "authors": ["Qingyu Zhang", "Puzhuo Liu", "Peng Di", "Chenxiong Qian"], "first_author": "Qingyu Zhang", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Alignment", "summary": "本文提出了CODEFUSE-COMMITEVAL——基于ApacheCM并通过规则化突变与双重验证构建的首个用于提交信息与代码差异不一致（MCI）检测的基准数据集，并在六个开源大模型与多种增强策略下进行了系统评估与类型分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.19875v1", "published": "2025-11-25", "update_time": "2025-11-25", "download_time": "2025-12-05 10:05:50"}
{"id": "2511.20709", "title": "DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation", "abstract": "Large language models (LLMs) and autonomous coding agents are increasingly used to generate software across a wide range of domains. Yet a core requirement remains unmet: ensuring that generated code is secure without compromising its functional correctness. Existing benchmarks and evaluations for secure code generation fall short-many measure only vulnerability reduction, disregard correctness preservation, or evaluate security and functionality on separate datasets, violating the fundamental need for simultaneous joint evaluation. We present DUALGAUGE, the first fully automated benchmarking framework designed to rigorously evaluate the security and correctness of LLM-generated code in unison. Given the lack of datasets enabling joint evaluation of secure code generation, we also present DUALGAUGE-BENCH, a curated benchmark suite of diverse coding tasks, each paired with manually validated test suites for both security and functionality, designed for full coverage of specification requirements. At the core of DUALGAUGE is an agentic program executor, which runs a program against given tests in sandboxed environments, and an LLM-based evaluator, which assesses both correctness and vulnerability behavior against expected outcomes. We rigorously evaluated and ensured the quality of DUALGAUGE-BENCH and the accuracy of DUALGAUGE, and applied DUALGAUGE to benchmarking ten leading LLMs on DUALGAUGE-BENCH across thousands of test scenarios. Our results reveal critical gaps in correct and secure code generation by these LLMs, for which our open-source system and datasets help accelerate progress via reproducible, scalable, and rigorous evaluation.", "arxiv_url": "https://arxiv.org/abs/2511.20709", "authors": ["Abhijeet Pathak", "Suvadra Barua", "Dinesh Gudimetla", "Rupam Patir", "Jiawei Guo", "Hongxin Hu", "Haipeng Cai"], "first_author": "Abhijeet Pathak", "category": ["Technical", "Benchmark", "Empirical"], "field": "Software Testing", "task": "Testing automation", "summary": "本文提出DUALGAUGE及DUALGAUGE-BENCH，一个自动化的联合功能与安全性基准测试框架与覆盖驱动的测试套件，通过沙箱执行、依赖自动解析和基于LLM的语义评估来同时评估LLM生成代码的正确性与可被利用的漏洞，并对多款主流模型进行了大规模测试揭示关键缺陷。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.20709v1", "published": "2025-11-24", "update_time": "2025-11-24", "download_time": "2025-12-05 10:06:15"}
{"id": "2511.19635", "title": "Agint: Agentic Graph Compilation for Software Engineering Agents", "abstract": "LLM-based coding agents are increasingly common but still face challenges in context management, latency, reliability, reproducibility, and scalability. We present Agint, an agentic graph compiler, interpreter, and runtime that incrementally and hierarchically converts natural-language instructions into typed, effect-aware code DAGs. Agint introduces explicit type floors (text to data to spec to code) grounded in semantic graph transformations and a hybrid LLM and function-based JIT runtime. This enables dynamic graph refinement, reproducible and optimizable execution, speculative evaluation, and interoperability with existing developer tools. Agint's typed graph bindings improve reliability and allow concurrent composition of concurrent codebases by construction, supporting accelerated development with smaller and faster models, lower latency, efficient context utilization, and higher throughput. Hierarchical compilation allows scalable graph edits, while the graph structure supports reproducibility and efficient parallel generation. Agint provides a composable unix-style toolchain: dagify (DAG compiler), dagent (hybrid JIT runtime), schemagin (schema generator), and datagin (data transformer) for realtime, low-latency code and dataflow creation. Human developers and coding agents refine graphs through the Agint CLI, while non-technical users use Agint Flow GUI for visual editing, conversational refinement, and debugging to promote prototype agentic workflows to production code. This continuous co-creation model allows teams to prototype quickly, refine seamlessly, and deploy reliably, bridging natural language, compiler methods, and developer tooling to enable a new generation of composable, team-centric coding agents at scale.", "arxiv_url": "https://arxiv.org/abs/2511.19635", "authors": ["Abhi Chivukula", "Jay Somasundaram", "Vijay Somasundaram"], "first_author": "Abhi Chivukula", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出Agint——一种将自然语言意图逐步编译为类型化、具效应感知的代码DAG并通过混合LLM/函数JIT运行时进行增量解析、并行执行与可重现部署的代理图编译器与运行时，旨在提高代码生成的可靠性、低延迟和可组合性。", "quality": "High", "conference": "NeurIPS Workshop: Deep Learning for Code in the Agentic Era 2025", "pdf_url": "https://arxiv.org/pdf/2511.19635v1", "published": "2025-11-24", "update_time": "2025-11-24", "download_time": "2025-12-05 10:06:46"}
{"id": "2511.23408", "title": "Evaluating LLMs for One-Shot Patching of Real and Artificial Vulnerabilities", "abstract": "Automated vulnerability patching is crucial for software security, and recent advancements in Large Language Models (LLMs) present promising capabilities for automating this task. However, existing research has primarily assessed LLMs using publicly disclosed vulnerabilities, leaving their effectiveness on related artificial vulnerabilities largely unexplored. In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs, such as OpenAI's GPT variants, LLaMA, DeepSeek, and Mistral models, using both real and artificial vulnerabilities. Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities. Our results reveal that LLMs patch real vulnerabilities more effectively compared to artificial ones. Additionally, our analysis reveals significant variability across LLMs in terms of overlapping (multiple LLMs patching the same vulnerabilities) and complementarity (vulnerabilities patched exclusively by a single LLM), emphasizing the importance of selecting appropriate LLMs for effective vulnerability patching.", "arxiv_url": "https://arxiv.org/abs/2511.23408", "authors": ["Aayush Garg", "Zanis Ali Khan", "Renzo Degiovanni", "Qiang Tang"], "first_author": "Aayush Garg", "category": ["Empirical"], "field": "Quality Management", "task": "Vulnerability Repair", "summary": "本文通过对14款主流LLM在15个真实漏洞及41个对应人工漏洞上进行一次性补丁生成并以PoV测试执行验证，发现LLM对真实漏洞的修复效果优于人工漏洞且不同模型在修复上存在显著的互补性与重叠性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.23408v1", "published": "2025-11-28", "update_time": "2025-11-28", "download_time": "2025-12-05 10:07:12"}
{"id": "2511.23321", "title": "Chart2Code-MoLA: Efficient Multi-Modal Code Generation via Adaptive Expert Routing", "abstract": "Chart-to-code generation is a critical task in automated data visualization, translating complex chart structures into executable programs. While recent Multi-modal Large Language Models (MLLMs) improve chart representation, existing approaches still struggle to achieve cross-type generalization, memory efficiency, and modular design. To address these challenges, this paper proposes C2C-MoLA, a multimodal framework that synergizes Mixture of Experts (MoE) with Low-Rank Adaptation (LoRA). The MoE component uses a complexity-aware routing mechanism with domain-specialized experts and load-balanced sparse gating, dynamically allocating inputs based on learnable structural metrics like element count and chart complexity. LoRA enables parameter-efficient updates for resource-conscious tuning, further supported by a tailored training strategy that aligns routing stability with semantic accuracy. Experiments on Chart2Code-160k show that the proposed model improves generation accuracy by up to 17%, reduces peak GPU memory by 18%, and accelerates convergence by 20%, when compared to standard fine-tuning and LoRA-only baselines, particularly on complex charts. Ablation studies validate optimal designs, such as 8 experts and rank-8 LoRA, and confirm scalability for real-world multimodal code generation.", "arxiv_url": "https://arxiv.org/abs/2511.23321", "authors": ["Yifei Wang", "Jacky Keung", "Zhenyu Mao", "Jingyu Zhang", "Yuchen Cao"], "first_author": "Yifei Wang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Translation", "summary": "本文提出C2C-MoLA，一种将Mixture of Experts与LoRA结合的多模态图表到代码生成框架，通过复杂度感知的动态路由、参数高效微调与语法语义联合训练在Chart2Code-160k上显著提升生成准确率、降低显存占用并加速收敛。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.23321v1", "published": "2025-11-28", "update_time": "2025-11-28", "download_time": "2025-12-05 10:07:36"}
{"id": "2401.12554", "title": "Can Large Language Models Write Parallel Code?", "abstract": "Large language models are increasingly becoming a popular tool for software development. Their ability to model and generate source code has been demonstrated in a variety of contexts, including code completion, summarization, translation, and lookup. However, they often struggle to generate code for complex programs. In this paper, we study the capabilities of state-of-the-art language models to generate parallel code. In order to evaluate language models, we create a benchmark, ParEval, consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing. We use ParEval to evaluate the effectiveness of several state-of-the-art open- and closed-source language models on these tasks. We introduce novel metrics for evaluating the performance of generated code, and use them to explore how well each large language model performs for 12 different computational problem types and six different parallel programming models.", "arxiv_url": "https://arxiv.org/abs/2401.12554", "authors": ["Daniel Nichols", "Joshua H. Davis", "Zhaojun Xie", "Arjun Rajaram", "Abhinav Bhatele"], "first_author": "Daniel Nichols", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出了用于并行/HPC代码生成与翻译的基准ParEval、以及新的性能评估指标（speedup_n@k 和 efficiency_n@k），并用该基准评估多种开源与闭源大模型，发现模型普遍难以生成高质量并行代码，尤其在MPI和稀疏/非结构化问题上表现最差。", "quality": "High", "conference": "HPDC 2024", "pdf_url": "https://arxiv.org/pdf/2401.12554v3", "published": "2024-01-23", "update_time": "2024-05-14", "download_time": "2025-12-05 10:08:05"}
{"id": "2401.01062", "title": "Experimenting a New Programming Practice with LLMs", "abstract": "The recent development on large language models makes automatically constructing small programs possible. It thus has the potential to free software engineers from low-level coding and allow us to focus on the perhaps more interesting parts of software development, such as requirement engineering and system testing. In this project, we develop a prototype named AISD (AI-aided Software Development), which is capable of taking high-level (potentially vague) user requirements as inputs, generates detailed use cases, prototype system designs, and subsequently system implementation. Different from existing attempts, AISD is designed to keep the user in the loop, i.e., by repeatedly taking user feedback on use cases, high-level system designs, and prototype implementations through system testing. AISD has been evaluated with a novel benchmark of non-trivial software projects. The experimental results suggest that it might be possible to imagine a future where software engineering is reduced to requirement engineering and system testing only.", "arxiv_url": "https://arxiv.org/abs/2401.01062", "authors": ["Simiao Zhang", "Jiaping Wang", "Guoliang Dong", "Jun Sun", "Yueling Zhang", "Geguang Pu"], "first_author": "Simiao Zhang", "category": ["Technical", "Benchmark"], "field": "Requirements & Design", "task": "Elicitation", "summary": "本文提出AISD，一个以保持用户全程参与的LLM驱动软件开发框架，并构建了用于评估自动化软件开发能力的新基准CAASD，实验证明在用例生成、设计迭代与原型测试下对比现有方法有显著提升。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2401.01062v1", "published": "2024-01-02", "update_time": "2024-01-02", "download_time": "2025-12-05 10:08:32"}
{"id": "2512.01010", "title": "Chain of Unit-Physics: A Primitive-Centric Approach to Scientific Code Synthesis", "abstract": "Agentic large language models are proposed as autonomous code generators for scientific computing, yet their reliability in high-stakes problems remains unclear. Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community. To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation. The framework is evaluated on a nontrivial combustion task, used here as a representative benchmark for scientific problem with realistic physical constraints. Closed-weight systems and code-focused agentic variants fail to produce correct end-to-end solvers, despite tool and web access, exhibiting four recurrent error classes: interface (syntax/API) hallucinations, overconfident assumptions, numerical/physical incoherence, and configuration fragility. Open-weight models with chain-of-thought (CoT) decoding reduce interface errors but still yield incorrect solutions. On the benchmark task, the proposed framework converges within 5-6 iterations, matches the human-expert implementation (mean error of $3.1\\times10^{-3}$ %), with a $\\sim$33.4 % faster runtime and a $\\sim$30 % efficient memory usage at a cost comparable to mid-sized commercial APIs, yielding a practical template for physics-grounded scientific code generation. As datasets and models evolve, zero-shot code accuracy will improve; however, the Chain of Unit-Physics framework goes further by embedding first-principles analysis that is foundational to scientific codes.", "arxiv_url": "https://arxiv.org/abs/2512.01010", "authors": ["Vansh Sharma", "Venkat Raman"], "first_author": "Vansh Sharma", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "summary": "本文提出 Chain of Unit-Physics 框架，通过人类专家编写的基于第一性原理的“单元物理”可测试约束与多智能体迭代生成/验证流程，显著提升基于LLM的科学计算代码合成的物理一致性与可靠性，并在一个12自由度燃烧求解器任务上达到接近人工实现的精度同时提升性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.01010v1", "published": "2025-11-30", "update_time": "2025-11-30", "download_time": "2025-12-05 10:08:59"}
{"id": "2512.00867", "title": "The AI Attribution Paradox: Transparency as Social Strategy in Open-Source Software Development", "abstract": "AI coding assistants have transformed software development, raising questions about transparency and attribution practices. We examine the \"AI attribution paradox\": how developers strategically balance acknowledging AI assistance with managing community scrutiny. Analyzing 14,300 GitHub commits across 7,393 repositories from 2023-2025, we investigated attribution strategies and community responses across eight major AI tools. Results reveal widespread AI usage (95.2% of commits) but strategic attribution: only 29.5% employ explicit disclosure, with dramatic tool variation (Claude 80.5% versus Copilot 9.0%). Explicit attribution triggers modest scrutiny (23% more questions and 21% more comments) but tool choice matters 20-30 times more for predicting reception. Community sentiment remains neutral regardless of attribution type, suggesting curiosity rather than hostility. Temporal analyses show rapid norm evolution: explicit attribution increased from near-zero in early 2024 to 40% by late 2025, indicating community adaptation. These findings illuminate attribution as strategic communication rather than simple transparency, advancing understanding of algorithmic accountability and norm formation during technological transitions. We discuss implications for developers navigating disclosure decisions, platforms designing attribution mechanisms, and researchers studying emergent practices in AI-augmented collaborative work.", "arxiv_url": "https://arxiv.org/abs/2512.00867", "authors": ["Obada Kraishan"], "first_author": "Obada Kraishan", "category": ["Empirical"], "field": "Human-AI Collaboration", "task": "Attribution & Transparency in Open-Source Software", "summary": "基于对2023–2025年14,300次GitHub提交的实证分析，本文揭示了开发者在开源项目中对AI生成代码的归属披露是一种战略性沟通（“AI归属悖论”），并比较了不同工具、社区响应及其随时间的演进。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.00867v1", "published": "2025-11-30", "update_time": "2025-11-30", "download_time": "2025-12-05 10:09:31"}
{"id": "2512.01939", "title": "An Empirical Study of Agent Developer Practices in AI Agent Frameworks", "abstract": "The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers.", "arxiv_url": "https://arxiv.org/abs/2512.01939", "authors": ["Yanlin Wang", "Xinyi Xu", "Jiachi Chen", "Tingting Bi", "Wenchao Gu", "Zibin Zheng"], "first_author": "Yanlin Wang", "category": ["Empirical"], "field": "Requirements & Design", "task": "Management", "summary": "本文通过对1,575个LLM代理项目及11,910条开发者讨论进行大规模挖掘与分析，提出了基于SDLC的代理开发挑战分类并比较了十个主流代理框架在学习成本、开发效率、功能抽象、性能优化与可维护性五个维度的差异与优劣。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.01939v1", "published": "2025-12-01", "update_time": "2025-12-01", "download_time": "2025-12-05 10:10:06"}
{"id": "2512.01690", "title": "Generating REST API Tests With Descriptive Names", "abstract": "Automated test generation has become a key technique for ensuring software quality, particularly in modern API-based architectures. However, automatically generated test cases are typically assigned non-descriptive names (e.g., test0, test1), which reduces their readability and hinders their usefulness during comprehension and maintenance. In this work, we present three novel deterministic techniques to generate REST API test names. We then compare eight techniques in total for generating descriptive names for REST API tests automatically produced by the fuzzer EvoMaster, using 10 test cases generated for 9 different open-source APIs. The eight techniques include rule-based heuristics and large language model (LLM)-based approaches. Their effectiveness was empirically evaluated through two surveys (involving up to 39 people recruited via LinkedIn). Our results show that a rule-based approach achieves the highest clarity ratings among deterministic methods, performs on par with state-of-the-art LLM-based models such as Gemini and GPT-4o, and significantly outperforms GPT-3.5.   To further evaluate the practical impact of our results, an industrial case study was carried out with practitioners who actively use EvoMaster at Volkswagen AG. A developer questionnaire was then carried out based on the use of EvoMaster on four different APIs by four different users, for a total of 74 evaluated test cases. Feedback from practitioners further confirms that descriptive names produced by this approach improve test suite readability.   These findings highlight that lightweight, deterministic techniques can serve as effective alternatives to computationally expensive and security-sensitive LLM-based approaches for automated system-level test naming, providing a practical step toward more developer-friendly API test generation.", "arxiv_url": "https://arxiv.org/abs/2512.01690", "authors": ["Philip Garrett", "Juan P. Galeotti", "Andrea Arcuri", "Alexander Poth", "Olsi Rrjolli"], "first_author": "Philip Garrett", "category": ["Technical", "Empirical"], "field": "Software Testing", "task": "Test Generation", "summary": "本文提出三种确定性方法为自动生成的REST API测试用例生成描述性名称，并通过用户调研（39名参与者）与大众汽车的工业案例比较规则与LLM方法，结果表明基于规则的方法在可读性上与先进LLM（如Gemini、GPT-4o）相当且已集成至EvoMaster作为默认策略。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.01690v1", "published": "2025-12-01", "update_time": "2025-12-01", "download_time": "2025-12-05 10:10:33"}
{"id": "2503.01449", "title": "Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection", "abstract": "Recent advancements in generative AI have led to the widespread adoption of large language models (LLMs) in software engineering, addressing numerous long-standing challenges. However, a comprehensive study examining the capabilities of LLMs in software vulnerability detection (SVD), a crucial aspect of software security, is currently lacking. Existing research primarily focuses on evaluating LLMs using C/C++ datasets. It typically explores only one or two strategies among prompt engineering, instruction tuning, and sequence classification fine-tuning for open-source LLMs. Consequently, there is a significant knowledge gap regarding the effectiveness of diverse LLMs in detecting vulnerabilities across various programming languages. To address this knowledge gap, we present a comprehensive empirical study evaluating the performance of LLMs on the SVD task. We have compiled a comprehensive dataset comprising 8,260 vulnerable functions in Python, 7,505 in Java, and 28,983 in JavaScript. We assess five open-source LLMs using multiple approaches, including prompt engineering, instruction tuning, and sequence classification fine-tuning. These LLMs are benchmarked against five fine-tuned small language models and two open-source static application security testing tools. Furthermore, we explore two avenues to improve LLM performance on SVD: a) Data perspective: Retraining models using downsampled balanced datasets. b) Model perspective: Investigating ensemble learning methods that combine predictions from multiple LLMs. Our comprehensive experiments demonstrate that SVD remains a challenging task for LLMs. This study provides a thorough understanding of the role of LLMs in SVD and offers practical insights for future advancements in leveraging generative AI to enhance software security practices.", "arxiv_url": "https://arxiv.org/abs/2503.01449", "authors": ["Ting Zhang", "Chengran Yang", "Yindu Su", "Martin Weyssow", "Hung Nguyen", "Tan Bui", "Hong Jin Kang", "Yikun Li", "Eng Lieh Ouh", "Lwin Khin Shar", "David Lo"], "first_author": "Ting Zhang", "category": ["Empirical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "summary": "本文构建了包含 Python、Java、JavaScript 函数级漏洞的大规模数据集，并对五个开源大模型在提示工程、指令微调和序列分类微调下进行全面基准评测，比较其与小型模型和静态扫描工具的表现，同时探索通过均衡重采样与模型集成提升检测效果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2503.01449v1", "published": "2025-03-03", "update_time": "2025-03-03", "download_time": "2025-12-05 10:10:55"}
{"id": "2512.01609", "title": "GPTrace: Effective Crash Deduplication Using LLM Embeddings", "abstract": "Fuzzing is a highly effective method for uncovering software vulnerabilities, but analyzing the resulting data typically requires substantial manual effort. This is amplified by the fact that fuzzing campaigns often find a large number of crashing inputs, many of which share the same underlying bug. Crash deduplication is the task of finding such duplicate crashing inputs and thereby reducing the data that needs to be examined. Many existing deduplication approaches rely on comparing stack traces or other information that is collected when a program crashes. Although various metrics for measuring the similarity of such pieces of information have been proposed, many do not yield satisfactory deduplication results. In this work, we present GPTrace, a deduplication workflow that leverages a large language model to evaluate the similarity of various data sources associated with crashes by computing embedding vectors and supplying those as input to a clustering algorithm. We evaluate our approach on over 300 000 crashing inputs belonging to 50 ground truth labels from 14 different targets. The deduplication results produced by GPTrace show a noticeable improvement over hand-crafted stack trace comparison methods and even more complex state-of-the-art approaches that are less flexible.", "arxiv_url": "https://arxiv.org/abs/2512.01609", "authors": ["Patrick Herter", "Vincent Ahlrichs", "Ridvan Açilan", "Julian Horsch"], "first_author": "Patrick Herter", "category": ["Technical"], "field": "Software Testing", "task": "Testing automation", "summary": "GPTrace提出一种利用大语言模型生成堆栈跟踪和ASan报告的嵌入向量并对其进行聚类的崩溃去重工作流，从而在多目标大规模模糊测试崩溃集上显著优于现有基于栈跟踪的去重方法。", "quality": "High", "conference": "ICSE 2026", "pdf_url": "https://arxiv.org/pdf/2512.01609v1", "published": "2025-12-01", "update_time": "2025-12-01", "download_time": "2025-12-05 10:11:28"}
{"id": "2512.01396", "title": "BackportBench: A Multilingual Benchmark for Automated Backporting of Patches", "abstract": "Many modern software projects evolve rapidly to incorporate new features and security patches. It is important for users to update their dependencies to safer versions, but many still use older, vulnerable package versions because upgrading can be difficult and may break their existing codebase. Software developers can mitigate this problem by backporting security patches to older releases. However, manually backporting is time-consuming and error-prone. The effectiveness of existing automated backporting techniques on general software remains unclear since they typically target only code-hunk or function-level patch porting scenarios and are evaluated with imperfect metrics.   To facilitate the development and evaluation of automated backporting techniques, we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem. BackportBench is a multilingual benchmark that contains 202 patch backporting problems from PyPI, Maven, and npm, each with executable Docker environments and relevant test cases. We evaluated existing patch porting methods and LLM-based techniques that have the potential to adapt to this task using BackportBench. The results show that the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes. However, the performance varies across different programming languages. Based on the findings, we draw several implications for researchers and software practitioners in future work on automated backporting.", "arxiv_url": "https://arxiv.org/abs/2512.01396", "authors": ["Zhiqing Zhong", "Jiaming Huang", "Pinjia He"], "first_author": "Zhiqing Zhong", "category": ["Benchmark", "Empirical"], "field": "Maintenance", "task": "Patch Backporting", "summary": "本文提出 BackportBench——一个包含来自 PyPI、Maven 和 npm 的 202 个多语言可执行补丁回移任务的基准，并用该基准评估现有补丁移植方法与多种 LLM 技术以分析其在不同语言与场景下的表现差异。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.01396v1", "published": "2025-12-01", "update_time": "2025-12-01", "download_time": "2025-12-05 10:11:49"}
{"id": "2512.01356", "title": "LAURA: Enhancing Code Review Generation with Context-Enriched Retrieval-Augmented LLM", "abstract": "Code review is critical for ensuring software quality and maintainability. With the rapid growth in software scale and complexity, code review has become a bottleneck in the development process because of its time-consuming and knowledge-intensive nature and the shortage of experienced developers willing to review code. Several approaches have been proposed for automatically generating code reviews based on retrieval, neural machine translation, pre-trained models, or large language models (LLMs). These approaches mainly leverage historical code changes and review comments. However, a large amount of crucial information for code review, such as the context of code changes and prior review knowledge, has been overlooked. This paper proposes an LLM-based review knowledge-augmented, context-aware framework for code review generation, named LAURA. The framework integrates review exemplar retrieval, context augmentation, and systematic guidance to enhance the performance of ChatGPT-4o and DeepSeek v3 in generating code review comments. Besides, given the extensive low-quality reviews in existing datasets, we also constructed a high-quality dataset. Experimental results show that for both models, LAURA generates review comments that are either completely correct or at least helpful to developers in 42.2% and 40.4% of cases, respectively, significantly outperforming SOTA baselines. Furthermore, our ablation studies demonstrate that all components of LAURA contribute positively to improving comment quality.", "arxiv_url": "https://arxiv.org/abs/2512.01356", "authors": ["Yuxin Zhang", "Yuxia Zhang", "Zeyu Sun", "Yanjie Jiang", "Hui Liu"], "first_author": "Yuxin Zhang", "category": ["Technical", "Benchmark"], "field": "Maintenance", "task": "Code Review", "summary": "本文提出LAURA框架，通过上下文增强、相似评审示例检索与系统化引导对LLM进行检索增强生成代码评审，并构建了包含301,256条高质量diff‑comment序列的数据集，从而显著提升了ChatGPT-4o和DeepSeek v3的评审质量。", "quality": "High", "conference": "ASE 2025", "pdf_url": "https://arxiv.org/pdf/2512.01356v1", "published": "2025-12-01", "update_time": "2025-12-01", "download_time": "2025-12-05 10:12:12"}
{"id": "2512.01255", "title": "Large Language Models Cannot Reliably Detect Vulnerabilities in JavaScript: The First Systematic Benchmark and Evaluation", "abstract": "Researchers have proposed numerous methods to detect vulnerabilities in JavaScript, especially those assisted by Large Language Models (LLMs). However, the actual capability of LLMs in JavaScript vulnerability detection remains questionable, necessitating systematic evaluation and comprehensive benchmarks. Unfortunately, existing benchmarks suffer from three critical limitations: (1) incomplete coverage, such as covering a limited subset of CWE types; (2) underestimation of LLM capabilities caused by unreasonable ground truth labeling; and (3) overestimation due to unrealistic cases such as using isolated vulnerable files rather than complete projects.   In this paper, we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection that directly address these limitations: (1) comprehensiveness, (2) no underestimation, and (3) no overestimation. Guided by these principles, we propose FORGEJS, the first automatic benchmark generation framework for evaluating LLMs' capability in JavaScript vulnerability detection. Then, we use FORGEJS to construct ARENAJS-the first systematic benchmark for LLM-based JavaScript vulnerability detection-and further propose JUDGEJS, an automatic evaluation framework.   We conduct the first systematic evaluation of LLMs for JavaScript vulnerability detection, leveraging JUDGEJS to assess seven popular commercial LLMs on ARENAJS. The results show that LLMs not only exhibit limited reasoning capabilities, but also suffer from severe robustness defects, indicating that reliable JavaScript vulnerability detection with LLMs remains an open challenge.", "arxiv_url": "https://arxiv.org/abs/2512.01255", "authors": ["Qingyuan Fei", "Xin Liu", "Song Li", "Shujiang Wu", "Jianwei Hou", "Ping Chen", "Zifeng Kang"], "first_author": "Qingyuan Fei", "category": ["Benchmark", "Empirical", "Technical"], "field": "Quality Management", "task": "Vulnerability Detection", "summary": "本文提出了面向JavaScript漏洞检测的自动基准生成框架FORGEJS，构建了系统性基准ARENAJS和自动评估框架JUDGEJS，并使用其对多款主流商用LLM进行系统评估，揭示了模型在推理能力、鲁棒性和可部署性方面的显著缺陷。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.01255v1", "published": "2025-12-01", "update_time": "2025-12-01", "download_time": "2025-12-05 10:12:36"}
{"id": "2512.02795", "title": "Towards Observation Lakehouses: Living, Interactive Archives of Software Behavior", "abstract": "Code-generating LLMs are trained largely on static artifacts (source, comments, specifications) and rarely on materializations of run-time behavior. As a result, they readily internalize buggy or mislabeled code. Since non-trivial semantic properties are undecidable in general, the only practical way to obtain ground-truth functionality is by dynamic observation of executions. In prior work, we addressed representation with Sequence Sheets, Stimulus-Response Matrices (SRMs), and Stimulus-Response Cubes (SRCs) to capture and compare behavior across tests, implementations, and contexts. These structures make observation data analyzable offline and reusable, but they do not by themselves provide persistence, evolution, or interactive analytics at scale. In this paper, therefore, we introduce observation lakehouses that operationalize continual SRCs: a tall, append-only observations table storing every actuation (stimulus, response, context) and SQL queries that materialize SRC slices on demand. Built on Apache Parquet + Iceberg + DuckDB, the lakehouse ingests data from controlled pipelines (LASSO) and CI pipelines (e.g., unit test executions), enabling n-version assessment, behavioral clustering, and consensus oracles without re-execution. On a 509-problem benchmark, we ingest $\\approx$8.6M observation rows ($<$51MiB) and reconstruct SRM/SRC views and clusters in $<$100ms on a laptop, demonstrating that continual behavior mining is practical without a distributed cluster of machines. This makes behavioral ground truth first-class alongside other run-time data and provides an infrastructure path toward behavior-aware evaluation and training. The Observation Lakehouse, together with the accompanying dataset, is publicly available as an open-source project on GitHub: https://github.com/SoftwareObservatorium/observation-lakehouse", "arxiv_url": "https://arxiv.org/abs/2512.02795", "authors": ["Marcus Kessel"], "first_author": "Marcus Kessel", "category": ["Technical", "Benchmark"], "field": "Software Testing", "task": "Testing automation", "summary": "本文提出 Observation Lakehouse —— 一个基于 Parquet + Iceberg + DuckDB 的可持续、可交互的观测数据湖屋，用于以细粒度调用步骤记录运行时行为并按需重构 SRM/SRC，从而实现无需重执行即可进行 n 版评估、行为聚类与共识 Oracle 并开源了相应数据集与实现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.02795v1", "published": "2025-12-02", "update_time": "2025-12-02", "download_time": "2025-12-05 10:13:02"}
{"id": "2512.02750", "title": "\"Can you feel the vibes?\": An exploration of novice programmer engagement with vibe coding", "abstract": "Emerging alongside generative AI and the broader trend of AI-assisted coding, the term \"vibe coding\" refers to creating software via natural language prompts rather than direct code authorship. This approach promises to democratize software development, but its educational implications remain underexplored. This paper reports on a one-day educational hackathon investigating how novice programmers and mixed-experience teams engage with vibe coding. We organized an inclusive event at a Brazilian public university with 31 undergraduate participants from computing and non-computing disciplines, divided into nine teams. Through observations, an exit survey, and semi-structured interviews, we examined creative processes, tool usage patterns, collaboration dynamics, and learning outcomes. Findings reveal that vibe coding enabled rapid prototyping and cross-disciplinary collaboration, with participants developing prompt engineering skills and delivering functional demonstrations within time constraints. However, we observed premature convergence in ideation, uneven code quality requiring rework, and limited engagement with core software engineering practices. Teams adopted sophisticated workflows combining multiple AI tools in pipeline configurations, with human judgment remaining essential for critical refinement. The short format (9 hours) proved effective for confidence-building among newcomers while accommodating participants with limited availability. We conclude that vibe coding hackathons can serve as valuable low-stakes learning environments when coupled with explicit scaffolds for divergent thinking, critical evaluation of AI outputs, and realistic expectations about production quality.", "arxiv_url": "https://arxiv.org/abs/2512.02750", "authors": ["Kiev Gama", "Filipe Calegario", "Victoria Jackson", "Alexander Nolte", "Luiz Augusto Morais", "Vinicius Garcia"], "first_author": "Kiev Gama", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Prompting", "summary": "本文报道了一次在巴西公立大学举办的一日“vibe coding”教育黑客松，通过观察、问卷与访谈分析31名本科生在以自然语言提示驱动的软件开发中的协作、工具使用与学习成果，发现其有利于快速原型和跨学科合作但存在早期思路收敛、代码质量不均与对工程实践参与有限等问题，建议通过引导发散思维与批判性评估来提升教学效果。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.02750v1", "published": "2025-12-02", "update_time": "2025-12-02", "download_time": "2025-12-05 10:13:27"}
{"id": "2511.15665", "title": "Quantum-Guided Test Case Minimization for LLM-Based Code Generation", "abstract": "Precisely controlling Large Language Models (LLMs) to generate efficient and concise code is a central challenge in software engineering. We introduce a framework based on Test-Driven Development (TDD) that transforms code specification into a combinatorial optimization task. The framework first prompts an LLM to generate a test suite, then formulates the Test Case Minimization (TCM) problem as a Quadratic Unconstrained Binary Optimization (QUBO) model. This QUBO paradigm is compatible with both classical solvers and emerging hardware such as quantum annealers. Experimentally, quantum annealing solves the core TCM task 16 times faster than simulated annealing. This performance underpins our end-to-end framework, which reduces total token consumption by 36.5\\% and significantly improves code quality. This work demonstrates a powerful synergy between generative AI and combinatorial optimization in software engineering, highlighting the critical importance of precise model formulation.", "arxiv_url": "https://arxiv.org/abs/2511.15665", "authors": ["Huixiang Zhang", "Mahzabeen Emu"], "first_author": "Huixiang Zhang", "category": ["Technical"], "field": "Software Testing", "task": "Test Generation", "summary": "本文提出一个结合TDD与QUBO优化的端到端框架：先用LLM生成带功能标签的冗余测试集，再将测试用例最小化问题建模为QUBO并使用量子退火（或经典求解器）求解，以显著减少token消耗并提升LLM生成代码的质量与效率；实验证明量子退火在核心优化上比模拟退火快约16倍，整体token减少36.5%、代码复杂度降低26.1%、生成时间缩短54.3%。", "quality": "Middle", "conference": "IEEE CASCON 2025", "pdf_url": "https://arxiv.org/pdf/2511.15665v1", "published": "2025-11-19", "update_time": "2025-11-19", "download_time": "2025-12-05 16:54:31"}
{"id": "2511.17417", "title": "CREST: Improving Interpretability and Effectiveness of Troubleshooting at Ericsson through Criterion-Specific Trouble Report Retrieval", "abstract": "The rapid evolution of the telecommunication industry necessitates efficient troubleshooting processes to maintain network reliability, software maintainability, and service quality. Trouble Reports (TRs), which document issues in Ericsson's production system, play a critical role in facilitating the timely resolution of software faults. However, the complexity and volume of TR data, along with the presence of diverse criteria that reflect different aspects of each fault, present challenges for retrieval systems. Building on prior work at Ericsson, which utilized a two-stage workflow, comprising Initial Retrieval (IR) and Re-Ranking (RR) stages, this study investigates different TR observation criteria and their impact on the performance of retrieval models. We propose \\textbf{CREST} (\\textbf{C}riteria-specific \\textbf{R}etrieval via \\textbf{E}nsemble of \\textbf{S}pecialized \\textbf{T}R models), a criterion-driven retrieval approach that leverages specialized models for different TR fields to improve both effectiveness and interpretability, thereby enabling quicker fault resolution and supporting software maintenance. CREST utilizes specialized models trained on specific TR criteria and aggregates their outputs to capture diverse and complementary signals. This approach leads to enhanced retrieval accuracy, better calibration of predicted scores, and improved interpretability by providing relevance scores for each criterion, helping users understand why specific TRs were retrieved. Using a subset of Ericsson's internal TRs, this research demonstrates that criterion-specific models significantly outperform a single model approach across key evaluation metrics. This highlights the importance of all targeted criteria used in this study for optimizing the performance of retrieval systems.", "arxiv_url": "https://arxiv.org/abs/2511.17417", "authors": ["Soroush Javdan", "Pragash Krishnamoorthy", "Olga Baysal"], "first_author": "Soroush Javdan", "category": ["Technical", "Empirical"], "field": "Maintenance", "task": "Trouble Report Retrieval", "summary": "本文提出 CREST，一种基于准则的故障报告检索方法，通过为故障报告中不同的观察准则训练专门模型并融合其输出，提高检索准确性、得分校准与可解释性，从而加速 Ericsson 的故障排查与维护流程。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.17417v1", "published": "2025-11-21", "update_time": "2025-11-21", "download_time": "2025-12-05 17:24:08"}
{"id": "2512.03421", "title": "Exploring the Potential and Limitations of Large Language Models for Novice Program Fault Localization", "abstract": "Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. Traditional methods like Spectrum-Based Fault Localization (SBFL) and Mutation-Based Fault Localization (MBFL) help identify faults but often lack the ability to understand code context, making them less effective for beginners. In recent years, Large Language Models (LLMs) have shown promise in overcoming these limitations by utilizing their ability to understand program syntax and semantics. LLM-based fault localization provides more accurate and context-aware results than traditional techniques. This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. Advanced models with reasoning capabilities, such as OpenAI o3 and DeepSeekR1, achieve superior accuracy with minimal reliance on prompt engineering. In contrast, models without reasoning capabilities, like GPT-4, require carefully designed prompts to maintain performance. While LLMs perform well in simple fault localization, their accuracy decreases as problem difficulty increases, though top models maintain robust performance in the BugT dataset. Over-reasoning is another challenge, where some models generate excessive explanations that hinder fault localization clarity. Additionally, the computational cost of deploying LLMs remains a significant barrier for real-time debugging. LLM's explanations demonstrate significant value for novice programmer assistance, with one-year experience participants consistently rating them highly. Our findings demonstrate the potential of LLMs to improve debugging efficiency while stressing the need for further refinement in their reasoning and computational efficiency for practical adoption.", "arxiv_url": "https://arxiv.org/abs/2512.03421", "authors": ["Hexiang Xu", "Hengyuan Liu", "Yonghao Wu", "Xiaolan Kang", "Xiang Chen", "Yong Liu"], "first_author": "Hexiang Xu", "category": ["Empirical", "Benchmark"], "field": "Quality Management", "task": "Bug Localization", "summary": "本文构建了新的BugT数据集并在Codeflaws、Condefects与BugT上系统评估多种开源与闭源LLM，实证分析了它们在新手程序故障定位中的性能、难度影响、推理过度与计算成本，并通过用户研究验证了LLM解释对新手的教学价值。", "quality": "High", "conference": "The Journal of Systems & Software", "pdf_url": "https://arxiv.org/pdf/2512.03421v1", "published": "2025-12-03", "update_time": "2025-12-03", "download_time": "2025-12-05 17:25:17"}
{"id": "2512.03420", "title": "HarnessAgent: Scaling Automatic Fuzzing Harness Construction with Tool-Augmented LLM Pipelines", "abstract": "Large language model (LLM)-based techniques have achieved notable progress in generating harnesses for program fuzzing. However, applying them to arbitrary functions (especially internal functions) \\textit{at scale} remains challenging due to the requirement of sophisticated contextual information, such as specification, dependencies, and usage examples. State-of-the-art methods heavily rely on static or incomplete context provisioning, causing failure of generating functional harnesses. Furthermore, LLMs tend to exploit harness validation metrics, producing plausible yet logically useless code. % Therefore, harness generation across large and diverse projects continues to face challenges in reliable compilation, robust code retrieval, and comprehensive validation.   To address these challenges, we present HarnessAgent, a tool-augmented agentic framework that achieves fully automated, scalable harness construction over hundreds of OSS-Fuzz targets. HarnessAgent introduces three key innovations: 1) a rule-based strategy to identify and minimize various compilation errors; 2) a hybrid tool pool for precise and robust symbol source code retrieval; and 3) an enhanced harness validation pipeline that detects fake definitions. We evaluate HarnessAgent on 243 target functions from OSS-Fuzz projects (65 C projects and 178 C++ projects). It improves the three-shot success rate by approximately 20\\% compared to state-of-the-art techniques, reaching 87\\% for C and 81\\% for C++. Our one-hour fuzzing results show that more than 75\\% of the harnesses generated by HarnessAgent increase the target function coverage, surpassing the baselines by over 10\\%. In addition, the hybrid tool-pool system of HarnessAgent achieves a response rate of over 90\\% for source code retrieval, outperforming Fuzz Introspector by more than 30\\%.", "arxiv_url": "https://arxiv.org/abs/2512.03420", "authors": ["Kang Yang", "Yunhang Zhang", "Zichuan Li", "GuanHong Tao", "Jun Xu", "XiaoJing Liao"], "first_author": "Kang Yang", "category": ["Technical", "Empirical"], "field": "Software Testing", "task": "Test Generation", "summary": "本文提出HarnessAgent，一种工具增强的agent框架，通过编译错误分流、混合检索工具池与强化验证流水线，实现对大型C/C++代码库中内部函数的自动化模糊测试驱动（harness）构建，并在243个OSS-Fuzz目标上显著提高生成成功率与模糊覆盖率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.03420v1", "published": "2025-12-03", "update_time": "2025-12-03", "download_time": "2025-12-05 17:25:58"}
{"id": "2512.05073", "title": "David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?", "abstract": "Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated agentic AI framework on NVIDIA's Comprehensive Verilog Design Problems(CVDP) benchmark. Results show that agentic workflows: through task decomposition, iterative feedback, and correction - not only unlock near-LLM performance at a fraction of the cost but also create learning opportunities for agents, paving the way for efficient, adaptive solutions in complex design tasks.", "arxiv_url": "https://arxiv.org/abs/2512.05073", "authors": ["Shashwat Shankar", "Subhranshu Pandey", "Innocent Dengkhw Mochahari", "Bhabesh Mali", "Animesh Basak Chowdhury", "Sukanta Bhattacharjee", "Chandan Karfa"], "first_author": "Shashwat Shankar", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Prompting", "summary": "本文提出并在NVIDIA CVDP基准上评估了一个为小型语言模型（SLM）定制的多智能体 agentic AI 框架，通过任务分解、SLM 优化提示、迭代验证与回滚机制，使低成本 SLM 在 Verilog 硬件设计与理解任务中接近或匹配大模型性能并显著降低能耗与成本。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.05073v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-06 23:50:18"}
{"id": "2512.04680", "title": "Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap", "abstract": "Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this paper aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI's within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.", "arxiv_url": "https://arxiv.org/abs/2512.04680", "authors": ["Jialong Li", "Mingyue Zhang", "Nianyu Li", "Danny Weyns", "Zhi Jin", "Kenji Tei"], "first_author": "Jialong Li", "category": ["Survey"], "field": "Self-Adaptive Systems", "task": "GenAI for MAPE-K (Monitoring, Analysis, Planning, Execution) and Human-on-the-loop Interaction", "summary": "本文综述了将生成式AI（尤其是大型语言模型）应用于自适应系统的最新进展，分析其在增强MAPE‑K各功能和改善人机协作中的潜力与挑战，并提出了面向研究与实践的路线图与缓解策略。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04680v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-06 23:50:52"}
{"id": "2512.04702", "title": "POLARIS: Is Multi-Agentic Reasoning the Next Wave in Engineering Self-Adaptive Systems?", "abstract": "The growing scale, complexity, interconnectivity, and autonomy of modern software ecosystems introduce unprecedented uncertainty, challenging the foundations of traditional self-adaptation. Existing approaches, typically rule-driven controllers or isolated learning components, struggle to generalize to novel contexts or coordinate responses across distributed subsystems, leaving them ill-equipped for emergent unknown unknowns. Recent discussions on Self-Adaptation 2.0 emphasize an equal partnership between AI and adaptive systems, merging learning-driven intelligence with adaptive control for predictive and proactive behavior. Building on this foundation, we introduce POLARIS, a three-layer multi-agentic self-adaptation framework that advances beyond reactive adaptation. POLARIS integrates: (1) a low-latency Adapter layer for monitoring and safe execution, (2) a transparent Reasoning layer that generates and verifies plans using tool-aware, explainable agents, and (3) a Meta layer that records experiences and meta-learns improved adaptation policies over time. Through shared knowledge and predictive models, POLARIS handles uncertainty, learns from past actions, and evolves its strategies, enabling systems that anticipate change and maintain resilient, goal-directed behavior. Preliminary evaluation on two self-adaptive exemplars, SWIM and SWITCH, shows that POLARIS consistently outperforms state-of-the-art baselines. We argue this marks a shift toward Self-Adaptation 3.0, akin to Software 3.0: a paradigm where systems not only learn from their environment but also reason about and evolve their own adaptation processes, continuously improving to meet novel challenges.", "arxiv_url": "https://arxiv.org/abs/2512.04702", "authors": ["Divyansh Pandey", "Vyakhya Gupta", "Prakhar Singhal", "Karthik Vaidhyanathan"], "first_author": "Divyansh Pandey", "category": ["Technical"], "field": "Self-Adaptive Systems", "task": "Multi-Agentic Runtime Adaptation (LLM-based reasoning & meta-learning)", "summary": "提出POLARIS——一个三层多智能体自适应框架，结合低延迟适配器、可解释的推理代理和元学习来实现预测性、可验证且能随经验持续演化的自适应系统，并在SWIM和SWITCH示例上显示出优于现有基线的性能提升。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04702v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-07 01:57:21"}
{"id": "2512.04673", "title": "Cross-Task Benchmarking and Evaluation of General-Purpose and Code-Specific Large Language Models", "abstract": "Large Language Models (LLMs) have revolutionized both general natural language processing and domain-specific applications such as code synthesis, legal reasoning, and finance. However, while prior studies have explored individual model capabilities, a systematic cross-domain comparison that unifies linguistic, reasoning, and code understanding abilities remains underexplored. In this work, we present a comprehensive evaluation of five general-purpose and three code-specific state-of-the-art LLMs across six diverse benchmarks encompassing linguistic competence, mathematical reasoning, and trustworthiness. Additionally, we analyze model behavior on the CoNaLa dataset for code explanation, comparing natural language and code-specialized LLMs. Our findings reveal that models optimized for code (e.g., CodeLLaMA variants) exhibit strong reasoning and syntactic precision, that even for non-coding tasks can show measurable performance gains, in contrast to general-purpose models like Mistral-7B and Llama-3-8B.", "arxiv_url": "https://arxiv.org/abs/2512.04673", "authors": ["Gunjan Das", "Paheli Bhattacharya", "Rishabh Gupta"], "first_author": "Gunjan Das", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Summarization", "summary": "本文对多款通用与代码专用大型语言模型在六个自然语言与推理基准及CoNaLa代码解释任务上进行了系统的横向评估与对比，发现代码优化模型在推理能力和语法精确性上具有显著优势。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04673v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-07 01:57:56"}
{"id": "2512.05100", "title": "Structured Document Translation via Format Reinforcement Learning", "abstract": "Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \\textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.", "arxiv_url": "https://arxiv.org/abs/2512.05100", "authors": ["Haiyue Song", "Johannes Eschbach-Dymanus", "Hour Kaing", "Sumire Honda", "Hideki Tanaka", "Bianka Buschbeck", "Masao Utiyama"], "first_author": "Haiyue Song", "category": ["Technical"], "field": "Structured Document Translation", "task": "XML/HTML Structured Document Translation", "summary": "本文提出FORMATRL，一种基于Group Relative Policy Optimization的强化学习方法，通过TreeSim和Node-chrF等结构感知奖励以及StrucAUC评估，将结构信息直接纳入模型优化以提升带XML/HTML标记的软件文档翻译的结构保真度与翻译质量。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.05100v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-07 01:59:06"}
{"id": "2512.04785", "title": "ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications", "abstract": "AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.", "arxiv_url": "https://arxiv.org/abs/2512.04785", "authors": ["Eranga Bandara", "Amin Hass", "Ross Gore", "Sachin Shetty", "Ravi Mukkamala", "Safdar H. Bouk", "Xueping Liang", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "first_author": "Eranga Bandara", "category": ["Technical", "Benchmark"], "field": "Requirements & Design", "task": "Analysis", "summary": "本文提出ASTRIDE——一个面向智能体式AI应用的自动化威胁建模平台，扩展STRIDE引入AI特有威胁类别并结合微调的视觉-语言模型与推理LLM从架构图端到端自动生成可解释的威胁模型。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04785v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-07 02:04:42"}
{"id": "2512.04611", "title": "PBFuzz: Agentic Directed Fuzzing for PoV Generation", "abstract": "Proof-of-Vulnerability (PoV) input generation is a critical task in software security and supports downstream applications such as path generation and validation. Generating a PoV input requires solving two sets of constraints: (1) reachability constraints for reaching vulnerable code locations, and (2) triggering constraints for activating the target vulnerability. Existing approaches, including directed greybox fuzzing and LLM-assisted fuzzing, struggle to efficiently satisfy these constraints. This work presents an agentic method that mimics human experts. Human analysts iteratively study code to extract semantic reachability and triggering constraints, form hypotheses about PoV triggering strategies, encode them as test inputs, and refine their understanding using debugging feedback. We automate this process with an agentic directed fuzzing framework called PBFuzz. PBFuzz tackles four challenges in agentic PoV generation: autonomous code reasoning for semantic constraint extraction, custom program-analysis tools for targeted inference, persistent memory to avoid hypothesis drift, and property-based testing for efficient constraint solving while preserving input structure. Experiments on the Magma benchmark show strong results. PBFuzz triggered 57 vulnerabilities, surpassing all baselines, and uniquely triggered 17 vulnerabilities not exposed by existing fuzzers. PBFuzz achieved this within a 30-minute budget per target, while conventional approaches use 24 hours. Median time-to-exposure was 339 seconds for PBFuzz versus 8680 seconds for AFL++ with CmpLog, giving a 25.6x efficiency improvement with an API cost of 1.83 USD per vulnerability.", "arxiv_url": "https://arxiv.org/abs/2512.04611", "authors": ["Haochen Zeng", "Andrew Bao", "Jiajun Cheng", "Chengyu Song"], "first_author": "Haochen Zeng", "category": ["Technical"], "field": "Software Testing", "task": "Test Generation", "summary": "本文提出PBFuzz——一种基于LLM代理的定向模糊测试框架，通过自主演绎代码约束、工具编排、持久化记忆与基于属性的测试生成PoV输入，并在Magma基准上显著优于现有方法。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04611v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-08 01:51:09"}
{"id": "2512.04538", "title": "Completion by Comprehension: Guiding Code Generation with Multi-Granularity Understanding", "abstract": "As code completion task from function-level to repository-level, leveraging contextual information from large-scale codebases becomes a core challenge. However, existing retrieval-augmented generation (RAG) methods typically treat code as plain natural language, relying primarily on shallow semantic matching while overlooking structural semantics and code-specific dependencies. This limits their ability to capture control flow and underlying intent, ultimately constraining the quality of generated code. Therefore, we propose CoCo, a novel framework that enables code Completion by Comprehension of multi-granularity context from large-scale code repositories. CoCo employs static code analysis to extract structured context at the function, file, and project levels, capturing execution logic and semantic dependencies. It then adopts an graph-based multi-granularity context selection mechanism to filter out redundant information and remove noise. Consequently, the information is converted into natural language in a consistent manner, thereby functioning as explicit contextual prompts to guide subsequent code completion. Additionally, a structure-aware code re-ranker mechanism ensures alignment at both semantic and structural levels. Extensive experiments on CrossCodeEval and RepoEval benchmarks demonstrate that CoCo consistently surpasses state-of-the-art baselines, achieving up to 20.2% gains in EM. Moreover, the framework is model-agnostic and can be seamlessly integrated into existing methods, leading to significant performance.", "arxiv_url": "https://arxiv.org/abs/2512.04538", "authors": ["Xinkui Zhao", "Rongkai Liu", "Yifan Zhang", "Chen Zhi", "Lufei Zhang", "Guanjie Cheng", "Yueshen Xu", "Shuiguang Deng", "Jianwei Yin"], "first_author": "Xinkui Zhao", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出CoCo框架，通过静态代码分析提取函数、文件和仓库三级结构化上下文，使用图式多粒度上下文选择与结构感知重排序将关键信息转为自然语言提示，从而指导检索增强的模型进行仓库级代码补全并在基准上显著优于现有方法。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04538v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-08 01:51:40"}
{"id": "2512.04738", "title": "OsmT: Bridging OpenStreetMap Queries and Natural Language with Open-source Tag-aware Language Models", "abstract": "Bridging natural language and structured query languages is a long-standing challenge in the database community. While recent advances in language models have shown promise in this direction, existing solutions often rely on large-scale closed-source models that suffer from high inference costs, limited transparency, and lack of adaptability for lightweight deployment. In this paper, we present OsmT, an open-source tag-aware language model specifically designed to bridge natural language and Overpass Query Language (OverpassQL), a structured query language for accessing large-scale OpenStreetMap (OSM) data. To enhance the accuracy and structural validity of generated queries, we introduce a Tag Retrieval Augmentation (TRA) mechanism that incorporates contextually relevant tag knowledge into the generation process. This mechanism is designed to capture the hierarchical and relational dependencies present in the OSM database, addressing the topological complexity inherent in geospatial query formulation. In addition, we define a reverse task, OverpassQL-to-Text, which translates structured queries into natural language explanations to support query interpretation and improve user accessibility. We evaluate OsmT on a public benchmark against strong baselines and observe consistent improvements in both query generation and interpretation. Despite using significantly fewer parameters, our model achieves competitive accuracy, demonstrating the effectiveness of open-source pre-trained language models in bridging natural language and structured query languages within schema-rich geospatial environments.", "arxiv_url": "https://arxiv.org/abs/2512.04738", "authors": ["Zhuoyue Wan", "Wentao Hu", "Chen Jason Zhang", "Yuanfeng Song", "Shuaimin Li", "Ruiqiang Xiao", "Xiao-Yong Wei", "Raymond Chi-Wing Wong"], "first_author": "Zhuoyue Wan", "category": ["Technical"], "field": "Natural Language Interfaces for Structured Geospatial Data", "task": "Text-to-OverpassQL and OverpassQL-to-Text (bidirectional NL ↔ OverpassQL translation)", "summary": "本文提出OSMT，一种开源的标签感知预训练语言模型，并通过Tag Retrieval Augmentation和混合预训练策略实现对自然语言与OverpassQL的双向翻译，在参数量较小的情况下仍能在Text-to-OverpassQL和OverpassQL-to-Text任务上取得有竞争力的性能与更好的结构化生成质量。", "quality": "High", "conference": "42nd IEEE International Conference on Data Engineering (ICDE 2026)", "pdf_url": "https://arxiv.org/pdf/2512.04738v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-08 02:00:39"}
{"id": "2512.04419", "title": "Solving LLM Repetition Problem in Production: A Comprehensive Study of Multiple Solutions", "abstract": "The repetition problem, where Large Language Models (LLMs) continuously generate repetitive content without proper termination, poses a critical challenge in production deployments, causing severe performance degradation and system stalling. This paper presents a comprehensive investigation and multiple practical solutions for the repetition problem encountered in real-world batch code interpretation tasks.   We identify three distinct repetition patterns: (1) business rule generation repetition, (2) method call relationship analysis repetition, and (3) PlantUML diagram syntax generation repetition. Through rigorous theoretical analysis based on Markov models, we establish that the root cause lies in greedy decoding's inability to escape repetitive loops, exacerbated by self-reinforcement effects.   Our comprehensive experimental evaluation demonstrates three viable solutions: (1) Beam Search decoding with early_stopping=True serves as a universal post-hoc mechanism that effectively resolves all three repetition patterns; (2) presence_penalty hyperparameter provides an effective solution specifically for BadCase 1; and (3) Direct Preference Optimization (DPO) fine-tuning offers a universal model-level solution for all three BadCases.   The primary value of this work lies in combining first-hand production experience with extensive experimental validation. Our main contributions include systematic theoretical analysis of repetition mechanisms, comprehensive evaluation of multiple solutions with task-specific applicability mapping, identification of early_stopping as the critical parameter for Beam Search effectiveness, and practical production-ready solutions validated in real deployment environments.", "arxiv_url": "https://arxiv.org/abs/2512.04419", "authors": ["Weiwei Wang", "Weijie Zou", "Jiyong Min"], "first_author": "Weiwei Wang", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Prompting", "summary": "本文结合生产环境中的一手经验、理论分析与大规模实验，提出并验证了三类可行的解决方案（启用early_stopping的Beam Search、presence_penalty参数调整和基于DPO的微调）以在批量代码解析任务中根治或缓解LLM的重复生成问题。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04419v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-08 02:07:41"}
{"id": "2512.05908", "title": "Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures", "abstract": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.", "arxiv_url": "https://arxiv.org/abs/2512.05908", "authors": ["Amirkia Rafiei Oskooei", "S. Selcan Yukcu", "Mehmet Cevheri Bozoglan", "Mehmet S. Aktas"], "first_author": "Amirkia Rafiei Oskooei", "category": ["Technical", "Empirical"], "field": "Quality Management", "task": "Bug Localization", "summary": "本文提出将多仓库微服务代码库转化为分层的自然语言摘要，并采用先将错误报告路由到相关仓库、再自上而下进行目录与文件级搜索的两阶段 LLM 推理流程，实现对大规模工业系统的可解释缺陷定位，显著优于检索与 RAG 基线。", "quality": "High", "conference": "LLM4Code Workshop, ICSE 2026", "pdf_url": "https://arxiv.org/pdf/2512.05908v1", "published": "2025-12-05", "update_time": "2025-12-05", "download_time": "2025-12-09 01:48:46"}
{"id": "2512.05887", "title": "Bootstrapping Fuzzers for Compilers of Low-Resource Language Dialects Using Language Models", "abstract": "Modern extensible compiler frameworks-such as MLIR-enable rapid creation of domain-specific language dialects. This flexibility, however, makes correctness harder to ensure as the same extensibility that accelerates development also complicates maintaining the testing infrastructure. Extensible languages require automated test generation that is both dialect-agnostic (works across dialects without manual adaptation) and dialect-effective (targets dialect-specific features to find bugs). Existing approaches typically sacrifice one of these goals by either requiring manually constructed seed corpora for each dialect, or by failing to be effective. We present a dialect-agnostic and dialect-effective grammar-based and coverage-guided fuzzing approach for extensible compilers that combines two key insights from existing work: (i) the grammars of dialects, which already encode the structural and type constraints, can often be extracted automatically from the dialect specification; and (ii) these grammars can be used in combination with pre-trained large language models to automatically generate representative and diverse seed inputs from the full dialect space without requiring any manual input or training data. These seeds can then be used to bootstrap coverage-guided fuzzers. We built this approach into a tool, Germinator. When evaluated on six MLIR projects spanning 91 dialects, Germinator generated seeds improve line coverage by 10-120% over grammar-based baselines. We compare against grammar-based baselines because they are the only class of existing automatic seed generators that can be applied uniformly across MLIR's heterogeneous dialect ecosystem. Germinator discovers 88 previously unknown bugs (40 confirmed), including 23 in dialects with no prior automated test generators, demonstrating effective and controllable testing of low-resource dialects at scale.", "arxiv_url": "https://arxiv.org/abs/2512.05887", "authors": ["Sairam Vaidya", "Marcel Böhme", "Loris D'Antoni"], "first_author": "Sairam Vaidya", "category": ["Technical", "Empirical"], "field": "Software Testing", "task": "Test Generation", "summary": "本文提出 Germinator：一种从 MLIR 方言的 TableGen 自动提取语法并结合语法约束的大型语言模型生成多样化种子以引导覆盖驱动的模糊测试，从而在 91 个方言上显著提升覆盖率并发现大量未知缺陷。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.05887v1", "published": "2025-12-05", "update_time": "2025-12-05", "download_time": "2025-12-09 01:49:12"}
{"id": "2512.05962", "title": "Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity", "abstract": "Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the \"mode-seeking\" or \"zero-forcing\" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $α$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.", "arxiv_url": "https://arxiv.org/abs/2512.05962", "authors": ["Germán Kruszewski", "Pierre Erbacher", "Jos Rozen", "Marc Dymetman"], "first_author": "Germán Kruszewski", "category": ["Technical", "Empirical"], "field": "Reasoning & Verification", "task": "Theorem Proving / Proof Search", "summary": "本文提出了DMVR框架与α-DPG方法，通过以验证器过滤构造显式目标分布并利用α-散度在mode-seeking与mass-covering之间插值，从而可控地在精确性与多样性之间权衡，并在Lean定理证明基准上取得覆盖率-精度前沿上的最优表现，显著缓解了RLVR类方法导致的多样性下降。 ", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.05962v1", "published": "2025-12-05", "update_time": "2025-12-05", "download_time": "2025-12-09 01:50:02"}
{"id": "2512.05908", "title": "Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures", "abstract": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.", "arxiv_url": "https://arxiv.org/abs/2512.05908", "authors": ["Amirkia Rafiei Oskooei", "S. Selcan Yukcu", "Mehmet Cevheri Bozoglan", "Mehmet S. Aktas"], "first_author": "Amirkia Rafiei Oskooei", "category": ["Technical", "Empirical"], "field": "Quality Management", "task": "Bug Localization", "summary": "本文将多仓库微服务代码转换为层次化的自然语言摘要，并通过仓库路由与目录/文件自上而下的两阶段NL‑to‑NL搜索，利用LLM实现可解释且可扩展的多仓库缺陷定位，在工业级DNext系统上显著优于RAG与检索基线。", "quality": "High", "conference": "LLM4Code Workshop, ICSE 2026", "pdf_url": "https://arxiv.org/pdf/2512.05908v1", "published": "2025-12-05", "update_time": "2025-12-05", "download_time": "2025-12-09 01:52:21"}
{"id": "2311.07989", "title": "Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code", "abstract": "In this work we systematically review the recent advancements in software engineering with language models, covering 70+ models, 40+ evaluation tasks, 180+ datasets, and 900 related works. Unlike previous works, we integrate software engineering (SE) with natural language processing (NLP) by discussing the perspectives of both sides: SE applies language models for development automation, while NLP adopts SE tasks for language model evaluation. We break down code processing models into general language models represented by the GPT family and specialized models that are specifically pretrained on code, often with tailored objectives. We discuss the relations and differences between these models, and highlight the historical transition of code modeling from statistical models and RNNs to pretrained Transformers and LLMs, which is exactly the same course that had been taken by NLP. We also go beyond programming and review LLMs' application in other software engineering activities including requirement engineering, testing, deployment, and operations in an endeavor to provide a global view of NLP in SE, and identify key challenges and potential future directions in this domain. We keep the survey open and updated on GitHub at https://github.com/codefuse-ai/Awesome-Code-LLM.", "arxiv_url": "https://arxiv.org/abs/2311.07989", "authors": ["Ziyin Zhang", "Chaoyu Chen", "Bingchang Liu", "Cong Liao", "Zi Gong", "Hang Yu", "Jianguo Li", "Rui Wang"], "first_author": "Ziyin Zhang", "category": ["Survey"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["Taxonomy of Code LMs", "SE–NLP Integration", "Code Modeling History", "Full Software Lifecycle Coverage"], "summary": "该论文系统综述代码语言模型及其在软件工程全生命周期中的应用，并统一了NLP与SE两个社区的视角。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2311.07989v7", "published": "2023-11-14", "update_time": "2024-06-26", "download_time": "2025-12-10 15:34:24"}
{"id": "2509.14856", "title": "CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects", "abstract": "Automated code review (CR) is a key application for Large Language Models (LLMs), but progress is hampered by a \"reality gap\": existing benchmarks evaluate models on isolated sub-tasks using simplified, context-poor data. This fails to reflect the holistic context-rich nature of real-world CR. To bridge this gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware benchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601 high-quality instances from 70 Python projects covering nine Pull-Request (PR) problem domains, where each instance provides rich, multi-faceted context including the associated issue, PR details, and repository state, enabling end-to-end evaluation. Beyond superficial metrics, we also propose a novel evaluation framework that combines rule-based checks for location and syntax with model-based judgments of review quality. We present the first large-scale assessment of state-of-the-art LLMs on this comprehensive CR task. Our results establish crucial baselines and reveal that (1) no single LLM dominates all aspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive performance; and (3) different LLMs exhibit varying robustness to redundant context. These findings highlight the necessity of holistic, multi-dimensional evaluation and provide actionable insights for advancing truly intelligent yet practical CR assistants.", "arxiv_url": "https://arxiv.org/abs/2509.14856", "authors": ["Hanyang Guo", "Xunjin Zheng", "Zihan Liao", "Hang Yu", "Peng DI", "Ziyin Zhang", "Hong-Ning Dai"], "first_author": "Hanyang Guo", "category": ["Benchmark"], "field": "Maintenance", "task": "Code Review", "tags": ["Repository-level Context", "End-to-End Evaluation", "Comprehensive Code Review", "Holistic Evaluation Framework"], "summary": "该论文提出首个面向端到端代码审查的全面性基准，通过提供丰富仓库级上下文并结合规则与模型评估框架，更真实地衡量LLM在实际代码审查任务中的能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2509.14856v3", "published": "2025-09-18", "update_time": "2025-10-23", "download_time": "2025-12-10 15:34:47"}
{"id": "2505.16901", "title": "Code Graph Model (CGM): A Graph-Integrated Large Language Model for Repository-Level Software Engineering Tasks", "abstract": "Recent advances in Large Language Models (LLMs) have shown promise in function-level code generation, yet repository-level software engineering tasks remain challenging. Current solutions predominantly rely on proprietary LLM agents, which introduce unpredictability and limit accessibility, raising concerns about data privacy and model customization. This paper investigates whether open-source LLMs can effectively address repository-level tasks without requiring agent-based approaches. We demonstrate this is possible by enabling LLMs to comprehend functions and files within codebases through their semantic information and structural dependencies. To this end, we introduce Code Graph Models (CGMs), which integrate repository code graph structures into the LLM's attention mechanism and map node attributes to the LLM's input space using a specialized adapter. When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. This performance ranks first among open weight models, second among methods with open-source systems, and eighth overall, surpassing the previous best open-source model-based method by 12.33%.", "arxiv_url": "https://arxiv.org/abs/2505.16901", "authors": ["Hongyuan Tao", "Ying Zhang", "Zhenhao Tang", "Hongen Peng", "Xukun Zhu", "Bingchang Liu", "Yingguang Yang", "Ziyin Zhang", "Zhaogui Xu", "Haipeng Zhang", "Linchao Zhu", "Rui Wang", "Hang Yu", "Jianguo Li", "Peng Di"], "first_author": "Hongyuan Tao", "category": ["Technical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Repository-level Issue Fixing", "Graph-Integrated Attention", "Code Graph Representation", "Agentless RAG"], "summary": "该论文提出通过将代码图结构融入LLM并结合无代理的图检索框架，实现开源模型在仓库级缺陷修复任务中的大幅性能提升。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2505.16901v4", "published": "2025-05-22", "update_time": "2025-06-23", "download_time": "2025-12-10 15:35:12"}
{"id": "2409.04183", "title": "GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding", "abstract": "Programming languages possess rich semantic information - such as data flow - that is represented by graphs and not available from the surface form of source code. Recent code language models have scaled to billions of parameters, but model source code solely as text tokens while ignoring any other structural information. Conversely, models that do encode structural information of code make modifications to the Transformer architecture, limiting their scale and compatibility with pretrained LLMs. In this work, we take the best of both worlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph neural networks and cross-modal alignment technologies to inject the structural information of code into LLMs as an auxiliary task during finetuning. This framework is both model-agnostic and task-agnostic, as it can be applied to any code LLM for any code downstream task, and requires the structural graph data only at training time from a corpus unrelated to the finetuning data, while incurring no cost at inference time over the baseline LLM. Experiments on five code tasks with seven different baseline LLMs ranging in size from 350M to 14B validate the effectiveness of GALLa, demonstrating consistent improvement over the baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder.", "arxiv_url": "https://arxiv.org/abs/2409.04183", "authors": ["Ziyin Zhang", "Hang Yu", "Shijie Li", "Peng Di", "Jianguo Li", "Rui Wang"], "first_author": "Ziyin Zhang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Graph Alignment", "AST Integration", "DFG Integration", "GNN Adapters", "Cross‑Modal Representation"], "summary": "该论文提出GALLa框架利用图结构信息对代码LLM进行对齐，从而在多种代码理解与生成任务中提升模型性能。", "quality": "High", "conference": "ACL 2025", "pdf_url": "https://arxiv.org/pdf/2409.04183v4", "published": "2024-09-06", "update_time": "2025-09-23", "download_time": "2025-12-10 15:35:38"}
{"id": "2212.09420", "title": "Large Language Models Meet NL2Code: A Survey", "abstract": "The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are \"Large Size, Premium Data, Expert Tuning\". In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.", "arxiv_url": "https://arxiv.org/abs/2212.09420", "authors": ["Daoguang Zan", "Bei Chen", "Fengji Zhang", "Dianjie Lu", "Bingchao Wu", "Bei Guan", "Yongji Wang", "Jian-Guang Lou"], "first_author": "Daoguang Zan", "category": ["Survey"], "field": "Coding Assistant", "task": "NL2Code", "tags": ["NL2Code", "Code Generation", "Model Comparison", "Benchmark Analysis"], "summary": "该论文系统综述了27种面向NL2Code任务的大语言模型，总结其技术特征、性能表现与未来挑战。", "quality": "High", "conference": "ACL 2023", "pdf_url": "https://arxiv.org/pdf/2212.09420v2", "published": "2022-12-19", "update_time": "2023-05-08", "download_time": "2025-12-10 15:35:55"}
{"id": "2212.10079", "title": "A Survey on Pretrained Language Models for Neural Code Intelligence", "abstract": "As the complexity of modern software continues to escalate, software engineering has become an increasingly daunting and error-prone endeavor. In recent years, the field of Neural Code Intelligence (NCI) has emerged as a promising solution, leveraging the power of deep learning techniques to tackle analytical tasks on source code with the goal of improving programming efficiency and minimizing human errors within the software industry. Pretrained language models have become a dominant force in NCI research, consistently delivering state-of-the-art results across a wide range of tasks, including code summarization, generation, and translation. In this paper, we present a comprehensive survey of the NCI domain, including a thorough review of pretraining techniques, tasks, datasets, and model architectures. We hope this paper will serve as a bridge between the natural language and programming language communities, offering insights for future research in this rapidly evolving field.", "arxiv_url": "https://arxiv.org/abs/2212.10079", "authors": ["Yichen Xu", "Yanqiao Zhu"], "first_author": "Yichen Xu", "category": ["Survey"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["Tokenization Strategies", "Structure Extraction", "Pretraining Paradigms", "Neural Code Intelligence"], "summary": "该论文系统综述了面向神经代码智能的预训练语言模型在预处理、模型设计与下游任务中的方法与进展。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2212.10079v1", "published": "2022-12-20", "update_time": "2022-12-20", "download_time": "2025-12-10 15:36:12"}
{"id": "2302.04026", "title": "An Empirical Comparison of Pre-Trained Models of Source Code", "abstract": "While a large number of pre-trained models of source code have been successfully developed and applied to a variety of software engineering (SE) tasks in recent years, our understanding of these pre-trained models is arguably fairly limited. With the goal of advancing our understanding of these models, we perform the first systematic empirical comparison of 19 recently-developed pre-trained models of source code on 13 SE tasks. To gain additional insights into these models, we adopt a recently-developed 4-dimensional categorization of pre-trained models, and subsequently investigate whether there are correlations between different categories of pre-trained models and their performances on different SE tasks.", "arxiv_url": "https://arxiv.org/abs/2302.04026", "authors": ["Changan Niu", "Chuanyi Li", "Vincent Ng", "Dongxiao Chen", "Jidong Ge", "Bin Luo"], "first_author": "Changan Niu", "category": ["Empirical"], "field": "New Field: Model Evaluation for SE", "task": "New Task: Pre-trained Model Comparison", "tags": ["Empirical Comparison", "Code Pre-trained Models", "Cross-task Evaluation", "Model Categorization"], "summary": "该论文系统性地对19种源码预训练模型在13项软件工程任务上的表现进行了实证比较，并分析其类别与性能之间的关联。", "quality": "High", "conference": "ICSE 2023", "pdf_url": "https://arxiv.org/pdf/2302.04026v1", "published": "2023-02-08", "update_time": "2023-02-08", "download_time": "2025-12-10 15:36:31"}
{"id": "2512.07814", "title": "Understanding Privacy Risks in Code Models Through Training Dynamics: A Causal Approach", "abstract": "Large language models for code (LLM4Code) have greatly improved developer productivity but also raise privacy concerns due to their reliance on open-source repositories containing abundant personally identifiable information (PII). Prior work shows that commercial models can reproduce sensitive PII, yet existing studies largely treat PII as a single category and overlook the heterogeneous risks among different types. We investigate whether distinct PII types vary in their likelihood of being learned and leaked by LLM4Code, and whether this relationship is causal. Our methodology includes building a dataset with diverse PII types, fine-tuning representative models of different scales, computing training dynamics on real PII data, and formulating a structural causal model to estimate the causal effect of learnability on leakage. Results show that leakage risks differ substantially across PII types and correlate with their training dynamics: easy-to-learn instances such as IP addresses exhibit higher leakage, while harder types such as keys and passwords leak less frequently. Ambiguous types show mixed behaviors. This work provides the first causal evidence that leakage risks are type-dependent and offers guidance for developing type-aware and learnability-aware defenses for LLM4Code.", "arxiv_url": "https://arxiv.org/abs/2512.07814", "authors": ["Hua Yang", "Alejandro Velasco", "Sen Fang", "Bowen Xu", "Denys Poshyvanyk"], "first_author": "Hua Yang", "category": ["Empirical", "Benchmark"], "field": "Privacy & Security", "task": "PII Leakage Analysis (causal)", "tags": ["PII Types", "Training Dynamics", "Memorization and Leakage", "Structural Causal Model", "PII Dataset from Code", "LLM4Code Fine-tuning", "Type-aware Privacy Risk", "Leakage Attack Evaluation"], "summary": "本文构建多类型PII数据集并在不同规模和架构的代码模型上计算训练动态，基于结构因果模型实证证明不同PII类型的可学性与推理时泄露风险存在因果关联，为类型感知的防护策略提供依据。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.07814v1", "published": "2025-12-08", "update_time": "2025-12-08", "download_time": "2025-12-10 01:51:03"}
{"id": "2512.07666", "title": "Bridging Code Graphs and Large Language Models for Better Code Understanding", "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in code intelligence tasks such as code generation, summarization, and translation. However, their reliance on linearized token sequences limits their ability to understand the structural semantics of programs. While prior studies have explored graphaugmented prompting and structure-aware pretraining, they either suffer from prompt length constraints or require task-specific architectural changes that are incompatible with large-scale instructionfollowing LLMs. To address these limitations, this paper proposes CGBridge, a novel plug-and-play method that enhances LLMs with Code Graph information through an external, trainable Bridge module. CGBridge first pre-trains a code graph encoder via selfsupervised learning on a large-scale dataset of 270K code graphs to learn structural code semantics. It then trains an external module to bridge the modality gap among code, graph, and text by aligning their semantics through cross-modal attention mechanisms. Finally, the bridge module generates structure-informed prompts, which are injected into a frozen LLM, and is fine-tuned for downstream code intelligence tasks. Experiments show that CGBridge achieves notable improvements over both the original model and the graphaugmented prompting method. Specifically, it yields a 16.19% and 9.12% relative gain in LLM-as-a-Judge on code summarization, and a 9.84% and 38.87% relative gain in Execution Accuracy on code translation. Moreover, CGBridge achieves over 4x faster inference than LoRA-tuned models, demonstrating both effectiveness and efficiency in structure-aware code understanding.", "arxiv_url": "https://arxiv.org/abs/2512.07666", "authors": ["Zeqi Chen", "Zhaoyang Chu", "Yi Gui", "Feng Guo", "Yao Wan", "Chuan Shi"], "first_author": "Zeqi Chen", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["Code Property Graph (CPG)", "GNN-based code graph encoder", "Self-supervised graph pretraining", "Cross-modal attention / alignment", "Bridge module (plug-and-play)", "Structure-informed soft prompts", "Graph-text contrastive learning", "Code translation"], "summary": "本文提出CGBridge——一种可插拔的桥接模块，通过对约27万条异构代码图进行自监督预训练、采用跨模态对齐（图-文本对比与匹配）并生成结构感知提示注入冻结的LLM，从而在代码摘要与翻译等代码理解任务上显著提升性能并提高推理效率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.07666v1", "published": "2025-12-08", "update_time": "2025-12-08", "download_time": "2025-12-10 01:51:28"}
{"id": "2512.07631", "title": "The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds", "abstract": "When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\\Itotal$ bits to identify a solution and gains $\\Istep$ bits per action at cost $\\Cstep$, yielding an effective cost $\\Ceff = (\\Itotal/\\Istep), \\Cstep$ that predicts resource requirements before search. We prove that $\\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \\", "arxiv_url": "https://arxiv.org/abs/2512.07631", "authors": ["Shahar Lutati"], "first_author": "Shahar Lutati", "category": ["Technical"], "field": "Agent Decision & Planning", "task": "Solvability Prediction / Resource Allocation", "tags": ["Agent Capability Problem", "Solvability Prediction", "Information-Theoretic Bounds", "Effective Cost (Ceffective)", "Mutual Information per Action", "Stopping Time Analysis", "Lorden's Inequality", "Gaussian Process Approximation", "Resource Allocation for LLM-based Agents"], "summary": "本文提出了代理能力问题（ACP），用信息论视角将解决所需信息量 Itotal 与每步信息增益 Is 及每步代价 Cs 结合成 Ceffective，并给出该量的下界与概率性上界以在资源受限下预测并引导代理求解可行性，实验以高斯过程近似验证了理论预测的有效性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.07631v1", "published": "2025-12-08", "update_time": "2025-12-08", "download_time": "2025-12-10 01:55:01"}
{"id": "2308.10620", "title": "Large Language Models for Software Engineering: A Systematic Literature Review", "abstract": "Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a systematic literature review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We select and analyze 395 research papers from January 2017 to January 2024 to answer four key research questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, preprocessing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and flagging promising areas for future study. Our artifacts are publicly available at https://github.com/xinyi-hou/LLM4SE_SLR.", "arxiv_url": "https://arxiv.org/abs/2308.10620", "authors": ["Xinyi Hou", "Yanjie Zhao", "Yue Liu", "Zhou Yang", "Kailong Wang", "Li Li", "Xiapu Luo", "David Lo", "John Grundy", "Haoyu Wang"], "first_author": "Xinyi Hou", "category": ["Survey"], "field": "LLM4SE", "task": "Systematic Literature Review", "tags": ["Systematic literature review", "LLM categorization for software engineering", "Data sourcing and preprocessing practices", "Prompt engineering strategies", "Instruction‑tuning & parameter‑efficient adaptation", "Evaluation metrics and benchmarking practices", "Mapping LLMs to 85 specific SE tasks", "Challenges and future research directions in LLM4SE"], "summary": "本文对2017至2024年间395篇将大型语言模型应用于软件工程的研究进行了系统文献综述，归类LLM类型与SE任务、分析数据处理与调优/评估方法，并总结应用场景、挑战与未来研究方向。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2308.10620v6", "published": "2023-08-21", "update_time": "2024-04-10", "download_time": "2025-12-10 16:14:25"}
{"id": "2308.11396", "title": "Towards an Understanding of Large Language Models in Software Engineering Tasks", "abstract": "Large Language Models (LLMs) have drawn widespread attention and research due to their astounding performance in text generation and reasoning tasks. Derivative products, like ChatGPT, have been extensively deployed and highly sought after. Meanwhile, the evaluation and optimization of LLMs in software engineering tasks, such as code generation, have become a research focus. However, there is still a lack of systematic research on applying and evaluating LLMs in software engineering. Therefore, this paper comprehensively investigate and collate the research and products combining LLMs with software engineering, aiming to answer two questions: (1) What are the current integrations of LLMs with software engineering? (2) Can LLMs effectively handle software engineering tasks? To find the answers, we have collected related literature as extensively as possible from seven mainstream databases and selected 123 timely papers published starting from 2022 for analysis. We have categorized these papers in detail and reviewed the current research status of LLMs from the perspective of seven major software engineering tasks, hoping this will help researchers better grasp the research trends and address the issues when applying LLMs. Meanwhile, we have also organized and presented papers with evaluation content to reveal the performance and effectiveness of LLMs in various software engineering tasks, guiding researchers and developers to optimize.", "arxiv_url": "https://arxiv.org/abs/2308.11396", "authors": ["Zibin Zheng", "Kaiwen Ning", "Qingyuan Zhong", "Jiachi Chen", "Wenqing Chen", "Lianghong Guo", "Weicheng Wang", "Yanlin Wang"], "first_author": "Zibin Zheng", "category": ["Survey", "Empirical"], "field": "LLM for Software Engineering", "task": "Survey & Cross-task Evaluation", "tags": ["Systematic Literature Review", "Cross-task Evaluation", "Code Generation", "Test Case Generation", "Vulnerability Detection", "Tool/Product Survey", "Research Trend Analysis"], "summary": "本文系统收集并分析了自2022年起的123篇相关论文，综述了大模型在代码生成、测试、缺陷与安全检测等软件工程各类任务中的应用、评估结果与研究趋势并总结了存在的挑战。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2308.11396v3", "published": "2023-08-22", "update_time": "2024-12-10", "download_time": "2025-12-10 16:15:12"}
{"id": "2310.17903", "title": "Pitfalls in Language Models for Code Intelligence: A Taxonomy and Survey", "abstract": "Modern language models (LMs) have been successfully employed in source code generation and understanding, leading to a significant increase in research focused on learning-based code intelligence, such as automated bug repair, and test case generation. Despite their great potential, language models for code intelligence (LM4Code) are susceptible to potential pitfalls, which hinder realistic performance and further impact their reliability and applicability in real-world deployment. Such challenges drive the need for a comprehensive understanding - not just identifying these issues but delving into their possible implications and existing solutions to build more reliable language models tailored to code intelligence. Based on a well-defined systematic research approach, we conducted an extensive literature review to uncover the pitfalls inherent in LM4Code. Finally, 67 primary studies from top-tier venues have been identified. After carefully examining these studies, we designed a taxonomy of pitfalls in LM4Code research and conducted a systematic study to summarize the issues, implications, current solutions, and challenges of different pitfalls for LM4Code systems. We developed a comprehensive classification scheme that dissects pitfalls across four crucial aspects: data collection and labeling, system design and learning, performance evaluation, and deployment and maintenance. Through this study, we aim to provide a roadmap for researchers and practitioners, facilitating their understanding and utilization of LM4Code in reliable and trustworthy ways.", "arxiv_url": "https://arxiv.org/abs/2310.17903", "authors": ["Xinyu She", "Yue Liu", "Yanjie Zhao", "Yiling He", "Li Li", "Chakkrit Tantithamthavorn", "Zhan Qin", "Haoyu Wang"], "first_author": "Xinyu She", "category": ["Survey"], "field": "Model Reliability & Trustworthiness", "task": "Pitfalls Taxonomy & Evaluation", "tags": ["Pitfall Taxonomy", "Data collection and labeling issues", "Dataset contamination and leakage", "Evaluation bias and metric validity", "Model robustness and reliability", "Deployment and maintenance risks", "Mitigation strategies and best practices", "Reproducibility and transparency"], "summary": "本文基于系统文献综述构建了面向代码智能的语言模型陷阱分类法，系统梳理了数据、模型设计、性能评估与部署维护等方面的常见问题、影响及现有应对策略，并提出未来研究方向与实践建议。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2310.17903v1", "published": "2023-10-27", "update_time": "2023-10-27", "download_time": "2025-12-10 16:15:54"}
{"id": "2312.15223", "title": "A Survey on Large Language Models for Software Engineering", "abstract": "Software Engineering (SE) is the systematic design, development, maintenance, and management of software applications underpinning the digital infrastructure of our modern world. Very recently, the SE community has seen a rapidly increasing number of techniques employing Large Language Models (LLMs) to automate a broad range of SE tasks. Nevertheless, existing information of the applications, effects, and possible limitations of LLMs within SE is still not well-studied.   In this paper, we provide a systematic survey to summarize the current state-of-the-art research in the LLM-based SE community. We summarize 62 representative LLMs of Code across three model architectures, 15 pre-training objectives across four categories, and 16 downstream tasks across five categories. We then present a detailed summarization of the recent SE studies for which LLMs are commonly utilized, including 947 studies for 112 specific code-related tasks across five crucial phases within the SE workflow. We also discuss several critical aspects during the integration of LLMs into SE, such as empirical evaluation, benchmarking, security and reliability, domain tuning, compressing and distillation. Finally, we highlight several challenges and potential opportunities on applying LLMs for future SE studies, such as exploring domain LLMs and constructing clean evaluation datasets. Overall, our work can help researchers gain a comprehensive understanding about the achievements of the existing LLM-based SE studies and promote the practical application of these techniques. Our artifacts are publicly available and will be continuously updated at the living repository: https://github.com/iSEngLab/AwesomeLLM4SE.", "arxiv_url": "https://arxiv.org/abs/2312.15223", "authors": ["Quanjun Zhang", "Chunrong Fang", "Yang Xie", "Yaxin Zhang", "Yun Yang", "Weisong Sun", "Shengcheng Yu", "Zhenyu Chen"], "first_author": "Quanjun Zhang", "category": ["Survey"], "field": "LLMs for Software Engineering", "task": "Comprehensive survey of LLM applications across the SE workflow", "tags": ["Model architecture taxonomy for code LLMs", "Pre-training objective categorization", "Downstream code-task taxonomy", "Mapping of 112 code-related tasks to SE phases", "Evaluation and benchmarking practices for code LLMs", "Security and reliability concerns in LLM4SE", "Domain adaptation / domain tuning for code models", "Model compression and distillation for code LLMs", "Living repository for tracking LLM4SE research"], "summary": "本文对将大语言模型应用于软件工程的研究进行了系统综述，汇总了代码领域的模型类别、预训练目标、下游任务与112项具体任务，讨论了评估、基准、可靠性、领域调优与模型压缩等整合问题并提供持续更新的资源库与未来研究方向。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2312.15223v2", "published": "2023-12-23", "update_time": "2024-09-08", "download_time": "2025-12-10 16:16:35"}
{"id": "2401.00288", "title": "Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit", "abstract": "Code intelligence leverages machine learning techniques to extract knowledge from extensive code corpora, with the aim of developing intelligent tools to improve the quality and productivity of computer programming. Currently, there is already a thriving research community focusing on code intelligence, with efforts ranging from software engineering, machine learning, data mining, natural language processing, and programming languages. In this paper, we conduct a comprehensive literature review on deep learning for code intelligence, from the aspects of code representation learning, deep learning techniques, and application tasks. We also benchmark several state-of-the-art neural models for code intelligence, and provide an open-source toolkit tailored for the rapid prototyping of deep-learning-based code intelligence models. In particular, we inspect the existing code intelligence models under the basis of code representation learning, and provide a comprehensive overview to enhance comprehension of the present state of code intelligence. Furthermore, we publicly release the source code and data resources to provide the community with a ready-to-use benchmark, which can facilitate the evaluation and comparison of existing and future code intelligence models (https://xcodemind.github.io). At last, we also point out several challenging and promising directions for future research.", "arxiv_url": "https://arxiv.org/abs/2401.00288", "authors": ["Yao Wan", "Yang He", "Zhangqian Bi", "Jianguo Zhang", "Hongyu Zhang", "Yulei Sui", "Guandong Xu", "Hai Jin", "Philip S. Yu"], "first_author": "Yao Wan", "category": ["Survey", "Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Representation Learning", "tags": ["Code Representation Learning", "Pre-trained Code Language Models", "Open-source Benchmarking Toolkit", "Cross-task Evaluation (summarization, search, completion, type inference)", "AST-path and Structural Code Features", "Evaluation Protocols and Metrics"], "summary": "本文全面综述了基于深度学习的代码智能研究，提出并开源了用于快速原型和评测的工具平台（NaturalCC），并对多种代码预训练模型在代码摘要、代码检索、代码补全与类型推断等任务上进行了统一基准评测。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2401.00288v1", "published": "2023-12-30", "update_time": "2023-12-30", "download_time": "2025-12-10 16:17:26"}
{"id": "2512.08867", "title": "SimpleDevQA: Benchmarking Large Language Models on Development Knowledge QA", "abstract": "The Development Knowledge Question Answering (Dev Knowledge QA) task aims to provide natural language answers to knowledge-seeking questions during software development. To investigate its importance and to what extent it has been explored, we analyze real user-LLM dialogues from WildChat and find that: (1) The Dev Knowledge QA task accounts for 39.6% of interactions(highest among all tasks), revealing broad knowledge needs beyond code generation (32.3%). (2) Only 27.5% of real Dev Knowledge QA dialogues focus on code understanding, leaving out development knowledge-seeking. (3) Only 17.1% of real-world Dev Knowledge QA dialogues can be used for constructing a benchmark. Existing benchmarks have two primary limitations for evaluating the Dev Knowledge QA capability of LLMs. First, existing benchmarks offer a limited development knowledge scope, mainly focusing on code understanding and neglecting broader knowledge during development. Second, some benchmarks are not built from real user queries. To bridge this gap, we design a three-phase pipeline that transforms real-world dialogue into simple development knowledge-seeking QA pairs. Through this pipeline, we introduce SimpleDevQA, a multilingual benchmark derived from real user dialogues. It contains 2,740 QA pairs in three languages (English, Chinese, and Russian), and focuses on questions with unique, short, and verifiable answers for accurate and simple evaluation. Experiments show that: Code LLMs generally outperform general LLMs of similar scale; Knowledge injection with the Retrieval-Augmented Generation (RAG) strategy can boost LLM accuracy by 11.3% on average; LLMs show systematic overconfidence in Dev Knowledge QA, and the answering accuracy of LLMs shows a positive correlation with their stated confidence; Generally, LLMs with stronger code generation performance also exhibit stronger performance in Dev Knowledge QA.", "arxiv_url": "https://arxiv.org/abs/2512.08867", "authors": ["Jing Zhang", "Lianghong Guo", "Yanlin Wang", "Mingwei Liu", "Jiachi Chen", "Yuchi Ma", "Ensheng Shi", "Terry Yue Zhuo", "Hongyu Zhang", "Zibin Zheng"], "first_author": "Jing Zhang", "category": ["Benchmark", "Empirical"], "field": "Development Knowledge", "task": "Development Knowledge QA", "tags": ["Benchmark Construction", "Dev Knowledge QA", "Multilingual Dataset", "WildChat Dialogue Mining", "Data Pipeline", "Reference Retrieval", "RAG (Retrieval-Augmented Generation)", "LLM Evaluation", "Factuality Verification", "Human Annotation", "Closed-source vs Open-source Comparison", "Code LLM vs General LLM Performance", "Overconfidence Analysis"], "summary": "本文提出SimpleDevQA——一个基于真实用户对话、覆盖英中俄三语的开发知识问答基准及其三阶段构建流水线，并通过对18种主流LLM的评测揭示了RAG可提升性能、闭源/代码模型普遍优于开源/通用模型以及模型过度自信等现象。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.08867v1", "published": "2025-12-09", "update_time": "2025-12-09", "download_time": "2025-12-11 01:52:20"}
{"id": "2512.08810", "title": "Multicalibration for LLM-based Code Generation", "abstract": "As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.", "arxiv_url": "https://arxiv.org/abs/2512.08810", "authors": ["Viola Campos", "Robin Kuschnereit", "Adrian Ulges"], "first_author": "Viola Campos", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Uncertainty Estimation & Calibration", "tags": ["Multicalibration", "Post-hoc calibration", "Uncertainty estimation", "Group-aware calibration (complexity/code-length/prompt-length/language)", "Brier Skill Score", "Expected Calibration Error", "Token likelihood aggregation", "CALIBRI dataset", "Function synthesis benchmarks", "Code LLM evaluation (Qwen3/GPT-OSS/DeepSeek)"], "summary": "本文首次将多组校准(multicalibration)方法应用于代码生成LLM，通过按复杂度、代码/提示长度及编程语言分组的后验校准显著提高置信度估计的可靠性，并发布了包含生成代码、似然与正确性标签的CALIBRI数据集以促进后续研究。", "quality": "High", "conference": "AI-SQE 2026 (The 1st International Workshop on AI for Software Quality Evaluation: Judgment, Metrics, Benchmarks, and Beyond) 2026", "pdf_url": "https://arxiv.org/pdf/2512.08810v1", "published": "2025-12-09", "update_time": "2025-12-09", "download_time": "2025-12-11 01:52:53"}
{"id": "2512.08769", "title": "A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows", "abstract": "Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.", "arxiv_url": "https://arxiv.org/abs/2512.08769", "authors": ["Eranga Bandara", "Ross Gore", "Peter Foytik", "Sachin Shetty", "Ravi Mukkamala", "Abdul Rahman", "Xueping Liang", "Safdar H. Bouk", "Amin Hass", "Sachini Rajapakse", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "first_author": "Eranga Bandara", "category": ["Survey", "Technical"], "field": "Requirements & Design", "task": "Management", "tags": ["Agentic AI", "Multi-Agent Workflows", "Model Context Protocol (MCP)", "Tool Integration", "Orchestration", "Deterministic Execution", "Responsible AI", "Deployment & Containerization", "Prompt Management", "Single-Responsibility Agents", "Case Study: Multimodal News Analysis"], "summary": "本文提供了一套面向生产的 agentic AI 工作流的端到端工程指南，包含工作流分解、多代理设计模式、Model Context Protocol、工具集成、确定性编排、责任化 AI 考量及部署策略，并通过多模态新闻分析案例演示最佳实践。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.08769v1", "published": "2025-12-09", "update_time": "2025-12-09", "download_time": "2025-12-11 01:57:12"}
{"id": "2204.02311", "title": "PaLM: Scaling Language Modeling with Pathways", "abstract": "Large language models have been shown to achieve remarkable performance across a variety of natural language tasks using few-shot learning, which drastically reduces the number of task-specific training examples needed to adapt the model to a particular application. To further our understanding of the impact of scale on few-shot learning, we trained a 540-billion parameter, densely activated, Transformer language model, which we call Pathways Language Model PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML system which enables highly efficient training across multiple TPU Pods. We demonstrate continued benefits of scaling by achieving state-of-the-art few-shot learning results on hundreds of language understanding and generation benchmarks. On a number of these tasks, PaLM 540B achieves breakthrough performance, outperforming the finetuned state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark. A significant number of BIG-bench tasks showed discontinuous improvements from model scale, meaning that performance steeply increased as we scaled to our largest model. PaLM also has strong capabilities in multilingual tasks and source code generation, which we demonstrate on a wide array of benchmarks. We additionally provide a comprehensive analysis on bias and toxicity, and study the extent of training data memorization with respect to model scale. Finally, we discuss the ethical considerations related to large language models and discuss potential mitigation strategies.", "arxiv_url": "https://arxiv.org/abs/2204.02311", "authors": ["Aakanksha Chowdhery", "Sharan Narang", "Jacob Devlin", "Maarten Bosma", "Gaurav Mishra", "Adam Roberts", "Paul Barham", "Hyung Won Chung", "Charles Sutton", "Sebastian Gehrmann", "Parker Schuh", "Kensen Shi", "Sasha Tsvyashchenko", "Joshua Maynez", "Abhishek Rao", "Parker Barnes", "Yi Tay", "Noam Shazeer", "Vinodkumar Prabhakaran", "Emily Reif", "Nan Du", "Ben Hutchinson", "Reiner Pope", "James Bradbury", "Jacob Austin", "Michael Isard", "Guy Gur-Ari", "Pengcheng Yin", "Toju Duke", "Anselm Levskaya", "Sanjay Ghemawat", "Sunipa Dev", "Henryk Michalewski", "Xavier Garcia", "Vedant Misra", "Kevin Robinson", "Liam Fedus", "Denny Zhou", "Daphne Ippolito", "David Luan", "Hyeontaek Lim", "Barret Zoph", "Alexander Spiridonov", "Ryan Sepassi", "David Dohan", "Shivani Agrawal", "Mark Omernick", "Andrew M. Dai", "Thanumalayan Sankaranarayana Pillai", "Marie Pellat", "Aitor Lewkowycz", "Erica Moreira", "Rewon Child", "Oleksandr Polozov", "Katherine Lee", "Zongwei Zhou", "Xuezhi Wang", "Brennan Saeta", "Mark Diaz", "Orhan Firat", "Michele Catasta", "Jason Wei", "Kathy Meier-Hellstern", "Douglas Eck", "Jeff Dean", "Slav Petrov", "Noah Fiedel"], "first_author": "Aakanksha Chowdhery", "category": ["Technical"], "field": "Pretraining & Scaling", "task": "Autoregressive pretraining and multi‑pod training infrastructure", "tags": ["Multi-Pod TPU v4 scaling", "Pipeline-free distributed training", "Autoregressive pretraining at 540B parameters", "Few-shot generalization evaluation", "Multi-step mathematical reasoning", "Code generation and evaluation", "Memorization and data contamination analysis", "Bias and toxicity analysis", "Training stability and hyperparameter techniques", "FLOPs and hardware utilization optimization"], "summary": "本文提出并训练了一个5400亿参数的自回归语言模型，利用跨多Pod的高效分布式训练基础设施进行规模化预训练，在少样本学习、多步骤推理、代码生成与多语种任务上取得了显著进展，并对记忆、数据污染、偏见与毒性等进行了系统分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2204.02311v5", "published": "2022-04-05", "update_time": "2022-10-05", "download_time": "2025-12-11 15:35:20"}
{"id": "2211.05100", "title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model", "abstract": "Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.", "arxiv_url": "https://arxiv.org/abs/2211.05100", "authors": ["BigScience Workshop", ":", "Teven Le Scao", "Angela Fan", "Christopher Akiki", "Ellie Pavlick", "Suzana Ilić", "Daniel Hesslow", "Roman Castagné", "Alexandra Sasha Luccioni", "François Yvon", "Matthias Gallé", "Jonathan Tow", "Alexander M. Rush", "Stella Biderman", "Albert Webson", "Pawan Sasanka Ammanamanchi", "Thomas Wang", "Benoît Sagot", "Niklas Muennighoff", "Albert Villanova del Moral", "Olatunji Ruwase", "Rachel Bawden", "Stas Bekman", "Angelina McMillan-Major", "Iz Beltagy", "Huu Nguyen", "Lucile Saulnier", "Samson Tan", "Pedro Ortiz Suarez", "Victor Sanh", "Hugo Laurençon", "Yacine Jernite", "Julien Launay", "Margaret Mitchell", "Colin Raffel", "Aaron Gokaslan", "Adi Simhi", "Aitor Soroa", "Alham Fikri Aji", "Amit Alfassy", "Anna Rogers", "Ariel Kreisberg Nitzav", "Canwen Xu", "Chenghao Mou", "Chris Emezue", "Christopher Klamm", "Colin Leong", "Daniel van Strien", "David Ifeoluwa Adelani", "Dragomir Radev", "Eduardo González Ponferrada", "Efrat Levkovizh", "Ethan Kim", "Eyal Bar Natan", "Francesco De Toni", "Gérard Dupont", "Germán Kruszewski", "Giada Pistilli", "Hady Elsahar", "Hamza Benyamina", "Hieu Tran", "Ian Yu", "Idris Abdulmumin", "Isaac Johnson", "Itziar Gonzalez-Dios", "Javier de la Rosa", "Jenny Chim", "Jesse Dodge", "Jian Zhu", "Jonathan Chang", "Jörg Frohberg", "Joseph Tobing", "Joydeep Bhattacharjee", "Khalid Almubarak", "Kimbo Chen", "Kyle Lo", "Leandro Von Werra", "Leon Weber", "Long Phan", "Loubna Ben allal", "Ludovic Tanguy", "Manan Dey", "Manuel Romero Muñoz", "Maraim Masoud", "María Grandury", "Mario Šaško", "Max Huang", "Maximin Coavoux", "Mayank Singh", "Mike Tian-Jian Jiang", "Minh Chien Vu", "Mohammad A. Jauhar", "Mustafa Ghaleb", "Nishant Subramani", "Nora Kassner", "Nurulaqilla Khamis", "Olivier Nguyen", "Omar Espejel", "Ona de Gibert", "Paulo Villegas", "Peter Henderson", "Pierre Colombo", "Priscilla Amuok", "Quentin Lhoest", "Rheza Harliman", "Rishi Bommasani", "Roberto Luis López", "Rui Ribeiro", "Salomey Osei", "Sampo Pyysalo", "Sebastian Nagel", "Shamik Bose", "Shamsuddeen Hassan Muhammad", "Shanya Sharma", "Shayne Longpre", "Somaieh Nikpoor", "Stanislav Silberberg", "Suhas Pai", "Sydney Zink", "Tiago Timponi Torrent", "Timo Schick", "Tristan Thrush", "Valentin Danchev", "Vassilina Nikoulina", "Veronika Laippala", "Violette Lepercq", "Vrinda Prabhu", "Zaid Alyafeai", "Zeerak Talat", "Arun Raja", "Benjamin Heinzerling", "Chenglei Si", "Davut Emre Taşar", "Elizabeth Salesky", "Sabrina J. Mielke", "Wilson Y. Lee", "Abheesht Sharma", "Andrea Santilli", "Antoine Chaffin", "Arnaud Stiegler", "Debajyoti Datta", "Eliza Szczechla", "Gunjan Chhablani", "Han Wang", "Harshit Pandey", "Hendrik Strobelt", "Jason Alan Fries", "Jos Rozen", "Leo Gao", "Lintang Sutawika", "M Saiful Bari", "Maged S. Al-shaibani", "Matteo Manica", "Nihal Nayak", "Ryan Teehan", "Samuel Albanie", "Sheng Shen", "Srulik Ben-David", "Stephen H. Bach", "Taewoon Kim", "Tali Bers", "Thibault Fevry", "Trishala Neeraj", "Urmish Thakker", "Vikas Raunak", "Xiangru Tang", "Zheng-Xin Yong", "Zhiqing Sun", "Shaked Brody", "Yallow Uri", "Hadar Tojarieh", "Adam Roberts", "Hyung Won Chung", "Jaesung Tae", "Jason Phang", "Ofir Press", "Conglong Li", "Deepak Narayanan", "Hatim Bourfoune", "Jared Casper", "Jeff Rasley", "Max Ryabinin", "Mayank Mishra", "Minjia Zhang", "Mohammad Shoeybi", "Myriam Peyrounette", "Nicolas Patry", "Nouamane Tazi", "Omar Sanseviero", "Patrick von Platen", "Pierre Cornette", "Pierre François Lavallée", "Rémi Lacroix", "Samyam Rajbhandari", "Sanchit Gandhi", "Shaden Smith", "Stéphane Requena", "Suraj Patil", "Tim Dettmers", "Ahmed Baruwa", "Amanpreet Singh", "Anastasia Cheveleva", "Anne-Laure Ligozat", "Arjun Subramonian", "Aurélie Névéol", "Charles Lovering", "Dan Garrette", "Deepak Tunuguntla", "Ehud Reiter", "Ekaterina Taktasheva", "Ekaterina Voloshina", "Eli Bogdanov", "Genta Indra Winata", "Hailey Schoelkopf", "Jan-Christoph Kalo", "Jekaterina Novikova", "Jessica Zosa Forde", "Jordan Clive", "Jungo Kasai", "Ken Kawamura", "Liam Hazan", "Marine Carpuat", "Miruna Clinciu", "Najoung Kim", "Newton Cheng", "Oleg Serikov", "Omer Antverg", "Oskar van der Wal", "Rui Zhang", "Ruochen Zhang", "Sebastian Gehrmann", "Shachar Mirkin", "Shani Pais", "Tatiana Shavrina", "Thomas Scialom", "Tian Yun", "Tomasz Limisiewicz", "Verena Rieser", "Vitaly Protasov", "Vladislav Mikhailov", "Yada Pruksachatkun", "Yonatan Belinkov", "Zachary Bamberger", "Zdeněk Kasner", "Alice Rueda", "Amanda Pestana", "Amir Feizpour", "Ammar Khan", "Amy Faranak", "Ana Santos", "Anthony Hevia", "Antigona Unldreaj", "Arash Aghagol", "Arezoo Abdollahi", "Aycha Tammour", "Azadeh HajiHosseini", "Bahareh Behroozi", "Benjamin Ajibade", "Bharat Saxena", "Carlos Muñoz Ferrandis", "Daniel McDuff", "Danish Contractor", "David Lansky", "Davis David", "Douwe Kiela", "Duong A. Nguyen", "Edward Tan", "Emi Baylor", "Ezinwanne Ozoani", "Fatima Mirza", "Frankline Ononiwu", "Habib Rezanejad", "Hessie Jones", "Indrani Bhattacharya", "Irene Solaiman", "Irina Sedenko", "Isar Nejadgholi", "Jesse Passmore", "Josh Seltzer", "Julio Bonis Sanz", "Livia Dutra", "Mairon Samagaio", "Maraim Elbadri", "Margot Mieskes", "Marissa Gerchick", "Martha Akinlolu", "Michael McKenna", "Mike Qiu", "Muhammed Ghauri", "Mykola Burynok", "Nafis Abrar", "Nazneen Rajani", "Nour Elkott", "Nour Fahmy", "Olanrewaju Samuel", "Ran An", "Rasmus Kromann", "Ryan Hao", "Samira Alizadeh", "Sarmad Shubber", "Silas Wang", "Sourav Roy", "Sylvain Viguier", "Thanh Le", "Tobi Oyebade", "Trieu Le", "Yoyo Yang", "Zach Nguyen", "Abhinav Ramesh Kashyap", "Alfredo Palasciano", "Alison Callahan", "Anima Shukla", "Antonio Miranda-Escalada", "Ayush Singh", "Benjamin Beilharz", "Bo Wang", "Caio Brito", "Chenxi Zhou", "Chirag Jain", "Chuxin Xu", "Clémentine Fourrier", "Daniel León Periñán", "Daniel Molano", "Dian Yu", "Enrique Manjavacas", "Fabio Barth", "Florian Fuhrimann", "Gabriel Altay", "Giyaseddin Bayrak", "Gully Burns", "Helena U. Vrabec", "Imane Bello", "Ishani Dash", "Jihyun Kang", "John Giorgi", "Jonas Golde", "Jose David Posada", "Karthik Rangasai Sivaraman", "Lokesh Bulchandani", "Lu Liu", "Luisa Shinzato", "Madeleine Hahn de Bykhovetz", "Maiko Takeuchi", "Marc Pàmies", "Maria A Castillo", "Marianna Nezhurina", "Mario Sänger", "Matthias Samwald", "Michael Cullan", "Michael Weinberg", "Michiel De Wolf", "Mina Mihaljcic", "Minna Liu", "Moritz Freidank", "Myungsun Kang", "Natasha Seelam", "Nathan Dahlberg", "Nicholas Michio Broad", "Nikolaus Muellner", "Pascale Fung", "Patrick Haller", "Ramya Chandrasekhar", "Renata Eisenberg", "Robert Martin", "Rodrigo Canalli", "Rosaline Su", "Ruisi Su", "Samuel Cahyawijaya", "Samuele Garda", "Shlok S Deshmukh", "Shubhanshu Mishra", "Sid Kiblawi", "Simon Ott", "Sinee Sang-aroonsiri", "Srishti Kumar", "Stefan Schweter", "Sushil Bharati", "Tanmay Laud", "Théo Gigant", "Tomoya Kainuma", "Wojciech Kusa", "Yanis Labrak", "Yash Shailesh Bajaj", "Yash Venkatraman", "Yifan Xu", "Yingxin Xu", "Yu Xu", "Zhe Tan", "Zhongli Xie", "Zifan Ye", "Mathilde Bras", "Younes Belkada", "Thomas Wolf"], "first_author": "BigScience Workshop", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["Decoder-only Transformer", "Multilingual pretraining corpus", "Mixed natural-language and programming-language pretraining", "Multitask prompted fine-tuning", "Responsible open-access licensing", "Collaborative large-scale training engineering"], "summary": "本文介绍了由全球协作开发的1760亿参数开源多语种解码器Transformer模型及其用于混合自然语言与编程语言的大规模语料的预训练、随后通过多任务提示微调提升性能，并以负责任许可公开发布模型与代码。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2211.05100v4", "published": "2022-11-09", "update_time": "2023-06-27", "download_time": "2025-12-11 15:36:19"}
{"id": "2412.18843", "title": "Improving the Readability of Automatically Generated Tests using Large Language Models", "abstract": "Search-based test generators are effective at producing unit tests with high coverage. However, such automatically generated tests have no meaningful test and variable names, making them hard to understand and interpret by developers. On the other hand, large language models (LLMs) can generate highly readable test cases, but they are not able to match the effectiveness of search-based generators, in terms of achieved code coverage.   In this paper, we propose to combine the effectiveness of search-based generators with the readability of LLM generated tests. Our approach focuses on improving test and variable names produced by search-based tools, while keeping their semantics (i.e., their coverage) unchanged.   Our evaluation on nine industrial and open source LLMs show that our readability improvement transformations are overall semantically-preserving and stable across multiple repetitions. Moreover, a human study with ten professional developers, show that our LLM-improved tests are as readable as developer-written tests, regardless of the LLM employed.", "arxiv_url": "https://arxiv.org/abs/2412.18843", "authors": ["Matteo Biagiola", "Gianluca Ghislotti", "Paolo Tonella"], "first_author": "Matteo Biagiola", "category": ["Technical", "Empirical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Automated unit-test readability improvement", "Identifier and test-name renaming", "Semantic-preserving transformations", "Multi-step prompting to limit context", "Coverage-preservation validation", "Stability analysis across LLM runs", "Human developer readability study", "Integration with search-based generators (Evosuite)"], "summary": "本文提出一种基于多步提示的技术，利用大型语言模型对搜索驱动（如Evosuite）自动生成的单元测试进行语义保留的标识符与测试名重命名以提升可读性，并通过九个LLM的实验与人工评估验证了其在保持覆盖率的同时可读性和稳定性。", "quality": "High", "conference": "IEEE Conference on Software Testing, Verification and Validation (ICST) 2025", "pdf_url": "https://arxiv.org/pdf/2412.18843v1", "published": "2024-12-25", "update_time": "2024-12-25", "download_time": "2025-12-11 15:40:05"}
{"id": "2501.00217", "title": "The Potential of LLMs in Automating Software Testing: From Generation to Reporting", "abstract": "Having a high quality software is essential in software engineering, which requires robust validation and verification processes during testing activities. Manual testing, while effective, can be time consuming and costly, leading to an increased demand for automated methods. Recent advancements in Large Language Models (LLMs) have significantly influenced software engineering, particularly in areas like requirements analysis, test automation, and debugging. This paper explores an agent-oriented approach to automated software testing, using LLMs to reduce human intervention and enhance testing efficiency. The proposed framework integrates LLMs to generate unit tests, visualize call graphs, and automate test execution and reporting. Evaluations across multiple applications in Python and Java demonstrate the system's high test coverage and efficient operation. This research underscores the potential of LLM-powered agents to streamline software testing workflows while addressing challenges in scalability and accuracy.", "arxiv_url": "https://arxiv.org/abs/2501.00217", "authors": ["Betim Sherifi", "Khaled Slhoub", "Fitzroy Nembhard"], "first_author": "Betim Sherifi", "category": ["Technical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Multi-agent testing framework", "Automated unit test generation", "Natural-language prompt-driven testing", "Call-graph DOT visualization", "Automated test execution and PDF reporting", "Code file locator and extraction", "Cross-language evaluation (Python, Java)", "Test rationale generation"], "summary": "本文提出了一个由多智能体与大型语言模型驱动的自动化软件测试框架，能够基于自然语言输入自动生成单元测试、可视化调用图并执行测试与生成报告，并在多个 Python 与 Java 应用上进行案例评估。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2501.00217v1", "published": "2024-12-31", "update_time": "2024-12-31", "download_time": "2025-12-11 15:40:40"}
{"id": "2501.01329", "title": "The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for Test Case Generation", "abstract": "Test cases are essential for validating the reliability and quality of software applications. Recent studies have demonstrated the capability of Large Language Models (LLMs) to generate useful test cases for given source code. However, the existing work primarily relies on human-written plain prompts, which often leads to suboptimal results since the performance of LLMs can be highly influenced by the prompts. Moreover, these approaches use the same prompt for all LLMs, overlooking the fact that different LLMs might be best suited to different prompts. Given the wide variety of possible prompt formulations, automatically discovering the optimal prompt for each LLM presents a significant challenge. Although there are methods on automated prompt optimization in the natural language processing field, they are hard to produce effective prompts for the test case generation task. First, the methods iteratively optimize prompts by simply combining and mutating existing ones without proper guidance, resulting in prompts that lack diversity and tend to repeat the same errors in the generated test cases. Second, the prompts are generally lack of domain contextual knowledge, limiting LLMs' performance in the task.", "arxiv_url": "https://arxiv.org/abs/2501.01329", "authors": ["Shuzheng Gao", "Chaozheng Wang", "Cuiyun Gao", "Xiaoqian Jiao", "Chun Yong Chong", "Shan Gao", "Michael Lyu"], "first_author": "Shuzheng Gao", "category": ["Technical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Prompt Optimization for Testing", "Diversity-Guided Prompt Generation", "Failure-Driven Rule Induction", "Domain Contextual Knowledge Extraction", "Tailored Prompts per LLM", "Reflection-based Error Mitigation", "Cross-file Inheritance and Invocation Context"], "summary": "本文提出MAPS，一种用于测试用例生成的自动化LLM定制提示优化方法，结合多样性引导、基于失败的规则归纳与领域上下文提取，为不同LLM生成定制化提示并显著提升线覆盖与分支覆盖率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2501.01329v1", "published": "2025-01-02", "update_time": "2025-01-02", "download_time": "2025-12-11 15:41:12"}
{"id": "2501.07425", "title": "Enhancing LLM's Ability to Generate More Repository-Aware Unit Tests Through Precise Contextual Information Injection", "abstract": "Though many learning-based approaches have been proposed for unit test generation and achieved remarkable performance, they still have limitations in relying on task-specific datasets. Recently, Large Language Models (LLMs) guided by prompt engineering have gained attention for their ability to handle a broad range of tasks, including unit test generation. Despite their success, LLMs may exhibit hallucinations when generating unit tests for focal methods or functions due to their lack of awareness regarding the project's global context. These hallucinations may manifest as calls to non-existent methods, as well as incorrect parameters or return values, such as mismatched parameter types or numbers. While many studies have explored the role of context, they often extract fixed patterns of context for different models and focal methods, which may not be suitable for all generation processes (e.g., excessive irrelevant context could lead to redundancy, preventing the model from focusing on essential information). To overcome this limitation, we propose RATester, which enhances the LLM's ability to generate more repository-aware unit tests through global contextual information injection. To equip LLMs with global knowledge similar to that of human testers, we integrate the language server gopls, which provides essential features (e.g., definition lookup) to assist the LLM. When RATester encounters an unfamiliar identifier (e.g., an unfamiliar struct name), it first leverages gopls to fetch relevant definitions and documentation comments, and then uses this global knowledge to guide the LLM. By utilizing gopls, RATester enriches the LLM's knowledge of the project's global context, thereby reducing hallucinations during unit test generation.", "arxiv_url": "https://arxiv.org/abs/2501.07425", "authors": ["Xin Yin", "Chao Ni", "Xinrui Li", "Liushan Chen", "Guojun Ma", "Xiaohu Yang"], "first_author": "Xin Yin", "category": ["Technical", "Benchmark"], "field": "Software Testing", "task": "Test Generation", "tags": ["Repository-aware context injection", "Language-server-assisted definition lookup", "Prompt augmentation for unit tests", "Hallucination mitigation", "Golang unit test generation", "Mutation testing evaluation", "Line-coverage improvement", "Model-agnostic testing framework"], "summary": "提出RATester，通过集成语言服务器以注入仓库级全局上下文信息，增强LLM生成更具仓库感的Golang单元测试，从而减少幻觉并显著提升覆盖率与变异检测效果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2501.07425v1", "published": "2025-01-13", "update_time": "2025-01-13", "download_time": "2025-12-11 15:41:48"}
{"id": "2501.10200", "title": "Test Wars: A Comparative Study of SBST, Symbolic Execution, and LLM-Based Approaches to Unit Test Generation", "abstract": "Generating tests automatically is a key and ongoing area of focus in software engineering research. The emergence of Large Language Models (LLMs) has opened up new opportunities, given their ability to perform a wide spectrum of tasks. However, the effectiveness of LLM-based approaches compared to traditional techniques such as search-based software testing (SBST) and symbolic execution remains uncertain. In this paper, we perform an extensive study of automatic test generation approaches based on three tools: EvoSuite for SBST, Kex for symbolic execution, and TestSpark for LLM-based test generation. We evaluate tools performance on the GitBug Java dataset and compare them using various execution-based and feature-based metrics. Our results show that while LLM-based test generation is promising, it falls behind traditional methods in terms of coverage. However, it significantly outperforms them in mutation scores, suggesting that LLMs provide a deeper semantic understanding of code. LLM-based approach also performed worse than SBST and symbolic execution-based approaches w.r.t. fault detection capabilities. Additionally, our feature-based analysis shows that all tools are primarily affected by the complexity and internal dependencies of the class under test (CUT), with LLM-based approaches being especially sensitive to the CUT size.", "arxiv_url": "https://arxiv.org/abs/2501.10200", "authors": ["Azat Abdullin", "Pouria Derakhshanfar", "Annibale Panichella"], "first_author": "Azat Abdullin", "category": ["Empirical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Comparative evaluation of SBST, symbolic execution, and LLM-based tests", "Coverage vs. mutation-score tradeoffs", "Statistical analysis of nondeterministic LLM outputs", "Data-leakage-aware benchmark selection", "Sensitivity to class-under-test complexity and size", "Automated, extensible test-generation evaluation pipeline", "Execution- and feature-based metrics", "Repeatability via multiple seeds and independent sessions"], "summary": "本文在为避免数据泄露的GitBug Java基准上，使用多次随机重复与统计检验比较了EvoSuite（SBST）、Kex（符号执行）和基于LLM的TestSpark的单元测试生成性能，发现LLM在变异得分上表现优异但在覆盖率与缺陷检测上落后，并分析了类复杂度等对方法效果的影响。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2501.10200v1", "published": "2025-01-17", "update_time": "2025-01-17", "download_time": "2025-12-11 15:42:22"}
{"id": "2501.11086", "title": "Can LLM Generate Regression Tests for Software Commits?", "abstract": "Large Language Models (LLMs) have shown tremendous promise in automated software engineering. In this paper, we investigate the opportunities of LLMs for automatic regression test generation for programs that take highly structured, human-readable inputs, such as XML parsers or JavaScript interpreters. Concretely, we explore the following regression test generation scenarios for such programs that have so far been difficult to test automatically in the absence of corresponding input grammars:   $\\bullet$ Bug finding. Given a code change (e.g., a commit or pull request), our LLM-based approach generates a test case with the objective of revealing any bugs that might be introduced if that change is applied.   $\\bullet$ Patch testing. Given a patch, our LLM-based approach generates a test case that fails before but passes after the patch. This test can be added to the regression test suite to catch similar bugs in the future.   We implement Cleverest, a feedback-directed, zero-shot LLM-based regression test generation technique, and evaluate its effectiveness on 22 commits to three subject programs: Mujs, Libxml2, and Poppler. For programs using more human-readable file formats, like XML or JavaScript, we found Cleverest performed very well. It generated easy-to-understand bug-revealing or bug-reproduction test cases for the majority of commits in just under three minutes -- even when only the code diff or commit message (unless it was too vague) was given. For programs with more compact file formats, like PDF, as expected, it struggled to generate effective test cases. However, the LLM-supplied test cases are not very far from becoming effective (e.g., when used as a seed by a greybox fuzzer or as a starting point by the developer).", "arxiv_url": "https://arxiv.org/abs/2501.11086", "authors": ["Jing Liu", "Seongmin Lee", "Eleonora Losiouk", "Marcel Böhme"], "first_author": "Jing Liu", "category": ["Technical", "Empirical"], "field": "Software Testing", "task": "Test Generation", "tags": ["Feedback-directed zero-shot test generation", "Commit-diff and commit-message prompting", "Regression test generation for structured human-readable inputs", "Patch testing and bug reproduction", "Seed generation for greybox fuzzers", "Comparison to directed greybox fuzzing", "Hyperparameter and ablation analysis", "Limitations on compact/binary input formats"], "summary": "本文提出 Cleverest——一种基于大语言模型的反馈驱动零样本回归测试生成技术，能够从提交的 diff 或提交信息自动合成针对人类可读结构化输入（如 XML/JavaScript）的回归测试，并在多数案例中快速找到或复现 bug，且可作为灰盒模糊测试的种子提升发现能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2501.11086v1", "published": "2025-01-19", "update_time": "2025-01-19", "download_time": "2025-12-11 15:43:00"}
{"id": "2504.08703", "title": "SWE-PolyBench: A multi-language benchmark for repository level evaluation of coding agents", "abstract": "Coding agents powered by large language models have shown impressive capabilities in software engineering tasks, but evaluating their performance across diverse programming languages and real-world scenarios remains challenging. We introduce SWE-PolyBench, a new multi-language benchmark for repository-level, execution-based evaluation of coding agents. SWE-PolyBench contains 2110 instances from 21 repositories and includes tasks in Java (165), JavaScript (1017), TypeScript (729) and Python (199), covering bug fixes, feature additions, and code refactoring. We provide a task and repository-stratified subsample (SWE-PolyBench500) and release an evaluation harness allowing for fully automated evaluation. To enable a more comprehensive comparison of coding agents, this work also presents a novel set of metrics rooted in syntax tree analysis. We evaluate leading open source coding agents on SWE-PolyBench, revealing their strengths and limitations across languages, task types, and complexity classes. Our experiments show that current agents exhibit uneven performances across languages and struggle with complex problems while showing higher performance on simpler tasks. SWE-PolyBench aims to drive progress in developing more versatile and robust AI coding assistants for real-world software engineering. Our datasets and code are available at: https://github.com/amazon-science/SWE-PolyBench", "arxiv_url": "https://arxiv.org/abs/2504.08703", "authors": ["Muhammad Shihab Rashid", "Christian Bock", "Yuan Zhuang", "Alexander Buchholz", "Tim Esler", "Simon Valentin", "Luca Franceschi", "Martin Wistuba", "Prabhu Teja Sivaprasad", "Woo Jung Kim", "Anoop Deoras", "Giovanni Zappella", "Laurent Callot"], "first_author": "Muhammad Shihab Rashid", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Repository-level code edits", "Pull-request-derived tasks", "Multi-language benchmark", "Execution-based evaluation with unit tests", "Concrete Syntax Tree (CST) node metrics", "File- and node-retrieval metrics", "Stratified 500-sample subset for fast iteration", "Bug/feature/refactor task taxonomy", "Cross-language robustness analysis", "Automated evaluation harness"], "summary": "本文提出SWE-PolyBench，一个包含2110个基于Pull Request的多语言（Java/JavaScript/TypeScript/Python）仓库级执行型基准，并引入基于语法树的文件与节点检索度量与自动化评估工具以评测编码代理在不同语言与复杂度下的表现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2504.08703v3", "published": "2025-04-11", "update_time": "2025-04-23", "download_time": "2025-12-11 15:46:42"}
{"id": "2504.21205", "title": "SecRepoBench: Benchmarking Code Agents for Secure Code Completion in Real-World Repositories", "abstract": "This paper introduces SecRepoBench, a benchmark to evaluate code agents on secure code completion in real-world repositories. SecRepoBench has 318 code completion tasks in 27 C/C++ repositories, covering 15 CWEs. We evaluate 28 standalone LLMs and 13 code agents across 3 state-of-the-art agent frameworks using our benchmark. We find that state-of-the-art LLMs struggle with generating correct and secure code completions. However, code agents significantly outperform standalone LLMs. We show that SecRepoBench is more difficult than the prior state-of-the-art benchmark. Finally, our comprehensive analysis provides insights into potential directions for enhancing the ability of code agents to write correct and secure code in real-world repositories.", "arxiv_url": "https://arxiv.org/abs/2504.21205", "authors": ["Chihao Shen", "Connor Dilgren", "Purva Chiniya", "Luke Griffith", "Yu Ding", "Yizheng Chen"], "first_author": "Chihao Shen", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Repository-level secure code completion", "C/C++ memory-safety and CWE coverage", "Developer-written unit-test correctness evaluation", "OSS-Fuzz PoC crash-based security testing", "AST-based masked region generation", "Semantic-preserving code mutation to prevent memorization", "Context retriever vs. agent-framework comparison", "Automated compile-and-test evaluation pipeline"], "summary": "本文提出了SecRepoBench——一个包含27个真实C/C++仓库、318个安全敏感代码补全任务的基准，结合开发者单元测试与OSS‑Fuzz PoC同时评估代码正确性与安全性，并用于比较多种独立LLM与代码代理的表现，发现代理显著优于独立模型但整体仍不足以保证正确与安全。 ", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2504.21205v2", "published": "2025-04-29", "update_time": "2025-11-05", "download_time": "2025-12-11 15:47:14"}
{"id": "2505.04606", "title": "OmniGIRL: A Multilingual and Multimodal Benchmark for GitHub Issue Resolution", "abstract": "The GitHub issue resolution task aims to resolve issues reported in repositories automatically. With advances in large language models (LLMs), this task has gained increasing attention, and several benchmarks are proposed to evaluate the issue resolution ability of LLMs. However, existing benchmarks have three main limitations. First, current benchmarks focus on a single programming language, limiting the evaluation of issues from repositories across different languages. Second, they usually cover a narrow range of domains, which may fail to represent the diversity of real-world issues. Third, existing benchmarks rely solely on textual information in issue descriptions, overlooking multimodal information such as images in issues. In this paper, we propose OmniGIRL, a GitHub Issue ResoLution benchmark that is multilingual, multimodal, and multi-domain. OmniGIRL includes 959 task instances, which are collected from repositories across four programming languages (i.e., Python, JavaScript, TypeScript, and Java) and eight different domains. Our evaluation shows that current LLMs show limited performances on OmniGIRL. Notably, the best-performing model, GPT-4o, resolves only 8.6% of the issues. Besides, we find that current LLMs struggle to resolve issues requiring understanding images. The best performance is achieved by Claude-3.5-Sonnet, which resolves only 10.5% of the issues with image information. Finally, we analyze the reasons behind current LLMs' failure on OmniGIRL, providing insights for future improvements.", "arxiv_url": "https://arxiv.org/abs/2505.04606", "authors": ["Lianghong Guo", "Wei Tao", "Runhan Jiang", "Yanlin Wang", "Jiachi Chen", "Xilin Liu", "Yuchi Ma", "Mingzhi Mao", "Hongyu Zhang", "Zibin Zheng"], "first_author": "Lianghong Guo", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Multilingual issue resolution", "Multimodal (image) issue cases", "Cross‑language evaluation", "Multi‑domain repository selection", "Test-driven patch application", "Multi-file modification analysis", "LLM failure modes: formatting & parsing"], "summary": "OmniGIRL 提出一个包含 959 条实例的多语言、多模态、多领域的 GitHub 问题修复基准，并评估与分析现有大模型在跨语言、多文件和含图像问题上的有限表现与失败原因。", "quality": "High", "conference": "ISSTA 2025", "pdf_url": "https://arxiv.org/pdf/2505.04606v1", "published": "2025-05-07", "update_time": "2025-05-07", "download_time": "2025-12-11 15:48:00"}
{"id": "2505.16975", "title": "SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development", "abstract": "Large Language Models (LLMs) have shown strong capability in diverse software engineering tasks, e.g. code completion, bug fixing, and document generation. However, feature-driven development (FDD), a highly prevalent real-world task that involves developing new functionalities for large, existing codebases, remains underexplored. We therefore introduce SWE-Dev, the first large-scale dataset (with 14,000 training and 500 test samples) designed to evaluate and train autonomous coding systems on real-world feature development tasks. To ensure verifiable and diverse training, SWE-Dev uniquely provides all instances with a runnable environment and its developer-authored executable unit tests. This collection not only provides high-quality data for Supervised Fine-Tuning (SFT), but also enables Reinforcement Learning (RL) by delivering accurate reward signals from executable unit tests. Our extensive evaluations on SWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent Systems (MAS), reveal that FDD is a profoundly challenging frontier for current AI (e.g., Claude-3.7-Sonnet achieves only 22.45\\% Pass@3 on the hard test split). Crucially, we demonstrate that SWE-Dev serves as an effective platform for model improvement: fine-tuning on training set enabled a 7B model comparable to GPT-4o on \\textit{hard} split, underscoring the value of its high-quality training data. Code is available here \\href{https://github.com/DorothyDUUU/SWE-Dev}{https://github.com/DorothyDUUU/SWE-Dev}.", "arxiv_url": "https://arxiv.org/abs/2505.16975", "authors": ["Yaxin Du", "Yuzhu Cai", "Yifan Zhou", "Cheng Wang", "Yu Qian", "Xianghe Pang", "Qian Liu", "Yue Hu", "Siheng Chen"], "first_author": "Yaxin Du", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Feature-driven development", "Executable unit-test supervision", "Runnable repository environments", "Cross-file refactoring and additions", "Test-based reinforcement learning rewards", "Supervised fine-tuning for repository tasks", "Multi-agent collaboration for coding", "Functional correctness evaluation", "Long-context code generation"], "summary": "该论文构建并公开了SWE-Dev——一个含1.45万可执行训练样本与500测试样本的仓库级功能开发基准，提供可运行环境与单元测试以评估并用于SFT、RL和多智能体训练，展示了在真实特性开发任务中对模型评估与训练的有效性与挑战。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2505.16975v2", "published": "2025-05-22", "update_time": "2025-06-19", "download_time": "2025-12-11 15:48:35"}
{"id": "2505.20411", "title": "SWE-rebench: An Automated Pipeline for Task Collection and Decontaminated Evaluation of Software Engineering Agents", "abstract": "LLM-based agents have shown promising capabilities in a growing range of software engineering (SWE) tasks. However, advancing this field faces two critical challenges. First, high-quality training data is scarce, especially data that reflects real-world SWE scenarios, where agents must interact with development environments, execute code and adapt behavior based on the outcomes of their actions. Existing datasets are either limited to one-shot code generation or comprise small, manually curated collections of interactive tasks, lacking both scale and diversity. Second, the lack of fresh interactive SWE tasks affects evaluation of rapidly improving models, as static benchmarks quickly become outdated due to contamination issues. To address these limitations, we introduce a novel, automated, and scalable pipeline to continuously extract real-world interactive SWE tasks from diverse GitHub repositories. Using this pipeline, we construct SWE-rebench, a public dataset comprising over 21,000 interactive Python-based SWE tasks, suitable for reinforcement learning of SWE agents at scale. Additionally, we use continuous supply of fresh tasks collected using SWE-rebench methodology to build a contamination-free benchmark for agentic software engineering. We compare results of various LLMs on this benchmark to results on SWE-bench Verified and show that performance of some language models might be inflated due to contamination issues.", "arxiv_url": "https://arxiv.org/abs/2505.20411", "authors": ["Ibragim Badertdinov", "Alexander Golubev", "Maksim Nekrashevich", "Anton Shevtsov", "Simon Karasik", "Andrei Andriushchenko", "Maria Trofimova", "Daria Litvintseva", "Boris Yangel"], "first_author": "Ibragim Badertdinov", "category": ["Benchmark", "Technical"], "field": "Software Testing", "task": "Testing automation", "tags": ["Automated task mining", "Executable environment configuration", "Test-driven verification", "Decontamination-aware benchmarking", "Continuous dataset collection", "Reinforcement-learning-ready tasks", "GitHub PR/issue mining", "Distributed pipeline for large-scale processing"], "summary": "本文提出一个可扩展的自动化管道从 GitHub 挖掘可执行的交互式软件工程任务，发布了包含 21,000+ Python 任务的 SWE-rebench 数据集并建立了持续更新且去污染的基准与排行榜以评估软件工程代理。", "quality": "High", "conference": "NeurIPS 2025", "pdf_url": "https://arxiv.org/pdf/2505.20411v2", "published": "2025-05-26", "update_time": "2025-11-04", "download_time": "2025-12-11 15:49:05"}
{"id": "2505.20749", "title": "Can Agents Fix Agent Issues?", "abstract": "LLM-based agent systems are emerging as a new software paradigm and have been widely adopted across diverse domains such as medicine, robotics, and programming. However, maintaining these systems requires substantial effort, as they are inevitably prone to bugs and continually evolve to meet changing external requirements. Therefore, automatically resolving agent issues (i.e., bug reports or feature requests) is a crucial and challenging task. While recent software engineering (SE) agents (e.g., SWE-agent) have shown promise in addressing issues in traditional software systems, it remains unclear how effectively they can resolve real-world issues in agent systems, which differ significantly from traditional software. To fill this gap, we first manually analyze 201 real-world agent issues and identify common categories of agent issues. We then spend 500 person-hours constructing AgentIssue-Bench, a reproducible benchmark comprising 50 agent issue resolution tasks (each with an executable environment and failure-triggering tests). We further evaluate state-of-the-art SE agents on AgentIssue-Bench and reveal their limited effectiveness (i.e., with only 0.67% - 4.67% resolution rates). These results underscore the unique challenges of maintaining agent systems compared to traditional software, highlighting the need for further research to develop advanced SE agents for resolving agent issues. Data and code are available at https://github.com/alfin06/AgentIssue-Bench.", "arxiv_url": "https://arxiv.org/abs/2505.20749", "authors": ["Alfin Wijaya Rahardja", "Junwei Liu", "Weitong Chen", "Zhenpeng Chen", "Yiling Lou"], "first_author": "Alfin Wijaya Rahardja", "category": ["Empirical", "Benchmark"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Agent issue taxonomy", "Reproducible agent-issue benchmark", "Dockerized failure reproduction", "Failure-triggering tests", "SE agent empirical evaluation", "LLM nondeterminism and volatility", "Agent-specific failures (model binding, memory, tool use)", "Qualitative resolution analysis"], "summary": "本文通过对201个真实Agent系统问题的人工分析构建了包含50个可复现任务的AGENTISSUE-BENCH基准，并评估多种现有SE agents，结果显示其在修复Agent问题上成功率极低，从而揭示了维护Agent系统的独特挑战。", "quality": "High", "conference": "NeurIPS 2025", "pdf_url": "https://arxiv.org/pdf/2505.20749v4", "published": "2025-05-27", "update_time": "2025-10-24", "download_time": "2025-12-11 15:49:37"}
{"id": "2505.22583", "title": "GitGoodBench: A Novel Benchmark For Evaluating Agentic Performance On Git", "abstract": "Benchmarks for Software Engineering (SE) AI agents, most notably SWE-bench, have catalyzed progress in programming capabilities of AI agents. However, they overlook critical developer workflows such as Version Control System (VCS) operations. To address this issue, we present GitGoodBench, a novel benchmark for evaluating AI agent performance on VCS tasks. GitGoodBench covers three core Git scenarios extracted from permissive open-source Python, Java, and Kotlin repositories. Our benchmark provides three datasets: a comprehensive evaluation suite (900 samples), a rapid prototyping version (120 samples), and a training corpus (17,469 samples). We establish baseline performance on the prototyping version of our benchmark using GPT-4o equipped with custom tools, achieving a 21.11% solve rate overall. We expect GitGoodBench to serve as a crucial stepping stone toward truly comprehensive SE agents that go beyond mere programming.", "arxiv_url": "https://arxiv.org/abs/2505.22583", "authors": ["Tobias Lindenbauer", "Egor Bogomolov", "Yaroslav Zharov"], "first_author": "Tobias Lindenbauer", "category": ["Benchmark"], "field": "Version Control & Collaboration", "task": "Git VCS", "tags": ["Agentic VCS evaluation", "Merge conflict resolution benchmark", "Interactive rebase planning", "Iterative commit reconstruction", "File-Commit Chain sampling", "Repository selection & filtering heuristics", "Agent trajectory training corpus", "Tool-enabled agent baseline evaluation"], "summary": "本文提出 GitGoodBench，一个用于评估 AI 代理在 Git 版本控制任务（合并冲突解决、交互式变基、迭代提交重构）上的端到端基准，发布了评估集、轻量集与用于收集轨迹的训练集并给出基线结果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2505.22583v1", "published": "2025-05-28", "update_time": "2025-05-28", "download_time": "2025-12-11 15:50:19"}
{"id": "2505.23932", "title": "SwingArena: Competitive Programming Arena for Long-context GitHub Issue Solving", "abstract": "We present SwingArena, a competitive evaluation framework for Large Language Models (LLMs) that closely mirrors real-world software development workflows. Unlike traditional static benchmarks, SwingArena models the collaborative process of software iteration by pairing LLMs as submitters, who generate patches, and reviewers, who create test cases and verify the patches through continuous integration (CI) pipelines. To support these interactive evaluations, we introduce a retrieval-augmented code generation (RACG) module that efficiently handles long-context challenges by providing syntactically and semantically relevant code snippets from large codebases, supporting multiple programming languages (C++, Python, Rust, and Go). This enables the framework to scale across diverse tasks and contexts while respecting token limitations. Our experiments, using over 400 high-quality real-world GitHub issues selected from a pool of 2,300 issues, show that models like GPT-4o excel at aggressive patch generation, whereas DeepSeek and Gemini prioritize correctness in CI validation. SwingArena presents a scalable and extensible methodology for evaluating LLMs in realistic, CI-driven software development settings. More details are available on our project page: swing-bench.github.io", "arxiv_url": "https://arxiv.org/abs/2505.23932", "authors": ["Wendong Xu", "Jing Xiong", "Chenyang Zhao", "Qiujiang Chen", "Haoran Wang", "Hui Shen", "Zhongwei Wan", "Jianbo Dai", "Taiqiang Wu", "He Xiao", "Chaofan Tao", "Z. Morley Mao", "Ying Sheng", "Zhijiang Guo", "Hongxia Yang", "Bei Yu", "Lingpeng Kong", "Quanquan Gu", "Ngai Wong"], "first_author": "Wendong Xu", "category": ["Benchmark", "Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["CI-driven evaluation", "Submitter–reviewer interaction", "Retrieval-augmented code generation", "BM25 sparse retrieval with dense reranking", "Syntax-aware code chunking", "Adversarial patch and test generation", "Multi-language repository support", "GitHub Actions pipeline simulation", "Long-context code understanding"], "summary": "SWINGARENA 提出一个通过模拟完整 CI 流水线、提交者—评审循环和检索增强的长上下文代码检索来评估 LLM 在真实多语言 GitHub issue 修复场景中表现的可扩展基准与框架。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2505.23932v2", "published": "2025-05-29", "update_time": "2025-06-02", "download_time": "2025-12-11 15:50:55"}
{"id": "2507.05281", "title": "CoreCodeBench: A Configurable Multi-Scenario Repository-Level Benchmark", "abstract": "As Large Language Models (LLMs) demonstrate increasingly sophisticated code processing capabilities, evaluating their performance on engineering-level code remains challenging. Existing repository-level benchmarks primarily focus on single scenarios, such as code generation or bug fixing, without adequately capturing the diversity and complexity of real-world software or project engineering workflows. Furthermore, these benchmarks suffer from limited controllability in question positioning and reliability issues in their generated test cases. To address these limitations, we present CorePipe, a fully automated pipeline that converts repositories into comprehensive test cases, and introduce CoreCodeBench, a configurable multi-scenario repository-level benchmark. To simulate real engineering scenarios, CorePipe generates three types of atomic questions (Development, BugFix, and Test-Driven Development) specifically targeting core code segments. These atomic questions are further combined into three types of composite questions, with difficulty levels flexibly adjusted through hyperparameter tuning. CoreCodeBench provides a comprehensive and extensive repository-level benchmark to investigate the applicability of LLMs in real-world engineering projects. Experiments with 16 LLMs across diverse scenarios reveal varying capabilities and offer multi-dimensional insights into LLM performance in engineering contexts. The code for CorePipe is available at https://github.com/AGI-Eval-Official/CoreCodeBench, and the data for CoreCodeBench can be accessed at https://huggingface.co/collections/tubehhh/corecodebench-68256d2faabf4b1610a08caa.", "arxiv_url": "https://arxiv.org/abs/2507.05281", "authors": ["Lingyue Fu", "Hao Guan", "Bolun Zhang", "Haowei Yuan", "Yaoming Zhu", "Jun Xu", "Zongyu Wang", "Lin Qiu", "Xunliang Cai", "Xuezhi Cao", "Weiwen Liu", "Weinan Zhang", "Yong Yu"], "first_author": "Lingyue Fu", "category": ["Benchmark", "Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Repository-level benchmark generation", "Automated repository-to-test pipeline", "Core code segment identification", "Atomic and composite problem composition", "Configurable difficulty and position control", "Test-driven development scenario synthesis", "Quality inspection of generated tests", "Cross-file contextual reasoning evaluation"], "summary": "本文提出了自动化管道CorePipe并发布CORECODEBENCH，一个可配置的多场景仓库级基准，用于从真实代码库生成高质量的开发、修复和TDD测试用例并评估LLM在工程级代码任务中的表现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2507.05281v1", "published": "2025-07-04", "update_time": "2025-07-04", "download_time": "2025-12-11 15:51:30"}
{"id": "2507.09866", "title": "Turning the Tide: Repository-based Code Reflection", "abstract": "Code large language models (LLMs) enhance programming by understanding and generating code across languages, offering intelligent feedback, bug detection, and code updates through reflection, improving development efficiency and accessibility. While benchmarks (e.g. HumanEval/LiveCodeBench) evaluate code generation and real-world relevance, previous works ignore the scenario of modifying code in repositories. Considering challenges remaining in improving reflection capabilities and avoiding data contamination in dynamic benchmarks, we introduce LiveRepoReflection, a challenging benchmark for evaluating code understanding and generation in multi-file repository contexts, featuring 1,888 rigorously filtered test cases across $6$ programming languages to ensure diversity, correctness, and high difficulty. Further, we create RepoReflection-Instruct, a large-scale, quality-filtered instruction-tuning dataset derived from diverse sources, used to train RepoReflectionCoder through a two-turn dialogue process involving code generation and error-driven repair. The leaderboard evaluates over 40 LLMs to reflect the model performance of repository-based code reflection.", "arxiv_url": "https://arxiv.org/abs/2507.09866", "authors": ["Wei Zhang", "Jian Yang", "Jiaxi Yang", "Ya Wang", "Zhoujun Li", "Zeyu Cui", "Binyuan Hui", "Junyang Lin"], "first_author": "Wei Zhang", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Repository-level code reflection", "Multi-file dependency reasoning", "Unit-test-driven problem selection", "Cross-execution verification", "Dynamic decontamination and deduplication", "Simulated multi-turn instruction generation", "Difficulty filtering via multi-model pass rates"], "summary": "本文提出了 LiveRepoReflection——一个面向多文件仓库情境的动态高难度代码反思基准，构建了用于指令微调的 RepoReflection-Instruct 数据集并训练出 RepoReflectionCoder，同时通过自动化管线、沙箱交叉执行和多模型评估建立排行榜以量化仓库级别的代码理解与修复能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2507.09866v1", "published": "2025-07-14", "update_time": "2025-07-14", "download_time": "2025-12-11 15:52:08"}
{"id": "2507.12415", "title": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?", "abstract": "Code performance optimization is paramount in real-world software engineering and critical for production-level systems. While Large Language Models (LLMs) have demonstrated impressive capabilities in code generation and bug fixing, their proficiency in enhancing code performance at the repository level remains largely unexplored. To address this gap, we introduce SWE-Perf, the first benchmark specifically designed to systematically evaluate LLMs on code performance optimization tasks within authentic repository contexts. SWE-Perf comprises 140 carefully curated instances, each derived from performance-improving pull requests from popular GitHub repositories. Each benchmark instance includes the relevant codebase, target functions, performance-related tests, expert-authored patches, and executable environments. Through a comprehensive evaluation of representative methods that span file-level and repo-level approaches (e.g., Agentless and OpenHands), we reveal a substantial capability gap between existing LLMs and expert-level optimization performance, highlighting critical research opportunities in this emerging field.", "arxiv_url": "https://arxiv.org/abs/2507.12415", "authors": ["Xinyi He", "Qian Liu", "Mingzhe Du", "Lin Yan", "Zhijie Fan", "Yiming Huang", "Zejian Yuan", "Zejun Ma"], "first_author": "Xinyi He", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Performance Optimization", "tags": ["Repository-level performance benchmark", "Pull-request mining for performance improvements", "Runtime-based evaluation metrics", "Performance-oriented unit tests", "Executable Docker environments", "Expert-patch gold standards", "Oracle (file-level) vs realistic (repo-level) evaluations", "Repository-scale optimization challenges"], "summary": "SWE-Perf 提出并发布了第一个评估语言模型在真实代码仓库中进行代码性能优化能力的基准数据集，包含 140 个基于性能改进的 PR 实例、可执行环境、性能测试与专家补丁，并在文件级与仓库级设置下对多种方法进行了系统评估，揭示模型与专家之间显著的性能差距。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2507.12415v1", "published": "2025-07-16", "update_time": "2025-07-16", "download_time": "2025-12-11 15:52:42"}
{"id": "2509.04078", "title": "RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging Evaluation of Large Language Models", "abstract": "Large Language Models (LLMs) have exhibited significant proficiency in code debugging, especially in automatic program repair, which may substantially reduce the time consumption of developers and enhance their efficiency. Significant advancements in debugging datasets have been made to promote the development of code debugging. However, these datasets primarily focus on assessing the LLM's function-level code repair capabilities, neglecting the more complex and realistic repository-level scenarios, which leads to an incomplete understanding of the LLM's challenges in repository-level debugging. While several repository-level datasets have been proposed, they often suffer from limitations such as limited diversity of tasks, languages, and error types. To mitigate this challenge, this paper introduces RepoDebug, a multi-task and multi-language repository-level code debugging dataset with 22 subtypes of errors that supports 8 commonly used programming languages and 3 debugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs, where Claude 3.5 Sonnect, the best-performing model, still cannot perform well in repository-level debugging.", "arxiv_url": "https://arxiv.org/abs/2509.04078", "authors": ["Jingjing Liu", "Zeming Liu", "Zihao Cheng", "Mengliang He", "Xiaoming Shi", "Yuhang Guo", "Xiangrong Zhu", "Yuanfang Guo", "Yunhong Wang", "Haifeng Wang"], "first_author": "Jingjing Liu", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Repository-level debugging", "Multi-language benchmark", "Multi-task evaluation (identification, localization, repair)", "22 bug subtypes taxonomy", "AST-based bug injection (tree-sitter)", "Cross-repository train/test split to avoid data leakage", "Automated filtering plus manual validation", "Metrics distinguishing single vs. multiple error localization", "Empirical LLM comparison across languages and error types"], "summary": "本文提出了RepoDebug，一个覆盖8种语言、3类调试任务和22种错误子类型的仓库级多语言多任务代码调试基准，并用多款大模型评估显示现有模型在仓库级调试上仍存在显著不足。", "quality": "High", "conference": "EMNLP 2025", "pdf_url": "https://arxiv.org/pdf/2509.04078v2", "published": "2025-09-04", "update_time": "2025-09-08", "download_time": "2025-12-11 15:53:22"}
{"id": "2509.16941", "title": "SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?", "abstract": "We introduce SWE-Bench Pro, a substantially more challenging benchmark that builds upon the best practices of SWE-BENCH [25], but is explicitly designed to capture realistic, complex, enterprise-level problems beyond the scope of SWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of 41 actively maintained repositories spanning business applications, B2B services, and developer tools. The benchmark is partitioned into a public set with open access to problems sourced from 11 repositories, a held-out set of 12 repositories and a commercial set of 18 proprietary repositories where we have formal partnership agreements with early-stage startups. Problems in the held-out and the commercial set are not publicly accessible, but we release results on the commercial set. Our benchmark features long-horizon tasks that may require hours to days for a professional software engineer to complete, often involving patches across multiple files and substantial code modifications. All tasks are human-verified and augmented with sufficient context to ensure resolvability. To better understand these limitations, we cluster the failure modes observed in the collected agent trajectories for a clearer characterization of the error patterns exhibited by current models. Overall, SWE-BENCH PRO provides a contamination-resistant testbed that more faithfully captures the complexity and diversity of real-world software development, advancing the pursuit of truly autonomous software engineering agents at a professional level.", "arxiv_url": "https://arxiv.org/abs/2509.16941", "authors": ["Xiang Deng", "Jeff Da", "Edwin Pan", "Yannis Yiming He", "Charles Ide", "Kanak Garg", "Niklas Lauffer", "Andrew Park", "Nitin Pasari", "Chetan Rane", "Karmini Sampath", "Maya Krishnan", "Srivatsa Kundurthy", "Sean Hendryx", "Zifan Wang", "Vijay Bharadwaj", "Jeff Holm", "Raja Aluri", "Chen Bo Calvin Zhang", "Noah Jacobson", "Bing Liu", "Brad Kenstler"], "first_author": "Xiang Deng", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Contamination-resistant curation", "Copyleft (GPL) licensing strategy", "Private commercial repositories", "Long-horizon multi-file code patches", "Human-in-the-loop task augmentation", "Unit-test-based verification", "Held-out and commercial test partitions", "Trajectory-level failure clustering", "Repository-level evaluation protocols", "Diagnostic analyses of agent failures"], "summary": "本文提出了一个面向企业级、长时程、多文件修改的污染抵抗软件工程基准，通过仅选用强制开源许可仓库并引入私有商用代码库、人工增强与单元测试验证来构建和评估能解决真实工程问题的代码代理，并对失败轨迹进行聚类分析以诊断当前模型局限。 ", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2509.16941v2", "published": "2025-09-21", "update_time": "2025-11-14", "download_time": "2025-12-11 15:53:52"}
{"id": "2510.14509", "title": "E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task", "abstract": "The rapid advancement in large language models (LLMs) has demonstrated significant potential in End-to-End Software Development (E2ESD). However, existing E2ESD benchmarks are limited by coarse-grained requirement specifications and unreliable evaluation protocols, hindering a true understanding of current framework capabilities. To address these limitations, we present E2EDev, a novel benchmark grounded in the principles of Behavior-Driven Development (BDD), which evaluates the capabilities of E2ESD frameworks by assessing whether the generated software meets user needs through mimicking real user interactions (Figure 1). E2EDev comprises (i) a fine-grained set of user requirements, (ii) multiple BDD test scenarios with corresponding Python step implementations for each requirement, and (iii) a fully automated testing pipeline built on the Behave framework. To ensure its quality while reducing the annotation effort, E2EDev leverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework (HITL-MAA). By evaluating various E2ESD frameworks and LLM backbones with E2EDev, our analysis reveals a persistent struggle to effectively solve these tasks, underscoring the critical need for more effective and cost-efficient E2ESD solutions. Our codebase and benchmark are publicly available at https://github.com/SCUNLP/E2EDev.", "arxiv_url": "https://arxiv.org/abs/2510.14509", "authors": ["Jingyao Liu", "Chen Huang", "Zhizhao Guan", "Wenqiang Lei", "Yang Deng"], "first_author": "Jingyao Liu", "category": ["Benchmark", "Technical"], "field": "Requirements & Design", "task": "Specification & Validation", "tags": ["Behavior-Driven Development", "Executable BDD test scenarios", "Human-in-the-Loop Multi-Agent Annotation", "Behave framework integration", "Fine-grained user requirement decomposition", "End-to-end software generation evaluation", "Multi-agent vs single-agent E2ESD comparison", "Error analysis and token-cost evaluation"], "summary": "该论文提出了基于BDD的E2EDev基准与人机协同多代理注释框架，通过可执行的BDD测试和自动化管道评估端到端软件生成框架的需求满足情况并分析其性能与开销。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2510.14509v2", "published": "2025-10-16", "update_time": "2025-10-24", "download_time": "2025-12-11 15:54:37"}
{"id": "2511.02352", "title": "SWE-Sharp-Bench: A Reproducible Benchmark for C# Software Engineering Tasks", "abstract": "AI coding agents have shown great progress on Python software engineering benchmarks like SWE-Bench, and for other languages like Java and C in benchmarks like Multi-SWE-Bench. However, C# -- a prominent enterprise language ranking #5 in the TIOBE index -- remains absent from such benchmarks. We introduce SWE-Sharp-Bench, a reproducible software engineering benchmark for C# featuring 150 instances from 17 repositories. Evaluating identical model-agent configurations across languages reveals a significant performance gap: while 70% of Python tasks in SWE-Bench Verified are solved, only 40% of our C# tasks are resolved. We open-source SWE-Sharp-Bench and our entire curation pipeline.", "arxiv_url": "https://arxiv.org/abs/2511.02352", "authors": ["Sanket Mhatre", "Yasharth Bajpai", "Sumit Gulwani", "Emerson Murphy-Hill", "Gustavo Soares"], "first_author": "Sanket Mhatre", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["C#/.NET ecosystem", "Reproducible containerized environments", "NuGet/MSBuild dependency resolution", "Execution-based filtering (pass→fail→pass)", "Patch complexity analysis", "Repository-level pull-request tasks", "Agent resolution-rate evaluation", "Multi-file coordinated edits"], "summary": "本文提出SWE-Sharp-Bench——首个面向C#/.NET生态的可复现软件工程基准（150个实例、17个仓库）并开源构建管道，实验证明当前大模型代理在C#上解决率显著低于Python，主要受复杂多文件补丁和依赖/构建管理影响。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.02352v3", "published": "2025-11-04", "update_time": "2025-11-18", "download_time": "2025-12-11 15:55:19"}
{"id": "2511.03404", "title": "Towards Realistic Project-Level Code Generation via Multi-Agent Collaboration and Semantic Architecture Modeling", "abstract": "In recent years, Large Language Models (LLMs) have achieved remarkable progress in automated code generation. In real-world software engineering, the growing demand for rapid iteration and continuous delivery underscores the importance of project-level code generation, where LLMs are expected to generate complete software projects directly from complex user requirements. Although existing studies have made initial explorations, they still face key limitations, including unrealistic datasets and unreliable evaluation metrics that fail to reflect real-world complexity, the semantic gap between human-written requirements and machine-interpretable structures, and difficulties in managing hierarchical dependencies and maintaining quality throughout the generation process. To address these limitations, we first introduce CodeProjectEval, a project-level code generation dataset built from 18 real-world repositories with 12.7 files and 2,388.6 lines of code per task on average, supplemented with documentation and executable test cases for automatic evaluation. We further propose ProjectGen, a multi-agent framework that decomposes projects into architecture design, skeleton generation, and code filling stages with iterative refinement and memory-based context management. Within this framework, we introduce the Semantic Software Architecture Tree (SSAT), a structured and semantically rich representation that effectively bridges user requirements and source code implementation. Experiments show that ProjectGen achieves state-of-the-art performance, passing 52/124 test cases on the small-scale project-level code generation dataset DevBench, a 57% improvement over the baseline approaches, and 310 test cases on CodeProjectEval, representing an improvement of roughly tenfold compared to the baselines.", "arxiv_url": "https://arxiv.org/abs/2511.03404", "authors": ["Qianhui Zhao", "Li Zhang", "Fang Liu", "Junhang Cheng", "Chengru Wu", "Junchen Ai", "Qiaoyuanhe Meng", "Lichen Zhang", "Xiaoli Lian", "Shubin Song", "Yuanping Guo"], "first_author": "Qianhui Zhao", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Project-level Code Generation", "Multi-agent Collaboration", "Semantic Software Architecture Tree", "Architecture-aware Synthesis", "Skeleton-and-fill Pipeline", "Iterative Refinement", "Memory-based Context Management", "Executable Test-case Evaluation", "Hierarchical Dependency Management"], "summary": "该论文提出了真实项目级代码生成数据集CodeProjectEval，并设计了多智能体ProjectGen框架与语义软件架构树（SSAT）作为中间表征，通过架构设计、骨架生成与代码填充的分阶段迭代及记忆化上下文管理显著提升了项目级代码生成的可执行性与质量。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.03404v1", "published": "2025-11-05", "update_time": "2025-11-05", "download_time": "2025-12-11 15:55:52"}
{"id": "2511.06090", "title": "SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?", "abstract": "Optimizing the performance of large-scale software repositories demands expertise in code reasoning and software engineering (SWE) to reduce runtime while preserving program correctness. However, most benchmarks emphasize what to fix rather than how to fix code. We introduce SWE-fficiency, a benchmark for evaluating repository-level performance optimization on real workloads. Our suite contains 498 tasks across nine widely used data-science, machine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a complete codebase and a slow workload, an agent must investigate code semantics, localize bottlenecks and relevant tests, and produce a patch that matches or exceeds expert speedup while passing the same unit tests. To enable this how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests for performance-improving edits, combining keyword filtering, static analysis, coverage tooling, and execution validation to both confirm expert speedup baselines and identify relevant repository unit tests. Empirical evaluation of state-of-the-art agents reveals significant underperformance. On average, agents achieve less than 0.15x the expert speedup: agents struggle in localizing optimization opportunities, reasoning about execution across functions, and maintaining correctness in proposed edits. We release the benchmark and accompanying data pipeline to facilitate research on automated performance engineering and long-horizon software reasoning.", "arxiv_url": "https://arxiv.org/abs/2511.06090", "authors": ["Jeffrey Jian Ma", "Milad Hashemi", "Amir Yazdanbakhsh", "Kevin Swersky", "Ofir Press", "Enhui Li", "Vijay Janapa Reddi", "Parthasarathy Ranganathan"], "first_author": "Jeffrey Jian Ma", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Repository-level performance optimization", "Workload-driven patch generation", "Test localization via coverage", "Automated PR scraping pipeline", "Oracle-free evaluation", "Speedup ratio metric", "Real-world Python scientific libraries", "Correctness-preserving edits", "Long-horizon codebase reasoning"], "summary": "本文提出SWE-FFICIENCY基准与可复现的数据采集管道，包含498个来自真实Python科学计算仓库的性能优化任务，要求在不破坏仓库单元测试的前提下通过代码修改加速指定工作负载，并发现现有语言模型在定位瓶颈、跨函数执行推理和保持正确性方面远落后于专家（平均仅达专家提速的0.15倍）。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.06090v2", "published": "2025-11-08", "update_time": "2025-11-11", "download_time": "2025-12-11 15:56:27"}
{"id": "1706.03762", "title": "Attention Is All You Need", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.", "arxiv_url": "https://arxiv.org/abs/1706.03762", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "first_author": "Ashish Vaswani", "category": ["Technical"], "field": "Model Architecture", "task": "Self-Attention Transformer for Sequence Transduction", "tags": ["Self-Attention", "Multi-Head Attention", "Scaled Dot-Product Attention", "Positional Encoding", "Encoder-Decoder Stack", "Position-wise Feed-Forward", "Parallelizable Training", "Autoregressive Decoding & Masking"], "summary": "本文提出了Transformer——一种完全基于自注意力的序列转换模型，通过多头注意力、位置编码与逐点前馈网络替代循环和卷积，从而显著提升并行性与翻译性能。", "quality": "High", "conference": "NeurIPS 2017", "pdf_url": "https://arxiv.org/pdf/1706.03762v7", "published": "2017-06-12", "update_time": "2023-08-02", "download_time": "2025-12-11 15:57:25"}
{"id": "1704.04856", "title": "A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes", "abstract": "We propose a model to automatically describe changes introduced in the source code of a program using natural language. Our method receives as input a set of code commits, which contains both the modifications and message introduced by an user. These two modalities are used to train an encoder-decoder architecture. We evaluated our approach on twelve real world open source projects from four different programming languages. Quantitative and qualitative results showed that the proposed approach can generate feasible and semantically sound descriptions not only in standard in-project settings, but also in a cross-project setting.", "arxiv_url": "https://arxiv.org/abs/1704.04856", "authors": ["Pablo Loyola", "Edison Marrese-Taylor", "Yutaka Matsuo"], "first_author": "Pablo Loyola", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["Commit Message Generation", "Diff-level Summarization", "Attention-based Encoder-Decoder", "LSTM Decoder with Beam Search", "Line-level Code Tokenization", "Atomic-commit Assumption", "Cross-project Evaluation", "Neural MT Baseline Comparison"], "summary": "本文提出一种基于注意力的编码-解码神经网络，用于从源码变更(diff/commit)自动生成自然语言描述，并在12个开源项目上进行原位与跨项目实证评估以验证效果。", "quality": "Middle", "conference": "ACL 2017", "pdf_url": "https://arxiv.org/pdf/1704.04856v1", "published": "2017-04-17", "update_time": "2017-04-17", "download_time": "2025-12-11 16:26:23"}
{"id": "1708.09492", "title": "Automatically Generating Commit Messages from Diffs using Neural Machine Translation", "abstract": "Commit messages are a valuable resource in comprehension of software evolution, since they provide a record of changes such as feature additions and bug repairs. Unfortunately, programmers often neglect to write good commit messages. Different techniques have been proposed to help programmers by automatically writing these messages. These techniques are effective at describing what changed, but are often verbose and lack context for understanding the rationale behind a change. In contrast, humans write messages that are short and summarize the high level rationale. In this paper, we adapt Neural Machine Translation (NMT) to automatically \"translate\" diffs into commit messages. We trained an NMT algorithm using a corpus of diffs and human-written commit messages from the top 1k Github projects. We designed a filter to help ensure that we only trained the algorithm on higher-quality commit messages. Our evaluation uncovered a pattern in which the messages we generate tend to be either very high or very low quality. Therefore, we created a quality-assurance filter to detect cases in which we are unable to produce good messages, and return a warning instead.", "arxiv_url": "https://arxiv.org/abs/1708.09492", "authors": ["Siyuan Jiang", "Ameer Armaly", "Collin McMillan"], "first_author": "Siyuan Jiang", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["Diff-to-commit translation", "Diff-level summarization", "Neural Machine Translation for code changes", "Verb-Direct-Object filtering", "Quality-assurance filter for generation", "Attentional RNN encoder-decoder", "Human evaluation"], "summary": "本文提出将 git diff 作为输入、使用注意力 RNN 的神经机器翻译模型自动生成一行高层次的提交信息，并通过动词-直接宾语过滤与质量保证过滤器在约2M 对提交数据上进行训练与人工评估。", "quality": "High", "conference": "ASE (IEEE/ACM International Conference on Automated Software Engineering) 2017", "pdf_url": "https://arxiv.org/pdf/1708.09492v1", "published": "2017-08-30", "update_time": "2017-08-30", "download_time": "2025-12-11 16:27:07"}
{"id": "1912.02972", "title": "ATOM: Commit Message Generation Based on Abstract Syntax Tree and Hybrid Ranking", "abstract": "Commit messages record code changes (e.g., feature modifications and bug repairs) in natural language, and are useful for program comprehension. Due to the frequent updates of software and time cost, developers are generally unmotivated to write commit messages for code changes. Therefore, automating the message writing process is necessitated. Previous studies on commit message generation have been benefited from generation models or retrieval models, but the code structure of changed code, i.e., AST, which can be important for capturing code semantics, has not been explicitly involved. Moreover, although generation models have the advantages of synthesizing commit messages for new code changes, they are not easy to bridge the semantic gap between code and natural languages which could be mitigated by retrieval models. In this paper, we propose a novel commit message generation model, named ATOM, which explicitly incorporates the abstract syntax tree for representing code changes and integrates both retrieved and generated messages through hybrid ranking. Specifically, the hybrid ranking module can prioritize the most accurate message from both retrieved and generated messages regarding one code change. We evaluate the proposed model ATOM on our dataset crawled from 56 popular Java repositories. Experimental results demonstrate that ATOM increases the state-of-the-art models by 30.72% in terms of BLEU-4 (an accuracy measure that is widely used to evaluate text generation systems). Qualitative analysis also demonstrates the effectiveness of ATOM in generating accurate code commit messages.", "arxiv_url": "https://arxiv.org/abs/1912.02972", "authors": ["Shangqing Liu", "Cuiyun Gao", "Sen Chen", "Lun Yiu Nie", "Yang Liu"], "first_author": "Shangqing Liu", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["AST-based code representation", "AST-to-sequence encoding", "Hybrid retrieval-generation ranking", "Commit/diff-level summarization", "BiLSTM with attention for AST paths", "Function-level dataset cleaning"], "summary": "本文提出ATOM，一种显式利用抽象语法树进行代码变更表示并通过检索与生成结果的混合排序优先选取最准确提交信息的方法，同时发布了约16万条Java提交的基准数据集，显著提升了BLEU-4成绩。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/1912.02972v2", "published": "2019-12-06", "update_time": "2020-11-11", "download_time": "2025-12-11 16:27:38"}
{"id": "2105.14242", "title": "CommitBERT: Commit Message Generation Using Pre-Trained Programming Language Model", "abstract": "Commit message is a document that summarizes source code changes in natural language. A good commit message clearly shows the source code changes, so this enhances collaboration between developers. Therefore, our work is to develop a model that automatically writes the commit message.   To this end, we release 345K datasets consisting of code modification and commit messages in six programming languages (Python, PHP, Go, Java, JavaScript, and Ruby). Similar to the neural machine translation (NMT) model, using our dataset, we feed the code modification to the encoder input and the commit message to the decoder input and measure the result of the generated commit message with BLEU-4.   Also, we propose the following two training methods to improve the result of generating the commit message: (1) A method of preprocessing the input to feed the code modification to the encoder input. (2) A method that uses an initial weight suitable for the code domain to reduce the gap in contextual representation between programming language (PL) and natural language (NL). Training code, dataset, and pre-trained weights are available at https://github.com/graykode/commit-autosuggestions", "arxiv_url": "https://arxiv.org/abs/2105.14242", "authors": ["Tae-Hwan Jung"], "first_author": "Tae-Hwan Jung", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["Commit message generation", "Git diff added/deleted pair representation", "Cross-language dataset (6 languages)", "Pretrained code→NL transfer", "Transformer encoder-decoder fine-tuning", "Dataset curation and license filtering", "Evaluation with BLEU-4"], "summary": "本文发布了一个包含345K对添加/删除代码片段与提交消息的多语言数据集，并通过在预训练的代码域语言模型上进行微调，以区分新增/删除部分的二元输入显著提升了提交消息生成效果。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2105.14242v1", "published": "2021-05-29", "update_time": "2021-05-29", "download_time": "2025-12-11 16:28:08"}
{"id": "2107.05373", "title": "On the Evaluation of Commit Message Generation Models: An Experimental Study", "abstract": "Commit messages are natural language descriptions of code changes, which are important for program understanding and maintenance. However, writing commit messages manually is time-consuming and laborious, especially when the code is updated frequently. Various approaches utilizing generation or retrieval techniques have been proposed to automatically generate commit messages. To achieve a better understanding of how the existing approaches perform in solving this problem, this paper conducts a systematic and in-depth analysis of the state-of-the-art models and datasets. We find that: (1) Different variants of the BLEU metric are used in previous works, which affects the evaluation and understanding of existing methods. (2) Most existing datasets are crawled only from Java repositories while repositories in other programming languages are not sufficiently explored. (3) Dataset splitting strategies can influence the performance of existing models by a large margin. Some models show better performance when the datasets are split by commit, while other models perform better when the datasets are split by timestamp or by project. Based on our findings, we conduct a human evaluation and find the BLEU metric that best correlates with the human scores for the task. We also collect a large-scale, information-rich, and multi-language commit message dataset MCMD and evaluate existing models on this dataset. Furthermore, we conduct extensive experiments under different dataset splitting strategies and suggest the suitable models under different scenarios. Based on the experimental results and findings, we provide feasible suggestions for comprehensively evaluating commit message generation models and discuss possible future research directions. We believe this work can help practitioners and researchers better evaluate and select models for automatic commit message generation.", "arxiv_url": "https://arxiv.org/abs/2107.05373", "authors": ["Wei Tao", "Yanlin Wang", "Ensheng Shi", "Lun Du", "Shi Han", "Hongyu Zhang", "Dongmei Zhang", "Wenqiang Zhang"], "first_author": "Wei Tao", "category": ["Empirical", "Benchmark", "Survey"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["BLEU variant comparison", "Metric–human correlation", "Dataset splitting strategies (commit/timestamp/project)", "Multi‑language, information‑rich commit corpus", "Just‑In‑Time evaluation scenario", "Cross‑project generalization", "Retrieval vs generation vs hybrid approaches", "Commit metadata usage (timestamp, repo, SHA)"], "summary": "本文系统性评估了现有的提交消息生成模型与数据集，比对不同BLEU变体与人工评分的相关性，分析数据切分策略对模型性能的影响，并发布了一个大规模多语言提交消息数据集MCMD及相应评估建议。", "quality": "High", "conference": "International Conference on Software Maintenance and Evolution (ICSME 2021)", "pdf_url": "https://arxiv.org/pdf/2107.05373v3", "published": "2021-07-12", "update_time": "2021-07-26", "download_time": "2025-12-11 16:28:38"}
{"id": "2308.00147", "title": "Delving into Commit-Issue Correlation to Enhance Commit Message Generation Models", "abstract": "Commit message generation (CMG) is a challenging task in automated software engineering that aims to generate natural language descriptions of code changes for commits. Previous methods all start from the modified code snippets, outputting commit messages through template-based, retrieval-based, or learning-based models. While these methods can summarize what is modified from the perspective of code, they struggle to provide reasons for the commit. The correlation between commits and issues that could be a critical factor for generating rational commit messages is still unexplored.   In this work, we delve into the correlation between commits and issues from the perspective of dataset and methodology. We construct the first dataset anchored on combining correlated commits and issues. The dataset consists of an unlabeled commit-issue parallel part and a labeled part in which each example is provided with human-annotated rational information in the issue. Furthermore, we propose \\tool (\\underline{Ex}traction, \\underline{Gro}unding, \\underline{Fi}ne-tuning), a novel paradigm that can introduce the correlation between commits and issues into the training phase of models. To evaluate whether it is effective, we perform comprehensive experiments with various state-of-the-art CMG models. The results show that compared with the original models, the performance of \\tool-enhanced models is significantly improved.", "arxiv_url": "https://arxiv.org/abs/2308.00147", "authors": ["Liran Wang", "Xunzhu Tang", "Yichen He", "Changyu Ren", "Shuhua Shi", "Chaoran Yan", "Zhoujun Li"], "first_author": "Liran Wang", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["Commit-Issue Correlation", "Rational Information Extraction", "Issue-Grounded Code Representation", "Extraction-Grounding-Fine-tuning Paradigm", "Commit-Issue Parallel Dataset", "Human-Annotated Rationale Labels", "Training-Time Issue Augmentation"], "summary": "本文构建了首个提交—问题并行数据集并提出ExGroFi（提取—落地—微调）范式，通过将问题报告中的理性信息引入训练显著提升了自动生成提交信息的合理性与质量。", "quality": "High", "conference": "ASE (IEEE/ACM International Conference on Automated Software Engineering) 2023", "pdf_url": "https://arxiv.org/pdf/2308.00147v2", "published": "2023-07-31", "update_time": "2023-09-28", "download_time": "2025-12-11 16:29:10"}
{"id": "2308.07655", "title": "From Commit Message Generation to History-Aware Commit Message Completion", "abstract": "Commit messages are crucial to software development, allowing developers to track changes and collaborate effectively. Despite their utility, most commit messages lack important information since writing high-quality commit messages is tedious and time-consuming. The active research on commit message generation (CMG) has not yet led to wide adoption in practice. We argue that if we could shift the focus from commit message generation to commit message completion and use previous commit history as additional context, we could significantly improve the quality and the personal nature of the resulting commit messages.   In this paper, we propose and evaluate both of these novel ideas. Since the existing datasets lack historical data, we collect and share a novel dataset called CommitChronicle, containing 10.7M commits across 20 programming languages. We use this dataset to evaluate the completion setting and the usefulness of the historical context for state-of-the-art CMG models and GPT-3.5-turbo. Our results show that in some contexts, commit message completion shows better results than generation, and that while in general GPT-3.5-turbo performs worse, it shows potential for long and detailed messages. As for the history, the results show that historical information improves the performance of CMG models in the generation task, and the performance of GPT-3.5-turbo in both generation and completion.", "arxiv_url": "https://arxiv.org/abs/2308.07655", "authors": ["Aleksandra Eliseeva", "Yaroslav Sokolov", "Egor Bogomolov", "Yaroslav Golubev", "Danny Dig", "Timofey Bryksin"], "first_author": "Aleksandra Eliseeva", "category": ["Technical", "Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["Commit message completion", "History-aware generation", "Prefix-conditioned suggestion", "Diff-based textual representation", "Large-scale multilingual commit history", "Dataset filter bias analysis", "Evaluation of generation vs completion", "Personalization of commit messages"], "summary": "本文将提交信息生成重新表述为基于已输入前缀的提交信息补全并将历史提交作为上下文，构建并发布包含约1070万提交的多语言历史数据集，对多种CMG模型与LLM进行了系统评估，证明在若干场景下补全与历史信息能提升性能。", "quality": "High", "conference": "ASE (IEEE/ACM International Conference on Automated Software Engineering) 2023", "pdf_url": "https://arxiv.org/pdf/2308.07655v1", "published": "2023-08-15", "update_time": "2023-08-15", "download_time": "2025-12-11 16:29:48"}
{"id": "2303.12570", "title": "RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation", "abstract": "The task of repository-level code completion is to continue writing the unfinished code based on a broader context of the repository. While for automated code completion tools, it is difficult to utilize the useful information scattered in different files. We propose RepoCoder, a simple, generic, and effective framework to address the challenge. It streamlines the repository-level code completion process by incorporating a similarity-based retriever and a pre-trained code language model in an iterative retrieval-generation pipeline. RepoCoder makes effective utilization of repository-level information for code completion and has the ability to generate code at various levels of granularity. Moreover, we propose a new benchmark RepoEval, which consists of the latest and high-quality real-world repositories covering line, API invocation, and function body completion scenarios. Experimental results indicate that RepoCoder significantly improves the In-File completion baseline by over 10% in all settings and consistently outperforms the vanilla retrieval-augmented code completion approach. Furthermore, we validate the effectiveness of RepoCoder through comprehensive analysis, providing valuable insights for future research. Our source code and benchmark are publicly available: https://github.com/microsoft/CodeT/tree/main/RepoCoder", "arxiv_url": "https://arxiv.org/abs/2303.12570", "authors": ["Fengji Zhang", "Bei Chen", "Yue Zhang", "Jacky Keung", "Jin Liu", "Daoguang Zan", "Yi Mao", "Jian-Guang Lou", "Weizhu Chen"], "first_author": "Fengji Zhang", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Iterative retrieval-generation", "Repository-level context retrieval", "Retrieval-augmented code completion", "Sliding-window code snippet indexing", "Query grounding with model-generated code", "Multi-granularity completion (line/API/function)", "Unit-test-based evaluation"], "summary": "本文提出RepoCoder，一种通过迭代检索-生成管道利用仓库级上下文进行代码补全的框架，并构建了包含行、API 调用与函数体多粒度且借助单元测试评估的RepoEval基准，实验证明显著提升补全性能。", "quality": "High", "conference": "EMNLP 2023", "pdf_url": "https://arxiv.org/pdf/2303.12570v3", "published": "2023-03-22", "update_time": "2023-10-20", "download_time": "2025-12-11 16:30:21"}
{"id": "2306.10763", "title": "Guiding Language Models of Code with Global Context using Monitors", "abstract": "Language models of code (LMs) work well when the surrounding code provides sufficient context. This is not true when it becomes necessary to use types, functionality or APIs defined elsewhere in the repository or a linked library, especially those not seen during training. LMs suffer from limited awareness of such global context and end up hallucinating.   Integrated development environments (IDEs) assist developers in understanding repository context using static analysis. We extend this assistance, enjoyed by developers, to LMs. We propose monitor-guided decoding (MGD) where a monitor uses static analysis to guide the decoding. We construct a repository-level dataset PragmaticCode for method-completion in Java and evaluate MGD on it. On models of varying parameter scale, by monitoring for type-consistent object dereferences, MGD consistently improves compilation rates and agreement with ground truth. Further, LMs with fewer parameters, when augmented with MGD, can outperform larger LMs. With MGD, SantaCoder-1.1B achieves better compilation rate and next-identifier match than the much larger text-davinci-003 model.   We also conduct a generalizability study to evaluate the ability of MGD to generalize to multiple programming languages (Java, C# and Rust), coding scenarios (e.g., correct number of arguments to method calls), and to enforce richer semantic constraints (e.g., stateful API protocols). Our data and implementation are available at https://github.com/microsoft/monitors4codegen .", "arxiv_url": "https://arxiv.org/abs/2306.10763", "authors": ["Lakshya A Agrawal", "Aditya Kanade", "Navin Goyal", "Shuvendu K. Lahiri", "Sriram K. Rajamani"], "first_author": "Lakshya A Agrawal", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Monitor-guided decoding", "Static-analysis-driven token masking", "Language Server Protocol integration", "Type-aware identifier resolution", "Decoding-time semantic constraints", "Repository-level context", "Stateful API protocol enforcement", "Compilation-focused evaluation"], "summary": "本文提出监视器引导解码（MGD），在解码阶段调用静态分析（基于LSP）对模型生成的令牌进行约束，从而利用仓库级全局上下文提升类型一致性、编译率和与真实代码的匹配，并公开了相应的仓库级数据集与工具实现。", "quality": "High", "conference": "NeurIPS 2023", "pdf_url": "https://arxiv.org/pdf/2306.10763v2", "published": "2023-06-19", "update_time": "2023-11-03", "download_time": "2025-12-11 16:30:53"}
{"id": "2306.10998", "title": "RepoFusion: Training Code Models to Understand Your Repository", "abstract": "Despite the huge success of Large Language Models (LLMs) in coding assistants like GitHub Copilot, these models struggle to understand the context present in the repository (e.g., imports, parent classes, files with similar names, etc.), thereby producing inaccurate code completions. This effect is more pronounced when using these assistants for repositories that the model has not seen during training, such as proprietary software or work-in-progress code projects. Recent work has shown the promise of using context from the repository during inference. In this work, we extend this idea and propose RepoFusion, a framework to train models to incorporate relevant repository context. Experiments on single-line code completion show that our models trained with repository context significantly outperform much larger code models as CodeGen-16B-multi ($\\sim73\\times$ larger) and closely match the performance of the $\\sim 70\\times$ larger StarCoderBase model that was trained with the Fill-in-the-Middle objective. We find these results to be a novel and compelling demonstration of the gains that training with repository context can bring. We carry out extensive ablation studies to investigate the impact of design choices such as context type, number of contexts, context length, and initialization within our framework. Lastly, we release Stack-Repo, a dataset of 200 Java repositories with permissive licenses and near-deduplicated files that are augmented with three types of repository contexts. Additionally, we are making available the code and trained checkpoints for our work. Our released resources can be found at \\url{https://huggingface.co/RepoFusion}.", "arxiv_url": "https://arxiv.org/abs/2306.10998", "authors": ["Disha Shrivastava", "Denis Kocetkov", "Harm de Vries", "Dzmitry Bahdanau", "Torsten Scholak"], "first_author": "Disha Shrivastava", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Repository-level context integration", "Fusion-in-Decoder adaptation", "Repo-level prompt proposals", "Context retrieval (BM25 and embedding-based)", "Surrounding-context augmentation", "Single-line code completion", "Ablation studies on context design", "Java repository corpus release"], "summary": "本文提出RepoFusion，通过将多个仓库级上下文用Fusion-in-Decoder训练融入代码模型以提升单行代码补全性能，并发布了带上下文增强的Java仓库语料与训练检查点，且小模型在多项评测中显著优于更大的基线模型。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2306.10998v1", "published": "2023-06-19", "update_time": "2023-06-19", "download_time": "2025-12-11 16:31:32"}
{"id": "2309.12499", "title": "CodePlan: Repository-level Coding using LLMs and Planning", "abstract": "Software engineering activities such as package migration, fixing errors reports from static analysis or testing, and adding type annotations or other specifications to a codebase, involve pervasively editing the entire repository of code. We formulate these activities as repository-level coding tasks.   Recent tools like GitHub Copilot, which are powered by Large Language Models (LLMs), have succeeded in offering high-quality solutions to localized coding problems. Repository-level coding tasks are more involved and cannot be solved directly using LLMs, since code within a repository is inter-dependent and the entire repository may be too large to fit into the prompt. We frame repository-level coding as a planning problem and present a task-agnostic framework, called CodePlan to solve it. CodePlan synthesizes a multi-step chain of edits (plan), where each step results in a call to an LLM on a code location with context derived from the entire repository, previous code changes and task-specific instructions. CodePlan is based on a novel combination of an incremental dependency analysis, a change may-impact analysis and an adaptive planning algorithm.   We evaluate the effectiveness of CodePlan on two repository-level tasks: package migration (C#) and temporal code edits (Python). Each task is evaluated on multiple code repositories, each of which requires inter-dependent changes to many files (between 2-97 files). Coding tasks of this level of complexity have not been automated using LLMs before. Our results show that CodePlan has better match with the ground truth compared to baselines. CodePlan is able to get 5/6 repositories to pass the validity checks (e.g., to build without errors and make correct code edits) whereas the baselines (without planning but with the same type of contextual information as CodePlan) cannot get any of the repositories to pass them.", "arxiv_url": "https://arxiv.org/abs/2309.12499", "authors": ["Ramakrishna Bairi", "Atharv Sonwane", "Aditya Kanade", "Vageesh D C", "Arun Iyer", "Suresh Parthasarathy", "Sriram Rajamani", "B. Ashok", "Shashank Shet"], "first_author": "Ramakrishna Bairi", "category": ["Technical"], "field": "Maintenance", "task": "Refactoring", "tags": ["Repository-level planning", "Incremental dependency analysis", "Change may-impact analysis", "Adaptive planning algorithm", "Edit-obligation plan graph", "Seed vs derived edit specification synthesis", "Oracle-driven validation (build/tests/type)", "Multi-file API migration"], "summary": "本文提出CodePlan，将仓库级代码编辑建模为规划问题，结合增量依赖分析、变更影响分析与自适应规划，生成多步LLM驱动的跨文件编辑以完成包迁移和时间性代码修改等仓库级维护任务并通过构建/验证保证正确性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2309.12499v1", "published": "2023-09-21", "update_time": "2023-09-21", "download_time": "2025-12-11 16:32:15"}
{"id": "2310.11248", "title": "CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion", "abstract": "Code completion models have made significant progress in recent years, yet current popular evaluation datasets, such as HumanEval and MBPP, predominantly focus on code completion tasks within a single file. This over-simplified setting falls short of representing the real-world software development scenario where repositories span multiple files with numerous cross-file dependencies, and accessing and understanding cross-file context is often required to complete the code correctly.   To fill in this gap, we propose CrossCodeEval, a diverse and multilingual code completion benchmark that necessitates an in-depth cross-file contextual understanding to complete the code accurately. CrossCodeEval is built on a diverse set of real-world, open-sourced, permissively-licensed repositories in four popular programming languages: Python, Java, TypeScript, and C#. To create examples that strictly require cross-file context for accurate completion, we propose a straightforward yet efficient static-analysis-based approach to pinpoint the use of cross-file context within the current file.   Extensive experiments on state-of-the-art code language models like CodeGen and StarCoder demonstrate that CrossCodeEval is extremely challenging when the relevant cross-file context is absent, and we see clear improvements when adding these context into the prompt. However, despite such improvements, the pinnacle of performance remains notably unattained even with the highest-performing model, indicating that CrossCodeEval is also capable of assessing model's capability in leveraging extensive context to make better code completion. Finally, we benchmarked various methods in retrieving cross-file context, and show that CrossCodeEval can also be used to measure the capability of code retrievers.", "arxiv_url": "https://arxiv.org/abs/2310.11248", "authors": ["Yangruibo Ding", "Zijian Wang", "Wasi Uddin Ahmad", "Hantian Ding", "Ming Tan", "Nihal Jain", "Murali Krishna Ramanathan", "Ramesh Nallapati", "Parminder Bhatia", "Dan Roth", "Bing Xiang"], "first_author": "Yangruibo Ding", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Cross-file Context", "Cross-file Code Completion Benchmark", "Static-analysis-based Example Selection", "Undefined-name Detection", "Multilingual (Python/Java/TypeScript/C#)", "Repository-level Context Retrieval", "Data-leakage Mitigation", "Prompt Context Augmentation"], "summary": "该论文提出了CROSSCODEEVAL——一个来自真实开源仓库的多语言跨文件代码补全基准，使用静态分析自动构造必须依赖跨文件上下文的样例，并评估了多种代码模型和检索方法以展示跨文件上下文对补全性能的重要性。", "quality": "High", "conference": "NeurIPS", "pdf_url": "https://arxiv.org/pdf/2310.11248v2", "published": "2023-10-17", "update_time": "2023-11-17", "download_time": "2025-12-11 16:32:51"}
{"id": "2404.00599", "title": "EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories", "abstract": "How to evaluate Large Language Models (LLMs) in code generation is an open question. Existing benchmarks demonstrate poor alignment with real-world code repositories and are insufficient to evaluate the coding abilities of LLMs. This paper proposes a new benchmark - EvoCodeBench to address the preceding problems, which has three primary advances. (1) EvoCodeBench aligns with real-world repositories in multiple dimensions, e.g., code distributions and dependency distributions. (2) EvoCodeBench offers comprehensive annotations (e.g., requirements, reference code, and reference dependencies), and robust evaluation metrics (e.g., Pass@k and Recall@k). (3) EvoCodeBench is an evolving benchmark to avoid data leakage. We build an automatic pipeline to update EvoCodeBench from the latest repositories. We release the first version - EvoCodeBench-2403, containing 275 samples from 25 real-world repositories. Based on EvoCodeBench, we propose repository-level code generation and evaluate 10 popular LLMs (e.g., gpt-4, gpt-3.5, DeepSeek Coder, StarCoder 2, CodeLLaMa, Gemma, and Qwen 1.5). Our experiments reveal the coding abilities of these LLMs in real-world repositories. For example, the highest Pass@1 of gpt-4 only is 20.73% in our experiments. We also analyze failed cases and summarize the shortcomings of existing LLMs in EvoCodeBench. We release EvoCodeBench, all prompts, and LLMs' completions for further community analysis.", "arxiv_url": "https://arxiv.org/abs/2404.00599", "authors": ["Jia Li", "Ge Li", "Xuanming Zhang", "Yihong Dong", "Zhi Jin"], "first_author": "Jia Li", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Repository-level code generation", "Evolving benchmark", "Dependency-aware evaluation", "Reference-dependency Recall@k", "Pass@k functional testing", "Automatic update pipeline", "Real-world repository alignment", "Standalone vs non-standalone functions"], "summary": "本文提出了EvoCodeBench——一个与真实开源仓库分布对齐、可周期更新的仓库级代码生成基准，提供依赖注释与测试用例并用以评估多款LLM在实际仓库情境下的代码生成能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2404.00599v1", "published": "2024-03-31", "update_time": "2024-03-31", "download_time": "2025-12-11 16:33:24"}
{"id": "2405.19856", "title": "DevEval: A Manually-Annotated Code Generation Benchmark Aligned with Real-World Code Repositories", "abstract": "How to evaluate the coding abilities of Large Language Models (LLMs) remains an open question. We find that existing benchmarks are poorly aligned with real-world code repositories and are insufficient to evaluate the coding abilities of LLMs.   To address the knowledge gap, we propose a new benchmark named DevEval, which has three advances. (1) DevEval aligns with real-world repositories in multiple dimensions, e.g., code distributions and dependency distributions. (2) DevEval is annotated by 13 developers and contains comprehensive annotations (e.g., requirements, original repositories, reference code, and reference dependencies). (3) DevEval comprises 1,874 testing samples from 117 repositories, covering 10 popular domains (e.g., Internet, Database). Based on DevEval, we propose repository-level code generation and evaluate 8 popular LLMs on DevEval (e.g., gpt-4, gpt-3.5, StarCoder 2, DeepSeek Coder, CodeLLaMa). Our experiments reveal these LLMs' coding abilities in real-world code repositories. For example, in our experiments, the highest Pass@1 of gpt-4-turbo is only 53.04%. We also analyze LLMs' failed cases and summarize their shortcomings. We hope DevEval can facilitate the development of LLMs in real code repositories. DevEval, prompts, and LLMs' predictions have been released.", "arxiv_url": "https://arxiv.org/abs/2405.19856", "authors": ["Jia Li", "Ge Li", "Yunfei Zhao", "Yongmin Li", "Huanyu Liu", "Hao Zhu", "Lecheng Wang", "Kaibo Liu", "Zheng Fang", "Lanshen Wang", "Jiazheng Ding", "Xuanming Zhang", "Yuqi Zhu", "Yihong Dong", "Zhi Jin", "Binhua Li", "Fei Huang", "Yongbin Li"], "first_author": "Jia Li", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Repository-level code generation", "Dependency-aware evaluation", "Recall@k for dependency recall", "Pass@k functional testing", "Manual developer annotations", "Real-world code & dependency distribution alignment", "Cross-file and intra-file dependency tracking", "Context-aware function synthesis"], "summary": "本文提出 DevEval —— 一个由13名开发者手工标注、与真实代码库分布对齐的代码生成基准（1,874个样例），定义了仓库级代码生成任务并引入依赖召回指标，对多款大模型进行了评测与失败案例分析。", "quality": "High", "conference": "ACL 2024", "pdf_url": "https://arxiv.org/pdf/2405.19856v1", "published": "2024-05-30", "update_time": "2024-05-30", "download_time": "2025-12-11 16:33:53"}
{"id": "2406.12902", "title": "JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large Language Models", "abstract": "Code generation benchmarks such as HumanEval are widely adopted to evaluate LLMs' capabilities. However, after consolidating the latest 24 benchmarks, we noticed three significant imbalances. First, imbalanced programming language. 95.8% of benchmarks involve Python, while only 5 benchmarks involve Java. Second, imbalanced code granularity. Function-/statement-level benchmarks account for over 83.3% of benchmarks. Only a mere handful extends to class-/project-levels, and all are limited to Python. Third, lacking advanced features. Existing benchmarks primarily assess basic coding skills, while overlooking advanced Object-Oriented Programming (OOP) features (i.e., encapsulation, inheritance, and polymorphism).   To fill these gaps, we propose JavaBench, a project-level Java benchmark that exercises OOP features. It comprises four Java projects with 389 methods in 106 Java classes. The test coverage is up to 92%, and JavaBench is attested by 282 undergraduate students, reaching a 90.93/100 average score (i.e., pass rate against the test suite), ensuring the quality of documentation, code skeleton, and tests. To better evaluate LLM's capability against JavaBench, we introduce a systematic evaluation design covering three context settings and five synthesis strategies at two granularities using three hierarchical metrics. Our extensive experiment yields several interesting findings. First, we noticed that regarding project-level Java programming, LLMs are far behind undergraduate students (no project can be correctly completed by any studied LLMs, and at most 41.17% Pass@5 in a more relaxed evaluation). Second, using method signature as prompt context may strike an ideal balance for project-level code generation. JavaBench is publicly available at https://github.com/java-bench/JavaBench.", "arxiv_url": "https://arxiv.org/abs/2406.12902", "authors": ["Jialun Cao", "Zhiyong Chen", "Jiarong Wu", "Shing-chi Cheung", "Chang Xu"], "first_author": "Jialun Cao", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Project-level Java generation", "Object-oriented features (encapsulation, inheritance, polymorphism)", "Method-signature context prompting", "Context ablation (max/min/selected)", "Synthesis strategies (holistic/independent/incremental)", "Hierarchical evaluation metrics (Completion/Compilation/Pass@k)", "Class-wise vs test-wise evaluation granularity", "Student-validated canonical solutions and human baseline", "High test coverage with mocking-based tests"], "summary": "本文提出了JavaBench——一个包含四个面向对象Java项目、由学生验证并带高覆盖测试的项目级基准，并给出系统化的多上下文、多合成策略评估，揭示当前LLM在面向对象Java生成上明显落后于本科生且在仅提供方法签名时表现最好。", "quality": "High", "conference": "ASE", "pdf_url": "https://arxiv.org/pdf/2406.12902v2", "published": "2024-06-10", "update_time": "2024-10-11", "download_time": "2025-12-11 16:34:30"}
{"id": "2406.06918", "title": "HumanEvo: An Evolution-aware Benchmark for More Realistic Evaluation of Repository-level Code Generation", "abstract": "To evaluate the repository-level code generation capabilities of Large Language Models (LLMs) in complex real-world software development scenarios, many evaluation methods have been developed. These methods typically leverage contextual code from the latest version of a project to assist LLMs in accurately generating the desired function. However, such evaluation methods fail to consider the dynamic evolution of software projects over time, which we refer to as evolution-ignored settings. This in turn results in inaccurate evaluation of LLMs' performance. In this paper, we conduct an empirical study to deeply understand LLMs' code generation performance within settings that reflect the evolution nature of software development. To achieve this, we first construct an evolution-aware repository-level code generation dataset, namely HumanEvo, equipped with an automated execution-based evaluation tool. Second, we manually categorize HumanEvo according to dependency levels to more comprehensively analyze the model's performance in generating functions with different dependency levels. Third, we conduct extensive experiments on HumanEvo with seven representative and diverse LLMs to verify the effectiveness of the proposed benchmark. We obtain several important findings through our experimental study. For example, we find that previous evolution-ignored evaluation methods result in inflated performance of LLMs, with performance overestimations ranging from 10.0% to 61.1% under different context acquisition methods, compared to the evolution-aware evaluation approach. Based on the findings, we give actionable suggestions for more realistic evaluation of LLMs on code generation. We also build a shared evolution-aware code generation toolbox to facilitate future research.", "arxiv_url": "https://arxiv.org/abs/2406.06918", "authors": ["Dewu Zheng", "Yanlin Wang", "Ensheng Shi", "Ruikai Zhang", "Yuchi Ma", "Hongyu Zhang", "Zibin Zheng"], "first_author": "Dewu Zheng", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Evolution-aware evaluation", "Future context leakage", "Useful context missing", "Repository rollback to base commit", "Dependency-level categorization", "Execution-based automated testing", "Temporal/context acquisition analysis", "Cross-language (Python and Java) benchmark"], "summary": "本文提出了HumanEvo——一个考虑代码库随时间演化的存储库级代码生成基准（含400个Python/Java任务与自动化执行评估），并实证表明忽略演化会导致LLM性能被显著高估，同时分析了依赖级别与上下文检索策略的影响。", "quality": "High", "conference": "International Conference on Software Engineering (ICSE 2025)", "pdf_url": "https://arxiv.org/pdf/2406.06918v2", "published": "2024-06-11", "update_time": "2025-03-18", "download_time": "2025-12-11 16:34:58"}
{"id": "2406.11927", "title": "On the Impacts of Contexts on Repository-Level Code Generation", "abstract": "CodeLLMs have gained widespread adoption for code generation tasks, yet their capacity to handle repository-level code generation with complex contextual dependencies remains underexplored. Our work underscores the critical importance of leveraging repository-level contexts to generate executable and functionally correct code. We present RepoExec, a novel benchmark designed to evaluate repository-level code generation, with a focus on three key aspects: executability, functional correctness through comprehensive test case generation, and accurate utilization of cross-file contexts. Our study examines a controlled scenario where developers specify essential code dependencies (contexts), challenging models to integrate them effectively. Additionally, we introduce an instruction-tuned dataset that enhances CodeLLMs' ability to leverage dependencies, along with a new metric, Dependency Invocation Rate (DIR), to quantify context utilization. Experimental results reveal that while pretrained LLMs demonstrate superior performance in terms of correctness, instruction-tuned models excel in context utilization and debugging capabilities. RepoExec offers a comprehensive evaluation framework for assessing code functionality and alignment with developer intent, thereby advancing the development of more reliable CodeLLMs for real-world applications. The dataset and source code are available at https://github.com/FSoft-AI4Code/RepoExec.", "arxiv_url": "https://arxiv.org/abs/2406.11927", "authors": ["Nam Le Hai", "Dung Manh Nguyen", "Nghi D. Q. Bui"], "first_author": "Nam Le Hai", "category": ["Benchmark", "Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Repository-level dependency handling", "Executable evaluation environment", "Dependency Invocation Rate (DIR)", "Automated high-coverage unit-test generation", "Dependency extraction tool", "Dependency-aware instruction tuning", "Multi-round test-driven debugging", "Context-size ablation (full/medium/small)"], "summary": "该论文提出了一个可执行的仓库级代码生成基准与评估范式（包含高覆盖单元测试与依赖提取工具）、引入依赖调用率（DIR）度量并通过对18个模型的实证评估和依赖增强的指令微调与多轮调试展示了依赖上下文对生成正确性与依赖利用的影响。", "quality": "High", "conference": "NAACL 2025", "pdf_url": "https://arxiv.org/pdf/2406.11927v4", "published": "2024-06-17", "update_time": "2025-02-09", "download_time": "2025-12-11 16:35:35"}
{"id": "2406.16801", "title": "RES-Q: Evaluating Code-Editing Large Language Model Systems at the Repository Scale", "abstract": "The instruction-following ability of Large Language Models (LLMs) has cultivated a class of LLM-based systems capable of approaching complex tasks such as making edits to large code repositories. Due to the high sensitivity and unpredictability of LLM behavior in response to changes in prompting, robust evaluation tools are needed to drive future iteration of these systems. We propose RES-Q, a natural language instruction-based benchmark for evaluating $\\textbf{R}$epository $\\textbf{E}$diting $\\textbf{S}$ystems, which consists of 100 handcrafted repository editing tasks derived from real GitHub commits. Given an edit instruction and a code repository, RES-Q evaluates an LLM system's ability to interpret the instruction, navigate the repository to gather relevant information, and construct an appropriate edit that satisfies the specified criteria. We argue that evaluating LLMs in this way addresses issues with traditional benchmarks and provides a more holistic assessment of a model's abilities. We evaluate various state-of-the-art LLMs as language agents in a repository-editing system built on Qurrent OS, our language agent development software. Despite their 1% pass@1 performance difference on HumanEval, we find Claude Sonnet 3.5 outperforms GPT-4o by 12% pass@1 on RES-Q, indicating RES-Q's capacity to differentiate model capability as traditional benchmarks approach saturation. We further investigate token efficiency, performance relationships with existing benchmarks, and interesting disparities between closed and open-source LLMs. Code and dataset are available at https://github.com/Qurrent-AI/RES-Q.", "arxiv_url": "https://arxiv.org/abs/2406.16801", "authors": ["Beck LaBash", "August Rosedale", "Alex Reents", "Lucas Negritto", "Colin Wiel"], "first_author": "Beck LaBash", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Repository-scale code editing", "Handcrafted test suites for verification", "Lazy (ambiguous) natural-language edit instructions", "Cross-file and multi-file patches", "Commit-derived realistic tasks", "Language-agent (LLM agent) evaluation", "Submission/evaluation harness for automated scoring", "Token-efficiency and benchmark saturation analysis"], "summary": "本文提出了RES-Q——由100个基于真实GitHub提交的仓库级代码编辑任务及手工测试套件组成的基准，并在语言代理系统中评估多种大型模型的仓库编辑能力且发布了提交评测环境。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2406.16801v2", "published": "2024-06-24", "update_time": "2024-06-25", "download_time": "2025-12-11 16:36:10"}
{"id": "2408.14354", "title": "SWE-bench-java: A GitHub Issue Resolving Benchmark for Java", "abstract": "GitHub issue resolving is a critical task in software engineering, recently gaining significant attention in both industry and academia. Within this task, SWE-bench has been released to evaluate issue resolving capabilities of large language models (LLMs), but has so far only focused on Python version. However, supporting more programming languages is also important, as there is a strong demand in industry. As a first step toward multilingual support, we have developed a Java version of SWE-bench, called SWE-bench-java. We have publicly released the dataset, along with the corresponding Docker-based evaluation environment and leaderboard, which will be continuously maintained and updated in the coming months. To verify the reliability of SWE-bench-java, we implement a classic method SWE-agent and test several powerful LLMs on it. As is well known, developing a high-quality multi-lingual benchmark is time-consuming and labor-intensive, so we welcome contributions through pull requests or collaboration to accelerate its iteration and refinement, paving the way for fully automated programming.", "arxiv_url": "https://arxiv.org/abs/2408.14354", "authors": ["Daoguang Zan", "Zhirong Huang", "Ailun Yu", "Shaoxin Lin", "Yifan Shi", "Wei Liu", "Dong Chen", "Zongshuai Qi", "Hao Yu", "Lei Yu", "Dezhi Ran", "Muhan Zeng", "Bo Shen", "Pan Bian", "Guangtai Liang", "Bei Guan", "Pengjie Huang", "Tao Xie", "Yongji Wang", "Qianxiang Wang"], "first_author": "Daoguang Zan", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Automated Program Repair", "Issue-to-Patch Generation", "Fail-to-Pass Test Extraction", "Docker-based Reproducible Evaluation", "Build Tool & JDK Inference", "Questionnaire-based Manual Verification", "Repository and Patch Mining", "Dependency Caching and Evaluation Optimization"], "summary": "本文提出并开源SWE-bench-java-verified——一个经手工验证的Java语言GitHub issue修复基准（91个可复现问题）、相应的Docker评测环境与排行榜，并基于该基准评估了多种模型与代理的修复能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2408.14354v1", "published": "2024-08-26", "update_time": "2024-08-26", "download_time": "2025-12-11 16:36:39"}
{"id": "2410.01353", "title": "Codev-Bench: How Do LLMs Understand Developer-Centric Code Completion?", "abstract": "Code completion, a key downstream task in code generation, is one of the most frequent and impactful methods for enhancing developer productivity in software development. As intelligent completion tools evolve, we need a robust evaluation benchmark that enables meaningful comparisons between products and guides future advancements. However, existing benchmarks focus more on coarse-grained tasks without industrial analysis resembling general code generation rather than the real-world scenarios developers encounter. Moreover, these benchmarks often rely on costly and time-consuming human annotation, and the standalone test cases fail to leverage minimal tests for maximum repository-level understanding and code coverage. To address these limitations, we first analyze business data from an industrial code completion tool and redefine the evaluation criteria to better align with the developer's intent and desired completion behavior throughout the coding process. Based on these insights, we introduce Codev-Agent, an agent-based system that automates repository crawling, constructs execution environments, extracts dynamic calling chains from existing unit tests, and generates new test samples to avoid data leakage, ensuring fair and effective comparisons. Using Codev-Agent, we present the Code-Development Benchmark (Codev-Bench), a fine-grained, real-world, repository-level, and developer-centric evaluation framework. Codev-Bench assesses whether a code completion tool can capture a developer's immediate intent and suggest appropriate code across diverse contexts, providing a more realistic benchmark for code completion in modern software development.", "arxiv_url": "https://arxiv.org/abs/2410.01353", "authors": ["Zhenyu Pan", "Rongyu Cao", "Yongchang Cao", "Yingwei Ma", "Binhua Li", "Fei Huang", "Han Liu", "Yongbin Li"], "first_author": "Zhenyu Pan", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Developer-centric evaluation", "Repository-level context", "Agent-based pipeline", "Dynamic call-chain extraction", "Automated test generation", "Execution environment reconstruction", "Data-leakage-aware sampling", "Industrial product feedback analysis"], "summary": "本文基于工业代码补全产品的使用反馈提出Codev-Agent自动化系统并构建了Codev-Bench，一个面向开发者、仓库级且细粒度的代码补全评测基准，通过仓库爬取、执行环境搭建、动态调用链提取与自动生成测试样例实现更真实和公平的模型评估。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2410.01353v3", "published": "2024-10-02", "update_time": "2024-10-24", "download_time": "2025-12-11 16:37:15"}
{"id": "2410.03859", "title": "SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?", "abstract": "Autonomous systems for software engineering are now capable of fixing bugs and developing features. These systems are commonly evaluated on SWE-bench (Jimenez et al., 2024a), which assesses their ability to solve software issues from GitHub repositories. However, SWE-bench uses only Python repositories, with problem statements presented predominantly as text and lacking visual elements such as images. This limited coverage motivates our inquiry into how existing systems might perform on unrepresented software engineering domains (e.g., front-end, game development, DevOps), which use different programming languages and paradigms. Therefore, we propose SWE-bench Multimodal (SWE-bench M), to evaluate systems on their ability to fix bugs in visual, user-facing JavaScript software. SWE-bench M features 617 task instances collected from 17 JavaScript libraries used for web interface design, diagramming, data visualization, syntax highlighting, and interactive mapping. Each SWE-bench M task instance contains at least one image in its problem statement or unit tests. Our analysis finds that top-performing SWE-bench systems struggle with SWE-bench M, revealing limitations in visual problem-solving and cross-language generalization. Lastly, we show that SWE-agent's flexible language-agnostic features enable it to substantially outperform alternatives on SWE-bench M, resolving 12% of task instances compared to 6% for the next best system.", "arxiv_url": "https://arxiv.org/abs/2410.03859", "authors": ["John Yang", "Carlos E. Jimenez", "Alex L. Zhang", "Kilian Lieret", "Joyce Yang", "Xindi Wu", "Ori Press", "Niklas Muennighoff", "Gabriel Synnaeve", "Karthik R. Narasimhan", "Diyi Yang", "Sida I. Wang", "Ofir Press"], "first_author": "John Yang", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Multimodal bug fixing", "UI screenshot comprehension", "Frontend JavaScript ecosystems", "Repository-level fail-to-pass evaluation", "Image-required task instances", "Cross-language generalization challenges", "Agent toolchain language-agnosticism"], "summary": "本文提出 SWE-bench Multimodal，一个包含图像/视频问题的 JavaScript 前端仓库级别缺陷修复基准，揭示现有自动化 LLM 系统在视觉问题理解与跨语言泛化上的不足并提供分析与改进建议。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2410.03859v1", "published": "2024-10-04", "update_time": "2024-10-04", "download_time": "2025-12-11 16:37:50"}
{"id": "2410.06992", "title": "SWE-Bench+: Enhanced Coding Benchmark for LLMs", "abstract": "Large Language Models (LLMs) in Software Engineering (SE) can offer assistance for coding. To facilitate a rigorous evaluation of LLMs in practical coding contexts, Carlos et al. introduced the SWE-bench dataset, which comprises 2,294 real-world GitHub issues and their corresponding pull requests, collected from 12 widely used Python repositories. Several impressive LLM-based toolkits recently are developed and evaluated on this dataset. However, a systematic evaluation of the quality of SWE-bench remains missing. In this paper, we addressed this gap by presenting an empirical analysis of the SWE-bench dataset. We conducted a manual screening of instances where SWEAgent + GPT-4 successfully resolved issues by comparing the model-generated patches with the actual pull requests. SWE-Agent+GPT-4 was at the top of SWE-bench leaderboard during the time of our study. Our analysis reveals some critical issues with the SWE-bench dataset: 1) 32.67% of the successful patches involve cheating as the solutions were directly provided in the issue report or the comments. We refer to as solution leakage problem. 2) 31.08% of the passed patches are suspicious patches due to weak test cases, i.e., the tests were not adequate to verify the correctness of a patch. When we filtered out these problematic issues, the resolution rate of SWE-Agent+GPT-4 dropped from 12.47% to 3.97%. We also observed that the same data quality issues also exist in the two variants of SWE-bench, i.e., SWE-bench Lite and SWE-Bench Verified. In addition, over 94% of the issues were created before LLM's knowledge cutoff dates, posing potential data leakage issues.", "arxiv_url": "https://arxiv.org/abs/2410.06992", "authors": ["Reem Aleithan", "Haoran Xue", "Mohammad Mahdi Mohajer", "Elijah Nnorom", "Gias Uddin", "Song Wang"], "first_author": "Reem Aleithan", "category": ["Empirical", "Benchmark"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Solution leakage detection", "Test adequacy / weak tests", "Temporal cutoff filtering", "Issue-to-patch evaluation", "Patch-level gold vs generated comparison", "Dataset curation for robust evaluation", "Leaderboard/result auditing"], "summary": "本文通过对现有SWE‑bench基准进行人工与实证分析，发现大量“解答泄露”和弱测试导致的可疑通过补丁并据此构建了避开模型训练截止期且去除泄露的新基准SWE‑Bench+，在新基准上模型的修复通过率显著下降。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2410.06992v2", "published": "2024-10-09", "update_time": "2024-10-10", "download_time": "2025-12-11 16:38:28"}
{"id": "2410.07331", "title": "DA-Code: Agent Data Science Code Generation Benchmark for Large Language Models", "abstract": "We introduce DA-Code, a code generation benchmark specifically designed to assess LLMs on agent-based data science tasks. This benchmark features three core elements: First, the tasks within DA-Code are inherently challenging, setting them apart from traditional code generation tasks and demanding advanced coding skills in grounding and planning. Second, examples in DA-Code are all based on real and diverse data, covering a wide range of complex data wrangling and analytics tasks. Third, to solve the tasks, the models must utilize complex data science programming languages, to perform intricate data processing and derive the answers. We set up the benchmark in a controllable and executable environment that aligns with real-world data analysis scenarios and is scalable. The annotators meticulously design the evaluation suite to ensure the accuracy and robustness of the evaluation. We develop the DA-Agent baseline. Experiments show that although the baseline performs better than other existing frameworks, using the current best LLMs achieves only 30.5% accuracy, leaving ample room for improvement. We release our benchmark at https://da-code-bench.github.io.", "arxiv_url": "https://arxiv.org/abs/2410.07331", "authors": ["Yiming Huang", "Jianwen Luo", "Yan Yu", "Yitong Zhang", "Fangyu Lei", "Yifan Wei", "Shizhu He", "Lifu Huang", "Xiao Liu", "Jun Zhao", "Kang Liu"], "first_author": "Yiming Huang", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Agent-based data science", "Interactive executable sandbox", "Data wrangling / EDA / ML pipeline", "Multi-source data (files, databases, documents)", "SQL+Python+Bash multi-language execution", "Autonomous iterative code editing", "Evaluation suite with red-teaming", "Baseline code agent framework"], "summary": "本文提出了DA-Code，一个包含500个真实复杂代理式数据科学任务的可执行基准与交互沙箱，并提供评估套件及基线代理以衡量大型语言模型在数据清洗、探索性分析与整个机器学习流水线上的自动化代码生成与推理能力。", "quality": "High", "conference": "EMNLP 2024", "pdf_url": "https://arxiv.org/pdf/2410.07331v2", "published": "2024-10-09", "update_time": "2024-10-11", "download_time": "2025-12-11 16:39:06"}
{"id": "2410.21647", "title": "Can Language Models Replace Programmers for Coding? REPOCOD Says 'Not Yet'", "abstract": "Recently, a number of repository-level code generation benchmarks-such as CoderEval, DevEval, RepoEval, RepoBench, and LongCodeArena-have emerged to evaluate the capabilities of large language models (LLMs) beyond standalone benchmarks like HumanEval and MBPP. Thus, a natural question is, would LLMs have similar performance in real world coding tasks as their performance in these benchmarks? Unfortunately, one cannot answer this question, since these benchmarks consist of short completions, synthetic examples, or focus on limited scale repositories, failing to represent real-world coding tasks.   To address these challenges, we create REPOCOD, a Python code-generation benchmark containing complex tasks with realistic dependencies in real-world large projects and appropriate metrics for evaluating source code. It includes 980 whole-function generation tasks from 11 popular projects, 50.8% of which require repository-level context. REPOCOD includes 314 developer-written test cases per instance for better evaluation. We evaluate ten LLMs on REPOCOD and find that none achieves more than 30% pass@1 on REPOCOD, indicating the necessity of building stronger LLMs that can help developers in real-world software development. In addition, we found that retrieval-augmented generation achieves better results than using target function dependencies as context.", "arxiv_url": "https://arxiv.org/abs/2410.21647", "authors": ["Shanchao Liang", "Yiran Hu", "Nan Jiang", "Lin Tan"], "first_author": "Shanchao Liang", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Repository-level dependencies", "Whole-function generation", "Developer-written unit tests for validation", "Static and dynamic analysis for test-function mapping", "Targeted test selection to reduce evaluation cost", "Retrieval-augmented generation vs dependency-context", "Large-scale real-world Python repositories", "Execution-based evaluation (unit-test based)"], "summary": "本文提出REPOCOD，一个包含来自11个大型Python项目的980个复杂仓库级全函数生成任务的基准，使用开发者编写的单元测试进行执行式评估并展示现有LLM在真实软件开发场景下表现仍然有限。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2410.21647v4", "published": "2024-10-29", "update_time": "2025-06-24", "download_time": "2025-12-11 16:39:44"}
{"id": "2410.21157", "title": "M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation", "abstract": "Repository-level code completion has drawn great attention in software engineering, and several benchmark datasets have been introduced. However, existing repository-level code completion benchmarks usually focus on a limited number of languages (<5), which cannot evaluate the general code intelligence abilities across different languages for existing code Large Language Models (LLMs). Besides, the existing benchmarks usually report overall average scores of different languages, where the fine-grained abilities in different completion scenarios are ignored. Therefore, to facilitate the research of code LLMs in multilingual scenarios, we propose a massively multilingual repository-level code completion benchmark covering 18 programming languages (called M2RC-EVAL), and two types of fine-grained annotations (i.e., bucket-level and semantic-level) on different completion scenarios are provided, where we obtain these annotations based on the parsed abstract syntax tree. Moreover, we also curate a massively multilingual instruction corpora M2RC- INSTRUCT dataset to improve the repository-level code completion abilities of existing code LLMs. Comprehensive experimental results demonstrate the effectiveness of our M2RC-EVAL and M2RC-INSTRUCT.", "arxiv_url": "https://arxiv.org/abs/2410.21157", "authors": ["Jiaheng Liu", "Ken Deng", "Congnan Liu", "Jian Yang", "Shukai Liu", "He Zhu", "Peng Zhao", "Linzheng Chai", "Yanan Wu", "Ke Jin", "Ge Zhang", "Zekun Wang", "Guoan Zhang", "Bangyu Xiang", "Wenbo Su", "Bo Zheng"], "first_author": "Jiaheng Liu", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Repository-level completion", "Multilingual evaluation (18 languages)", "AST-based cursor selection", "Bucket-level difficulty labeling", "Semantic-level code labels", "Cross-file context retrieval", "Multilingual instruction corpora for tuning"], "summary": "该论文提出了一个包含18种编程语言、基于AST提供桶级与语义级精细标注的仓库级多语言代码补全基准并发布配套的多语言指令语料，以评估并提升代码大模型的跨语言仓库级补全能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2410.21157v1", "published": "2024-10-28", "update_time": "2024-10-28", "download_time": "2025-12-11 16:40:18"}
{"id": "2411.18019", "title": "A Real-World Benchmark for Evaluating Fine-Grained Issue Solving Capabilities of Large Language Models", "abstract": "Automatically resolving software issues is crucial for software development in practice, impacting the software quality and user experience. The process of resolving real-world issues encompasses tasks such as question-answering (QA), fault localization, and code editing. Existing benchmarks such as HumanEval fall short in their ability to assess LLMs' proficiency in solving issues within a codebase. Although benchmarks like SWE-Bench are designed to evaluate the LLMs' capability to handle real-world GitHub issues, the end-to-end evaluation method cannot provide granular insights on the performance of subtasks involved in issue solving. To address existing deficiencies in benchmarking LLMs for practical software engineering tasks, we introduce FAUN-Eval, a benchmark specifically designed to evaluate the Fine-grAined issUe solviNg capabilities of LLMs. FAUN-Eval systematically assesses LLMs across three distinct tasks: QA, fault localization, and code editing. This benchmark is constructed using a dataset curated from 30 well-known GitHub repositories. For each entry, issue and pull request (PR) pairs are meticulously compiled and validated using cross-referencing and keyword verification methods. FAUN-Eval includes 300 entries and employs both LLM and manual checks to ensure data quality. We evaluate ten LLMs with FAUN-Eval, including four closed-source and six open-source models. Our experimental results reveal several key findings. We find that the top-performing LLMs differ across the different tasks. Additionally, features in issues may lead LLMs to generate incorrect information. Moreover, models may vary in their proficiency with texts of different lengths.", "arxiv_url": "https://arxiv.org/abs/2411.18019", "authors": ["Ruida Hu", "Chao Peng", "Jingyi Ren", "Bo Jiang", "Xiangxin Meng", "Qinyun Wu", "Pengfei Gao", "Xinchen Wang", "Cuiyun Gao"], "first_author": "Ruida Hu", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Fine-grained issue decomposition", "Real-world GitHub issue–PR pairing", "Fault localization evaluation", "Code edit/patch generation assessment", "Question-answering for issue triage", "Data quality verification (LLM + manual checks)", "Multi-task performance metrics (EM, CodeBLEU, Edit Similarity)", "Sensitivity to issue text features and length"], "summary": "本文提出FAUN-Eval，一个从真实GitHub仓库构建的细粒度基准与评估框架，用于分别评测大模型在问题问答、故障定位与代码编辑三项子任务上的能力并对多款模型进行对比分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.18019v1", "published": "2024-11-27", "update_time": "2024-11-27", "download_time": "2025-12-11 16:40:55"}
{"id": "2412.01769", "title": "Commit0: Library Generation from Scratch", "abstract": "With the goal of benchmarking generative systems beyond expert software development ability, we introduce Commit0, a benchmark that challenges AI agents to write libraries from scratch. Agents are provided with a specification document outlining the library's API as well as a suite of interactive unit tests, with the goal of producing an implementation of this API accordingly. The implementation is validated through running these unit tests. As a benchmark, Commit0 is designed to move beyond static one-shot code generation towards agents that must process long-form natural language specifications, adapt to multi-stage feedback, and generate code with complex dependencies. Commit0 also offers an interactive environment where models receive static analysis and execution feedback on the code they generate. Our experiments demonstrate that while current agents can pass some unit tests, none can yet fully reproduce full libraries. Results also show that interactive feedback is quite useful for models to generate code that passes more unit tests, validating the benchmarks that facilitate its use.", "arxiv_url": "https://arxiv.org/abs/2412.01769", "authors": ["Wenting Zhao", "Nan Jiang", "Celine Lee", "Justin T Chiu", "Claire Cardie", "Matthias Gallé", "Alexander M Rush"], "first_author": "Wenting Zhao", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Repository-level code generation", "Interactive test-driven synthesis", "Long-form specification processing", "Dependency-aware implementation ordering", "Execution & static-analysis feedback loop", "Starter-repo hole-filling", "Automated unit-test evaluation"], "summary": "COMMIT0 提出一个从零生成完整软件库的基准，提供长篇规范、起始仓库与交互式单元测试以评估和促进模型在多轮反馈、依赖管理与长上下文下的代码生成能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2412.01769v1", "published": "2024-12-02", "update_time": "2024-12-02", "download_time": "2025-12-11 16:41:31"}
{"id": "2412.11990", "title": "ExecRepoBench: Multi-level Executable Code Completion Evaluation", "abstract": "Code completion has become an essential tool for daily software development. Existing evaluation benchmarks often employ static methods that do not fully capture the dynamic nature of real-world coding environments and face significant challenges, including limited context length, reliance on superficial evaluation metrics, and potential overfitting to training datasets. In this work, we introduce a novel framework for enhancing code completion in software development through the creation of a repository-level benchmark ExecRepoBench and the instruction corpora Repo-Instruct, aim at improving the functionality of open-source large language models (LLMs) in real-world coding scenarios that involve complex interdependencies across multiple files. ExecRepoBench includes 1.2K samples from active Python repositories. Plus, we present a multi-level grammar-based completion methodology conditioned on the abstract syntax tree to mask code fragments at various logical units (e.g. statements, expressions, and functions). Then, we fine-tune the open-source LLM with 7B parameters on Repo-Instruct to produce a strong code completion baseline model Qwen2.5-Coder-Instruct-C based on the open-source model. Qwen2.5-Coder-Instruct-C is rigorously evaluated against existing benchmarks, including MultiPL-E and ExecRepoBench, which consistently outperforms prior baselines across all programming languages. The deployment of \\ourmethod{} can be used as a high-performance, local service for programming development\\footnote{\\url{https://execrepobench.github.io/}}.", "arxiv_url": "https://arxiv.org/abs/2412.11990", "authors": ["Jian Yang", "Jiajun Zhang", "Jiaxi Yang", "Ke Jin", "Lei Zhang", "Qiyao Peng", "Ken Deng", "Yibo Miao", "Tianyu Liu", "Zeyu Cui", "Binyuan Hui", "Junyang Lin"], "first_author": "Jian Yang", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Executable repository benchmark", "Unit-test-based evaluation", "AST-based multi-level masking", "Repository-level (cross-file) completion", "Instruction corpus for repo completion", "Data decontamination (leakage removal)", "Supervised fine-tuning for completion"], "summary": "本文提出了可执行的仓库级代码补全基准 EXECREPOBENCH 与基于抽象语法树的多级语法掩码指令语料 REPO-INSTRUCT，并在此基础上微调开源模型以实现基于单元测试的跨文件代码补全评测与改进。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2412.11990v1", "published": "2024-12-16", "update_time": "2024-12-16", "download_time": "2025-12-11 16:42:03"}
{"id": "2501.13699", "title": "DI-BENCH: Benchmarking Large Language Models on Dependency Inference with Testable Repositories at Scale", "abstract": "Large Language Models have advanced automated software development, however, it remains a challenge to correctly infer dependencies, namely, identifying the internal components and external packages required for a repository to successfully run. Existing studies highlight that dependency-related issues cause over 40\\% of observed runtime errors on the generated repository. To address this, we introduce DI-BENCH, a large-scale benchmark and evaluation framework specifically designed to assess LLMs' capability on dependency inference. The benchmark features 581 repositories with testing environments across Python, C#, Rust, and JavaScript. Extensive experiments with textual and execution-based metrics reveal that the current best-performing model achieves only a 42.9% execution pass rate, indicating significant room for improvement. DI-BENCH establishes a new viewpoint for evaluating LLM performance on repositories, paving the way for more robust end-to-end software synthesis.", "arxiv_url": "https://arxiv.org/abs/2501.13699", "authors": ["Linghao Zhang", "Junhao Wang", "Shilin He", "Chaoyun Zhang", "Yu Kang", "Bowen Li", "Jiaheng Wen", "Chengxing Xie", "Maoquan Wang", "Yufan Huang", "Elsie Nallipogu", "Qingwei Lin", "Yingnong Dang", "Saravan Rajmohan", "Dongmei Zhang", "Qi Zhang"], "first_author": "Linghao Zhang", "category": ["Benchmark", "Empirical"], "field": "Dependency & Build Management", "task": "Dependency Inference", "tags": ["CI-based execution evaluation", "Repository-level dependency extraction", "Masked build configuration reconstruction", "Cross-language coverage (Python, C#, Rust, JavaScript)", "Execution pass rate metric", "Dependency version resolution and mismatch", "Scalable curated testable repositories", "Precision/recall for dependency lists"], "summary": "DI-BENCH 提出一个包含581个可执行仓库、跨 Python/C#/Rust/JavaScript 的依赖推断基准，并通过复用 CI 流水线进行文本与执行级评估，揭示当前 LLM 在依赖推断上的显著不足（最佳执行率仅 42.9%）。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2501.13699v1", "published": "2025-01-23", "update_time": "2025-01-23", "download_time": "2025-12-11 16:42:35"}
{"id": "2502.00226", "title": "HackerRank-ASTRA: Evaluating Correctness & Consistency of Large Language Models on cross-domain multi-file project problems", "abstract": "Evaluating the real-world applicability of large language models (LLMs) provides valuable insights for their development and use in software development tasks. Existing benchmarks often focus on standalone coding problems or specific libraries, overlooking multi-file, project-based scenarios and lacking a rigorous evaluation of consistency. The HackerRank-ASTRA Benchmark introduces project-based coding problems that mirror real-world scenarios. It evaluates model consistency through 32 runs (k = 32) and median standard deviation while incorporating taxonomy-level analysis to assess sub-skill capabilities. Initial evaluations on 65 problems show that the top three models -- o1, o1-preview, and Claude-3.5-Sonnet-1022 -- achieved comparable average scores of 75%, with no statistically significant differences in performance. Notably, Claude-3.5-Sonnet-1022 demonstrated the highest consistency across problems, with low variability (SD = 0.0497), which was statistically significant compared to other models, highlighting its reliability for real-world software development tasks.", "arxiv_url": "https://arxiv.org/abs/2502.00226", "authors": ["Jun Xing", "Mayur Bhatia", "Sahil Phulwani", "Darshan Suresh", "Rafik Matta"], "first_author": "Jun Xing", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Multi-file project problems", "Cross-framework frontend tasks", "Consistency evaluation across runs", "Median standard deviation (k=32)", "Mean pass@1 and mean score metrics", "Taxonomy-level subskill analysis", "Long-context code generation", "High test-case coverage", "Industry-aligned proprietary problem bank"], "summary": "HackerRank-ASTRA 提出并发布了一个包含65个多文件、跨框架项目级前端开发问题的基准，并通过32次独立运行的中位标准差与多项正确性指标评估LLM在正确性与一致性及子技能层面的表现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2502.00226v1", "published": "2025-01-31", "update_time": "2025-01-31", "download_time": "2025-12-11 16:43:08"}
{"id": "2502.12115", "title": "SWE-Lancer: Can Frontier LLMs Earn $1 Million from Real-World Freelance Software Engineering?", "abstract": "We introduce SWE-Lancer, a benchmark of over 1,400 freelance software engineering tasks from Upwork, valued at \\$1 million USD total in real-world payouts. SWE-Lancer encompasses both independent engineering tasks--ranging from \\$50 bug fixes to \\$32,000 feature implementations--and managerial tasks, where models choose between technical implementation proposals. Independent tasks are graded with end-to-end tests triple-verified by experienced software engineers, while managerial decisions are assessed against the choices of the original hired engineering managers. We evaluate model performance and find that frontier models are still unable to solve the majority of tasks. To facilitate future research, we open-source a unified Docker image and a public evaluation split, SWE-Lancer Diamond (https://github.com/openai/SWELancer-Benchmark). By mapping model performance to monetary value, we hope SWE-Lancer enables greater research into the economic impact of AI model development.", "arxiv_url": "https://arxiv.org/abs/2502.12115", "authors": ["Samuel Miserendino", "Michele Wang", "Tejal Patwardhan", "Johannes Heidecke"], "first_author": "Samuel Miserendino", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Freelance job-derived benchmark", "End-to-end browser automation tests", "Economic payout mapping", "Managerial proposal selection evaluation", "Full-stack repository-level patching", "Triple-verified test validation", "Public/private evaluation splits"], "summary": "本文构建并公开了一个基于真实Upwork自由职业软件工程任务（共1,488个、总计约100万美元报酬）的基准，采用专业工程师编写并三重验证的端到端测试评估模型对真实全栈修补与管理决策的能力，并报告前沿模型在该基准上的表现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2502.12115v4", "published": "2025-02-17", "update_time": "2025-05-29", "download_time": "2025-12-11 16:43:43"}
{"id": "2503.06680", "title": "FEA-Bench: A Benchmark for Evaluating Repository-Level Code Generation for Feature Implementation", "abstract": "Implementing new features in repository-level codebases is a crucial application of code generation models. However, current benchmarks lack a dedicated evaluation framework for this capability. To fill this gap, we introduce FEA-Bench, a benchmark designed to assess the ability of large language models (LLMs) to perform incremental development within code repositories. We collect pull requests from 83 GitHub repositories and use rule-based and intent-based filtering to construct task instances focused on new feature development. Each task instance containing code changes is paired with relevant unit test files to ensure that the solution can be verified. The feature implementation requires LLMs to simultaneously possess code completion capabilities for new components and code editing abilities for other relevant parts in the code repository, providing a more comprehensive evaluation method of LLMs' automated software engineering capabilities. Experimental results show that LLMs perform significantly worse in the FEA-Bench, highlighting considerable challenges in such repository-level incremental code development.", "arxiv_url": "https://arxiv.org/abs/2503.06680", "authors": ["Wei Li", "Xin Zhang", "Zhongxin Guo", "Shaoguang Mao", "Wen Luo", "Guangyue Peng", "Yangyu Huang", "Houfeng Wang", "Scarlett Li"], "first_author": "Wei Li", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Repository-level incremental development", "Feature implementation from pull requests", "Unit-test-based execution validation", "Cross-file edits and new-file addition", "Automated PR-to-task collection pipeline", "Large-patch long-form code generation"], "summary": "本文提出FEA-Bench——一个从GitHub pull request构建的基准，用以评估LLM在仓库级别实现新功能（包括新增组件与跨文件编辑）能力，并以单元测试执行结果作为验证，实验证明当前模型在此任务上表现较差。", "quality": "High", "conference": "ACL 2025", "pdf_url": "https://arxiv.org/pdf/2503.06680v2", "published": "2025-03-09", "update_time": "2025-06-19", "download_time": "2025-12-11 16:44:20"}
{"id": "2503.06689", "title": "DependEval: Benchmarking LLMs for Repository Dependency Understanding", "abstract": "While large language models (LLMs) have shown considerable promise in code generation, real-world software development demands advanced repository-level reasoning. This includes understanding dependencies, project structures, and managing multi-file changes. However, the ability of LLMs to effectively comprehend and handle complex code repositories has yet to be fully explored. To address challenges, we introduce a hierarchical benchmark designed to evaluate repository dependency understanding (DependEval). Benchmark is based on 15,576 repositories collected from real-world websites. It evaluates models on three core tasks: Dependency Recognition, Repository Construction, and Multi-file Editing, across 8 programming languages from actual code repositories. Our evaluation of over 25 LLMs reveals substantial performance gaps and provides valuable insights into repository-level code understanding.", "arxiv_url": "https://arxiv.org/abs/2503.06689", "authors": ["Junjia Du", "Yadi Liu", "Hongcheng Guo", "Jiawei Wang", "Haojian Huang", "Yunyi Ni", "Zhoujun Li"], "first_author": "Junjia Du", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Repository dependency resolution", "Cross-file dependency parsing", "Repository structure generation from requirements", "Coordinated multi-file editing", "Hierarchical evaluation metrics", "Static import-based dependency extraction", "Call-chain extraction", "Multilingual repository benchmark"], "summary": "本文提出DependEval，一个覆盖8种编程语言、基于15,576个真实仓库的分层基准，用于评估LLM在依赖识别、仓库构建和多文件编辑等仓库级代码理解任务上的能力，并在25+模型上给出细粒度性能分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2503.06689v1", "published": "2025-03-09", "update_time": "2025-03-09", "download_time": "2025-12-11 16:45:03"}
{"id": "2503.07010", "title": "ProjectEval: A Benchmark for Programming Agents Automated Evaluation on Project-Level Code Generation", "abstract": "Recently, LLM agents have made rapid progress in improving their programming capabilities. However, existing benchmarks lack the ability to automatically evaluate from users' perspective, and also lack the explainability of the results of LLM agents' code generation capabilities. Thus, we introduce ProjectEval, a new benchmark for LLM agents project-level code generation's automated evaluation by simulating user interaction. ProjectEval is constructed by LLM with human reviewing. It has three different level inputs of natural languages or code skeletons. ProjectEval can evaluate the generated projects by user interaction simulation for execution, and by code similarity through existing objective indicators. Through ProjectEval, we find that systematic engineering project code, overall understanding of the project and comprehensive analysis capability are the keys for LLM agents to achieve practical projects. Our findings and benchmark provide valuable insights for developing more effective programming agents that can be deployed in future real-world production.", "arxiv_url": "https://arxiv.org/abs/2503.07010", "authors": ["Kaiyuan Liu", "Youcheng Pan", "Yang Xiang", "Daojing He", "Jing Li", "Yexing Du", "Tianrun Gao"], "first_author": "Kaiyuan Liu", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Project-level code generation benchmark", "User interaction simulation", "Execution-based automated testing", "Three-level input (NL / checklist / skeleton)", "Parameter-description alignment", "Semi-automated construction (LLM + human review)", "Web and console project evaluation", "Explainable evaluation metrics"], "summary": "本文提出了ProjectEval，一个通过模拟用户交互并结合三种输入级别与执行型测试套件，对编程代理项目级代码生成进行自动化且可解释评估的基准。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2503.07010v2", "published": "2025-03-10", "update_time": "2025-05-31", "download_time": "2025-12-11 16:45:49"}
{"id": "2503.07358", "title": "RepoST: Scalable Repository-Level Coding Environment Construction with Sandbox Testing", "abstract": "We present RepoST, a scalable method to construct environments that provide execution feedback for repository-level code generation for both training and evaluation. Unlike existing works that aim to build entire repositories for execution, which is challenging for both human and LLMs, we provide execution feedback with sandbox testing, which isolates a given target function and its dependencies to a separate script for testing. Sandbox testing reduces the complexity of external dependencies and enables constructing environments at a large scale. We use our method to construct RepoST-Train, a large-scale train set with 7,415 functions from 832 repositories. Training with the execution feedback provided by RepoST-Train leads to a performance gain of 5.5% Pass@1 on HumanEval and 3.5% Pass@1 on RepoEval. We also build an evaluation dataset, RepoST-Eval, and benchmark 12 code generation models.", "arxiv_url": "https://arxiv.org/abs/2503.07358", "authors": ["Yiqing Xie", "Alex Xie", "Divyanshu Sheth", "Pengfei Liu", "Daniel Fried", "Carolyn Rose"], "first_author": "Yiqing Xie", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Sandbox Testing", "Repository-level Environment Construction", "Function-level Sandboxing", "Automated Test Generation", "Equivalence Testing", "Mocking External APIs and Files", "AST-based Functionality Verification", "Iterative Debugging and Filtering", "Executable Evaluation Scripts", "Large-scale Train/Eval Dataset Construction"], "summary": "本文提出REPOST，通过将目标函数及其局部依赖沙箱化并由LLM生成测试与模拟资源，自动构建可执行的仓库级代码生成训练与评估环境，并发布大规模的训练集与评测集以提升模型性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2503.07358v1", "published": "2025-03-10", "update_time": "2025-03-10", "download_time": "2025-12-11 16:46:22"}
{"id": "2504.02605", "title": "Multi-SWE-bench: A Multilingual Benchmark for Issue Resolving", "abstract": "The task of issue resolving is to modify a codebase to generate a patch that addresses a given issue. However, existing benchmarks, such as SWE-bench, focus almost exclusively on Python, making them insufficient for evaluating Large Language Models (LLMs) across diverse software ecosystems. To address this, we introduce a multilingual issue-resolving benchmark, called Multi-SWE-bench, covering Java, TypeScript, JavaScript, Go, Rust, C, and C++. It includes a total of 1,632 high-quality instances, which were carefully annotated from 2,456 candidates by 68 expert annotators, ensuring that the benchmark can provide an accurate and reliable evaluation. Based on Multi-SWE-bench, we evaluate a series of state-of-the-art models using three representative methods (Agentless, SWE-agent, and OpenHands) and present a comprehensive analysis with key empirical insights. In addition, we launch a Multi-SWE-RL open-source community, aimed at building large-scale reinforcement learning (RL) training datasets for issue-resolving tasks. As an initial contribution, we release a set of 4,723 well-structured instances spanning seven programming languages, laying a solid foundation for RL research in this domain. More importantly, we open-source our entire data production pipeline, along with detailed tutorials, encouraging the open-source community to continuously contribute and expand the dataset. We envision our Multi-SWE-bench and the ever-growing Multi-SWE-RL community as catalysts for advancing RL toward its full potential, bringing us one step closer to the dawn of AGI.", "arxiv_url": "https://arxiv.org/abs/2504.02605", "authors": ["Daoguang Zan", "Zhirong Huang", "Wei Liu", "Hanwu Chen", "Linhao Zhang", "Shulin Xin", "Lu Chen", "Qi Liu", "Xiaojian Zhong", "Aoyan Li", "Siyao Liu", "Yongsheng Xiao", "Liangqiang Chen", "Yuyu Zhang", "Jing Su", "Tianyu Liu", "Rui Long", "Kai Shen", "Liang Xiang"], "first_author": "Daoguang Zan", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Multilingual issue-resolving benchmark", "Pull-request based instance curation", "Dockerized reproducible test environments", "Dual-stage human verification", "Cross-language generalization analysis", "Patch complexity & multi-file impact analysis", "Open RL training instance release", "Agentless vs agent-based evaluation"], "summary": "本文提出 Multi-SWE-bench，一个覆盖 Java、TypeScript、JavaScript、Go、Rust、C 与 C++ 的多语言 issue-resolving 基准（1,632 个人工验证实例）并发布 4,723 个用于 RL 的开放实例，提供可复现环境与严格的人工检验流程，同时对多种方法和模型进行了跨语言评测与细粒度表现分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2504.02605v1", "published": "2025-04-03", "update_time": "2025-04-03", "download_time": "2025-12-11 16:47:22"}
{"id": "2108.11590", "title": "AVATAR: A Parallel Corpus for Java-Python Program Translation", "abstract": "Program translation refers to migrating source code from one programming language to another. It has tremendous practical value in software development, as porting software across languages is time-consuming and costly. Automating program translation is of paramount importance in software migration, and recently researchers explored unsupervised approaches due to the unavailability of parallel corpora. However, the availability of pre-trained language models for programming languages enables supervised fine-tuning with a small number of labeled examples. Therefore, we present AVATAR, a collection of 9,515 programming problems and their solutions written in two popular languages, Java and Python. AVATAR is collected from competitive programming sites, online platforms, and open-source repositories. Furthermore, AVATAR includes unit tests for 250 examples to facilitate functional correctness evaluation. We benchmark several pre-trained language models fine-tuned on AVATAR. Experiment results show that the models lack in generating functionally accurate code.", "arxiv_url": "https://arxiv.org/abs/2108.11590", "authors": ["Wasi Uddin Ahmad", "Md Golam Rahman Tushar", "Saikat Chakraborty", "Kai-Wei Chang"], "first_author": "Wasi Uddin Ahmad", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Parallel Java-Python corpus", "Unit-test-based functional evaluation", "Competitive programming solutions", "Parallel function extraction", "AST and dataflow similarity metrics", "Solution diversity filtering", "Lexical vs. execution accuracy gap"], "summary": "本文构建了一个包含9,515道题及其Java/Python解法的并行语料并提供250个带单元测试的样例用于功能性评估，同时基准实验表明现有模型在生成功能正确代码方面仍有明显不足。", "quality": "High", "conference": "ACL 2023", "pdf_url": "https://arxiv.org/pdf/2108.11590v2", "published": "2021-08-26", "update_time": "2023-05-04", "download_time": "2025-12-11 16:50:32"}
{"id": "2206.08474", "title": "XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence", "abstract": "Recent advances in machine learning have significantly improved the understanding of source code data and achieved good performance on a number of downstream tasks. Open source repositories like GitHub enable this process with rich unlabeled code data. However, the lack of high quality labeled data has largely hindered the progress of several code related tasks, such as program translation, summarization, synthesis, and code search. This paper introduces XLCoST, Cross-Lingual Code SnippeT dataset, a new benchmark dataset for cross-lingual code intelligence. Our dataset contains fine-grained parallel data from 8 languages (7 commonly used programming languages and English), and supports 10 cross-lingual code tasks. To the best of our knowledge, it is the largest parallel dataset for source code both in terms of size and the number of languages. We also provide the performance of several state-of-the-art baseline models for each task. We believe this new dataset can be a valuable asset for the research community and facilitate the development and validation of new methods for cross-lingual code intelligence.", "arxiv_url": "https://arxiv.org/abs/2206.08474", "authors": ["Ming Zhu", "Aneesh Jain", "Karthik Suresh", "Roshan Ravindran", "Sindhu Tipirneni", "Chandan K. Reddy"], "first_author": "Ming Zhu", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Cross-lingual parallel snippets", "Snippet-level alignment", "Program-level alignment", "Multilingual code translation/summarization/synthesis", "Cross-lingual code search", "Fine-grained alignment from programming-problem solutions", "Baseline performance evaluation"], "summary": "本文提出并发布了 XLCoST，一个包含七种编程语言与英语的百万级细粒度并行代码片段数据集，支持十项跨语种代码任务并提供多种基线评测。 ", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2206.08474v1", "published": "2022-06-16", "update_time": "2022-06-16", "download_time": "2025-12-11 16:51:09"}
{"id": "2303.03004", "title": "xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval", "abstract": "Recently, pre-trained large language models (LLMs) have shown impressive abilities in generating codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level, and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap with a reference code rather than actual execution. We introduce xCodeEval, the largest executable multilingual multitask benchmark to date consisting of $25$M document-level coding examples ($16.5$B tokens) from about $7.5$K unique problems covering up to $11$ programming languages with execution-level parallelism. It features a total of $7$ tasks involving code understanding, generation, translation and retrieval. xCodeEval adopts an execution-based evaluation and offers a multilingual code execution engine, ExecEval that supports unit test based execution in all the $11$ languages. To address the challenge of balancing the distributions of text-code samples over multiple attributes in validation/test sets, we propose a novel data splitting and a data selection schema based on the geometric mean and graph-theoretic principle. Our experiments with OpenAI's LLMs (zero-shot) and open-LLMs (zero-shot and fine-tuned) on the tasks and languages demonstrate **xCodeEval** to be quite challenging as per the current advancements in language models.", "arxiv_url": "https://arxiv.org/abs/2303.03004", "authors": ["Mohammad Abdullah Matin Khan", "M Saiful Bari", "Xuan Long Do", "Weishi Wang", "Md Rizwan Parvez", "Shafiq Joty"], "first_author": "Mohammad Abdullah Matin Khan", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Multilingual Program Synthesis & Evaluation Benchmark", "tags": ["Execution-based evaluation", "Unit-test harness", "Multilingual code parallelism", "Large-scale Codeforces corpus", "Program synthesis benchmark", "Code translation and retrieval tasks", "Geometric-mean split strategy", "Graph-theoretic data selection", "Difficulty-stratified sampling", "Distributed execution engine"], "summary": "本文提出XCODEEVAL——一个包含约2500万可执行多语言多任务代码样本的大规模基准及其ExecEval执行引擎，并通过基于单元测试的执行评估、几何平均的数据切分和图论数据选择策略为代码理解、生成、翻译和检索任务提供标准化评测。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2303.03004v4", "published": "2023-03-06", "update_time": "2023-11-06", "download_time": "2025-12-11 16:51:45"}
{"id": "2308.08961", "title": "On the Evaluation of Neural Code Translation: Taxonomy and Benchmark", "abstract": "In recent years, neural code translation has gained increasing attention. While most of the research focuses on improving model architectures and training processes, we notice that the evaluation process and benchmark for code translation models are severely limited: they primarily treat source code as natural languages and provide a holistic accuracy score while disregarding the full spectrum of model capabilities across different translation types and complexity. In this paper, we present a comprehensive investigation of four state-of-the-art models and analyze in-depth the advantages and limitations of three existing benchmarks. Based on the empirical results, we develop a taxonomy that categorizes code translation tasks into four primary types according to their complexity and knowledge dependence: token level (type 1), syntactic level (type 2), library level (type 3), and algorithm level (type 4). We then conduct a thorough analysis of how existing approaches perform across these four categories. Our findings indicate that while state-of-the-art code translation models excel in type-1 and type-2 translations, they struggle with knowledge-dependent ones such as type-3 and type-4. Existing benchmarks are biased towards trivial translations, such as keyword mapping. To overcome these limitations, we construct G-TransEval, a new benchmark by manually curating type-3 and type-4 translation pairs and unit test cases. Results on our new benchmark suggest that G-TransEval can exhibit more comprehensive and finer-grained capability of code translation models and thus provide a more rigorous evaluation. Our studies also provide more insightful findings and suggestions for future research, such as building type-3 and type-4 training data and ensembling multiple pretraining approaches.", "arxiv_url": "https://arxiv.org/abs/2308.08961", "authors": ["Mingsheng Jiao", "Tingrui Yu", "Xuan Li", "Guanjie Qiu", "Xiaodong Gu", "Beijun Shen"], "first_author": "Mingsheng Jiao", "category": ["Empirical", "Benchmark"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Translation Taxonomy", "Token/Syntax/Library/Algorithm Levels", "Unit-test Functional Evaluation", "Benchmark Bias Analysis", "Cross-language Difficulty (dynamic→static)", "Fine-grained Evaluation Metrics (BLEU/CodeBLEU/CA)", "Curated High-Complexity Translation Pairs"], "summary": "本文提出针对神经代码翻译的四层复杂度分类（词法、语法、库、算法），通过细粒度实证分析揭示现有模型在库级与算法级翻译上的不足，并构建了包含高难度样本与单元测试的G-TransEval基准以实现更严格的评估。", "quality": "High", "conference": "International Conference on Automated Software Engineering (ASE 2023)", "pdf_url": "https://arxiv.org/pdf/2308.08961v1", "published": "2023-08-17", "update_time": "2023-08-17", "download_time": "2025-12-11 16:52:12"}
{"id": "2310.04951", "title": "CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation", "abstract": "Recent code translation techniques exploit neural machine translation models to translate source code from one programming language to another to satisfy production compatibility or to improve efficiency of codebase maintenance. Most existing code translation datasets only focus on a single pair of popular programming languages. To advance research on code translation and meet diverse requirements of real-world applications, we construct CodeTransOcean, a large-scale comprehensive benchmark that supports the largest variety of programming languages for code translation. CodeTransOcean consists of three novel multilingual datasets, namely, MultilingualTrans supporting translations between multiple popular programming languages, NicheTrans for translating between niche programming languages and popular ones, and LLMTrans for evaluating executability of translated code by large language models (LLMs). CodeTransOcean also includes a novel cross-framework dataset, DLTrans, for translating deep learning code across different frameworks. We develop multilingual modeling approaches for code translation and demonstrate their great potential in improving the translation quality of both low-resource and high-resource language pairs and boosting the training efficiency. We also propose a novel evaluation metric Debugging Success Rate@K for program-level code translation. Last but not least, we evaluate LLM ChatGPT on our datasets and investigate its potential for fuzzy execution predictions. We build baselines for CodeTransOcean and analyze challenges of code translation for guiding future research. The CodeTransOcean datasets and code are publicly available at https://github.com/WeixiangYAN/CodeTransOcean.", "arxiv_url": "https://arxiv.org/abs/2310.04951", "authors": ["Weixiang Yan", "Yuchen Tian", "Yunzhe Li", "Qian Chen", "Wen Wang"], "first_author": "Weixiang Yan", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Multilingual code translation", "Niche-to-popular language transfer", "Deep-learning-framework translation", "Execution-based evaluation", "Debugging Success Rate@K (DSR@K)", "AutoTransExecuter pipeline", "Fuzzy execution metric", "Multilingual modeling for low-resource pairs", "ChatGPT prompting and self-debugging study"], "summary": "本文提出了CodeTransOcean——一个覆盖多种主流与小众编程语言及深度学习框架的大规模代码翻译基准，包含多套数据集、自动化翻译-执行评估流水线与新的执行型评价指标，并展示了多语言建模与ChatGPT在代码翻译任务中的表现与挑战。", "quality": "High", "conference": "EMNLP 2023", "pdf_url": "https://arxiv.org/pdf/2310.04951v2", "published": "2023-10-08", "update_time": "2023-10-25", "download_time": "2025-12-11 16:52:47"}
{"id": "2411.06145", "title": "ClassEval-T: Evaluating Large Language Models in Class-Level Code Translation", "abstract": "In recent years, Large Language Models (LLMs) have dramatically advanced the performance of automated code translation, making their computational accuracy score reach up to over 80% on many previous benchmarks. However, most code samples in these benchmarks are short, standalone, statement/method-level, and algorithmic, which is not aligned with practical coding tasks. Therefore, it is still unknown the actual capability of LLMs in translating code samples written for daily development. To achieve this, we construct a class-level code translation benchmark, ClassEval-T, and make the first attempt to extensively assess recent LLMs' performance on class-level code translation. ClassEval-T is extended from ClassEval, a well-known class-level Python code generation benchmark consisting of multiple practical coding topics, such as database operation and game design, and diverse contextual dependencies (e.g., fields, methods, and libraries). It cost us 360 person-hours to accomplish the manual migration to Java and C++ with complete code samples and associated test suites. Subsequently, we design three translation strategies (i.e., holistic, min-dependency, and standalone) for class-level code translations and evaluate eight recent LLMs of commercial, general, and code kinds in diverse families and sizes on ClassEval-T. Experimental results demonstrate a remarkable performance drop compared with the most widely studied method-level code translation benchmark, and obvious discrepancies among LLMs appear, showing the effectiveness of ClassEval-T in measuring recent LLMs. Afterwards, we further discuss the usage scenarios for diverse translation strategies and LLMs' ability to dependency awareness when translating class samples. Finally, 1,243 failure cases made by the best-performing LLM under test are analyzed and categorized in this paper for practical guidance and future enlightenment.", "arxiv_url": "https://arxiv.org/abs/2411.06145", "authors": ["Pengyu Xue", "Linhao Wu", "Zhen Yang", "Chengyi Wang", "Xiang Li", "Yuxiang Zhang", "Jia Li", "Ruikai Jin", "Yifei Pei", "Zhaoyan Shen", "Xiran Lyu", "Jacky Wai Keung"], "first_author": "Pengyu Xue", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Class-level code translation", "Cross-language migration (Python, Java, C++)", "Holistic / min-dependency / standalone translation strategies", "Dependency-awareness evaluation", "High-coverage test suites for correctness & compilation", "Manual failure-case taxonomy"], "summary": "本文构建并公开了面向类级别的代码翻译基准ClassEval-T（含高覆盖测试用例），并用三种翻译策略评估多款LLM的跨语言翻译能力、依赖感知与失败模式，给出实践性分析与建议。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.06145v4", "published": "2024-11-09", "update_time": "2025-04-14", "download_time": "2025-12-11 16:53:16"}
{"id": "2411.13990", "title": "RustRepoTrans: Repository-level Code Translation Benchmark Targeting Rust", "abstract": "Recent advancements in large language models (LLMs) have demonstrated impressive capabilities in code translation, typically evaluated using benchmarks like CodeTransOcean and RepoTransBench. However, dependency-free benchmarks fail to capture real-world complexities by focusing primarily on simple function-level translations and overlooking repository-level context (e.g., dependencies). Full-repository translation benchmarks significantly exceed the current capabilities of existing models, resulting in performance bottlenecks that fail to provide actionable insights for guiding model development. Furthermore, existing benchmarks do not account for the scenario of incrementally translating new or modified modules from the source to the target language, which demands careful handling of repository-level contexts such as dependencies, cross-module references, and architectural divergence. Moreover, LLMs' effectiveness in translating to newer, low-resource languages like Rust remains largely underexplored. To address these gaps, we introduce RustRepoTrans, the first repository-level context code translation benchmark targeting incremental translation, comprising 375 tasks translating into Rust from C, Java, and Python. Using this benchmark, we evaluate seven representative LLMs, analyzing their errors to assess limitations in complex translation scenarios. Among them, DeepSeek-R1 performs best with 51.5% Pass@1, excelling in both basic functionality and additional translation abilities, such as noise robustness and syntactical difference identification. However, even DeepSeek-R1 experiences a 22.2% performance drop (Pass@1 from 73.7% to 51.5%) when handling repository-level context compared to previous benchmarks without such context.", "arxiv_url": "https://arxiv.org/abs/2411.13990", "authors": ["Guangsheng Ou", "Mingwei Liu", "Yuxuan Chen", "Yanlin Wang", "Xin Peng", "Zibin Zheng"], "first_author": "Guangsheng Ou", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Repository-level context", "Incremental code migration", "Rust migration challenges", "Dependency extraction", "Function-pair mining", "Test-case-driven verification", "Error taxonomy (dependency failures)", "Fine-grained translation metrics"], "summary": "本文提出RustRepoTrans——一个包含375个增量仓库级上下文代码翻译任务、面向从C/Java/Python到Rust迁移的基准，并评估七款LLM、归类错误并引入更细粒度的评估框架。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.13990v6", "published": "2024-11-21", "update_time": "2025-10-17", "download_time": "2025-12-11 16:53:49"}
{"id": "2412.17744", "title": "RepoTransBench: A Real-World Benchmark for Repository-Level Code Translation", "abstract": "Repository-level code translation refers to translating an entire code repository from one programming language to another while preserving the functionality of the source repository. Many benchmarks have been proposed to evaluate the performance of such code translators. However, previous benchmarks mostly provide fine-grained samples, focusing at either code snippet, function, or file-level code translation. Such benchmarks do not accurately reflect real-world demands, where entire repositories often need to be translated, involving longer code length and more complex functionalities. To address this gap, we propose a new benchmark, named RepoTransBench, which is a real-world repository-level code translation benchmark with an automatically executable test suite. We conduct experiments on RepoTransBench to evaluate the translation performance of 11 advanced LLMs. We find that the Success@1 score (test success in one attempt) of the best-performing LLM is only 7.33%. To further explore the potential of LLMs for repository-level code translation, we provide LLMs with error-related feedback to perform iterative debugging and observe an average 7.09% improvement on Success@1. However, even with this improvement, the Success@1 score of the best-performing LLM is only 21%, which may not meet the need for reliable automatic repository-level code translation. Finally, we conduct a detailed error analysis and highlight current LLMs' deficiencies in repository-level code translation, which could provide a reference for further improvements.", "arxiv_url": "https://arxiv.org/abs/2412.17744", "authors": ["Yanli Wang", "Yanlin Wang", "Suiquan Wang", "Daya Guo", "Jiachi Chen", "John Grundy", "Xilin Liu", "Yuchi Ma", "Mingzhi Mao", "Hongyu Zhang", "Zibin Zheng"], "first_author": "Yanli Wang", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Repository-level translation", "Execution-based test suites", "Iterative debugging with error feedback", "Resource and configuration file migration", "Compilability and functional correctness evaluation", "Real-world repository selection and filtering", "Error taxonomy for translation failures", "Success@k evaluation metric"], "summary": "本文提出了RepoTransBench——一个包含100个真实可执行仓库并附自动化测试套件的仓库级代码翻译基准，评估了多款大型模型并通过迭代调试与错误分析揭示了当前模型在仓库级翻译中的不足。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2412.17744v1", "published": "2024-12-23", "update_time": "2024-12-23", "download_time": "2025-12-11 16:54:25"}
{"id": "2501.16050", "title": "Skeleton-Guided-Translation: A Benchmarking Framework for Code Repository Translation with Fine-Grained Quality Evaluation", "abstract": "The advancement of large language models has intensified the need to modernize enterprise applications and migrate legacy systems to secure, versatile languages. However, existing code translation benchmarks primarily focus on individual functions, overlooking the complexities involved in translating entire repositories, such as maintaining inter-module coherence and managing dependencies. While some recent repository-level translation benchmarks attempt to address these challenges, they still face limitations, including poor maintainability and overly coarse evaluation granularity, which make them less developer-friendly. We introduce Skeleton-Guided-Translation, a framework for repository-level Java to C# code translation with fine-grained quality evaluation. It uses a two-step process: first translating the repository's structural \"skeletons\", then translating the full repository guided by these skeletons. Building on this, we present TRANSREPO-BENCH, a benchmark of high quality open-source Java repositories and their corresponding C# skeletons, including matching unit tests and build configurations. Our unit tests are fixed and can be applied across multiple or incremental translations without manual adjustments, enhancing automation and scalability in evaluations. Additionally, we develop fine-grained evaluation metrics that assess translation quality at the individual test case level, addressing traditional binary metrics' inability to distinguish when build failures cause all tests to fail. Evaluations using TRANSREPO-BENCH highlight key challenges and advance more accurate repository level code translation.", "arxiv_url": "https://arxiv.org/abs/2501.16050", "authors": ["Xing Zhang", "Jiaheng Wen", "Fangkai Yang", "Pu Zhao", "Yu Kang", "Junhao Wang", "Maoquan Wang", "Yufan Huang", "Elsie Nallipogu", "Qingwei Lin", "Yingnong Dang", "Saravan Rajmohan", "Dongmei Zhang", "Qi Zhang"], "first_author": "Xing Zhang", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Repository-level code translation", "Skeleton-guided translation", "Fine-grained unit-test evaluation", "Incremental/partial translation", "Cross-file dependency management", "Per-testcase translation scoring", "Build-config aware evaluation", "Java-to-C# migration"], "summary": "本文提出Skeleton-Guided-Translation框架并构建TRANSREPO-BENCH基准，通过先翻译仓库骨架再填充实现Java到C#的仓库级翻译，并基于单元测试引入细粒度的测试用例级别评价指标以提升可维护性与评估准确性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2501.16050v1", "published": "2025-01-27", "update_time": "2025-01-27", "download_time": "2025-12-11 16:54:54"}
{"id": "2504.15254", "title": "CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation", "abstract": "C-to-Rust transpilation is essential for modernizing legacy C code while enhancing safety and interoperability with modern Rust ecosystems. However, no dataset currently exists for evaluating whether a system can transpile C into safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset of 100 C repositories, each paired with manually-written interfaces in safe Rust as well as test cases that can be used to validate correctness of the transpilation. By considering entire repositories rather than isolated functions, CRUST-Bench captures the challenges of translating complex projects with dependencies across multiple files. The provided Rust interfaces provide explicit specifications that ensure adherence to idiomatic, memory-safe Rust patterns, while the accompanying test cases enforce functional correctness. We evaluate state-of-the-art large language models (LLMs) on this task and find that safe and idiomatic Rust generation is still a challenging problem for various state-of-the-art methods and techniques. We also provide insights into the errors LLMs usually make in transpiling code from C to safe Rust. The best performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot setting. Improvements on CRUST-Bench would lead to improved transpilation systems that can reason about complex scenarios and help in migrating legacy codebases from C into languages like Rust that ensure memory safety. You can find the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.", "arxiv_url": "https://arxiv.org/abs/2504.15254", "authors": ["Anirudh Khatry", "Robert Zhang", "Jia Pan", "Ziteng Wang", "Qiaochu Chen", "Greg Durrett", "Isil Dillig"], "first_author": "Anirudh Khatry", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["C-to-safe-Rust transpilation", "Repository-scale multi-file translation", "Manually authored Rust interface specifications", "Test-driven correctness and compile-run validation", "Pointer-to-ownership abstraction mapping", "Memory-safety and borrowing challenges", "LLM error analysis for transpilation", "Generate-then-repair evaluation loop"], "summary": "本文提出CRUST-Bench——包含100个C仓库、手工编写的安全Rust接口与测试用例的基准，用于评估并分析LLM在将多文件C项目转译为内存安全、惯用Rust代码时的性能与常见错误模式。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2504.15254v3", "published": "2025-04-21", "update_time": "2025-10-01", "download_time": "2025-12-11 16:55:25"}
{"id": "2202.10868", "title": "Neural Program Repair: Systems, Challenges and Solutions", "abstract": "Automated Program Repair (APR) aims to automatically fix bugs in the source code. Recently, as advances in Deep Learning (DL) field, there is a rise of Neural Program Repair (NPR) studies, which formulate APR as a translation task from buggy code to correct code and adopt neural networks based on encoder-decoder architecture. Compared with other APR techniques, NPR approaches have a great advantage in applicability because they do not need any specification (i.e., a test suite). Although NPR has been a hot research direction, there isn't any overview on this field yet. In order to help interested readers understand architectures, challenges and corresponding solutions of existing NPR systems, we conduct a literature review on latest studies in this paper. We begin with introducing the background knowledge on this field. Next, to be understandable, we decompose the NPR procedure into a series of modules and explicate various design choices on each module. Furthermore, we identify several challenges and discuss the effect of existing solutions. Finally, we conclude and provide some promising directions for future research.", "arxiv_url": "https://arxiv.org/abs/2202.10868", "authors": ["Wenkang Zhong", "Chuanyi Li", "Jidong Ge", "Bin Luo"], "first_author": "Wenkang Zhong", "category": ["Survey"], "field": "Quality Management", "task": "Bug Repair", "tags": ["NPR pipeline", "Context extraction strategies", "AST and structural representations", "Code abstraction and anonymization", "Encoder-decoder seq2seq generation", "Edit-based vs token-based patching", "Patch ranking and validation", "Evaluation: plausible vs correct patches"], "summary": "本文系统回顾了神经程序修复（NPR）研究，按预处理、输入表示、输出搜索和补丁排序四个阶段详细分析各类设计选择、面临的挑战与现有解决方案并提出未来方向。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2202.10868v2", "published": "2022-02-22", "update_time": "2022-09-21", "download_time": "2025-12-11 16:55:53"}
{"id": "2301.03270", "title": "A Survey of Learning-based Automated Program Repair", "abstract": "Automated program repair (APR) aims to fix software bugs automatically and plays a crucial role in software development and maintenance. With the recent advances in deep learning (DL), an increasing number of APR techniques have been proposed to leverage neural networks to learn bug-fixing patterns from massive open-source code repositories. Such learning-based techniques usually treat APR as a neural machine translation (NMT) task, where buggy code snippets (i.e., source language) are translated into fixed code snippets (i.e., target language) automatically. Benefiting from the powerful capability of DL to learn hidden relationships from previous bug-fixing datasets, learning-based APR techniques have achieved remarkable performance. In this paper, we provide a systematic survey to summarize the current state-of-the-art research in the learning-based APR community. We illustrate the general workflow of learning-based APR techniques and detail the crucial components, including fault localization, patch generation, patch ranking, patch validation, and patch correctness phases. We then discuss the widely-adopted datasets and evaluation metrics and outline existing empirical studies. We discuss several critical aspects of learning-based APR techniques, such as repair domains, industrial deployment, and the open science issue. We highlight several practical guidelines on applying DL techniques for future APR studies, such as exploring explainable patch generation and utilizing code features. Overall, our paper can help researchers gain a comprehensive understanding about the achievements of the existing learning-based APR techniques and promote the practical application of these techniques. Our artifacts are publicly available at \\url{https://github.com/QuanjunZhang/AwesomeLearningAPR}.", "arxiv_url": "https://arxiv.org/abs/2301.03270", "authors": ["Quanjun Zhang", "Chunrong Fang", "Yuxiang Ma", "Weisong Sun", "Zhenyu Chen"], "first_author": "Quanjun Zhang", "category": ["Survey"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Neural Machine Translation formulation", "Fault Localization", "Patch Generation", "Patch Ranking & Validation", "Code Representation: Sequence/Tree/Graph", "Pre-trained Models for APR", "Evaluation: Plausible vs Correct Patches", "Multilingual and Multi-hunk Repair", "Explainable Patch Generation", "Open Science and Reproducibility"], "summary": "本文系统综述了基于深度学习的自动程序修复（APR）研究，梳理了常见工作流程与关键组件、数据集与评测指标、实证研究、挑战与未来方向。", "quality": "High", "conference": "ACM Transactions on Software Engineering and Methodology (TOSEM) 2023", "pdf_url": "https://arxiv.org/pdf/2301.03270v3", "published": "2023-01-09", "update_time": "2023-11-01", "download_time": "2025-12-11 16:56:32"}
{"id": "2303.18184", "title": "A Survey on Automated Program Repair Techniques", "abstract": "With the rapid development and large-scale popularity of program software, modern society increasingly relies on software systems. However, the problems exposed by software have also come to the fore. Software defect has become an important factor troubling developers. In this context, Automated Program Repair (APR) techniques have emerged, aiming to automatically fix software defect problems and reduce manual debugging work. In particular, benefiting from the advances in deep learning, numerous learning-based APR techniques have emerged in recent years, which also bring new opportunities for APR research. To give researchers a quick overview of APR techniques' complete development and future opportunities, we revisit the evolution of APR techniques and discuss in depth the latest advances in APR research. In this paper, the development of APR techniques is introduced in terms of four different patch generation schemes: search-based, constraint-based, template-based, and learning-based. Moreover, we propose a uniform set of criteria to review and compare each APR tool, summarize the advantages and disadvantages of APR techniques, and discuss the current state of APR development. Furthermore, we introduce the research on the related technical areas of APR that have also provided a strong motivation to advance APR development. Finally, we analyze current challenges and future directions, especially highlighting the critical opportunities that large language models bring to APR research.", "arxiv_url": "https://arxiv.org/abs/2303.18184", "authors": ["Kai Huang", "Zhengzi Xu", "Su Yang", "Hongyu Sun", "Xuejun Li", "Zheng Yan", "Yuqing Zhang"], "first_author": "Kai Huang", "category": ["Survey"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Search-based Repair", "Constraint-based Repair", "Template-based Repair", "Learning-based Repair", "Evaluation Criteria for APR", "Patch Assessment and Overfitting", "Fault Localization", "Dataset Quality and Overlap", "Industrial Deployment Challenges", "LLM-enabled Program Repair"], "summary": "本文为自动程序修复（APR）领域的系统综述，按照搜索式、约束式、模板式和学习式四类修复策略回顾进展，提出统一评估标准，分析现有问题并展望包括大语言模型在内的未来研究方向。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2303.18184v3", "published": "2023-03-31", "update_time": "2023-05-13", "download_time": "2025-12-11 16:57:05"}
{"id": "1812.08693", "title": "An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation", "abstract": "Millions of open-source projects with numerous bug fixes are available in code repositories. This proliferation of software development histories can be leveraged to learn how to fix common programming bugs. To explore such a potential, we perform an empirical study to assess the feasibility of using Neural Machine Translation techniques for learning bug-fixing patches for real defects. First, we mine millions of bug-fixes from the change histories of projects hosted on GitHub, in order to extract meaningful examples of such bug-fixes. Next, we abstract the buggy and corresponding fixed code, and use them to train an Encoder-Decoder model able to translate buggy code into its fixed version. In our empirical investigation we found that such a model is able to fix thousands of unique buggy methods in the wild. Overall, this model is capable of predicting fixed patches generated by developers in 9-50% of the cases, depending on the number of candidate patches we allow it to generate. Also, the model is able to emulate a variety of different Abstract Syntax Tree operations and generate candidate patches in a split second.", "arxiv_url": "https://arxiv.org/abs/1812.08693", "authors": ["Michele Tufano", "Cody Watson", "Gabriele Bavota", "Massimiliano Di Penta", "Martin White", "Denys Poshyvanyk"], "first_author": "Michele Tufano", "category": ["Empirical", "Technical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Neural Machine Translation for Program Repair", "Method-level bug-fix pair mining from GitHub", "AST edit action emulation", "Code abstraction with idioms and identifier mapping", "GumTree-based fine-grained differencing", "RNN encoder-decoder with attention", "Candidate patch generation and top-k inference", "Syntactic correctness and edit-operation analysis"], "summary": "本文通过在大规模GitHub修复提交中挖掘方法级错误-修复对，使用编码器-解码器的神经机器翻译模型将有缺陷代码“翻译”成修复代码，并实证评估了该方法生成与开发者等效补丁的能力与效率。", "quality": "High", "conference": "ACM Transactions on Software Engineering and Methodology 2019", "pdf_url": "https://arxiv.org/pdf/1812.08693v2", "published": "2018-12-20", "update_time": "2019-05-21", "download_time": "2025-12-11 16:57:59"}
{"id": "1901.09102", "title": "On Learning Meaningful Code Changes via Neural Machine Translation", "abstract": "Recent years have seen the rise of Deep Learning (DL) techniques applied to source code. Researchers have exploited DL to automate several development and maintenance tasks, such as writing commit messages, generating comments and detecting vulnerabilities among others. One of the long lasting dreams of applying DL to source code is the possibility to automate non-trivial coding activities. While some steps in this direction have been taken (e.g., learning how to fix bugs), there is still a glaring lack of empirical evidence on the types of code changes that can be learned and automatically applied by DL. Our goal is to make this first important step by quantitatively and qualitatively investigating the ability of a Neural Machine Translation (NMT) model to learn how to automatically apply code changes implemented by developers during pull requests. We train and experiment with the NMT model on a set of 236k pairs of code components before and after the implementation of the changes provided in the pull requests. We show that, when applied in a narrow enough context (i.e., small/medium-sized pairs of methods before/after the pull request changes), NMT can automatically replicate the changes implemented by developers during pull requests in up to 36% of the cases. Moreover, our qualitative analysis shows that the model is capable of learning and replicating a wide variety of meaningful code changes, especially refactorings and bug-fixing activities. Our results pave the way for novel research in the area of DL on code, such as the automatic learning and applications of refactoring.", "arxiv_url": "https://arxiv.org/abs/1901.09102", "authors": ["Michele Tufano", "Jevgenija Pantiuchina", "Cody Watson", "Gabriele Bavota", "Denys Poshyvanyk"], "first_author": "Michele Tufano", "category": ["Technical", "Empirical", "Benchmark"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Method-level code transformations", "Neural Machine Translation (seq2seq)", "Identifier and literal abstraction", "Gerrit pull-request mining", "AST-based edit extraction", "Refactoring and bug-fix synthesis", "Concretization/mapping of abstract tokens", "Beam-search multi-hypothesis generation"], "summary": "该论文通过在从Gerrit挖掘的近24万对方法（改动前/后）上训练编码-解码神经机器翻译模型，定量与定性评估其学习并自动复现开发者在PR中实施的有意义代码变更（如重构与修复），在小/中等方法范围内最高可达36%的精确复现率，并公开了数据与工具。", "quality": "High", "conference": "ACM/IEEE International Conference on Software Engineering (ICSE 2019)", "pdf_url": "https://arxiv.org/pdf/1901.09102v1", "published": "2019-01-25", "update_time": "2019-01-25", "download_time": "2025-12-11 16:58:44"}
{"id": "2010.01544", "title": "Review4Repair: Code Review Aided Automatic Program Repairing", "abstract": "Context: Learning-based automatic program repair techniques are showing promise to provide quality fix suggestions for detected bugs in the source code of the software. These tools mostly exploit historical data of buggy and fixed code changes and are heavily dependent on bug localizers while applying to a new piece of code. With the increasing popularity of code review, dependency on bug localizers can be reduced. Besides, the code review-based bug localization is more trustworthy since reviewers' expertise and experience are reflected in these suggestions.   Objective: The natural language instructions scripted on the review comments are enormous sources of information about the bug's nature and expected solutions. However, none of the learning-based tools has utilized the review comments to fix programming bugs to the best of our knowledge. In this study, we investigate the performance improvement of repair techniques using code review comments.   Method: We train a sequence-to-sequence model on 55,060 code reviews and associated code changes. We also introduce new tokenization and preprocessing approaches that help to achieve significant improvement over state-of-the-art learning-based repair techniques.   Results: We boost the top-1 accuracy by 20.33% and top-10 accuracy by 34.82%. We could provide a suggestion for stylistics and non-code errors unaddressed by prior techniques.   Conclusion: We believe that the automatic fix suggestions along with code review generated by our approach would help developers address the review comment quickly and correctly and thus save their time and effort.", "arxiv_url": "https://arxiv.org/abs/2010.01544", "authors": ["Faria Huq", "Masum Hasan", "Mahim Anzum Haque Pantho", "Sazan Mahbub", "Anindya Iqbal", "Toufique Ahmed"], "first_author": "Faria Huq", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Review-to-Patch Generation", "Code Review Comment Utilization", "Pointer-Generator Seq2Seq", "Hard and Soft Tokenization", "Joint Natural Language and Code Modeling", "Stylistic and Non-functional Fixes", "Bug Localization via Reviews"], "summary": "本文提出Review4Repair，利用代码审查评论与代码变更联合训练指针生成序列到序列模型并采用新的硬/软分词预处理，在55,060条审查-修复对上显著提高了自动程序修复的top-1和top-10准确率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2010.01544v2", "published": "2020-10-04", "update_time": "2020-10-06", "download_time": "2025-12-11 16:59:33"}
{"id": "2108.04631", "title": "Megadiff: A Dataset of 600k Java Source Code Changes Categorized by Diff Size", "abstract": "This paper presents Megadiff, a dataset of source code diffs. It focuses on Java, with strict inclusion criteria based on commit message and diff size. Megadiff contains 663 029 Java diffs that can be used for research on commit comprehension, fault localization, automated program repair, and machine learning on code changes.", "arxiv_url": "https://arxiv.org/abs/2108.04631", "authors": ["Martin Monperrus", "Matias Martinez", "He Ye", "Fernanda Madeiral", "Thomas Durieux", "Zhongxing Yu"], "first_author": "Martin Monperrus", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Line-based unified diffs", "Java source-code change corpus", "Diff-size categorization (1–40 LOC)", "Full-file before-and-after context", "Fixing-commit heuristic (commit-message filtering)", "Raw unprocessed diffs for reproducibility", "Repository-scale collection and filtering"], "summary": "该论文发布了 Megadiﬀ，一个包含663,029条按改动行数（1–40行）分类、包含完整前后文件上下文且未加工的Java源代码差异数据集，旨在支持提交理解、程序修复和基于变更的机器学习研究。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2108.04631v1", "published": "2021-08-10", "update_time": "2021-08-10", "download_time": "2025-12-11 17:00:06"}
{"id": "2304.01102", "title": "RunBugRun -- An Executable Dataset for Automated Program Repair", "abstract": "Recently, we can notice a transition to data-driven techniques in Automated Program Repair (APR), in particular towards deep neural networks. This entails training on hundreds of thousands or even millions of non-executable code fragments. We would like to bring more attention to an aspect of code often neglected in Neural Program Repair (NPR), namely its execution. Code execution has several significant advantages. It allows for test-based evaluation of candidate fixes and can provide valuable information to aid repair. In this work we present a fully executable dataset of 450,000 small buggy/fixed program pairs originally submitted to programming competition websites written in eight different programming languages. Along with the dataset we provide infrastructure to compile, safely execute and test programs as well as fine-grained bug-type labels. To give a point of reference, we provide basic evaluation results for two baselines, one based on a generate-and-validate approach and one on deep learning. With this dataset we follow several goals: we want to lift Neural Program Repair beyond fully static code representations, foster the use of execution-based features and, by including several different languages, counterbalance the predominance of Java in the current landscape of APR datasets and benchmarks.", "arxiv_url": "https://arxiv.org/abs/2304.01102", "authors": ["Julian Aron Prenner", "Romain Robbes"], "first_author": "Julian Aron Prenner", "category": ["Benchmark"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Executable bug-fix pairs", "Polyglot (8 languages)", "Competitive programming submissions", "Test-case harness and sandboxed execution", "Fine-grained hierarchical bug labels", "Runtime errors and stack traces", "Curation and de-duplication pipeline", "Baseline evaluations for G&V and neural repair", "Cross-language knowledge transfer analysis"], "summary": "本文构建了一个包含45万条可执行错误/修复程序对的跨语言自动程序修复数据集，提供编译与沙箱执行基础设施、测试用例、细粒度错误标签并给出基线评估与分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2304.01102v1", "published": "2023-04-03", "update_time": "2023-04-03", "download_time": "2025-12-11 17:00:57"}
{"id": "2401.04621", "title": "DebugBench: Evaluating Debugging Capability of Large Language Models", "abstract": "Large Language Models (LLMs) have demonstrated exceptional coding capability. However, as another critical component of programming proficiency, the debugging capability of LLMs remains relatively unexplored. Previous evaluations of LLMs' debugging ability are significantly limited by the risk of data leakage, the scale of the dataset, and the variety of tested bugs. To overcome these deficiencies, we introduce `DebugBench', an LLM debugging benchmark consisting of 4,253 instances. It covers four major bug categories and 18 minor types in C++, Java, and Python. To construct DebugBench, we collect code snippets from the LeetCode community, implant bugs into source data with GPT-4, and assure rigorous quality checks. We evaluate two commercial and four open-source models in a zero-shot scenario. We find that (1) while closed-source models exhibit inferior debugging performance compared to humans, open-source models relatively lower pass rate scores; (2) the complexity of debugging notably fluctuates depending on the bug category; (3) incorporating runtime feedback has a clear impact on debugging performance which is not always helpful. As an extension, we also compare LLM debugging and code generation, revealing a strong correlation between them for closed-source models. These findings will benefit the development of LLMs in debugging.", "arxiv_url": "https://arxiv.org/abs/2401.04621", "authors": ["Runchu Tian", "Yining Ye", "Yujia Qin", "Xin Cong", "Yankai Lin", "Yinxu Pan", "Yesai Wu", "Haotian Hui", "Weichuan Liu", "Zhiyuan Liu", "Maosong Sun"], "first_author": "Runchu Tian", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Debugging benchmark", "Fine-grained bug taxonomy", "Automated bug implantation", "Data-leakage-aware curation", "Multi-language code snippets", "Runtime-feedback evaluation", "Zero-shot LLM assessment", "Error-type difficulty analysis", "Debugging vs. code-generation correlation", "Test-suite-based automatic evaluation"], "summary": "该论文提出DebugBench——一个包含4,253个实例、覆盖18种细分错误并在C++/Java/Python上构建的数据集，用以无泄露地评估LLM的调试能力，并通过零样本实验分析不同错误类别与运行时反馈对调试性能的影响。", "quality": "High", "conference": "ACL 2024", "pdf_url": "https://arxiv.org/pdf/2401.04621v3", "published": "2024-01-09", "update_time": "2024-06-06", "download_time": "2025-12-11 17:01:33"}
{"id": "2411.02310", "title": "MdEval: Massively Multilingual Code Debugging", "abstract": "Code large language models (LLMs) have made significant progress in code debugging by directly generating the correct code based on the buggy code snippet. Programming benchmarks, typically consisting of buggy code snippet and their associated test cases, are used to assess the debugging capabilities of LLMs. However, many existing benchmarks primarily focus on Python and are often limited in terms of language diversity (e.g., DebugBench and DebugEval). To advance the field of multilingual debugging with LLMs, we propose the first massively multilingual debugging benchmark, which includes 3.6K test samples of 18 programming languages and covers the automated program repair (APR) task, the code review (CR) task, and the bug identification (BI) task. Further, we introduce the debugging instruction corpora MDEVAL-INSTRUCT by injecting bugs into the correct multilingual queries and solutions (xDebugGen). Further, a multilingual debugger xDebugCoder trained on MDEVAL-INSTRUCT as a strong baseline specifically to handle the bugs of a wide range of programming languages (e.g. \"Missing Mut\" in language Rust and \"Misused Macro Definition\" in language C). Our extensive experiments on MDEVAL reveal a notable performance gap between open-source models and closed-source LLMs (e.g., GPT and Claude series), highlighting huge room for improvement in multilingual code debugging scenarios.", "arxiv_url": "https://arxiv.org/abs/2411.02310", "authors": ["Shukai Liu", "Linzheng Chai", "Jian Yang", "Jiajun Shi", "He Zhu", "Liran Wang", "Ke Jin", "Wei Zhang", "Hualei Zhu", "Shuyue Guo", "Tao Sun", "Jiaheng Liu", "Yunlong Duan", "Yu Hao", "Liqun Yang", "Guanglin Niu", "Ge Zhang", "Zhoujun Li"], "first_author": "Shukai Liu", "category": ["Benchmark", "Technical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Multilingual debugging benchmark", "Automated program repair", "Bug localization", "Bug identification", "Bug-injection data augmentation", "Instruction-tuning for debugging", "Round-trip code translation for bug generation", "Language-specific error taxonomy", "Human-annotated multilingual dataset", "Multitask evaluation and leaderboard"], "summary": "本文提出了首个覆盖20种编程语言的多语言代码调试基准与指令语料，通过三种注入错误策略生成调试训练对并构建多语言调试基线模型，对40个模型进行大规模评测以揭示多语言调试能力差距。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.02310v2", "published": "2024-11-04", "update_time": "2025-02-24", "download_time": "2025-12-11 17:02:09"}
{"id": "2501.09745", "title": "Suggesting Code Edits in Interactive Machine Learning Notebooks Using Large Language Models", "abstract": "Machine learning developers frequently use interactive computational notebooks, such as Jupyter notebooks, to host code for data processing and model training. Jupyter notebooks provide a convenient tool for writing machine learning pipelines and interactively observing outputs, however, maintaining Jupyter notebooks, e.g., to add new features or fix bugs, can be challenging due to the length and complexity of the notebooks. Moreover, there is no existing benchmark related to developer edits on Jupyter notebooks. To address this, we present the first dataset of 48,398 Jupyter notebook edits derived from 20,095 revisions of 792 machine learning repositories on GitHub, and perform the first study of the using LLMs to predict code edits in Jupyter notebooks. Our dataset captures granular details of cell-level and line-level modifications, offering a foundation for understanding real-world maintenance patterns in machine learning workflows. We observed that the edits on Jupyter notebooks are highly localized, with changes averaging only 166 lines of code in repositories. While larger models outperform smaller counterparts in code editing, all models have low accuracy on our dataset even after finetuning, demonstrating the complexity of real-world machine learning maintenance tasks. Our findings emphasize the critical role of contextual information in improving model performance and point toward promising avenues for advancing large language models' capabilities in engineering machine learning code.", "arxiv_url": "https://arxiv.org/abs/2501.09745", "authors": ["Bihui Jin", "Jiayue Wang", "Pengyu Nie"], "first_author": "Bihui Jin", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Jupyter notebook edit dataset", "Cell-level and line-level diffs", "GitHub ML repository mining", "Commit-message conditioned edit prediction", "Localized/incremental notebook edits", "Few-shot prompting for code edits", "Supervised fine-tuning and parameter-efficient tuning", "Context-aware evaluation of edit predictions", "Edit-similarity and code-centric metrics"], "summary": "本文构建并开源了一个包含48,398次Jupyter笔记本代码编辑的基准数据集，分析笔记本维护的局部编辑模式并通过few-shot与微调评估LLM在单元格/行级代码修改预测上的性能，发现模型在真实维护任务上表现仍然较差且对上下文依赖显著。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2501.09745v1", "published": "2025-01-16", "update_time": "2025-01-16", "download_time": "2025-12-11 17:02:40"}
{"id": "2506.04418", "title": "Characterizing Multi-Hunk Patches: Divergence, Proximity, and LLM Repair Challenges", "abstract": "Multi-hunk bugs, where fixes span disjoint regions of code, are common in practice, yet remain underrepresented in automated repair. Existing techniques and benchmarks pre-dominantly target single-hunk scenarios, overlooking the added complexity of coordinating semantically related changes across the codebase. In this work, we characterize HUNK4J, a dataset of multi-hunk patches derived from 372 real-world defects. We propose hunk divergence, a metric that quantifies the variation among edits in a patch by capturing lexical, structural, and file-level differences, while incorporating the number of hunks involved. We further define spatial proximity, a classification that models how hunks are spatially distributed across the program hierarchy. Our empirical study spanning six LLMs reveals that model success rates decline with increased divergence and spatial dispersion. Notably, when using the LLM alone, no model succeeds in the most dispersed Fragment class. These findings highlight a critical gap in LLM capabilities and motivate divergence-aware repair strategies.", "arxiv_url": "https://arxiv.org/abs/2506.04418", "authors": ["Noor Nashid", "Daniel Ding", "Keheliya Gallaba", "Ahmed E. Hassan", "Ali Mesbah"], "first_author": "Noor Nashid", "category": ["Benchmark", "Empirical", "Technical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Multi-hunk patch characterization", "Hunk divergence metric", "Spatial proximity classification", "Lexical/AST/file-level pairwise distance", "Divergence-aware repair analysis", "LLM-based program repair evaluation", "Context retrieval and scope effects"], "summary": "本文提出了衡量多块补丁内部差异的 hunk divergence 指标与表示补丁空间分布的 spatial proximity 分类，并基于372个真实多块缺陷构建基准和评估平台，对六种LLM的多块修复能力进行系统实证，揭示随着散度和分散程度增加模型修复成功率显著下降。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2506.04418v2", "published": "2025-06-04", "update_time": "2025-11-17", "download_time": "2025-12-11 17:03:20"}
{"id": "2509.21891", "title": "AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans", "abstract": "Fine-tuning large language models for code editing has typically relied on mining commits and pull requests. The working hypothesis has been that commit messages describe human intent in natural language, and patches to code describe the changes that implement that intent. However, much of the previously collected data is noisy: commit messages are terse, human-written commits commingle several unrelated edits, and many commits come from simple, rule-based bots.   The recent adoption of software engineering agents changes this landscape. Code changes co-authored by humans and agents tend to be more narrowly scoped and focused on clearer goals. Their commit messages, generated by LLMs, articulate intent and rationale in much greater detail. Moreover, when these changes land in public repositories, they are implicitly filtered by humans: maintainers discard low-quality commits to their projects.   We present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code, OpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August 2025. We describe the identification and curation pipeline, quantify adoption trends of these agents, and analyze the structural properties of the edits. Finally, we show that models fine-tuned on AgentPack can outperform models trained on prior human-only commit corpora, highlighting the potential of using public data from software engineering agents to train future code-editing models.", "arxiv_url": "https://arxiv.org/abs/2509.21891", "authors": ["Yangtian Zi", "Zixuan Wu", "Aleksander Boruch-Gruszecki", "Jonathan Bell", "Arjun Guha"], "first_author": "Yangtian Zi", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Agent–human coauthored commits", "Commit-signature identification", "GitHub public-timeline mining & curation pipeline", "Multi-file code-change diffs", "Agent-written tests", "Detailed commit rationale / intent", "Adoption trends of coding agents", "Fine-tuning for code-editing performance"], "summary": "本文提出AGENTPACK——一个包含1.3M条由软件工程代理与人类共同撰写的GitHub代码更改数据集，说明了识别与清洗流水线、量化代理採用与编辑结构特性，并展示用该数据集微调模型可显著提升代码编辑能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2509.21891v1", "published": "2025-09-26", "update_time": "2025-09-26", "download_time": "2025-12-11 17:03:51"}
{"id": "2509.25203", "title": "Generating High-Quality Datasets for Code Editing via Open-Source Language Models", "abstract": "Code editing plays a vital role in software engineering, requiring developers to adjust existing code according to natural language instructions while keeping functionality intact and avoiding unnecessary modifications. However, commit-based datasets commonly used for this task are often noisy, lack diversity, and fail to reflect the style of real-world edit instructions. To address this, we introduce OpenCodeEdit, an open-source pipeline that leverages multiple LLMs to synthesize realistic code-edit triplets. The pipeline produces both concise \"lazy\" instructions and more detailed \"descriptive\" ones, and applies filtering based on diffs and topics to guarantee data quality and variety. Using this process, we construct OCEDataFT, a curated dataset of 20K samples. Fine-tuning three advanced base models on OCEDataFT leads to significant performance boosts on the CanItEdit benchmark, with relative pass@1 improvements ranging from 4.50% to 20.79%. Notably, the resulting models achieve performance close to closed-source systems, narrowing the gap to GPT-4 to just 3.54%, without relying on proprietary resources or manual annotation.", "arxiv_url": "https://arxiv.org/abs/2509.25203", "authors": ["Zekai Zhang", "Mingwei Liu", "Zhenxi Chen", "Linxi Liang", "Yuxuan Chen", "Guangsheng Ou", "Yanlin Wang", "Dan Li", "Xin Peng", "Zibin Zheng"], "first_author": "Zekai Zhang", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Synthetic pre-edit/instruction/post-edit triplets", "Lazy vs descriptive instruction styles", "Multi-LLM data synthesis", "Diff- and topic-based filtering", "Instruction-tuning for code editing", "Reproducible open-source data pipeline", "Data pruning / 'less-is-more' effect", "Fine-tuning small open-weight code models", "Quantitative pass@1 performance gains"], "summary": "本文提出OpenCodeEdit，一种利用开源大模型合成高质量（前代码-指令-后代码）编辑三元组并通过差异与主题过滤的可复现数据生成管道，发布了20K样本的OCEDataFT数据集并证明其能显著提升开源小模型在指令驱动代码编辑任务上的性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2509.25203v3", "published": "2025-09-19", "update_time": "2025-10-07", "download_time": "2025-12-11 17:04:30"}
{"id": "1707.02275", "title": "A parallel corpus of Python functions and documentation strings for automated code documentation and code generation", "abstract": "Automated documentation of programming source code and automated code generation from natural language are challenging tasks of both practical and scientific interest. Progress in these areas has been limited by the low availability of parallel corpora of code and natural language descriptions, which tend to be small and constrained to specific domains.   In this work we introduce a large and diverse parallel corpus of a hundred thousands Python functions with their documentation strings (\"docstrings\") generated by scraping open source repositories on GitHub. We describe baseline results for the code documentation and code generation tasks obtained by neural machine translation. We also experiment with data augmentation techniques to further increase the amount of training data.   We release our datasets and processing scripts in order to stimulate research in these areas.", "arxiv_url": "https://arxiv.org/abs/1707.02275", "authors": ["Antonio Valerio Miceli Barone", "Rico Sennrich"], "first_author": "Antonio Valerio Miceli Barone", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["Parallel code-docstring corpus", "Docstring extraction from GitHub", "Top-level Python functions", "Synthetic docstrings via back-translation", "Code normalization and parsing", "NMT baselines with BPE subtokenization"], "summary": "本文构建并发布了一个包含约15万条Python函数及其docstring的平行语料库，提供了数据预处理脚本、基于神经机器翻译的代码文档化和代码生成基线以及通过反向翻译生成的合成docstring以便后续研究使用。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/1707.02275v1", "published": "2017-07-07", "update_time": "2017-07-07", "download_time": "2025-12-11 17:05:13"}
{"id": "1811.07234", "title": "Improving Automatic Source Code Summarization via Deep Reinforcement Learning", "abstract": "Code summarization provides a high level natural language description of the function performed by code, as it can benefit the software maintenance, code categorization and retrieval. To the best of our knowledge, most state-of-the-art approaches follow an encoder-decoder framework which encodes the code into a hidden space and then decode it into natural language space, suffering from two major drawbacks: a) Their encoders only consider the sequential content of code, ignoring the tree structure which is also critical for the task of code summarization, b) Their decoders are typically trained to predict the next word by maximizing the likelihood of next ground-truth word with previous ground-truth word given. However, it is expected to generate the entire sequence from scratch at test time. This discrepancy can cause an \\textit{exposure bias} issue, making the learnt decoder suboptimal. In this paper, we incorporate an abstract syntax tree structure as well as sequential content of code snippets into a deep reinforcement learning framework (i.e., actor-critic network). The actor network provides the confidence of predicting the next word according to current state. On the other hand, the critic network evaluates the reward value of all possible extensions of the current state and can provide global guidance for explorations. We employ an advantage reward composed of BLEU metric to train both networks. Comprehensive experiments on a real-world dataset show the effectiveness of our proposed model when compared with some state-of-the-art methods.", "arxiv_url": "https://arxiv.org/abs/1811.07234", "authors": ["Yao Wan", "Zhou Zhao", "Min Yang", "Guandong Xu", "Haochao Ying", "Jian Wu", "Philip S. Yu"], "first_author": "Yao Wan", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Summarization", "tags": ["AST-based LSTM", "Sequence and tree structure fusion", "Hybrid attention", "Actor-Critic reinforcement learning", "Exposure-bias mitigation", "BLEU-based advantage reward", "Pretraining actor and critic", "Python code summarization"], "summary": "本文提出将抽象语法树（AST）和源代码序列通过混合注意力融合，并引入基于BLEU奖励的actor-critic深度强化学习框架以缓解暴露偏差，从而提升自动代码摘要的生成质量。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/1811.07234v1", "published": "2018-11-17", "update_time": "2018-11-17", "download_time": "2025-12-11 17:05:55"}
{"id": "1909.09436", "title": "CodeSearchNet Challenge: Evaluating the State of Semantic Code Search", "abstract": "Semantic code search is the task of retrieving relevant code given a natural language query. While related to other information retrieval tasks, it requires bridging the gap between the language used in code (often abbreviated and highly technical) and natural language more suitable to describe vague concepts and ideas.   To enable evaluation of progress on code search, we are releasing the CodeSearchNet Corpus and are presenting the CodeSearchNet Challenge, which consists of 99 natural language queries with about 4k expert relevance annotations of likely results from CodeSearchNet Corpus. The corpus contains about 6 million functions from open-source code spanning six programming languages (Go, Java, JavaScript, PHP, Python, and Ruby). The CodeSearchNet Corpus also contains automatically generated query-like natural language for 2 million functions, obtained from mechanically scraping and preprocessing associated function documentation. In this article, we describe the methodology used to obtain the corpus and expert labels, as well as a number of simple baseline solutions for the task.   We hope that CodeSearchNet Challenge encourages researchers and practitioners to study this interesting task further and will host a competition and leaderboard to track the progress on the challenge. We are also keen on extending CodeSearchNet Challenge to more queries and programming languages in the future.", "arxiv_url": "https://arxiv.org/abs/1909.09436", "authors": ["Hamel Husain", "Ho-Hsiang Wu", "Tiferet Gazit", "Miltiadis Allamanis", "Marc Brockschmidt"], "first_author": "Hamel Husain", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Function-documentation pairing", "Semantic code retrieval", "Expert relevance annotations", "Cross-language corpus (6 languages)", "Heuristic preprocessing and deduplication", "Annotation interface and protocol", "NDCG ranking evaluation", "Neural+Elasticsearch baseline ensemble"], "summary": "本文发布了包含约200万条函数-文档对的CodeSearchNet语料库并提出了包含99条查询与约4026条专家相关性标注的CodeSearchNet挑战，介绍了数据收集与过滤流程、注释方法及若干基线检索模型以评估语义代码搜索进展。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/1909.09436v3", "published": "2019-09-20", "update_time": "2020-06-08", "download_time": "2025-12-11 17:06:35"}
{"id": "1801.01681", "title": "VulDeePecker: A Deep Learning-Based System for Vulnerability Detection", "abstract": "The automatic detection of software vulnerabilities is an important research problem. However, existing solutions to this problem rely on human experts to define features and often miss many vulnerabilities (i.e., incurring high false negative rate). In this paper, we initiate the study of using deep learning-based vulnerability detection to relieve human experts from the tedious and subjective task of manually defining features. Since deep learning is motivated to deal with problems that are very different from the problem of vulnerability detection, we need some guiding principles for applying deep learning to vulnerability detection. In particular, we need to find representations of software programs that are suitable for deep learning. For this purpose, we propose using code gadgets to represent programs and then transform them into vectors, where a code gadget is a number of (not necessarily consecutive) lines of code that are semantically related to each other. This leads to the design and implementation of a deep learning-based vulnerability detection system, called Vulnerability Deep Pecker (VulDeePecker). In order to evaluate VulDeePecker, we present the first vulnerability dataset for deep learning approaches. Experimental results show that VulDeePecker can achieve much fewer false negatives (with reasonable false positives) than other approaches. We further apply VulDeePecker to 3 software products (namely Xen, Seamonkey, and Libav) and detect 4 vulnerabilities, which are not reported in the National Vulnerability Database but were \"silently\" patched by the vendors when releasing later versions of these products; in contrast, these vulnerabilities are almost entirely missed by the other vulnerability detection systems we experimented with.", "arxiv_url": "https://arxiv.org/abs/1801.01681", "authors": ["Zhen Li", "Deqing Zou", "Shouhuai Xu", "Xinyu Ou", "Hai Jin", "Sujuan Wang", "Zhijun Deng", "Yuyi Zhong"], "first_author": "Zhen Li", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Code Gadgets", "Code Vectorization", "Bidirectional LSTM", "Vulnerability Pattern Learning", "Fine-grained Vulnerability Localization", "Buffer Error (CWE-119)", "Resource Management Error (CWE-399)", "Large-scale labeled code-gadget dataset"], "summary": "该论文提出以“code gadget”作为中间表示并将其向量化，采用双向LSTM学习漏洞模式构建深度学习漏洞检测系统并发布首个面向该方法的大规模数据集，从而显著降低漏报率并发现若干未登记录但被静默修复的漏洞。", "quality": "High", "conference": "NDSS (Network and Distributed System Security Symposium) 2018", "pdf_url": "https://arxiv.org/pdf/1801.01681v1", "published": "2018-01-05", "update_time": "2018-01-05", "download_time": "2025-12-11 17:07:33"}
{"id": "1807.04320", "title": "Automated Vulnerability Detection in Source Code Using Deep Representation Learning", "abstract": "Increasing numbers of software vulnerabilities are discovered every year whether they are reported publicly or discovered internally in proprietary code. These vulnerabilities can pose serious risk of exploit and result in system compromise, information leaks, or denial of service. We leveraged the wealth of C and C++ open-source code available to develop a large-scale function-level vulnerability detection system using machine learning. To supplement existing labeled vulnerability datasets, we compiled a vast dataset of millions of open-source functions and labeled it with carefully-selected findings from three different static analyzers that indicate potential exploits. The labeled dataset is available at: https://osf.io/d45bw/. Using these datasets, we developed a fast and scalable vulnerability detection tool based on deep feature representation learning that directly interprets lexed source code. We evaluated our tool on code from both real software packages and the NIST SATE IV benchmark dataset. Our results demonstrate that deep feature representation learning on source code is a promising approach for automated software vulnerability detection.", "arxiv_url": "https://arxiv.org/abs/1807.04320", "authors": ["Rebecca L. Russell", "Louis Kim", "Lei H. Hamilton", "Tomo Lazovich", "Jacob A. Harer", "Onur Ozdemir", "Paul M. Ellingwood", "Marc W. McConley"], "first_author": "Rebecca L. Russell", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Function-level vulnerability detection", "Lexed token representation with placeholder tokens", "Static-analyzer-derived binary labels", "Strict duplicate and near-duplicate removal", "Neural embeddings with CNN/RNN feature extraction", "Random-forest ensemble on learned features", "Large-scale mined C/C++ corpus"], "summary": "本文构建了一个由静态分析器标注的大规模C/C++函数语料并提出一种基于词法化表示、神经表征学习（CNN/RNN）与随机森林集成的自动漏洞检测方法，实验证明在真实软件包与基准集上效果良好。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/1807.04320v2", "published": "2018-07-11", "update_time": "2018-11-28", "download_time": "2025-12-11 17:08:13"}
{"id": "1807.06756", "title": "SySeVR: A Framework for Using Deep Learning to Detect Software Vulnerabilities", "abstract": "The detection of software vulnerabilities (or vulnerabilities for short) is an important problem that has yet to be tackled, as manifested by the many vulnerabilities reported on a daily basis. This calls for machine learning methods for vulnerability detection. Deep learning is attractive for this purpose because it alleviates the requirement to manually define features. Despite the tremendous success of deep learning in other application domains, its applicability to vulnerability detection is not systematically understood. In order to fill this void, we propose the first systematic framework for using deep learning to detect vulnerabilities in C/C++ programs with source code. The framework, dubbed Syntax-based, Semantics-based, and Vector Representations (SySeVR), focuses on obtaining program representations that can accommodate syntax and semantic information pertinent to vulnerabilities. Our experiments with 4 software products demonstrate the usefulness of the framework: we detect 15 vulnerabilities that are not reported in the National Vulnerability Database. Among these 15 vulnerabilities, 7 are unknown and have been reported to the vendors, and the other 8 have been \"silently\" patched by the vendors when releasing newer versions of the pertinent software products.", "arxiv_url": "https://arxiv.org/abs/1807.06756", "authors": ["Zhen Li", "Deqing Zou", "Shouhuai Xu", "Hai Jin", "Yawei Zhu", "Zhaoxuan Chen"], "first_author": "Zhen Li", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Syntax-based Vulnerability Candidates (SyVC)", "Semantics-based Vulnerability Candidates (SeVC)", "Program vectorization for vulnerability detection", "AST-driven syntax extraction", "Data- and control-dependency slicing", "Bidirectional recurrent networks (BGRU) for code classification", "Explainability of false positives/false negatives", "Large-scale 126-type labeled vulnerability dataset"], "summary": "本文提出SySeVR框架，通过提取基于语法的候选（SyVC）并扩展为包含数据与控制依赖的语义候选（SeVC），将SeVC编码为向量并使用深度神经网络（尤其双向GRU）检测C/C++源代码中的漏洞，同时发布了包含126类漏洞的数据集并在实际软件中发现若干未登记的漏洞。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/1807.06756v3", "published": "2018-07-18", "update_time": "2021-01-12", "download_time": "2025-12-11 17:08:59"}
{"id": "1902.02595", "title": "A Manually-Curated Dataset of Fixes to Vulnerabilities of Open-Source Software", "abstract": "Advancing our understanding of software vulnerabilities, automating their identification, the analysis of their impact, and ultimately their mitigation is necessary to enable the development of software that is more secure. While operating a vulnerability assessment tool that we developed and that is currently used by hundreds of development units at SAP, we manually collected and curated a dataset of vulnerabilities of open-source software and the commits fixing them. The data was obtained both from the National Vulnerability Database (NVD) and from project-specific Web resources that we monitor on a continuous basis. From that data, we extracted a dataset that maps 624 publicly disclosed vulnerabilities affecting 205 distinct open-source Java projects, used in SAP products or internal tools, onto the 1282 commits that fix them. Out of 624 vulnerabilities, 29 do not have a CVE identifier at all and 46, which do have a CVE identifier assigned by a numbering authority, are not available in the NVD yet. The dataset is released under an open-source license, together with supporting scripts that allow researchers to automatically retrieve the actual content of the commits from the corresponding repositories and to augment the attributes available for each instance. Also, these scripts allow to complement the dataset with additional instances that are not security fixes (which is useful, for example, in machine learning applications). Our dataset has been successfully used to train classifiers that could automatically identify security-relevant commits in code repositories. The release of this dataset and the supporting code as open-source will allow future research to be based on data of industrial relevance; also, it represents a concrete step towards making the maintenance of this dataset a shared effort involving open-source communities, academia, and the industry.", "arxiv_url": "https://arxiv.org/abs/1902.02595", "authors": ["Serena E. Ponta", "Henrik Plate", "Antonino Sabetta", "Michele Bezzi", "Cédric Dangremont"], "first_author": "Serena E. Ponta", "category": ["Benchmark"], "field": "Quality Management", "task": "Vulnerability Repair", "tags": ["Fix-commit mapping", "Manual curation", "Security-relevant commit labels", "Java open-source projects", "NVD augmentation", "Industrial-grade vulnerability corpus", "Negative-sample generation scripts", "Commit-level vulnerability metadata"], "summary": "本文发布了一个手工整理的开源Java漏洞与其修复提交的数据集，包含624个漏洞与1282个修复提交并提供脚本用于检索、扩展和生成负样本。", "quality": "High", "conference": "Proceedings of The 16th International Conference on Mining Software Repositories (Data Showcase track) 2019", "pdf_url": "https://arxiv.org/pdf/1902.02595v3", "published": "2019-02-07", "update_time": "2019-03-19", "download_time": "2025-12-11 17:09:32"}
{"id": "2001.02334", "title": "$μ$VulDeePecker: A Deep Learning-Based System for Multiclass Vulnerability Detection", "abstract": "Fine-grained software vulnerability detection is an important and challenging problem. Ideally, a detection system (or detector) not only should be able to detect whether or not a program contains vulnerabilities, but also should be able to pinpoint the type of a vulnerability in question. Existing vulnerability detection methods based on deep learning can detect the presence of vulnerabilities (i.e., addressing the binary classification or detection problem), but cannot pinpoint types of vulnerabilities (i.e., incapable of addressing multiclass classification). In this paper, we propose the first deep learning-based system for multiclass vulnerability detection, dubbed $μ$VulDeePecker. The key insight underlying $μ$VulDeePecker is the concept of code attention, which can capture information that can help pinpoint types of vulnerabilities, even when the samples are small. For this purpose, we create a dataset from scratch and use it to evaluate the effectiveness of $μ$VulDeePecker. Experimental results show that $μ$VulDeePecker is effective for multiclass vulnerability detection and that accommodating control-dependence (other than data-dependence) can lead to higher detection capabilities.", "arxiv_url": "https://arxiv.org/abs/2001.02334", "authors": ["Deqing Zou", "Sujuan Wang", "Shouhuai Xu", "Zhen Li", "Hai Jin"], "first_author": "Deqing Zou", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Multiclass vulnerability classification", "Code attention", "Code gadget extraction", "Control-dependence analysis", "System Dependency Graphs", "Feature-fusion BLSTM architecture", "API/library call vulnerability detection", "Fine-grained CWE-based labeling"], "summary": "本文提出µVulDeePecker，通过引入“code attention”并在code gadget中融合数据依赖与控制依赖、采用特征融合的BLSTM架构，对C/C++中基于库/API调用的漏洞进行多类细粒度检测并发布了包含40类漏洞的代码片段数据集以验证其有效性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2001.02334v1", "published": "2020-01-08", "update_time": "2020-01-08", "download_time": "2025-12-11 17:10:27"}
{"id": "2009.07235", "title": "Deep Learning based Vulnerability Detection: Are We There Yet?", "abstract": "Automated detection of software vulnerabilities is a fundamental problem in software security. Existing program analysis techniques either suffer from high false positives or false negatives. Recent progress in Deep Learning (DL) has resulted in a surge of interest in applying DL for automated vulnerability detection. Several recent studies have demonstrated promising results achieving an accuracy of up to 95% at detecting vulnerabilities. In this paper, we ask, \"how well do the state-of-the-art DL-based techniques perform in a real-world vulnerability prediction scenario?\". To our surprise, we find that their performance drops by more than 50%. A systematic investigation of what causes such precipitous performance drop reveals that existing DL-based vulnerability prediction approaches suffer from challenges with the training data (e.g., data duplication, unrealistic distribution of vulnerable classes, etc.) and with the model choices (e.g., simple token-based models). As a result, these approaches often do not learn features related to the actual cause of the vulnerabilities. Instead, they learn unrelated artifacts from the dataset (e.g., specific variable/function names, etc.). Leveraging these empirical findings, we demonstrate how a more principled approach to data collection and model design, based on realistic settings of vulnerability prediction, can lead to better solutions. The resulting tools perform significantly better than the studied baseline: up to 33.57% boost in precision and 128.38% boost in recall compared to the best performing model in the literature. Overall, this paper elucidates existing DL-based vulnerability prediction systems' potential issues and draws a roadmap for future DL-based vulnerability prediction research. In that spirit, we make available all the artifacts supporting our results: https://git.io/Jf6IA.", "arxiv_url": "https://arxiv.org/abs/2009.07235", "authors": ["Saikat Chakraborty", "Rahul Krishna", "Yangruibo Ding", "Baishakhi Ray"], "first_author": "Saikat Chakraborty", "category": ["Empirical", "Benchmark", "Technical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Data duplication and leakage", "Class imbalance in vulnerability datasets", "Graph-based program semantics", "Representation learning for class separation", "Learning dataset artifacts (identifier/name bias)", "Real-world vulnerability curation from issue/commit mining", "Feature-attribution explainability", "Evaluation generalizability gap"], "summary": "本文系统性评估了现有深度学习漏洞检测方法在真实世界场景中显著降级的原因（如数据重复、类不平衡和基于 token 的模型忽略语义），并通过公开从 Chromium 和 Debian 挖掘的真实漏洞数据集、去重与平衡策略以及基于图的语义表示和表征学习等改进显著提升了检测性能并给出未来研究建议。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2009.07235v1", "published": "2020-09-03", "update_time": "2020-09-03", "download_time": "2025-12-11 17:11:08"}
{"id": "2102.07995", "title": "D2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using Differential Analysis", "abstract": "Static analysis tools are widely used for vulnerability detection as they understand programs with complex behavior and millions of lines of code. Despite their popularity, static analysis tools are known to generate an excess of false positives. The recent ability of Machine Learning models to understand programming languages opens new possibilities when applied to static analysis. However, existing datasets to train models for vulnerability identification suffer from multiple limitations such as limited bug context, limited size, and synthetic and unrealistic source code. We propose D2A, a differential analysis based approach to label issues reported by static analysis tools. The D2A dataset is built by analyzing version pairs from multiple open source projects. From each project, we select bug fixing commits and we run static analysis on the versions before and after such commits. If some issues detected in a before-commit version disappear in the corresponding after-commit version, they are very likely to be real bugs that got fixed by the commit. We use D2A to generate a large labeled dataset to train models for vulnerability identification. We show that the dataset can be used to build a classifier to identify possible false alarms among the issues reported by static analysis, hence helping developers prioritize and investigate potential true positives first.", "arxiv_url": "https://arxiv.org/abs/2102.07995", "authors": ["Yunhui Zheng", "Saurabh Pujar", "Burn Lewis", "Luca Buratti", "Edward Epstein", "Bo Yang", "Jim Laredo", "Alessandro Morari", "Zhong Su"], "first_author": "Yunhui Zheng", "category": ["Benchmark", "Technical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Differential labeling from version pairs", "Auto-labeler pipeline", "Static-analyzer false-positive reduction", "Inter-procedural trace preservation", "Commit-pair based ground-truth heuristics", "Scalable parallel static analysis", "Static-analysis-derived features", "Label quality validation via manual review"], "summary": "本文提出D2A：一种基于版本差分静态分析和提交历史的自动标注方法，构建了大规模真实C/C++漏洞检测数据集并展示其在静态分析误报抑制任务中的有效性。", "quality": "High", "conference": "International Conference on Software Engineering (ICSE)", "pdf_url": "https://arxiv.org/pdf/2102.07995v1", "published": "2021-02-16", "update_time": "2021-02-16", "download_time": "2025-12-11 17:11:36"}
{"id": "2105.12787", "title": "Self-Supervised Bug Detection and Repair", "abstract": "Machine learning-based program analyses have recently shown the promise of integrating formal and probabilistic reasoning towards aiding software development. However, in the absence of large annotated corpora, training these analyses is challenging. Towards addressing this, we present BugLab, an approach for self-supervised learning of bug detection and repair. BugLab co-trains two models: (1) a detector model that learns to detect and repair bugs in code, (2) a selector model that learns to create buggy code for the detector to use as training data. A Python implementation of BugLab improves by up to 30% upon baseline methods on a test dataset of 2374 real-life bugs and finds 19 previously unknown bugs in open-source software.", "arxiv_url": "https://arxiv.org/abs/2105.12787", "authors": ["Miltiadis Allamanis", "Henry Jackson-Flux", "Marc Brockschmidt"], "first_author": "Miltiadis Allamanis", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Self-supervised adversarial co-training", "Bug selector-detector", "Syntax-tree rewrite operators", "Localization-and-rewrite prediction", "Heterogeneous code-graph representation", "Relational transformer encoding", "Curated real-world bug benchmark"], "summary": "本文提出BUGLAB——一种通过联合训练“选择器”（生成难以检测的语法重写错误）与“检测器”（定位并修复错误）的自监督方法，基于代码重写规则在未标注代码上学习缺陷定位与修复，并在Python实现上通过新整理的真实缺陷基准显著提升性能且在开源项目中发现若干新错误。", "quality": "High", "conference": "NeurIPS 2021", "pdf_url": "https://arxiv.org/pdf/2105.12787v3", "published": "2021-05-26", "update_time": "2021-11-16", "download_time": "2025-12-11 17:12:10"}
{"id": "2107.08760", "title": "CVEfixes: Automated Collection of Vulnerabilities and Their Fixes from Open-Source Software", "abstract": "Data-driven research on the automated discovery and repair of security vulnerabilities in source code requires comprehensive datasets of real-life vulnerable code and their fixes. To assist in such research, we propose a method to automatically collect and curate a comprehensive vulnerability dataset from Common Vulnerabilities and Exposures (CVE) records in the public National Vulnerability Database (NVD). We implement our approach in a fully automated dataset collection tool and share an initial release of the resulting vulnerability dataset named CVEfixes.   The CVEfixes collection tool automatically fetches all available CVE records from the NVD, gathers the vulnerable code and corresponding fixes from associated open-source repositories, and organizes the collected information in a relational database. Moreover, the dataset is enriched with meta-data such as programming language, and detailed code and security metrics at five levels of abstraction. The collection can easily be repeated to keep up-to-date with newly discovered or patched vulnerabilities. The initial release of CVEfixes spans all published CVEs up to 9 June 2021, covering 5365 CVE records for 1754 open-source projects that were addressed in a total of 5495 vulnerability fixing commits.   CVEfixes supports various types of data-driven software security research, such as vulnerability prediction, vulnerability classification, vulnerability severity prediction, analysis of vulnerability-related code changes, and automated vulnerability repair.", "arxiv_url": "https://arxiv.org/abs/2107.08760", "authors": ["Guru Prasad Bhandari", "Amara Naseer", "Leon Moonen"], "first_author": "Guru Prasad Bhandari", "category": ["Benchmark", "Survey"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["CVE-to-commit linking", "vulnerability-fix mining", "multi-granularity code artifacts", "CWE-based labeling", "CVSS severity enrichment", "code- and commit-level metrics", "language-agnostic collection", "automated repository mining tool"], "summary": "本文提出并开源了一种从NVD CVE记录自动收集开源项目中真实漏洞及其修复的工具与数据集，按多粒度（仓库/提交/文件/方法/CVE）组织并附加CWE分类、CVSS评分和多项代码度量，旨在支持漏洞检测、分类与自动修复等研究。", "quality": "High", "conference": "Proceedings of the 17th International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE '21) 2021", "pdf_url": "https://arxiv.org/pdf/2107.08760v1", "published": "2021-07-19", "update_time": "2021-07-19", "download_time": "2025-12-11 17:12:46"}
{"id": "2304.00409", "title": "DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection", "abstract": "We propose and release a new vulnerable source code dataset. We curate the dataset by crawling security issue websites, extracting vulnerability-fixing commits and source codes from the corresponding projects. Our new dataset contains 18,945 vulnerable functions spanning 150 CWEs and 330,492 non-vulnerable functions extracted from 7,514 commits. Our dataset covers 295 more projects than all previous datasets combined.   Combining our new dataset with previous datasets, we present an analysis of the challenges and promising research directions of using deep learning for detecting software vulnerabilities. We study 11 model architectures belonging to 4 families. Our results show that deep learning is still not ready for vulnerability detection, due to high false positive rate, low F1 score, and difficulty of detecting hard CWEs. In particular, we demonstrate an important generalization challenge for the deployment of deep learning-based models. We show that increasing the volume of training data may not further improve the performance of deep learning models for vulnerability detection, but might be useful to improve the generalization ability to unseen projects.   We also identify hopeful future research directions. We demonstrate that large language models (LLMs) are a promising research direction for ML-based vulnerability detection, outperforming Graph Neural Networks (GNNs) with code-structure features in our experiments. Moreover, developing source code specific pre-training objectives is a promising research direction to improve the vulnerability detection performance.", "arxiv_url": "https://arxiv.org/abs/2304.00409", "authors": ["Yizheng Chen", "Zhoujie Ding", "Lamya Alowain", "Xinyun Chen", "David Wagner"], "first_author": "Yizheng Chen", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["C/C++ function-level vulnerability labels", "Vulnerability-fixing-commit labeling", "Label noise quantification", "Cross-project generalization", "Code-specific pretraining objectives", "Transformer vs GNN comparison", "False positive rate analysis", "CWE diversity coverage"], "summary": "本文发布了DiverseVul，一个大规模且多样化的C/C++漏洞函数数据集，并通过对多种模型的系统评估发现：在更大训练集下基于代码预训练的Transformer优于GNN但总体F1和跨项目泛化仍然很差且标签噪声显著，强调需改进代码专用预训练与泛化方法。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2304.00409v2", "published": "2023-04-01", "update_time": "2023-08-09", "download_time": "2025-12-11 17:13:24"}
{"id": "2306.17193", "title": "Uncovering the Limits of Machine Learning for Automatic Vulnerability Detection", "abstract": "Recent results of machine learning for automatic vulnerability detection (ML4VD) have been very promising. Given only the source code of a function $f$, ML4VD techniques can decide if $f$ contains a security flaw with up to 70% accuracy. However, as evident in our own experiments, the same top-performing models are unable to distinguish between functions that contain a vulnerability and functions where the vulnerability is patched. So, how can we explain this contradiction and how can we improve the way we evaluate ML4VD techniques to get a better picture of their actual capabilities?   In this paper, we identify overfitting to unrelated features and out-of-distribution generalization as two problems, which are not captured by the traditional approach of evaluating ML4VD techniques. As a remedy, we propose a novel benchmarking methodology to help researchers better evaluate the true capabilities and limits of ML4VD techniques. Specifically, we propose (i) to augment the training and validation dataset according to our cross-validation algorithm, where a semantic preserving transformation is applied during the augmentation of either the training set or the testing set, and (ii) to augment the testing set with code snippets where the vulnerabilities are patched.   Using six ML4VD techniques and two datasets, we find (a) that state-of-the-art models severely overfit to unrelated features for predicting the vulnerabilities in the testing data, (b) that the performance gained by data augmentation does not generalize beyond the specific augmentations applied during training, and (c) that state-of-the-art ML4VD techniques are unable to distinguish vulnerable functions from their patches.", "arxiv_url": "https://arxiv.org/abs/2306.17193", "authors": ["Niklas Risse", "Marcel Böhme"], "first_author": "Niklas Risse", "category": ["Empirical", "Benchmark", "Technical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Semantic-preserving code transformations", "Overfitting to superficial features", "Cross-transformation generalization", "Patch-pair evaluation", "Data augmentation robustness", "Vuln–patch paired dataset", "Evaluation algorithms for ML4VD", "Out-of-distribution generalization"], "summary": "本文提出两种用于评估自动化漏洞检测（ML4VD）的算法并发布包含漏洞与对应补丁对的新数据集，实验证明现有基于令牌的模型严重依赖与漏洞无关的特征且无法区分漏洞与其补丁。", "quality": "High", "conference": "Proceedings of the 33rd USENIX Security Symposium (USENIX Security 2024) 2024", "pdf_url": "https://arxiv.org/pdf/2306.17193v2", "published": "2023-06-28", "update_time": "2024-06-06", "download_time": "2025-12-11 17:14:01"}
{"id": "2311.12420", "title": "How Far Have We Gone in Vulnerability Detection Using Large Language Models", "abstract": "As software becomes increasingly complex and prone to vulnerabilities, automated vulnerability detection is critically important, yet challenging. Given the significant successes of large language models (LLMs) in various tasks, there is growing anticipation of their efficacy in vulnerability detection. However, a quantitative understanding of their potential in vulnerability detection is still missing. To bridge this gap, we introduce a comprehensive vulnerability benchmark VulBench. This benchmark aggregates high-quality data from a wide range of CTF (Capture-the-Flag) challenges and real-world applications, with annotations for each vulnerable function detailing the vulnerability type and its root cause. Through our experiments encompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based models and static analyzers, we find that several LLMs outperform traditional deep learning approaches in vulnerability detection, revealing an untapped potential in LLMs. This work contributes to the understanding and utilization of LLMs for enhanced software security.", "arxiv_url": "https://arxiv.org/abs/2311.12420", "authors": ["Zeyu Gao", "Hao Wang", "Yuchen Zhou", "Wenyu Zhu", "Chao Zhang"], "first_author": "Zeyu Gao", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Function-level vulnerability labels", "CTF and real-world CVE aggregation", "Human expert annotation", "Binary and multi-class classification", "Root-cause vulnerability annotation", "Natural-language vulnerability descriptions", "LLM vs. static analyzer benchmarking", "Cross-model evaluation protocol", "Dataset quality curation"], "summary": "本文提出了VulBench——一个从CTF与真实漏洞数据源汇聚并经人工校验的高质量函数级漏洞基准，并在该基准上系统评估了16个LLM与多种深度学习模型和静态分析器，揭示了LLM在漏洞检测中的潜力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2311.12420v3", "published": "2023-11-21", "update_time": "2023-12-22", "download_time": "2025-12-11 17:14:38"}
{"id": "2403.18624", "title": "Vulnerability Detection with Code Language Models: How Far Are We?", "abstract": "In the context of the rising interest in code language models (code LMs) and vulnerability detection, we study the effectiveness of code LMs for detecting vulnerabilities. Our analysis reveals significant shortcomings in existing vulnerability datasets, including poor data quality, low label accuracy, and high duplication rates, leading to unreliable model performance in realistic vulnerability detection scenarios. Additionally, the evaluation methods used with these datasets are not representative of real-world vulnerability detection.   To address these challenges, we introduce PrimeVul, a new dataset for training and evaluating code LMs for vulnerability detection. PrimeVul incorporates a novel set of data labeling techniques that achieve comparable label accuracy to human-verified benchmarks while significantly expanding the dataset. It also implements a rigorous data de-duplication and chronological data splitting strategy to mitigate data leakage issues, alongside introducing more realistic evaluation metrics and settings. This comprehensive approach aims to provide a more accurate assessment of code LMs' performance in real-world conditions.   Evaluating code LMs on PrimeVul reveals that existing benchmarks significantly overestimate the performance of these models. For instance, a state-of-the-art 7B model scored 68.26% F1 on BigVul but only 3.09% F1 on PrimeVul. Attempts to improve performance through advanced training techniques and larger models like GPT-3.5 and GPT-4 were unsuccessful, with results akin to random guessing in the most stringent settings. These findings underscore the considerable gap between current capabilities and the practical requirements for deploying code LMs in security roles, highlighting the need for more innovative research in this domain.", "arxiv_url": "https://arxiv.org/abs/2403.18624", "authors": ["Yangruibo Ding", "Yanjun Fu", "Omniyyah Ibrahim", "Chawin Sitawarin", "Xinyun Chen", "Basel Alomair", "David Wagner", "Baishakhi Ray", "Yizheng Chen"], "first_author": "Yangruibo Ding", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Label Noise Analysis", "Data De-duplication", "Chronological Train-Test Split", "False-Positive-Constrained Metric", "Pairwise Vulnerable-vs-Fixed Evaluation", "Commit-based Labeling Pitfalls", "CWE Coverage Expansion", "Real-world Evaluation Protocols"], "summary": "本文分析了现有漏洞数据集的标签噪声与数据泄露问题，提出高质量去重的大规模漏洞数据集PRIMEVUL并引入VD‑S与成对评估等更真实的评估指南，实验证明现有代码语言模型在现实漏洞检测场景下表现严重不足。", "quality": "High", "conference": "IEEE/ACM International Conference on Software Engineering (ICSE 2025)", "pdf_url": "https://arxiv.org/pdf/2403.18624v2", "published": "2024-03-27", "update_time": "2024-07-10", "download_time": "2025-12-11 17:15:08"}
{"id": "2406.07595", "title": "VulDetectBench: Evaluating the Deep Capability of Vulnerability Detection with Large Language Models", "abstract": "Large Language Models (LLMs) have training corpora containing large amounts of program code, greatly improving the model's code comprehension and generation capabilities. However, sound comprehensive research on detecting program vulnerabilities, a more specific task related to code, and evaluating the performance of LLMs in this more specialized scenario is still lacking. To address common challenges in vulnerability analysis, our study introduces a new benchmark, VulDetectBench, specifically designed to assess the vulnerability detection capabilities of LLMs. The benchmark comprehensively evaluates LLM's ability to identify, classify, and locate vulnerabilities through five tasks of increasing difficulty. We evaluate the performance of 17 models (both open- and closed-source) and find that while existing models can achieve over 80% accuracy on tasks related to vulnerability identification and classification, they still fall short on specific, more detailed vulnerability analysis tasks, with less than 30% accuracy, making it difficult to provide valuable auxiliary information for professional vulnerability mining. Our benchmark effectively evaluates the capabilities of various LLMs at different levels in the specific task of vulnerability detection, providing a foundation for future research and improvements in this critical area of code security. VulDetectBench is publicly available at https://github.com/Sweetaroo/VulDetectBench.", "arxiv_url": "https://arxiv.org/abs/2406.07595", "authors": ["Yu Liu", "Lang Gao", "Mingxin Yang", "Yu Xie", "Ping Chen", "Xiaojin Zhang", "Wei Chen"], "first_author": "Yu Liu", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Vulnerability existence detection", "CWE type classification", "Root cause localization", "Trigger point identification", "Key objects/functions extraction", "C/C++ memory-safety vulnerabilities", "Mixed real-world and synthetic corpus", "Multi-task benchmark", "Cross-model comparative evaluation"], "summary": "本文提出VulDetectBench——一个面向C/C++漏洞检测的多任务基准（包含五个难度递增任务并结合真实与合成代码），对17个开闭源模型进行了比较评估，发现模型在漏洞存在检测与类型分类上表现良好但在根因与触发点等精细定位任务上表现不佳。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2406.07595v4", "published": "2024-06-11", "update_time": "2024-08-21", "download_time": "2025-12-11 17:15:44"}
{"id": "2411.17274", "title": "CleanVul: Automatic Function-Level Vulnerability Detection in Code Commits Using LLM Heuristics", "abstract": "Accurate identification of software vulnerabilities is crucial for system integrity. Vulnerability datasets, often derived from the National Vulnerability Database (NVD) or directly from GitHub, are essential for training machine learning models to detect these security flaws. However, these datasets frequently suffer from significant noise, typically 40% to 75%, due primarily to the automatic and indiscriminate labeling of all changes in vulnerability-fixing commits (VFCs) as vulnerability-related. This misclassification occurs because not all changes in a commit aimed at fixing vulnerabilities pertain to security threats; many are routine updates like bug fixes or test improvements.   This paper introduces the first methodology that uses the Large Language Model (LLM) with a heuristic enhancement to automatically identify vulnerability-fixing changes from VFCs, achieving an F1-score of 0.82. VulSifter was applied to a large-scale study, where we conducted a crawl of 127,063 repositories on GitHub, resulting in the acquisition of 5,352,105 commits. VulSifter involves utilizing an LLM to comprehend code semantics and contextual information, while applying heuristics to filter out unrelated changes. We then developed CleanVul, a high-quality dataset comprising 8,198 functions using our LLM heuristic enhancement approach, demonstrating Correctness (90.6%) comparable to established datasets such as SVEN and PrimeVul.   To evaluate the CleanVul dataset, we conducted experiments focusing on fine-tuning various LLMs on CleanVul and other high-quality datasets. Evaluation results reveal that LLMs fine-tuned on CleanVul not only exhibit enhanced accuracy but also superior generalization capabilities compared to those trained on uncleaned datasets. Specifically, models trained on CleanVul and tested on PrimeVul achieve accuracy higher than those trained and tested exclusively on PrimeVul.", "arxiv_url": "https://arxiv.org/abs/2411.17274", "authors": ["Yikun Li", "Ting Zhang", "Ratnadira Widyasari", "Yan Naing Tun", "Huu Hung Nguyen", "Tan Bui", "Ivana Clairine Irsan", "Yiran Cheng", "Xiang Lan", "Han Wei Ang", "Frank Liauw", "Martin Weyssow", "Hong Jin Kang", "Eng Lieh Ouh", "Lwin Khin Shar", "David Lo"], "first_author": "Yikun Li", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["LLM-based noise reduction", "Vulnerability-fixing commit analysis", "Function-level vulnerability labeling", "Heuristic filtering of code changes", "Commit diff and message contextualization", "Automated dataset curation pipeline", "Cross-dataset generalization evaluation"], "summary": "本文提出将大模型与启发式规则结合的VulSifter方法，自动识别并过滤漏洞修复提交中与漏洞无关的改动，构建高质量的函数级漏洞数据集CleanVul并验证其能提升漏洞检测模型的泛化能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.17274v7", "published": "2024-11-26", "update_time": "2025-09-11", "download_time": "2025-12-11 17:19:29"}
{"id": "2503.09433", "title": "CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards CWE Detection", "abstract": "Identifying vulnerabilities in source code is crucial, especially in critical software components. Existing methods such as static analysis, dynamic analysis, formal verification, and recently Large Language Models are widely used to detect security flaws. This paper introduces CASTLE (CWE Automated Security Testing and Low-Level Evaluation), a benchmarking framework for evaluating the vulnerability detection capabilities of different methods. We assess 13 static analysis tools, 10 LLMs, and 2 formal verification tools using a hand-crafted dataset of 250 micro-benchmark programs covering 25 common CWEs. We propose the CASTLE Score, a novel evaluation metric to ensure fair comparison. Our results reveal key differences: ESBMC (a formal verification tool) minimizes false positives but struggles with vulnerabilities beyond model checking, such as weak cryptography or SQL injection. Static analyzers suffer from high false positives, increasing manual validation efforts for developers. LLMs perform exceptionally well in the CASTLE dataset when identifying vulnerabilities in small code snippets. However, their accuracy declines, and hallucinations increase as the code size grows. These results suggest that LLMs could play a pivotal role in future security solutions, particularly within code completion frameworks, where they can provide real-time guidance to prevent vulnerabilities. The dataset is accessible at https://github.com/CASTLE-Benchmark.", "arxiv_url": "https://arxiv.org/abs/2503.09433", "authors": ["Richard A. Dubniczky", "Krisztofer Zoltán Horvát", "Tamás Bisztray", "Mohamed Amine Ferrag", "Lucas C. Cordeiro", "Norbert Tihanyi"], "first_author": "Richard A. Dubniczky", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["CWE micro-benchmarks", "compilable C snippets", "line-level vulnerability labeling", "benchmark scoring metric", "static analyzer false-positive analysis", "LLM detection scalability", "formal verification coverage"], "summary": "本文构建了一个包含250个可编译C微基准、覆盖25类CWE的漏洞检测基准并提出评估度量，通过比较静态分析器、形式验证工具与多款LLM揭示各方法在误报率、覆盖范围与随代码规模扩展时性能衰减的差异。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2503.09433v2", "published": "2025-03-12", "update_time": "2025-03-31", "download_time": "2025-12-11 17:19:57"}
{"id": "2503.22388", "title": "Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors", "abstract": "LLMs are transforming software development, yet current code generation and code repair benchmarks mainly assess syntactic and functional correctness in simple, single-error cases. LLMs' capabilities to autonomously find and fix runtime logical errors in complex data science code remain largely unexplored. To address this gap, we introduce DSDBench: the Data Science Debugging Benchmark, the first benchmark for systematic evaluation of LLMs on multi-hop error tracing and multi-bug detection in data science code debugging. DSDBench adapts datasets from existing data science task benchmarks, such as DABench and MatPlotBench, featuring realistic data science debugging tasks with automatically synthesized multi-hop, multi-bug code snippets. DSDBench includes 1,117 annotated samples with 741 cause-effect error pairs and runtime error messages. Evaluations of state-of-the-art LLMs on DSDBench show significant performance gaps, highlighting challenges in debugging logical runtime errors in data science code. DSDBench offers a crucial resource to evaluate and improve LLMs' debugging and reasoning capabilities, enabling more reliable AI-assisted data science in the future. DSDBench is publicly available at github.com/KevinCL16/DSDBench.", "arxiv_url": "https://arxiv.org/abs/2503.22388", "authors": ["Zhiyu Yang", "Shuo Wang", "Yukun Yan", "Yang Deng"], "first_author": "Zhiyu Yang", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Multi-hop Error Tracing", "Multi-Bug Detection", "Runtime Error Injection", "Cause-Effect Line Annotation", "Automated Error Injection Pipeline", "Library API Misuse", "Interactive Notebook Debugging", "Data Science Workflow Failures"], "summary": "本文提出并发布了DSDBench——一个针对数据科学脚本的基准数据集与自动化注入/对齐流水线，用以评估LLM在多跳与多错误运行时调试（定位根因并关联错误触发行）方面的能力，并通过大规模实证揭示了现有模型在此任务上的显著不足。", "quality": "High", "conference": "EMNLP 2025", "pdf_url": "https://arxiv.org/pdf/2503.22388v3", "published": "2025-03-28", "update_time": "2025-09-16", "download_time": "2025-12-11 17:20:27"}
{"id": "2505.19828", "title": "SecVulEval: Benchmarking LLMs for Real-World C/C++ Vulnerability Detection", "abstract": "Large Language Models (LLMs) have shown promise in software engineering tasks, but evaluating their effectiveness in vulnerability detection is challenging due to the lack of high-quality datasets. Most existing datasets are limited to function-level labels, ignoring finer-grained vulnerability patterns and crucial contextual information. Also, poor data quality such as mislabeling, inconsistent annotations, and duplicates can lead to inflated performance and weak generalization. Moreover, by including only the functions, these datasets miss broader program context, like data/control dependencies and interprocedural interactions, that are essential for accurately understanding real-world security flaws. Without this context, detection models are evaluated under unrealistic assumptions.   To address these limitations, this paper introduces SecVulEval, a benchmark designed to support fine-grained evaluation of LLMs and other detection methods with rich contextual information. SecVulEval focuses on real-world C/C++ vulnerabilities at the statement level. This granularity enables more precise evaluation of a model's ability to localize vulnerabilities, beyond simple binary classification at the function level. By incorporating rich contextual information, SecVulEval sets a new standard for vulnerability detection benchmarks in realistic scenarios. This benchmark includes 25,440 function samples covering 5,867 unique CVEs in C/C++ projects from 1999 to 2024. We evaluated the SOTA LLMs with a multi-agent-based approach. The evaluation on our dataset shows that the models are still far from accurately predicting vulnerable statements in a given function. The best-performing Claude-3.7-Sonnet model achieves 23.83% F1-score for detecting vulnerable statements with correct reasoning. Finally, we analyze the LLM outputs and provide insights into their behavior in vulnerability detection for C/C++.", "arxiv_url": "https://arxiv.org/abs/2505.19828", "authors": ["Md Basim Uddin Ahmed", "Nima Shiri Harzevili", "Jiho Shin", "Hung Viet Pham", "Song Wang"], "first_author": "Md Basim Uddin Ahmed", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Statement-level vulnerability localization", "Patch-derived labels and commit metadata", "CWE/CVE annotation", "Context extraction across five context levels", "Interprocedural and data/control dependency context", "Multi-agent LLM evaluation pipeline", "De-duplication and data quality curation", "Empirical F1 evaluation of LLM vulnerability localization"], "summary": "本文提出SECVULEVAL——一个面向真实C/C++项目、具备语句级漏洞标注与丰富上下文（函数参数、外部函数、类型定义、全局量、执行环境）和CVE/CWE元数据的基准，并通过多代理LLM评估显示当前模型在语句级漏洞定位上表现仍然很差。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2505.19828v1", "published": "2025-05-26", "update_time": "2025-05-26", "download_time": "2025-12-11 17:21:02"}
{"id": "2505.20630", "title": "SV-TrustEval-C: Evaluating Structure and Semantic Reasoning in Large Language Models for Source Code Vulnerability Analysis", "abstract": "As Large Language Models (LLMs) evolve in understanding and generating code, accurately evaluating their reliability in analyzing source code vulnerabilities becomes increasingly vital. While studies have examined LLM capabilities in tasks like vulnerability detection and repair, they often overlook the importance of both structure and semantic reasoning crucial for trustworthy vulnerability analysis. To address this gap, we introduce SV-TrustEval-C, a benchmark designed to evaluate LLMs' abilities for vulnerability analysis of code written in the C programming language through two key dimensions: structure reasoning - assessing how models identify relationships between code elements under varying data and control flow complexities; and semantic reasoning - examining their logical consistency in scenarios where code is structurally and semantically perturbed. Our results show that current LLMs are far from satisfactory in understanding complex code relationships and that their vulnerability analyses rely more on pattern matching than on robust logical reasoning. These findings underscore the effectiveness of the SV-TrustEval-C benchmark and highlight critical areas for enhancing the reasoning capabilities and trustworthiness of LLMs in real-world vulnerability analysis tasks. Our initial benchmark dataset is publicly available.", "arxiv_url": "https://arxiv.org/abs/2505.20630", "authors": ["Yansong Li", "Paula Branco", "Alexander M. Hoole", "Manish Marwah", "Hari Manassery Koduvely", "Guy-Vincent Jourdan", "Stephan Jou"], "first_author": "Yansong Li", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Structure reasoning", "Semantic reasoning", "Counterfactual code perturbations", "Goal-driven completion scenarios", "Predictive vulnerability scenarios", "Structure-oriented variants generator", "Data-flow perturbation", "Control-flow perturbation", "C language vulnerabilities", "Reasoning-based QA evaluation", "Pattern-matching vs logical reasoning"], "summary": "本文提出了 SV-TRUSTEVAL-C 基准，通过结构导向变体生成器对 C 语言代码进行语义和结构扰动，以评估 LLM 在漏洞分析中对结构与语义推理的能力，并通过多模型实验发现现有模型更依赖模式匹配而非稳健逻辑推理。", "quality": "High", "conference": "IEEE Symposium on Security and Privacy (SP) 2025", "pdf_url": "https://arxiv.org/pdf/2505.20630v1", "published": "2025-05-27", "update_time": "2025-05-27", "download_time": "2025-12-11 17:23:17"}
{"id": "2509.25242", "title": "A Benchmark for Localizing Code and Non-Code Issues in Software Projects", "abstract": "Accurate project localization (e.g., files and functions) for issue resolution is a critical first step in software maintenance. However, existing benchmarks for issue localization, such as SWE-Bench and LocBench, are limited. They focus predominantly on pull-request issues and code locations, ignoring other evidence and non-code files such as commits, comments, configurations, and documentation. To address this gap, we introduce MULocBench, a comprehensive dataset of 1,100 issues from 46 popular GitHub Python projects. Comparing with existing benchmarks, MULocBench offers greater diversity in issue types, root causes, location scopes, and file types, providing a more realistic testbed for evaluation. Using this benchmark, we assess the performance of state-of-the-art localization methods and five LLM-based prompting strategies. Our results reveal significant limitations in current techniques: even at the file level, performance metrics (Acc@5, F1) remain below 40%. This underscores the challenge of generalizing to realistic, multi-faceted issue resolution. To enable future research on project localization for issue resolution, we publicly release MULocBench at https://huggingface.co/datasets/somethingone/MULocBench.", "arxiv_url": "https://arxiv.org/abs/2509.25242", "authors": ["Zejun Zhang", "Jian Wang", "Qingyun Yang", "Yifan Pan", "Yi Tang", "Yi Li", "Zhenchang Xing", "Tian Zhang", "Xuandong Li", "Guoan Zhang"], "first_author": "Zejun Zhang", "category": ["Benchmark", "Empirical"], "field": "Quality Management", "task": "Bug Localization", "tags": ["Multi-granularity localization (file/class/function)", "Cross-artifact localization (code, config, docs, tests, assets)", "Resolution-evidence extraction from PRs/commits/comments", "Manual annotation & patch parsing", "LLM prompting strategies for localization", "Issue type and root-cause taxonomy", "Comparison of retrieval-/procedure-/agent-based localizers", "Real-world GitHub Python corpus"], "summary": "本文提出并公开了 MULocBench——一个涵盖46个流行 Python 项目、1100 个问题的定位基准，包含代码与非代码文件及详细位置标注，并通过实证比较现有定位方法与五种基于提示的 LLM 策略，揭示当前方法在真实场景下（文件/类/函数级）定位效果仍然较差（文件级 Acc@5 < 40%）。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2509.25242v1", "published": "2025-09-26", "update_time": "2025-09-26", "download_time": "2025-12-11 17:42:48"}
{"id": "2305.05959", "title": "Survey of Code Search Based on Deep Learning", "abstract": "Code writing is repetitive and predictable, inspiring us to develop various code intelligence techniques. This survey focuses on code search, that is, to retrieve code that matches a given query by effectively capturing the semantic similarity between the query and code. Deep learning, being able to extract complex semantics information, has achieved great success in this field. Recently, various deep learning methods, such as graph neural networks and pretraining models, have been applied to code search with significant progress. Deep learning is now the leading paradigm for code search. In this survey, we provide a comprehensive overview of deep learning-based code search. We review the existing deep learning-based code search framework which maps query/code to vectors and measures their similarity. Furthermore, we propose a new taxonomy to illustrate the state-of-the-art deep learning-based code search in a three-steps process: query semantics modeling, code semantics modeling, and matching modeling which involves the deep learning model training. Finally, we suggest potential avenues for future research in this promising field.", "arxiv_url": "https://arxiv.org/abs/2305.05959", "authors": ["Yutao Xie", "Jiayi Lin", "Hande Dong", "Lei Zhang", "Zhonghai Wu"], "first_author": "Yutao Xie", "category": ["Survey"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["NL-to-code retrieval", "Query intent modeling", "Code semantic representation", "Graph-based code representations", "Transformer-based code encoders", "Contrastive and ranking training objectives", "Evaluation metrics and benchmarking", "Training / pretraining strategies for retrieval"], "summary": "本文综述了基于深度学习的代码检索研究，提出了查询语义建模、代码语义建模与匹配建模的三步分类法，并总结了现有方法、评估指标与未来研究方向。", "quality": "High", "conference": "ACM Transactions on Software Engineering and Methodology 2023", "pdf_url": "https://arxiv.org/pdf/2305.05959v2", "published": "2023-05-10", "update_time": "2023-12-13", "download_time": "2025-12-11 17:43:30"}
{"id": "1803.09371", "title": "StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow", "abstract": "Stack Overflow (SO) has been a great source of natural language questions and their code solutions (i.e., question-code pairs), which are critical for many tasks including code retrieval and annotation. In most existing research, question-code pairs were collected heuristically and tend to have low quality. In this paper, we investigate a new problem of systematically mining question-code pairs from Stack Overflow (in contrast to heuristically collecting them). It is formulated as predicting whether or not a code snippet is a standalone solution to a question. We propose a novel Bi-View Hierarchical Neural Network which can capture both the programming content and the textual context of a code snippet (i.e., two views) to make a prediction. On two manually annotated datasets in Python and SQL domain, our framework substantially outperforms heuristic methods with at least 15% higher F1 and accuracy. Furthermore, we present StaQC (Stack Overflow Question-Code pairs), the largest dataset to date of ~148K Python and ~120K SQL question-code pairs, automatically mined from SO using our framework. Under various case studies, we demonstrate that StaQC can greatly help develop data-hungry models for associating natural language with programming language.", "arxiv_url": "https://arxiv.org/abs/1803.09371", "authors": ["Ziyu Yao", "Daniel S. Weld", "Wei-Peng Chen", "Huan Sun"], "first_author": "Ziyu Yao", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Alignment", "tags": ["question-code pairing", "standalone-solution identification", "text-code joint representation", "block-level hierarchical encoding", "Stack Overflow mining", "how-to-do-it question filtering", "code snippet role classification", "Python and SQL domains"], "summary": "本文提出一种同时利用文本上下文与代码内容的双视角分层神经网络来识别 Stack Overflow 回答中作为独立解法的代码片段，并基于此系统化地挖掘出大规模高质量的问答-代码配对数据集（Python 与 SQL）。", "quality": "High", "conference": "The Web Conference (WWW 2018)", "pdf_url": "https://arxiv.org/pdf/1803.09371v1", "published": "2018-03-26", "update_time": "2018-03-26", "download_time": "2025-12-11 17:44:02"}
{"id": "1805.08949", "title": "Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow", "abstract": "For tasks like code synthesis from natural language, code retrieval, and code summarization, data-driven models have shown great promise. However, creating these models require parallel data between natural language (NL) and code with fine-grained alignments. Stack Overflow (SO) is a promising source to create such a data set: the questions are diverse and most of them have corresponding answers with high-quality code snippets. However, existing heuristic methods (e.g., pairing the title of a post with the code in the accepted answer) are limited both in their coverage and the correctness of the NL-code pairs obtained. In this paper, we propose a novel method to mine high-quality aligned data from SO using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a probabilistic model to capture the correlation between NL and code using neural networks. These features are fed into a classifier that determines the quality of mined NL-code pairs. Experiments using Python and Java as test beds show that the proposed method greatly expands coverage and accuracy over existing mining methods, even when using only a small number of labeled examples. Further, we find that reasonable results are achieved even when training the classifier on one language and testing on another, showing promise for scaling NL-code mining to a wide variety of programming languages beyond those for which we are able to annotate data.", "arxiv_url": "https://arxiv.org/abs/1805.08949", "authors": ["Pengcheng Yin", "Bowen Deng", "Edgar Chen", "Bogdan Vasilescu", "Graham Neubig"], "first_author": "Pengcheng Yin", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Alignment", "tags": ["Neural correspondence features", "Bidirectional conditional probabilities", "Structural snippet features", "Intent–snippet–context segmentation", "Line-contiguous fragment enumeration", "Stack Overflow mining", "Cross-language transfer", "Annotation protocol and labeling tool"], "summary": "本文提出一种将手工结构特征与基于神经翻译模型的双向条件概率相结合的分类方法，用以从 Stack Overflow 自动挖掘高质量对齐的自然语言–代码片段对，并发布了标注数据与工具。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/1805.08949v1", "published": "2018-05-23", "update_time": "2018-05-23", "download_time": "2025-12-11 17:44:34"}
{"id": "1908.09804", "title": "Neural Code Search Evaluation Dataset", "abstract": "There has been an increase of interest in code search using natural language. Assessing the performance of such code search models can be difficult without a readily available evaluation suite. In this paper, we present an evaluation dataset consisting of natural language query and code snippet pairs, with the hope that future work in this area can use this dataset as a common benchmark. We also provide the results of two code search models ([1] and [6]) from recent work.   The evaluation dataset is available at https://github.com/facebookresearch/Neural-Code-Search-Evaluation-Dataset", "arxiv_url": "https://arxiv.org/abs/1908.09804", "authors": ["Hongyu Li", "Seohyun Kim", "Satish Chandra"], "first_author": "Hongyu Li", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Stack Overflow Q&A curation", "Method-level code extraction", "Android-focused search corpus", "GitHub commit-aware indexing", "Automatic code-to-code relevance labeling", "Evaluation metrics (MRR, Answered@k)"], "summary": "该论文发布了一个面向自然语言到代码检索的评估数据集，包含约4.7M个方法级搜索语料、287个经人工筛选的Stack Overflow问答对以及用于自动判定相关性的相似度标注和基准模型评测结果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/1908.09804v6", "published": "2019-08-26", "update_time": "2019-10-02", "download_time": "2025-12-11 17:45:08"}
{"id": "2008.12193", "title": "Neural Code Search Revisited: Enhancing Code Snippet Retrieval through Natural Language Intent", "abstract": "In this work, we propose and study annotated code search: the retrieval of code snippets paired with brief descriptions of their intent using natural language queries. On three benchmark datasets, we investigate how code retrieval systems can be improved by leveraging descriptions to better capture the intents of code snippets. Building on recent progress in transfer learning and natural language processing, we create a domain-specific retrieval model for code annotated with a natural language description. We find that our model yields significantly more relevant search results (with absolute gains up to 20.6% in mean reciprocal rank) compared to state-of-the-art code retrieval methods that do not use descriptions but attempt to compute the intent of snippets solely from unannotated code.", "arxiv_url": "https://arxiv.org/abs/2008.12193", "authors": ["Geert Heyman", "Tom Van Cutsem"], "first_author": "Geert Heyman", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Annotated code snippets", "Natural language intent descriptions", "Description-aware ranking", "Semantic-similarity fine-tuning", "Transfer learning for retrieval", "Benchmark dataset creation", "Snippet-description pairing heuristics"], "summary": "本文提出并研究“注释代码搜索”任务（带有自然语言意图描述的代码片段检索），构建三个基准数据集并通过对预训练模型的细化微调利用描述显著提升检索性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2008.12193v1", "published": "2020-08-27", "update_time": "2020-08-27", "download_time": "2025-12-11 17:45:44"}
{"id": "2102.04664", "title": "CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation", "abstract": "Benchmark datasets have a significant impact on accelerating research in programming language tasks. In this paper, we introduce CodeXGLUE, a benchmark dataset to foster machine learning research for program understanding and generation. CodeXGLUE includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison. CodeXGLUE also features three baseline systems, including the BERT-style, GPT-style, and Encoder-Decoder models, to make it easy for researchers to use the platform. The availability of such data and baselines can help the development and validation of new methods that can be applied to various program understanding and generation problems.", "arxiv_url": "https://arxiv.org/abs/2102.04664", "authors": ["Shuai Lu", "Daya Guo", "Shuo Ren", "Junjie Huang", "Alexey Svyatkovskiy", "Ambrosio Blanco", "Colin Clement", "Dawn Drain", "Daxin Jiang", "Duyu Tang", "Ge Li", "Lidong Zhou", "Linjun Shou", "Long Zhou", "Michele Tufano", "Ming Gong", "Ming Zhou", "Nan Duan", "Neel Sundaresan", "Shao Kun Deng", "Shengyu Fu", "Shujie Liu"], "first_author": "Shuai Lu", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["Multi-task code benchmark", "Program understanding and generation", "Cloze-style semantic probing", "Line-level code completion", "Code-to-code translation (Java↔C#)", "Natural-language code search (web queries, normalized identifiers)", "Documentation translation", "Clone detection", "Defect / vulnerability detection", "Baseline model suite and evaluation platform"], "summary": "本文提出并发布了 CodeXGLUE，一个包含14个数据集、覆盖10类代码理解与生成任务并提供评测平台与基线实现的多任务代码基准套件。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2102.04664v2", "published": "2021-02-09", "update_time": "2021-03-16", "download_time": "2025-12-11 17:46:31"}
{"id": "2105.13239", "title": "CoSQA: 20,000+ Web Queries for Code Search and Question Answering", "abstract": "Finding codes given natural language query isb eneficial to the productivity of software developers. Future progress towards better semantic matching between query and code requires richer supervised training resources. To remedy this, we introduce the CoSQA dataset.It includes 20,604 labels for pairs of natural language queries and codes, each annotated by at least 3 human annotators. We further introduce a contrastive learning method dubbed CoCLR to enhance query-code matching, which works as a data augmenter to bring more artificially generated training instances. We show that evaluated on CodeXGLUE with the same CodeBERT model, training on CoSQA improves the accuracy of code question answering by 5.1%, and incorporating CoCLR brings a further improvement of 10.5%.", "arxiv_url": "https://arxiv.org/abs/2105.13239", "authors": ["Junjie Huang", "Duyu Tang", "Linjun Shou", "Ming Gong", "Ke Xu", "Daxin Jiang", "Ming Zhou", "Nan Duan"], "first_author": "Junjie Huang", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Alignment", "tags": ["Web-search-query curation from search logs", "Query intent filtering via heuristic keywords", "Crowdsourced binary relevance annotations", "Function-with-documentation as code answer unit", "Contrastive learning for query-code matching", "Data augmentation via synthetic query-code pairs", "Query-code semantic retrieval evaluation", "Large-scale human-labeled code search benchmark"], "summary": "该论文发布了一个包含20,604对经过多人标注的真实网页查询与Python函数的查询-代码匹配数据集，并提出一种基于对比学习的数据增强方法以显著提升查询到代码的检索与问答性能。", "quality": "High", "conference": "ACL 2021", "pdf_url": "https://arxiv.org/pdf/2105.13239v1", "published": "2021-05-27", "update_time": "2021-05-27", "download_time": "2025-12-11 17:47:07"}
{"id": "2403.16702", "title": "ProCQA: A Large-scale Community-based Programming Question Answering Dataset for Code Search", "abstract": "Retrieval-based code question answering seeks to match user queries in natural language to relevant code snippets. Previous approaches typically rely on pretraining models using crafted bi-modal and uni-modal datasets to align text and code representations. In this paper, we introduce ProCQA, a large-scale programming question answering dataset extracted from the StackOverflow community, offering naturally structured mixed-modal QA pairs. To validate its effectiveness, we propose a modality-agnostic contrastive pre-training approach to improve the alignment of text and code representations of current code language models. Compared to previous models that primarily employ bimodal and unimodal pairs extracted from CodeSearchNet for pre-training, our model exhibits significant performance improvements across a wide range of code retrieval benchmarks.", "arxiv_url": "https://arxiv.org/abs/2403.16702", "authors": ["Zehan Li", "Jianfei Zhang", "Chuantao Yin", "Yuanxin Ouyang", "Wenge Rong"], "first_author": "Zehan Li", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Alignment", "tags": ["Mixed-modal (interleaved code and text) QA pairs", "Community Q&A sourcing (StackOverflow)", "Modality-agnostic contrastive pretraining", "Dual-encoder representation learning", "Retrieval-based code search", "Rule-based filtering and decontamination", "Large-scale multi-language corpus"], "summary": "本文提出ProCQA——一个从StackOverflow挖掘的约500万条跨11种编程语言的混合模态（代码与文本交织）问答数据集，并基于此提出模态不可知的对比预训练方法以提升代码与文本表示对齐与检索性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2403.16702v1", "published": "2024-03-25", "update_time": "2024-03-25", "download_time": "2025-12-11 17:47:46"}
{"id": "2406.11589", "title": "CoSQA+: Pioneering the Multi-Choice Code Search Benchmark with Test-Driven Agents", "abstract": "Semantic code search, retrieving code that matches a given natural language query, is an important task to improve productivity in software engineering. Existing code search datasets face limitations: they rely on human annotators who assess code primarily through semantic understanding rather than functional verification, leading to potential inaccuracies and scalability issues. Additionally, current evaluation metrics often overlook the multi-choice nature of code search. This paper introduces CoSQA+, pairing high-quality queries from CoSQA with multiple suitable codes. We develop an automated pipeline featuring multiple model-based candidate selections and the novel test-driven agent annotation system. Among a single Large Language Model (LLM) annotator and Python expert annotators (without test-based verification), agents leverage test-based verification and achieve the highest accuracy of 93.9%. Through extensive experiments, CoSQA+ has demonstrated superior quality over CoSQA. Models trained on CoSQA+ exhibit improved performance. We publicly release both CoSQA+_all, which contains 412,080 agent-annotated pairs, and CoSQA+_verified, which contains 1,000 human-verified pairs, at https://github.com/DeepSoftwareAnalytics/CoSQA_Plus.", "arxiv_url": "https://arxiv.org/abs/2406.11589", "authors": ["Jing Gong", "Yanghui Wu", "Linxi Liang", "Yanlin Wang", "Jiachi Chen", "Mingwei Liu", "Zibin Zheng"], "first_author": "Jing Gong", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Multi-choice code search", "Test-driven agent annotation", "Automatic test generation and execution", "Multi-model candidate selection", "Executable verification", "Human-verified gold subset", "MAP@10 evaluation", "Cross-language generalization"], "summary": "本文提出CoSQA+，通过多模型候选筛选与测试驱动的多智能体自动标注流水线构建面向多选场景的代码检索基准，并公开了大规模自动标注集与1000条人工验证子集以提升检索评估与模型性能。", "quality": "High", "conference": "IEEE Transactions on Software Engineering (TSE) 2025", "pdf_url": "https://arxiv.org/pdf/2406.11589v6", "published": "2024-06-17", "update_time": "2025-11-10", "download_time": "2025-12-11 17:48:25"}
{"id": "2407.02883", "title": "CoIR: A Comprehensive Benchmark for Code Information Retrieval Models", "abstract": "Despite the substantial success of Information Retrieval (IR) in various NLP tasks, most IR systems predominantly handle queries and corpora in natural language, neglecting the domain of code retrieval. Code retrieval is critically important yet remains under-explored, with existing methods and benchmarks inadequately representing the diversity of code in various domains and tasks. Addressing this gap, we present COIR (Code Information Retrieval Benchmark), a robust and comprehensive benchmark specifically designed to assess code retrieval capabilities. COIR comprises ten meticulously curated code datasets, spanning eight distinctive retrieval tasks across seven diverse domains. We first discuss the construction of COIR and its diverse dataset composition. Further, we evaluate nine widely used retrieval models using COIR, uncovering significant difficulties in performing code retrieval tasks even with state-of-the-art systems. To facilitate easy adoption and integration within existing research workflows, COIR has been developed as a user-friendly Python framework, readily installable via pip. It shares same data schema as other popular benchmarks like MTEB and BEIR, enabling seamless cross-benchmark evaluations. Through COIR, we aim to invigorate research in the code retrieval domain, providing a versatile benchmarking tool that encourages further development and exploration of code retrieval systems. https://github.com/CoIR-team/coir.", "arxiv_url": "https://arxiv.org/abs/2407.02883", "authors": ["Xiangyang Li", "Kuicai Dong", "Yi Quan Lee", "Wei Xia", "Hao Zhang", "Xinyi Dai", "Yasheng Wang", "Ruiming Tang"], "first_author": "Xiangyang Li", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Cross-domain code retrieval", "Text-to-code and code-to-text retrieval", "Code-to-code similarity and context retrieval", "Hybrid text-and-code queries", "Manual dataset curation and filtering", "Standardized evaluation pipeline (BEIR/MTEB-compatible)", "Overfitting analysis to existing leaderboards", "Zero-shot retrieval evaluation with nDCG/MAP"], "summary": "本文提出COIR，一个包含10个经人工审校的数据集、覆盖多语言与多种检索任务并兼容BEIR/MTEB的统一评测框架，用于全面评估代码信息检索模型的泛化能力。", "quality": "High", "conference": "ACL 2025", "pdf_url": "https://arxiv.org/pdf/2407.02883v3", "published": "2024-07-03", "update_time": "2025-06-06", "download_time": "2025-12-11 17:49:00"}
{"id": "2408.11081", "title": "What can Large Language Models Capture about Code Functional Equivalence?", "abstract": "Code-LLMs, LLMs pre-trained on large code corpora, have shown great progress in learning rich representations of the structure and syntax of code, successfully using it to generate or classify code fragments. At the same time, understanding if they are able to do so because they capture code semantics, and how well, is still an open question. In this paper, we tackle this problem by introducing SeqCoBench, a benchmark for systematically assessing how Code-LLMs can capture code functional equivalence. SeqCoBench contains over 20 code transformations that either preserve or alter the semantics of Python programs. We conduct extensive evaluations in different settings, including zero-shot and parameter-efficient finetuning methods on state-of-the-art (Code)-LLMs to see if they can discern semantically equivalent or different pairs of programs in SeqCoBench. We find that the performance gap between these LLMs and classical match-based retrieval scores is minimal, with both approaches showing a concerning lack of depth in understanding code semantics.", "arxiv_url": "https://arxiv.org/abs/2408.11081", "authors": ["Nickil Maveli", "Antonio Vergari", "Shay B. Cohen"], "first_author": "Nickil Maveli", "category": ["Benchmark", "Empirical"], "field": "Maintenance", "task": "Clone Detection", "tags": ["Semantic code clones", "Functional equivalence classification", "Semantic-preserving transformations", "Semantic-altering transformations", "Pairwise program comparison", "Parameter-efficient fine-tuning evaluation", "Comparison to match-based similarity metrics", "Python program transformation suite"], "summary": "本文构建了SeqCoBench基准，包含20余种对Python程序进行语义保持或改变的变换，用以系统评估代码LLM在判别函数功能等价性方面的能力，并发现这些模型在该任务上与传统匹配度量表现相近且整体理解深度不足。", "quality": "High", "conference": "NAACL 2025", "pdf_url": "https://arxiv.org/pdf/2408.11081v2", "published": "2024-08-20", "update_time": "2025-02-12", "download_time": "2025-12-11 17:49:40"}
{"id": "2506.11066", "title": "CoQuIR: A Comprehensive Benchmark for Code Quality-Aware Information Retrieval", "abstract": "Code retrieval is essential in modern software development, as it boosts code reuse and accelerates debugging. However, current benchmarks primarily emphasize functional relevance while neglecting critical dimensions of software quality. Motivated by this gap, we introduce CoQuIR, the first large-scale, multilingual benchmark specifically designed to evaluate quality-aware code retrieval across four key dimensions: correctness, efficiency, security, and maintainability. CoQuIR provides fine-grained quality annotations for 42,725 queries and 134,907 code snippets in 11 programming languages, and is accompanied by two quality-centric evaluation metrics: Pairwise Preference Accuracy and Margin-based Ranking Score. Using CoQuIR, we benchmark 23 retrieval models, covering both open-source and proprietary systems, and find that even top-performing models frequently fail to distinguish buggy or insecure code from their more robust counterparts. Furthermore, we conduct preliminary investigations into training methods that explicitly encourage retrievers to recognize code quality. Using synthetic datasets, we demonstrate promising improvements in quality-aware metrics across various models, without sacrificing semantic relevance. Downstream code generation experiments further validate the effectiveness of our approach. Overall, our work highlights the importance of integrating quality signals into code retrieval systems, laying the groundwork for more trustworthy and robust software development tools.", "arxiv_url": "https://arxiv.org/abs/2506.11066", "authors": ["Jiahui Geng", "Fengyu Cai", "Shaobo Cui", "Qing Li", "Liangwei Chen", "Chenyang Lyu", "Haonan Li", "Derui Zhu", "Walter Pretschner", "Heinz Koeppl", "Fakhri Karray"], "first_author": "Jiahui Geng", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Quality-aware retrieval", "Contrastive training for code quality", "Quality annotations: correctness, efficiency, security, maintainability", "Pairwise Preference Accuracy", "Margin-based Ranking Score", "Multilingual code corpus", "Contrastive code pairs and hard negatives", "Downstream retrieval-augmented generation evaluation"], "summary": "本文提出CoQuIR，一个面向正确性、效率、安全性和可维护性四个代码质量维度的大规模多语言代码检索基准，包含质量标注、质量感知评估指标，并通过对比训练提高检索器的质量识别能力，从而改善检索和下游生成的安全与可靠性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2506.11066v2", "published": "2025-05-31", "update_time": "2025-08-27", "download_time": "2025-12-11 17:50:17"}
{"id": "1912.03768", "title": "TypeWriter: Neural Type Prediction with Search-based Validation", "abstract": "Maintaining large code bases written in dynamically typed languages, such as JavaScript or Python, can be challenging due to the absence of type annotations: simple data compatibility errors proliferate, IDE support is limited, and APIs are hard to comprehend. Recent work attempts to address those issues through either static type inference or probabilistic type prediction. Unfortunately, static type inference for dynamic languages is inherently limited, while probabilistic approaches suffer from imprecision. This paper presents TypeWriter, the first combination of probabilistic type prediction with search-based refinement of predicted types. TypeWriter's predictor learns to infer the return and argument types for functions from partially annotated code bases by combining the natural language properties of code with programming language-level information. To validate predicted types, TypeWriter invokes a gradual type checker with different combinations of the predicted types, while navigating the space of possible type combinations in a feedback-directed manner. We implement the TypeWriter approach for Python and evaluate it on two code corpora: a multi-million line code base at Facebook and a collection of 1,137 popular open-source projects. We show that TypeWriter's type predictor achieves an F1 score of 0.64 (0.79) in the top-1 (top-5) predictions for return types, and 0.57 (0.80) for argument types, which clearly outperforms prior type prediction models. By combining predictions with search-based validation, TypeWriter can fully annotate between 14% to 44% of the files in a randomly selected corpus, while ensuring type correctness. A comparison with a static type inference tool shows that TypeWriter adds many more non-trivial types. TypeWriter currently suggests types to developers at Facebook and several thousands of types have already been accepted with minimal changes.", "arxiv_url": "https://arxiv.org/abs/1912.03768", "authors": ["Michael Pradel", "Georgios Gousios", "Jason Liu", "Satish Chandra"], "first_author": "Michael Pradel", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Neural type prediction", "Feedback-directed search", "Gradual type checker validation", "Function-level argument and return typing", "Identifier and docstring context", "Usage-token sequence embeddings", "Combinatorial search for consistent annotations", "Automated type annotation for Python"], "summary": "本文提出 TypeWriter：通过结合基于神经网络的函数参数/返回类型预测与基于反馈的搜索验证（使用渐进式类型检查器）自动为 Python 代码添加类型注解，并在大规模代码库上验证了其实用性与效果。", "quality": "High", "conference": "ICSE 2019", "pdf_url": "https://arxiv.org/pdf/1912.03768v2", "published": "2019-12-08", "update_time": "2020-03-06", "download_time": "2025-12-11 17:51:07"}
{"id": "2104.04706", "title": "ManyTypes4Py: A Benchmark Python Dataset for Machine Learning-based Type Inference", "abstract": "In this paper, we present ManyTypes4Py, a large Python dataset for machine learning (ML)-based type inference. The dataset contains a total of 5,382 Python projects with more than 869K type annotations. Duplicate source code files were removed to eliminate the negative effect of the duplication bias. To facilitate training and evaluation of ML models, the dataset was split into training, validation and test sets by files. To extract type information from abstract syntax trees (ASTs), a lightweight static analyzer pipeline is developed and accompanied with the dataset. Using this pipeline, the collected Python projects were analyzed and the results of the AST analysis were stored in JSON-formatted files. The ManyTypes4Py dataset is shared on zenodo and its tools are publicly available on GitHub.", "arxiv_url": "https://arxiv.org/abs/2104.04706", "authors": ["Amir M. Mir", "Evaldas Latoskinas", "Georgios Gousios"], "first_author": "Amir M. Mir", "category": ["Benchmark"], "field": "Type Inference", "task": "ML-based Type Inference", "tags": ["AST-based feature extraction", "Lightweight static analysis pipeline", "File-level deduplication (CD4Py)", "Seq2seq code representation with token-type alignment", "Natural and contextual type hints (ret_exprs, params_occur)", "Train/validation/test split by files", "Long-tail type distribution analysis", "mypy-based project selection"], "summary": "本文提出ManyTypes4Py——一个包含5,382个Python项目、约869,825条类型注解并附带轻量静态分析工具（LibSA4Py）与去重处理的用于机器学习类型推断的大型基准数据集。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2104.04706v1", "published": "2021-04-10", "update_time": "2021-04-10", "download_time": "2025-12-11 17:52:01"}
{"id": "2302.12163", "title": "Do Machine Learning Models Produce TypeScript Types That Type Check?", "abstract": "Type migration is the process of adding types to untyped code to gain assurance at compile time. TypeScript and other gradual type systems facilitate type migration by allowing programmers to start with imprecise types and gradually strengthen them. However, adding types is a manual effort and several migrations on large, industry codebases have been reported to have taken several years. In the research community, there has been significant interest in using machine learning to automate TypeScript type migration. Existing machine learning models report a high degree of accuracy in predicting individual TypeScript type annotations. However, in this paper we argue that accuracy can be misleading, and we should address a different question: can an automatic type migration tool produce code that passes the TypeScript type checker?   We present TypeWeaver, a TypeScript type migration tool that can be used with an arbitrary type prediction model. We evaluate TypeWeaver with three models from the literature: DeepTyper, a recurrent neural network; LambdaNet, a graph neural network; and InCoder, a general-purpose, multi-language transformer that supports fill-in-the-middle tasks. Our tool automates several steps that are necessary for using a type prediction model, (1) importing types for a project's dependencies; (2) migrating JavaScript modules to TypeScript notation; (3) inserting predicted type annotations into the program to produce TypeScript when needed; and (4) rejecting non-type predictions when needed.   We evaluate TypeWeaver on a dataset of 513 JavaScript packages, including packages that have never been typed before. With the best type prediction model, we find that only 21% of packages type check, but more encouragingly, 69% of files type check successfully.", "arxiv_url": "https://arxiv.org/abs/2302.12163", "authors": ["Ming-Ho Yee", "Arjun Guha"], "first_author": "Ming-Ho Yee", "category": ["Technical", "Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Type migration automation", "Type annotation weaving", "Type checking validation", "CommonJS to ECMAScript module conversion", "Dependency .d.ts importing", "Filtering non-type model outputs", "Empirical evaluation on npm packages", "Analysis of trivial vs. meaningful annotations"], "summary": "本文提出 TypeWeaver 工具并在 513 个 npm 包上评估多种机器学习类型预测模型，展示了在实际迁移中需要处理依赖类型导入、模块系统转换和非类型预测过滤等工程步骤，且尽管单注解准确率高，只有 21% 的包（69% 的文件）最终通过 TypeScript 类型检查。", "quality": "High", "conference": "European Conference on Object-Oriented Programming (ECOOP) 2023", "pdf_url": "https://arxiv.org/pdf/2302.12163v2", "published": "2023-02-23", "update_time": "2023-07-11", "download_time": "2025-12-11 17:52:33"}
{"id": "2303.09564", "title": "TypeT5: Seq2seq Type Inference using Static Analysis", "abstract": "There has been growing interest in automatically predicting missing type annotations in programs written in Python and JavaScript. While prior methods have achieved impressive accuracy when predicting the most common types, they often perform poorly on rare or complex types. In this paper, we present a new type inference method that treats type prediction as a code infilling task by leveraging CodeT5, a state-of-the-art seq2seq pre-trained language model for code. Our method uses static analysis to construct dynamic contexts for each code element whose type signature is to be predicted by the model. We also propose an iterative decoding scheme that incorporates previous type predictions in the model's input context, allowing information exchange between related code elements. Our evaluation shows that the proposed approach, TypeT5, not only achieves a higher overall accuracy (particularly on rare and complex types) but also produces more coherent results with fewer type errors -- while enabling easy user intervention.", "arxiv_url": "https://arxiv.org/abs/2303.09564", "authors": ["Jiayi Wei", "Greg Durrett", "Isil Dillig"], "first_author": "Jiayi Wei", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Type Inference for Python", "Static usage graph", "Context augmentation via static analysis", "Seq2seq code infilling", "Iterative two-pass decoding", "Coherence via type-checking constraints", "Interactive human-in-the-loop correction", "Handling parametric and user-defined types"], "summary": "本文提出TypeT5，将类型推断视为seq2seq代码填空任务，结合静态分析构造跨文件使用关系上下文并采用迭代解码以提高对罕见与复杂类型的一致性和准确率。", "quality": "High", "conference": "ICLR 2023", "pdf_url": "https://arxiv.org/pdf/2303.09564v1", "published": "2023-03-16", "update_time": "2023-03-16", "download_time": "2025-12-11 17:53:05"}
{"id": "2305.17145", "title": "Type Prediction With Program Decomposition and Fill-in-the-Type Training", "abstract": "TypeScript and Python are two programming languages that support optional type annotations, which are useful but tedious to introduce and maintain. This has motivated automated type prediction: given an untyped program, produce a well-typed output program. Large language models (LLMs) are promising for type prediction, but there are challenges: fill-in-the-middle performs poorly, programs may not fit into the context window, generated types may not type check, and it is difficult to measure how well-typed the output program is. We address these challenges by building OpenTau, a search-based approach for type prediction that leverages large language models. We propose a new metric for type prediction quality, give a tree-based program decomposition that searches a space of generated types, and present fill-in-the-type fine-tuning for LLMs. We evaluate our work with a new dataset for TypeScript type prediction, and show that 47.4% of files type check (14.5% absolute improvement) with an overall rate of 3.3 type errors per file. All code, data, and models are available at: https://github.com/GammaTauAI/opentau.", "arxiv_url": "https://arxiv.org/abs/2305.17145", "authors": ["Federico Cassano", "Ming-Ho Yee", "Noah Shinn", "Arjun Guha", "Steven Holtzen"], "first_author": "Federico Cassano", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Editing", "tags": ["Tree-based program decomposition", "Fill-in-the-type fine-tuning", "Typedness evaluation metric", "Search over type candidates", "Type-checking guided ranking", "Local type inference", "TypeScript gradual migration"], "summary": "本文提出OPENTAU，通过将程序递归分解为树状代码块、基于搜索的候选类型生成与一种名为fill-in-the-type的微调方法，并辅以新的typedness评估和TypeScript数据集，有效提升了自动类型注入的可检类型率与类型精确度。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2305.17145v1", "published": "2023-05-25", "update_time": "2023-05-25", "download_time": "2025-12-11 17:53:38"}
{"id": "2408.10718", "title": "CodeJudge-Eval: Can Large Language Models be Good Judges in Code Understanding?", "abstract": "Recent advancements in large language models (LLMs) have showcased impressive code generation capabilities, primarily evaluated through language-to-code benchmarks. However, these benchmarks may not fully capture a model's code understanding abilities. We introduce CodeJudge-Eval (CJ-Eval), a novel benchmark designed to assess LLMs' code understanding abilities from the perspective of code judging rather than code generation. CJ-Eval challenges models to determine the correctness of provided code solutions, encompassing various error types and compilation issues. By leveraging a diverse set of problems and a fine-grained judging system, CJ-Eval addresses the limitations of traditional benchmarks, including the potential memorization of solutions. Evaluation of 12 well-known LLMs on CJ-Eval reveals that even state-of-the-art models struggle, highlighting the benchmark's ability to probe deeper into models' code understanding abilities. Our codes and benchmark are available at \\url{https://github.com/CodeLLM-Research/CodeJudge-Eval}.", "arxiv_url": "https://arxiv.org/abs/2408.10718", "authors": ["Yuwei Zhao", "Ziyang Luo", "Yuchen Tian", "Hongzhan Lin", "Weixiang Yan", "Annan Li", "Jing Ma"], "first_author": "Yuwei Zhao", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["LLM-as-a-judge", "Code understanding evaluation", "Fine-grained verdict taxonomy", "Execution-grounded annotations", "Programming-contest problems", "Cross-model candidate solutions", "Memorization mitigation", "Multiple-choice judging"], "summary": "本文提出了CodeJudge-Eval基准，通过让模型对来自不同模型的候选代码进行细粒度（如AC/WA/TLE等）判定以评估代码理解能力，并在竞赛级题目上对多种模型的判题性能进行了实证分析，结果表明当前模型在判题任务上仍存在显著不足。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2408.10718v2", "published": "2024-08-20", "update_time": "2024-09-13", "download_time": "2025-12-11 17:54:13"}
{"id": "2512.09679", "title": "Understanding Chain-of-Thought Effectiveness in Code Generation: An Empirical and Information-Theoretic Analysis", "abstract": "Large language models (LLMs) achieve strong performance on code generation, but the mechanisms by which Chain-of-Thought (CoT) prompting helps remain unclear. We present a systematic empirical and information-theoretic study of CoT effectiveness in neural code generation, evaluating five paradigms (Zero-Shot, Zero-Shot CoT, Self-Planning, Structured CoT, Reasoning-CoT) across six Python benchmarks, a multilingual benchmark with 12 programming languages, and six models from 7B to 480B parameters, using conditional mutual information $I(Y;C|X)$ as a conceptual lens. Our results show that externally guided CoT consistently outperforms direct generation, with structured methods improving Pass@1 by 5--12\\% on average while using substantially fewer tokens than reflective reasoning, and that CoT benefits depend on language type systems and model capacity. We further find that reasoning \\emph{quality} is critical: high-quality structured CoT from strong generators yields significantly higher accuracy than lightweight alternatives with the same template, whereas naive Zero-Shot CoT can even degrade performance. These findings provide practical guidance for choosing CoT strategies based on model capacity, language characteristics, and task complexity.", "arxiv_url": "https://arxiv.org/abs/2512.09679", "authors": ["Naizhu Jin", "Zhong Li", "Guang Yang", "Tian Zhang", "Qingkai Zeng"], "first_author": "Naizhu Jin", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Chain-of-Thought prompting", "Structured CoT (SCoT)", "Self-planning prompts", "Reflective reasoning", "Conditional mutual information", "Information density metric", "Cross-language generalization", "Model scale / capacity effects", "Token-efficiency vs accuracy", "Reasoning quality evaluation"], "summary": "本文提出基于条件互信息的信息论框架，并通过跨语言、跨模型的大规模实证实验系统地评估多种 Chain-of-Thought 提示范式在代码生成中对准确性与效率的影响，发现结构化 CoT 在令牌效率和稳定性上优于反思式方法且推理质量与模型容量显著影响效果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.09679v1", "published": "2025-12-10", "update_time": "2025-12-10", "download_time": "2025-12-12 01:51:32"}
{"id": "2512.09627", "title": "LogICL: Distilling LLM Reasoning to Bridge the Semantic Gap in Cross-Domain Log Anomaly Detection", "abstract": "Effective log anomaly detection is critical to sustaining reliability in large-scale IT infrastructures. Transformer-based models require substantial resources and labeled data, exacerbating the cold-start problem in target domains where logs are scarce. Existing cross-domain methods leverage source logs but struggle with generalization due to reliance on surface lexical similarity, failing to capture latent semantic equivalence amid structural divergences. To address this, we propose LogICL, a framework distilling Large Language Model (LLM) reasoning into a lightweight encoder for cross-domain anomaly detection. During training, LogICL constructs a delta matrix measuring the utility of demonstrations selected via Maximal Marginal Relevance relative to zero-shot inference. The encoder is optimized via a multi-objective loss comprising an ICL-Guided term that aligns representations based on reasoning assistance utility, maximum mean discrepancy for domain alignment, and supervised contrastive loss. At inference, the optimized encoder retrieves reasoning-aware demonstrations using semantic similarity and delta scores, enabling frozen-LLM in-context learning with Chain-of-Thought for accurate and interpretable detection. Experiments on few-shot and zero-shot cross-domain benchmarks confirm LogICL achieves state-of-the-art performance across heterogeneous systems. Further analysis via visualizations and case studies confirms LogICL bridges the semantic gap beyond surface lexical similarity, effectively capturing latent semantic equivalence for rapid deployment.", "arxiv_url": "https://arxiv.org/abs/2512.09627", "authors": ["Jingwei Ye", "Zhi Wang", "Chenbin Su", "Jieshuai Yang", "Jiayi Ding", "Chunbo Liu", "Ge Chu"], "first_author": "Jingwei Ye", "category": ["Technical"], "field": "AIOps", "task": "Log Anomaly Detection", "tags": ["In-Context Learning Distillation", "Delta Matrix for Demonstration Utility", "Maximal Marginal Relevance Demonstration Selection", "ICL-guided Contrastive Representation Learning", "Maximum Mean Discrepancy Domain Alignment", "Supervised Contrastive Loss for Anomaly Discrimination", "Reasoning-aware Demonstration Retrieval", "Frozen-LLM Chain-of-Thought Inference", "Lightweight Log Sequence Encoder", "Cross-domain Few-shot/Zero-shot Adaptation"], "summary": "本文提出LogICL，通过将大模型的推理能力蒸馏到轻量级日志编码器并结合基于ICL的示例选择、delta矩阵衡量、MMD域对齐和对比损失，实现对异构系统的跨域少样本/零样本日志异常检测与可解释推理。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.09627v1", "published": "2025-12-10", "update_time": "2025-12-10", "download_time": "2025-12-12 01:52:03"}
{"id": "2512.09543", "title": "SWEnergy: An Empirical Study on Energy Efficiency in Agentic Issue Resolution Frameworks with SLMs", "abstract": "Context. LLM-based autonomous agents in software engineering rely on large, proprietary models, limiting local deployment. This has spurred interest in Small Language Models (SLMs), but their practical effectiveness and efficiency within complex agentic frameworks for automated issue resolution remain poorly understood.   Goal. We investigate the performance, energy efficiency, and resource consumption of four leading agentic issue resolution frameworks when deliberately constrained to using SLMs. We aim to assess the viability of these systems for this task in resource-limited settings and characterize the resulting trade-offs.   Method. We conduct a controlled evaluation of four leading agentic frameworks (SWE-Agent, OpenHands, Mini SWE Agent, AutoCodeRover) using two SLMs (Gemma-3 4B, Qwen-3 1.7B) on the SWE-bench Verified Mini benchmark. On fixed hardware, we measure energy, duration, token usage, and memory over 150 runs per configuration.   Results. We find that framework architecture is the primary driver of energy consumption. The most energy-intensive framework, AutoCodeRover (Gemma), consumed 9.4x more energy on average than the least energy-intensive, OpenHands (Gemma). However, this energy is largely wasted. Task resolution rates were near-zero, demonstrating that current frameworks, when paired with SLMs, consume significant energy on unproductive reasoning loops. The SLM's limited reasoning was the bottleneck for success, but the framework's design was the bottleneck for efficiency.   Conclusions. Current agentic frameworks, designed for powerful LLMs, fail to operate efficiently with SLMs. We find that framework architecture is the primary driver of energy consumption, but this energy is largely wasted due to the SLMs' limited reasoning. Viable low-energy solutions require shifting from passive orchestration to architectures that actively manage SLM weaknesses.", "arxiv_url": "https://arxiv.org/abs/2512.09543", "authors": ["Arihant Tripathy", "Ch Pavan Harshit", "Karthik Vaidhyanathan"], "first_author": "Arihant Tripathy", "category": ["Empirical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Energy-aware agentic frameworks", "Hardware-level CPU/GPU energy profiling", "Small-model multi-turn reasoning limitations", "Framework architectural trade-offs", "Resource-constrained autonomous bug repair", "Wasted inference energy from reasoning loops", "Reproducible energy measurement methodology", "Failure-mode analysis for agentic workflows"], "summary": "本文在固定硬件上用两种小型语言模型对四种代理式问题修复框架进行大规模能耗与性能评估，发现框架架构决定能耗且大部分能耗因SLM推理能力受限导致无效循环被浪费，表明需设计主动管理SLM弱点的新型架构以实现低能耗可行性。", "quality": "High", "conference": "AGENT (ICSE 2026 workshop) 2026", "pdf_url": "https://arxiv.org/pdf/2512.09543v1", "published": "2025-12-10", "update_time": "2025-12-10", "download_time": "2025-12-12 01:58:17"}
{"id": "2512.09108", "title": "Evolving Excellence: Automated Optimization of LLM-based Agents", "abstract": "Agentic AI systems built on large language models (LLMs) offer significant potential for automating complex workflows, from software development to customer support. However, LLM agents often underperform due to suboptimal configurations; poorly tuned prompts, tool descriptions, and parameters that typically require weeks of manual refinement. Existing optimization methods either are too complex for general use or treat components in isolation, missing critical interdependencies.   We present ARTEMIS, a no-code evolutionary optimization platform that jointly optimizes agent configurations through semantically-aware genetic operators. Given only a benchmark script and natural language goals, ARTEMIS automatically discovers configurable components, extracts performance signals from execution logs, and evolves configurations without requiring architectural modifications.   We evaluate ARTEMIS on four representative agent systems: the \\emph{ALE Agent} for competitive programming on AtCoder Heuristic Contest, achieving a \\textbf{$13.6\\%$ improvement} in acceptance rate; the \\emph{Mini-SWE Agent} for code optimization on SWE-Perf, with a statistically significant \\textbf{10.1\\% performance gain}; and the \\emph{CrewAI Agent} for cost and mathematical reasoning on Math Odyssey, achieving a statistically significant \\textbf{$36.9\\%$ reduction} in the number of tokens required for evaluation. We also evaluate the \\emph{MathTales-Teacher Agent} powered by a smaller open-source model (Qwen2.5-7B) on GSM8K primary-level mathematics problems, achieving a \\textbf{22\\% accuracy improvement} and demonstrating that ARTEMIS can optimize agents based on both commercial and local models.", "arxiv_url": "https://arxiv.org/abs/2512.09108", "authors": ["Paul Brookes", "Vardan Voskanyan", "Rafail Giavrimis", "Matthew Truscott", "Mina Ilieva", "Chrystalla Pavlou", "Alexandru Staicu", "Manal Adham", "Will Evers- Hood", "Jingzhi Gong", "Kejia Zhang", "Matvey Fedoseev", "Vishal Sharma", "Roman Bauer", "Zheng Wang", "Hema Nair", "Wei Jie", "Tianhua Xu", "Aurora Constantin", "Leslie Kanthan", "Michail Basios"], "first_author": "Paul Brookes", "category": ["Technical"], "field": "LLM Agents & Deployment", "task": "Agent Configuration Optimization", "tags": ["Black-box agent tuning", "Semantically-aware mutation and crossover", "No-code evolutionary platform", "Execution-log based fitness", "LLM-ensemble guided edits", "Joint textual and parametric optimization", "Prompt + tool description co-optimization", "Cost vs. performance trade-off analysis"], "summary": "本文提出Artemis——一个无代码的基于进化算法的代理配置自动优化平台，通过语义感知的变异与交叉操作并利用执行日志与基准反馈，自动联合调优提示、工具描述和参数，从而显著提升不同LLM代理在多种任务上的准确性与效率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.09108v1", "published": "2025-12-09", "update_time": "2025-12-09", "download_time": "2025-12-12 02:05:43"}
{"id": "2501.03447", "title": "CoReQA: Uncovering Potentials of Language Models in Code Repository Question Answering", "abstract": "Large language models that enhance software development tasks, such as code generation, code completion, and code question answering (QA), have been extensively studied in both academia and the industry. The models are integrated into popular intelligent IDEs like JetBrains and Cursor. Current benchmarks for evaluating models' code comprehension capabilities primarily focus on code generation or completion, often neglecting QA, which is a crucial aspect of understanding code. Existing code QA benchmarks are derived from code comments with predefined patterns (e.g., CodeQA) or focus on specific domains, such as education (e.g., CS1QA). These benchmarks fail to capture the real-world complexity of software engineering and user requirements for understanding code repositories. To address this gap, we introduce CoReQA, a benchmark for Code Repository-level question answering, constructed from GitHub issues and comments from 176 popular repositories across four programming languages. Since questions and answers may include both natural language and code snippets, traditional evaluation metrics such as BLEU are inadequate for assessing repository-level QA performance. Thus, we provide an LLM-as-a-judge framework to evaluate QA performance from five aspects. Based on CoReQA, we evaluate the performance of three baselines, including two short-context models using generic retrieval strategies and one long-context model that utilizes the entire repository context. Evaluation results show that state-of-the-art proprietary and long-context models struggle to address repository-level questions effectively. Our analysis highlights the limitations of language models in assisting developers in understanding repositories and suggests future directions for improving repository comprehension systems through effective context retrieval methodologies.", "arxiv_url": "https://arxiv.org/abs/2501.03447", "authors": ["Jialiang Chen", "Kaifa Zhao", "Jie Liu", "Chao Peng", "Jierui Liu", "Hang Zhu", "Pengfei Gao", "Ping Yang", "Shuiguang Deng"], "first_author": "Jialiang Chen", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Repository-level question answering", "GitHub issue-based QA pairs", "Automated QA generation from issue threads", "LLM-as-a-judge evaluation", "Retrieval-augmented generation (RAG)", "Long-context model evaluation", "Cross-file code comprehension", "Context retrieval strategies"], "summary": "本文构建了CoReQA——一个基于 GitHub issue 的仓库级代码问答基准，并提出自动化构建与 LLM-as-a-judge 评估流程，用以评测短/长上下文模型在跨文件检索与仓库理解上的能力与局限。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2501.03447v1", "published": "2025-01-07", "update_time": "2025-01-07", "download_time": "2025-12-12 21:32:08"}
{"id": "2502.12466", "title": "EquiBench: Benchmarking Large Language Models' Reasoning about Program Semantics via Equivalence Checking", "abstract": "As large language models (LLMs) become integral to code-related tasks, a central question emerges: Do LLMs truly understand program semantics? We introduce EquiBench, a new benchmark for evaluating LLMs through equivalence checking, i.e., determining whether two programs produce identical outputs for all possible inputs. Unlike prior code generation benchmarks, this task directly tests a model's ability to reason about program semantics. EquiBench consists of 2400 program pairs across four languages and six categories. These pairs are generated through program analysis, compiler scheduling, and superoptimization, ensuring high-confidence labels, nontrivial difficulty, and full automation. We evaluate 19 state-of-the-art LLMs and find that in the most challenging categories, the best accuracies are 63.8% and 76.2%, only modestly above the 50% random baseline. Further analysis reveals that models often rely on syntactic similarity rather than exhibiting robust reasoning about program semantics, highlighting current limitations. Our code and dataset are publicly available at https://github.com/Anjiang-Wei/equibench", "arxiv_url": "https://arxiv.org/abs/2502.12466", "authors": ["Anjiang Wei", "Jiannan Cao", "Ran Li", "Hongyu Chen", "Yuhui Zhang", "Ziheng Wang", "Yuan Liu", "Thiago S. F. X. Teixeira", "Diyi Yang", "Ke Wang", "Alex Aiken"], "first_author": "Anjiang Wei", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Program Equivalence", "Equivalence-checking Benchmark", "Dead Code Elimination", "Superoptimization-generated Assembly Pairs", "Compiler Scheduling Transformations", "CUDA tensor scheduling with FP tolerance", "Algorithmic/variable-renaming transformations (OJ)", "Automated transformation and labeling pipeline", "Syntactic-similarity bias analysis", "Evaluation of prompting (few-shot, CoT) on semantic reasoning"], "summary": "本文提出 EquiBench：一个包含2400对跨四种语言与六类等价/不等价程序对的自动构造基准，用以衡量大模型对程序语义（等价性判定）的推理能力，并通过对19个模型的评估揭示模型常依赖句法相似性而非稳健语义推理，且在最难类别上性能接近随机基线。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2502.12466v3", "published": "2025-02-18", "update_time": "2025-09-19", "download_time": "2025-12-12 21:32:49"}
{"id": "2503.04359", "title": "LONGCODEU: Benchmarking Long-Context Language Models on Long Code Understanding", "abstract": "Current advanced long-context language models offer great potential for real-world software engineering applications. However, progress in this critical domain remains hampered by a fundamental limitation: the absence of a rigorous evaluation framework for long code understanding. To gap this obstacle, we propose a long code understanding benchmark LONGCODEU from four aspects (8 tasks) to evaluate LCLMs' long code understanding ability required for practical applications, including code unit perception, intra-code unit understanding, inter-code unit relation understanding, and long code documentation understanding. We evaluate 9 popular LCLMs on LONGCODEU (i.e., 6 general models and 3 code models). Our experimental results reveal key limitations in current LCLMs' capabilities for long code understanding. Particularly, the performance of LCLMs drops dramatically when the long code length is greater than 32K, falling far short of their claimed 128K-1M context windows. In the four aspects, inter-code unit relation understanding is the most challenging for LCLMs. Our study provides valuable insights for optimizing LCLMs and driving advancements in software engineering.", "arxiv_url": "https://arxiv.org/abs/2503.04359", "authors": ["Jia Li", "Xuyuan Guo", "Lei Li", "Kechi Zhang", "Ge Li", "Jia Li", "Zhengwei Tao", "Fang Liu", "Chongyang Tao", "Yuqi Zhu", "Zhi Jin"], "first_author": "Jia Li", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["Long-context evaluation", "Repository-level code understanding", "Inter-code-unit relation reasoning", "Function (code unit) identification", "Long documentation comprehension", "Temporal filtering to reduce data contamination", "Context-length stress testing (up to 128K tokens)"], "summary": "该论文提出了LONGCODEU基准，收集真实仓库中的超长代码并从代码单元感知、单位内理解、单位间关系理解和长文档理解四个方面设计8项任务，对多款长上下文模型进行评测以揭示其在超32K上下文长度下的性能瓶颈。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2503.04359v1", "published": "2025-03-06", "update_time": "2025-03-06", "download_time": "2025-12-12 21:33:30"}
{"id": "2506.00750", "title": "CodeSense: a Real-World Benchmark and Dataset for Code Semantic Reasoning", "abstract": "Understanding and reasoning about code semantics is essential for enhancing code LLMs' abilities to solve real-world software engineering (SE) tasks. Although several code reasoning benchmarks exist, most rely on synthetic datasets or educational coding problems and focus on coarse-grained reasoning tasks such as input/output prediction, limiting their effectiveness in evaluating LLMs in practical SE contexts. To bridge this gap, we propose CodeSense, the first benchmark that makes available a spectrum of fine-grained code reasoning tasks concerned with the software engineering of real-world code. We collected Python, C and Java software projects from real-world repositories. We executed tests from these repositories, collected their execution traces, and constructed a ground truth dataset for fine-grained semantic reasoning tasks. We then performed comprehensive evaluations on state-of-the-art LLMs. Our results show a clear performance gap for the models to handle fine-grained reasoning tasks. Although prompting techniques such as chain-of-thought and in-context learning helped, the lack of code semantics in LLMs fundamentally limit models' capabilities of code reasoning. Besides dataset, benchmark and evaluation, our work produced an execution tracing framework and tool set that make it easy to collect ground truth for fine-grained SE reasoning tasks, offering a strong basis for future benchmark construction and model post training. Our code and data are located at https://codesense-bench.github.io/.", "arxiv_url": "https://arxiv.org/abs/2506.00750", "authors": ["Monoshi Kumar Roy", "Simin Chen", "Benjamin Steenhoek", "Jinjun Peng", "Gail Kaiser", "Baishakhi Ray", "Wei Le"], "first_author": "Monoshi Kumar Roy", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Fine-grained statement-level semantics", "Execution trace instrumentation", "Dynamic value collection", "Branch condition prediction", "Loop iteration inference", "Pointer and aliasing reasoning", "Real-world multi-language projects (Python/C/Java)", "Semantic annotation toolchain", "Benchmark construction and public leaderboard", "Evaluation of prompting strategies (CoT, few-shot, ICL)"], "summary": "CodeSense 提出并公开了基于真实 Python、C 和 Java 项目的细粒度代码语义推理基准、执行跟踪工具与带标注的数据集，并在 14 个主流大模型上进行系统评估以揭示其语义推理能力的局限性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2506.00750v2", "published": "2025-05-31", "update_time": "2025-10-02", "download_time": "2025-12-12 21:34:37"}
{"id": "2507.05269", "title": "CoRe: Benchmarking LLMs Code Reasoning Capabilities through Static Analysis Tasks", "abstract": "Large language models (LLMs) have been widely adopted across diverse domains of software engineering, such as code generation, program repair, and vulnerability detection. These applications require understanding beyond surface-level code patterns: value propagation, control flow, and interdependence between program elements. However, existing benchmarks primarily evaluate end-to-end outcomes, such as whether code is correctly repaired or generated, leaving the models' ability for program semantic reasoning underexplored. This work presents CORE, a high-quality, human-verified benchmark designed to evaluate LLMs on fundamental static analysis tasks. CORE includes 12,553 task instances spanning data dependency, control dependency, and information flow across programs written in C/C++, Java, and Python. To ensure semantic diversity and reasoning complexity, we propose a semantics-aware diverse sampling strategy that selects targets and task instances based on structural coverage and dependency depth. We evaluate 10 mainstream LLMs and show that, while they perform well at identifying dependencies, models still struggle with tasks that require deeper semantic understanding and multi-step reasoning. We further conduct qualitative analyses to uncover key challenges, such as complex control structures and backward dependency patterns, offering insights into improving LLMs' code reasoning capabilities.", "arxiv_url": "https://arxiv.org/abs/2507.05269", "authors": ["Danning Xie", "Mingwei Zheng", "Xuwei Liu", "Jiannan Wang", "Chengpeng Wang", "Lin Tan", "Xiangyu Zhang"], "first_author": "Danning Xie", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Static analysis benchmarking", "Data-dependency tasks", "Control-dependency tasks", "Information-flow tasks", "Semantics-aware diverse sampling", "Human-verified semi-automated annotation", "Multilingual (C/C++, Java, Python)", "Trace generation and source enumeration", "Dependency depth and structural coverage", "Failure modes: complex control and backward dependencies"], "summary": "本文提出CORE基准——包含12,553条经人工验证的多语言静态分析任务（数据依赖、控制依赖、信息流），并通过语义感知抽样与半自动注释管线评测10个主流LLM，发现模型在深度多步语义推理上仍存在明显不足。", "quality": "High", "conference": "NeurIPS", "pdf_url": "https://arxiv.org/pdf/2507.05269v2", "published": "2025-07-03", "update_time": "2025-11-07", "download_time": "2025-12-12 21:35:32"}
{"id": "2509.14635", "title": "SWE-QA: Can Language Models Answer Repository-level Code Questions?", "abstract": "Understanding and reasoning about entire software repositories is an essential capability for intelligent software engineering tools. While existing benchmarks such as CoSQA and CodeQA have advanced the field, they predominantly focus on small, self-contained code snippets. These setups fail to capture the complexity of real-world repositories, where effective understanding and reasoning often require navigating multiple files, understanding software architecture, and grounding answers in long-range code dependencies. In this paper, we present SWE-QA, a repository-level code question answering (QA) benchmark designed to facilitate research on automated QA systems in realistic code environments. SWE-QA involves 576 high-quality question-answer pairs spanning diverse categories, including intention understanding, cross-file reasoning, and multi-hop dependency analysis. To construct SWE-QA, we first crawled 77,100 GitHub issues from 11 popular repositories. Based on an analysis of naturally occurring developer questions extracted from these issues, we developed a two-level taxonomy of repository-level questions and constructed a set of seed questions for each category. For each category, we manually curated and validated questions and collected their corresponding answers. As a prototype application, we further develop SWE-QA-Agent, an agentic framework in which LLM agents reason and act to find answers automatically. We evaluate six advanced LLMs on SWE-QA under various context augmentation strategies. Experimental results highlight the promise of LLMs, particularly our SWE-QA-Agent framework, in addressing repository-level QA, while also revealing open challenges and pointing to future research directions.", "arxiv_url": "https://arxiv.org/abs/2509.14635", "authors": ["Weihan Peng", "Yuling Shi", "Yuhang Wang", "Xinyun Zhang", "Beijun Shen", "Xiaodong Gu"], "first_author": "Weihan Peng", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["Repository-level QA benchmark", "Cross-file reasoning", "Multi-hop dependency analysis", "Issue-derived question taxonomy", "Seed-based question instantiation pipeline", "ReAct-style agent with tool usage", "Context augmentation (RAG) evaluation", "Rubric-guided human evaluation"], "summary": "本文提出SWE-QA——一个包含12个开源仓库共576个高质量仓库级问答对的基准，并构建SWE-QA-Agent代理与多种上下文增强策略对LLM在跨文件和多跳代码推理任务上的性能进行评估与分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2509.14635v1", "published": "2025-09-18", "update_time": "2025-09-18", "download_time": "2025-12-12 21:36:22"}
{"id": "2511.02778", "title": "VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation", "abstract": "Code has emerged as a precise and executable medium for reasoning and action in the agent era. Yet, progress has largely focused on language-centric tasks such as program synthesis and debugging, leaving visual-centric coding underexplored. Inspired by how humans reason over sketches, we advocate SVG code as a compact, interpretable, and executable visual representation. We introduce VCode, a benchmark that reframes multimodal understanding as code generation: given an image, a model must produce SVG that preserves symbolic meaning for downstream reasoning. VCode covers three domains - general commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel evaluation protocol in which a policy model answers questions over rendered SVGs; correct answers indicate faithful symbolic preservation. Empirically, frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap between language-centric and visual-centric coding. To close this gap, we introduce VCoder, an agentic framework that augments VLMs along two axes: (i) Thinking with Revision, which iteratively analyzes discrepancies and refines SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply structured cues such as objects, shapes, and text beyond the model's intrinsic capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities score well overall yet remain limited in professional knowledge and 3D reasoning. VCoder delivers a 12.3-point overall gain over the top-performing Claude-4-Opus. Human studies show that both humans and VLMs perform worse on rendered SVGs, their consistency reveals the promise of symbolic visual representation. The benchmark and code are available at https://github.com/CSU-JPG/VCode.", "arxiv_url": "https://arxiv.org/abs/2511.02778", "authors": ["Kevin Qinghong Lin", "Yuhao Zheng", "Hangyu Ran", "Dantong Zhu", "Dongxing Mao", "Linjie Li", "Philip Torr", "Alex Jinpeng Wang"], "first_author": "Kevin Qinghong Lin", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["SVG as symbolic visual representation", "Image→SVG code generation", "Render→VQA evaluation (CodeVQA)", "Iterative discrepancy-driven refinement", "Integration of perception tools (detectors/segmenters)", "Symbolic abstraction for visual reasoning", "3D/spatial relation preservation", "Human vs. model consistency study"], "summary": "本文提出VCode，一个将自然图像转换为可执行且可解释的SVG符号化视觉表示的多模态编码基准，并通过CodeVQA评估协议验证符号保真性，同时引入VCoder（迭代修订+视觉工具）以显著提升图像到SVG的生成质量。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.02778v1", "published": "2025-11-04", "update_time": "2025-11-04", "download_time": "2025-12-12 21:43:43"}
{"id": "2512.04355", "title": "Counting Without Running: Evaluating LLMs' Reasoning About Code Complexity", "abstract": "Modern GPU software stacks demand developers who can anticipate performance bottlenecks before ever launching a kernel; misjudging floating-point workloads upstream can derail tuning, scheduling, and even hardware procurement. Yet despite rapid progress in code generation, today's Large Language Models (LLMs) are rarely tested on this kind of forward-looking reasoning. We close that gap with gpuFLOPBench, a benchmark that asks models to \"count without running\" by predicting single and double-precision FLOP counts for 577 CUDA kernels drawn from HeCBench, annotated with ground-truth profiles and eight execution attributes that distinguish trivially analyzable code from kernels whose FLOPs depend on hidden compiler or runtime behavior. Evaluating current closed-source reasoning models shows clear but uneven progress: the newest LLMs achieve perfect classification on straightforward kernels but still incur multiple order-of-magnitude errors whenever implicit FLOPs arise from division, intrinsic math functions, or common subexpressions. These results surface a core limitation of existing code assistants -- the inability to internalize hardware-specific microcode effects -- and position gpuFLOPBench as a focused testbed for developing LLM tooling that can reason about performance with the same rigor as experienced GPU developers. Sources are available at our repository: https://github.com/Scientific-Computing-Lab/gpuFLOPBench", "arxiv_url": "https://arxiv.org/abs/2512.04355", "authors": ["Gregory Bolet", "Giorgis Georgakoudis", "Konstantinos Parasyris", "Harshitha Menon", "Niranjan Hasabnis", "Kirk W. Cameron", "Gal Oren"], "first_author": "Gregory Bolet", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Performance Optimization", "tags": ["Static FLOP counting", "CUDA kernel FLOP estimation", "Execution-attribute annotations", "Implicit/hidden FLOPs (division, intrinsics, templates)", "Single vs double-precision FLOP labels", "Prompting for structured performance predictions", "Hardware/microcode execution effects", "Benchmark for static performance reasoning"], "summary": "本文提出GPUFLOPBENCH数据集与评测框架，包含577个CUDA内核的单/双精度FLOP计数与执行属性标注，并评估现有封闭式推理模型在静态预测代码FLOP时的表现，揭示其在处理隐式FLOP来源（如除法、内建函数、编译器/运行时行为）时的显著失败模式。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04355v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-12 21:46:22"}
{"id": "2208.04415", "title": "Deep Learning Driven Natural Languages Text to SQL Query Conversion: A Survey", "abstract": "With the future striving toward data-centric decision-making, seamless access to databases is of utmost importance. There is extensive research on creating an efficient text-to-sql (TEXT2SQL) model to access data from the database. Using a Natural language is one of the best interfaces that can bridge the gap between the data and results by accessing the database efficiently, especially for non-technical users. It will open the doors and create tremendous interest among users who are well versed in technical skills or not very skilled in query languages. Even if numerous deep learning-based algorithms are proposed or studied, there still is very challenging to have a generic model to solve the data query issues using natural language in a real-work scenario. The reason is the use of different datasets in different studies, which comes with its limitations and assumptions. At the same time, we do lack a thorough understanding of these proposed models and their limitations with the specific dataset it is trained on. In this paper, we try to present a holistic overview of 24 recent neural network models studied in the last couple of years, including their architectures involving convolutional neural networks, recurrent neural networks, pointer networks, reinforcement learning, generative models, etc. We also give an overview of the 11 datasets that are widely used to train the models for TEXT2SQL technologies. We also discuss the future application possibilities of TEXT2SQL technologies for seamless data queries.", "arxiv_url": "https://arxiv.org/abs/2208.04415", "authors": ["Ayush Kumar", "Parth Nagarkar", "Prabhav Nalhe", "Sanjeev Vijayakumar"], "first_author": "Ayush Kumar", "category": ["Survey"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Text-to-SQL", "Cross-domain semantic parsing", "Schema linking and grounding", "Complex SQL composition (joins, nested/nested queries)", "Conversational/contextual Text-to-SQL", "Evaluation metrics (exact match, component F1)"], "summary": "本文系统综述了近年基于深度学习的文本到SQL转换研究，比较了24种模型架构与11个常用数据集，并分析了数据集特性、评估指标及未来挑战。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2208.04415v1", "published": "2022-08-08", "update_time": "2022-08-08", "download_time": "2025-12-12 21:47:09"}
{"id": "2208.13629", "title": "A Survey on Text-to-SQL Parsing: Concepts, Methods, and Future Directions", "abstract": "Text-to-SQL parsing is an essential and challenging task. The goal of text-to-SQL parsing is to convert a natural language (NL) question to its corresponding structured query language (SQL) based on the evidences provided by relational databases. Early text-to-SQL parsing systems from the database community achieved a noticeable progress with the cost of heavy human engineering and user interactions with the systems. In recent years, deep neural networks have significantly advanced this task by neural generation models, which automatically learn a mapping function from an input NL question to an output SQL query. Subsequently, the large pre-trained language models have taken the state-of-the-art of the text-to-SQL parsing task to a new level. In this survey, we present a comprehensive review on deep learning approaches for text-to-SQL parsing. First, we introduce the text-to-SQL parsing corpora which can be categorized as single-turn and multi-turn. Second, we provide a systematical overview of pre-trained language models and existing methods for text-to-SQL parsing. Third, we present readers with the challenges faced by text-to-SQL parsing and explore some potential future directions in this field.", "arxiv_url": "https://arxiv.org/abs/2208.13629", "authors": ["Bowen Qin", "Binyuan Hui", "Lihan Wang", "Min Yang", "Jinyang Li", "Binhua Li", "Ruiying Geng", "Rongyu Cao", "Jian Sun", "Luo Si", "Fei Huang", "Yongbin Li"], "first_author": "Bowen Qin", "category": ["Survey"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Single-turn vs Multi-turn Parsing", "Schema Linking and Table-grounding", "Table-aware Pretraining Objectives", "Graph-based Schema Encoding", "Sketch-based and Grammar-guided Decoding", "AST/Tree-based SQL Generation", "Execution- and Exact-match Evaluation Metrics", "Cross-domain and Contextual Generalization"], "summary": "本文为文本到SQL解析的综合性综述，系统介绍了相关数据集、表格预训练模型、编码与解码方法，并讨论了评估指标、挑战与未来研究方向。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2208.13629v1", "published": "2022-08-29", "update_time": "2022-08-29", "download_time": "2025-12-12 21:48:20"}
{"id": "1709.00103", "title": "Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning", "abstract": "A significant amount of the world's knowledge is stored in relational databases. However, the ability for users to retrieve facts from a database is limited due to a lack of understanding of query languages such as SQL. We propose Seq2SQL, a deep neural network for translating natural language questions to corresponding SQL queries. Our model leverages the structure of SQL queries to significantly reduce the output space of generated queries. Moreover, we use rewards from in-the-loop query execution over the database to learn a policy to generate unordered parts of the query, which we show are less suitable for optimization via cross entropy loss. In addition, we will publish WikiSQL, a dataset of 80654 hand-annotated examples of questions and SQL queries distributed across 24241 tables from Wikipedia. This dataset is required to train our model and is an order of magnitude larger than comparable datasets. By applying policy-based reinforcement learning with a query execution environment to WikiSQL, our model Seq2SQL outperforms attentional sequence to sequence models, improving execution accuracy from 35.9% to 59.4% and logical form accuracy from 23.4% to 48.3%.", "arxiv_url": "https://arxiv.org/abs/1709.00103", "authors": ["Victor Zhong", "Caiming Xiong", "Richard Socher"], "first_author": "Victor Zhong", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["NL-to-SQL generation", "Execution-based policy gradient", "Schema-aware pointer decoding", "Modeling unordered WHERE conditions", "Structured SQL decomposition (aggregation/select/where)"], "summary": "本文提出Seq2SQL，一种将自然语言问题翻译为结构化SQL查询的模型，通过将SQL解码分解为聚合、SELECT和WHERE三部分并使用基于执行结果的强化学习训练，同时发布大规模手工标注的SQL问答数据集以提升评估与训练效果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/1709.00103v7", "published": "2017-08-31", "update_time": "2017-11-09", "download_time": "2025-12-12 21:49:12"}
{"id": "1809.08887", "title": "Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task", "abstract": "We present Spider, a large-scale, complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables, covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task where different complex SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and the exact same programs in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 12.4% exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task are publicly available at https://yale-lily.github.io/spider", "arxiv_url": "https://arxiv.org/abs/1809.08887", "authors": ["Tao Yu", "Rui Zhang", "Kai Yang", "Michihiro Yasunaga", "Dongxu Wang", "Zifan Li", "James Ma", "Irene Li", "Qingning Yao", "Shanelle Roman", "Zilin Zhang", "Dragomir Radev"], "first_author": "Tao Yu", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Cross-domain text-to-SQL", "Complex multi-table SQL (JOIN/GROUP BY/NESTED)", "Database-schema-aware semantic parsing", "Database-split generalization evaluation", "Human expert annotation and review pipeline", "Large-scale relational schema collection"], "summary": "该论文提出并公开了Spider，一个包含200个多表数据库、10,181个自然语言问题和5,693个复杂SQL的大规模人工标注跨领域文本到SQL数据集，并定义了数据库分割的泛化任务与基线评估。", "quality": "High", "conference": "EMNLP 2018", "pdf_url": "https://arxiv.org/pdf/1809.08887v5", "published": "2018-09-24", "update_time": "2019-02-02", "download_time": "2025-12-12 21:50:00"}
{"id": "2010.12773", "title": "Structure-Grounded Pretraining for Text-to-SQL", "abstract": "Learning to capture text-table alignment is essential for tasks like text-to-SQL. A model needs to correctly recognize natural language references to columns and values and to ground them in the given database schema. In this paper, we present a novel weakly supervised Structure-Grounded pretraining framework (StruG) for text-to-SQL that can effectively learn to capture text-table alignment based on a parallel text-table corpus. We identify a set of novel prediction tasks: column grounding, value grounding and column-value mapping, and leverage them to pretrain a text-table encoder. Additionally, to evaluate different methods under more realistic text-table alignment settings, we create a new evaluation set Spider-Realistic based on Spider dev set with explicit mentions of column names removed, and adopt eight existing text-to-SQL datasets for cross-database evaluation. STRUG brings significant improvement over BERT-LARGE in all settings. Compared with existing pretraining methods such as GRAPPA, STRUG achieves similar performance on Spider, and outperforms all baselines on more realistic sets. The Spider-Realistic dataset is available at https://doi.org/10.5281/zenodo.5205322.", "arxiv_url": "https://arxiv.org/abs/2010.12773", "authors": ["Xiang Deng", "Ahmed Hassan Awadallah", "Christopher Meek", "Oleksandr Polozov", "Huan Sun", "Matthew Richardson"], "first_author": "Xiang Deng", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Structure-grounded pretraining", "Text-table alignment", "Column grounding", "Value grounding", "Column-value mapping", "Weakly-supervised alignment labeling", "Schema linking for SQL generation", "Cross-database generalization"], "summary": "本文提出STRUG，一种基于并行文本-表格语料通过列对齐、值对齐和列-值映射等弱监督预训练任务来增强文本到SQL的结构对齐能力，并构建更现实的评估设置以验证其跨库泛化效果。", "quality": "High", "conference": "NAACL 2021", "pdf_url": "https://arxiv.org/pdf/2010.12773v3", "published": "2020-10-24", "update_time": "2022-08-31", "download_time": "2025-12-12 21:52:10"}
{"id": "2106.01065", "title": "Towards Robustness of Text-to-SQL Models against Synonym Substitution", "abstract": "Recently, there has been significant progress in studying neural networks to translate text descriptions into SQL queries. Despite achieving good performance on some public benchmarks, existing text-to-SQL models typically rely on the lexical matching between words in natural language (NL) questions and tokens in table schemas, which may render the models vulnerable to attacks that break the schema linking mechanism. In this work, we investigate the robustness of text-to-SQL models to synonym substitution. In particular, we introduce Spider-Syn, a human-curated dataset based on the Spider benchmark for text-to-SQL translation. NL questions in Spider-Syn are modified from Spider, by replacing their schema-related words with manually selected synonyms that reflect real-world question paraphrases. We observe that the accuracy dramatically drops by eliminating such explicit correspondence between NL questions and table schemas, even if the synonyms are not adversarially selected to conduct worst-case adversarial attacks. Finally, we present two categories of approaches to improve the model robustness. The first category of approaches utilizes additional synonym annotations for table schemas by modifying the model input, while the second category is based on adversarial training. We demonstrate that both categories of approaches significantly outperform their counterparts without the defense, and the first category of approaches are more effective.", "arxiv_url": "https://arxiv.org/abs/2106.01065", "authors": ["Yujian Gan", "Xinyun Chen", "Qiuping Huang", "Matthew Purver", "John R. Woodward", "Jinxia Xie", "Pengsheng Huang"], "first_author": "Yujian Gan", "category": ["Benchmark", "Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Synonym substitution robustness", "Schema linking vulnerability", "Human-curated paraphrase benchmark", "Schema annotation augmentation", "Adversarial training for robustness", "Cell-value paraphrasing", "Cross-domain text-to-SQL evaluation", "Input-level defense (multiple schema annotations)"], "summary": "本文构建了一个人工整理的同义替换问句基准以评估并改进 text-to-SQL 模型对模式词同义替换的鲁棒性，提出通过多重模式注释的输入修改和对抗训练两类防御方法并验证其有效性，且输入修改方法在资源消耗更低的情况下表现更好。", "quality": "High", "conference": "ACL 2021", "pdf_url": "https://arxiv.org/pdf/2106.01065v2", "published": "2021-06-02", "update_time": "2021-06-19", "download_time": "2025-12-12 21:52:49"}
{"id": "2106.05006", "title": "Text-to-SQL in the Wild: A Naturally-Occurring Dataset Based on Stack Exchange Data", "abstract": "Most available semantic parsing datasets, comprising of pairs of natural utterances and logical forms, were collected solely for the purpose of training and evaluation of natural language understanding systems. As a result, they do not contain any of the richness and variety of natural-occurring utterances, where humans ask about data they need or are curious about. In this work, we release SEDE, a dataset with 12,023 pairs of utterances and SQL queries collected from real usage on the Stack Exchange website. We show that these pairs contain a variety of real-world challenges which were rarely reflected so far in any other semantic parsing dataset, propose an evaluation metric based on comparison of partial query clauses that is more suitable for real-world queries, and conduct experiments with strong baselines, showing a large gap between the performance on SEDE compared to other common datasets.", "arxiv_url": "https://arxiv.org/abs/2106.05006", "authors": ["Moshe Hazoom", "Vibhor Malik", "Ben Bogin"], "first_author": "Moshe Hazoom", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["naturally-occurring SQL queries", "web-forum query logs", "under-specified questions", "parameterized queries", "nested subqueries", "high template diversity", "data cleaning & validation", "partial-clause evaluation (PCM-F1)", "query canonization & anonymization"], "summary": "本文发布了SEDE，一个由Stack Exchange Data Explorer真实用户生成的12,023条自然语言与对应SQL对的数据集，分析了其中的真实世界挑战（如欠定问题与参数化查询）、提出了基于部分子句匹配的评估指标PCM-F1，并展示了在该数据集上现有模型性能明显下降。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2106.05006v1", "published": "2021-06-09", "update_time": "2021-06-09", "download_time": "2025-12-12 21:53:32"}
{"id": "1806.09029", "title": "Improving Text-to-SQL Evaluation Methodology", "abstract": "To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.", "arxiv_url": "https://arxiv.org/abs/1806.09029", "authors": ["Catherine Finegan-Dollak", "Jonathan K. Kummerfeld", "Li Zhang", "Karthik Ramanathan", "Sesh Sadasivam", "Rui Zhang", "Dragomir Radev"], "first_author": "Catherine Finegan-Dollak", "category": ["Benchmark", "Empirical", "Technical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Query-based dataset split", "SQL canonicalization", "Variable anonymization analysis", "Template-based slot-filling baseline", "Dataset standardization and error fixing", "Human vs. automatically generated question complexity", "Paraphrase plus entity replacement augmentation", "Duplicate query deduplication"], "summary": "本文通过标准化并修复多个现有 text-to-SQL 数据集、引入更具挑战性的问答样本、提出基于 SQL 查询的训练/测试切分以及分析变量匿名化的影响，从而改进和规范了 text-to-SQL 的评估方法。", "quality": "High", "conference": "ACL 2018", "pdf_url": "https://arxiv.org/pdf/1806.09029v1", "published": "2018-06-23", "update_time": "2018-06-23", "download_time": "2025-12-12 21:55:04"}
{"id": "1909.05378", "title": "CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases", "abstract": "We present CoSQL, a corpus for building cross-domain, general-purpose database (DB) querying dialogue systems. It consists of 30k+ turns plus 10k+ annotated SQL queries, obtained from a Wizard-of-Oz (WOZ) collection of 3k dialogues querying 200 complex DBs spanning 138 domains. Each dialogue simulates a real-world DB query scenario with a crowd worker as a user exploring the DB and a SQL expert retrieving answers with SQL, clarifying ambiguous questions, or otherwise informing of unanswerable questions. When user questions are answerable by SQL, the expert describes the SQL and execution results to the user, hence maintaining a natural interaction flow. CoSQL introduces new challenges compared to existing task-oriented dialogue datasets:(1) the dialogue states are grounded in SQL, a domain-independent executable representation, instead of domain-specific slot-value pairs, and (2) because testing is done on unseen databases, success requires generalizing to new domains. CoSQL includes three tasks: SQL-grounded dialogue state tracking, response generation from query results, and user dialogue act prediction. We evaluate a set of strong baselines for each task and show that CoSQL presents significant challenges for future research. The dataset, baselines, and leaderboard will be released at https://yale-lily.github.io/cosql.", "arxiv_url": "https://arxiv.org/abs/1909.05378", "authors": ["Tao Yu", "Rui Zhang", "He Yang Er", "Suyi Li", "Eric Xue", "Bo Pang", "Xi Victoria Lin", "Yi Chern Tan", "Tianze Shi", "Zihan Li", "Youxuan Jiang", "Michihiro Yasunaga", "Sungrok Shim", "Tao Chen", "Alexander Fabbri", "Zifan Li", "Luyao Chen", "Yuwen Zhang", "Shreya Dixit", "Vincent Zhang", "Caiming Xiong", "Richard Socher", "Walter S Lasecki", "Dragomir Radev"], "first_author": "Tao Yu", "category": ["Benchmark"], "field": "Natural Language Interfaces to Databases", "task": "Conversational Text-to-SQL (SQL-grounded dialogue state tracking, response generation from executed SQL/results, user dialogue act prediction)", "tags": ["Wizard-of-Oz data collection", "Cross-domain complex databases", "SQL-grounded dialogue state tracking", "Natural language response generation from SQL and results", "User dialogue act prediction (clarify/unanswerable)", "Execution-grounded annotations", "Unseen-database split for generalization", "Multi-turn conversational query interactions", "Baselines and public leaderboard"], "summary": "CoSQL 提出并发布了一个大规模跨域会话式文本到 SQL 语料库（含对话、SQL 标注、系统回应与对话行为标签），并为 SQL-驱动的对话状态跟踪、基于查询结果的自然语言回复生成和用户对话行为预测等任务提供基线与挑战性评估。", "quality": "High", "conference": "EMNLP 2019", "pdf_url": "https://arxiv.org/pdf/1909.05378v1", "published": "2019-09-11", "update_time": "2019-09-11", "download_time": "2025-12-12 21:58:40"}
{"id": "2109.05157", "title": "Exploring Underexplored Limitations of Cross-Domain Text-to-SQL Generalization", "abstract": "Recently, there has been significant progress in studying neural networks for translating text descriptions into SQL queries under the zero-shot cross-domain setting. Despite achieving good performance on some public benchmarks, we observe that existing text-to-SQL models do not generalize when facing domain knowledge that does not frequently appear in the training data, which may render the worse prediction performance for unseen domains. In this work, we investigate the robustness of text-to-SQL models when the questions require rarely observed domain knowledge. In particular, we define five types of domain knowledge and introduce Spider-DK (DK is the abbreviation of domain knowledge), a human-curated dataset based on the Spider benchmark for text-to-SQL translation. NL questions in Spider-DK are selected from Spider, and we modify some samples by adding domain knowledge that reflects real-world question paraphrases. We demonstrate that the prediction accuracy dramatically drops on samples that require such domain knowledge, even if the domain knowledge appears in the training set, and the model provides the correct predictions for related training samples.", "arxiv_url": "https://arxiv.org/abs/2109.05157", "authors": ["Yujian Gan", "Xinyun Chen", "Matthew Purver"], "first_author": "Yujian Gan", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Cross-domain text-to-SQL", "Domain knowledge categories", "Human-curated challenge set", "Robustness and generalization evaluation", "Omitted-column and multi-column mention handling", "Cell-value synonym substitution", "Boolean-like condition inference", "Schema-item vs SQL-structure conflict"], "summary": "本文提出了面向跨域 Text-to-SQL 的挑战性数据集，通过将领域知识归为五类（如省略列、多列推理、同义词替换、布尔类条件等）并对535条示例进行人工构造与评估，展示了现有模型在需要特定领域知识的样本上准确率显著下降，表明模型未能有效泛化训练集中出现的领域知识。", "quality": "High", "conference": "EMNLP 2021", "pdf_url": "https://arxiv.org/pdf/2109.05157v1", "published": "2021-09-11", "update_time": "2021-09-11", "download_time": "2025-12-12 21:59:48"}
{"id": "2305.03111", "title": "Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs", "abstract": "Text-to-SQL parsing, which aims at converting natural language instructions into executable SQLs, has gained increasing attention in recent years. In particular, Codex and ChatGPT have shown impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema with few rows of database contents leaving the gap between academic study and real-world applications. To mitigate this gap, we present Bird, a big benchmark for large-scale database grounded in text-to-SQL tasks, containing 12,751 pairs of text-to-SQL data and 95 databases with a total size of 33.4 GB, spanning 37 professional domains. Our emphasis on database values highlights the new challenges of dirty database contents, external knowledge between NL questions and database contents, and SQL efficiency, particularly in the context of massive databases. To solve these problems, text-to-SQL models must feature database value comprehension in addition to semantic parsing. The experimental results demonstrate the significance of database values in generating accurate text-to-SQLs for big databases. Furthermore, even the most effective text-to-SQL models, i.e. ChatGPT, only achieves 40.08% in execution accuracy, which is still far from the human result of 92.96%, proving that challenges still stand. Besides, we also provide an efficiency analysis to offer insights into generating text-to-efficient-SQLs that are beneficial to industries. We believe that BIRD will contribute to advancing real-world applications of text-to-SQL research. The leaderboard and source code are available: https://bird-bench.github.io/.", "arxiv_url": "https://arxiv.org/abs/2305.03111", "authors": ["Jinyang Li", "Binyuan Hui", "Ge Qu", "Jiaxi Yang", "Binhua Li", "Bowen Li", "Bailin Wang", "Bowen Qin", "Rongyu Cao", "Ruiying Geng", "Nan Huo", "Xuanhe Zhou", "Chenhao Ma", "Guoliang Li", "Kevin C. C. Chang", "Fei Huang", "Reynold Cheng", "Yongbin Li"], "first_author": "Jinyang Li", "category": ["Benchmark", "Empirical"], "field": "Text-to-SQL & Databases", "task": "Text-to-SQL Benchmark (Large-scale, value-grounded, efficiency-aware)", "tags": ["Large-scale database values", "Noisy / dirty value handling", "External knowledge grounding", "SQL execution efficiency", "Valid Efficiency Score (VES)", "Crowdsourced NL–SQL annotation", "Double-blind annotation workflow", "Domain-diverse databases", "Hidden test set for leakage avoidance", "FT vs ICL evaluation (T5 vs LLMs)", "Execution-based accuracy evaluation"], "summary": "该论文提出了BIRD基准，包含95个真实大规模数据库（33.4GB）与12,751对文本到SQL样本，强调数据库值的噪声处理、外部知识推理与查询效率并提出VES指标及多模型基线评测。", "quality": "High", "conference": "NeurIPS 2023", "pdf_url": "https://arxiv.org/pdf/2305.03111v3", "published": "2023-05-04", "update_time": "2023-11-15", "download_time": "2025-12-12 22:21:49"}
{"id": "2406.07860", "title": "BookSQL: A Large Scale Text-to-SQL Dataset for Accounting Domain", "abstract": "Several large-scale datasets (e.g., WikiSQL, Spider) for developing natural language interfaces to databases have recently been proposed. These datasets cover a wide breadth of domains but fall short on some essential domains, such as finance and accounting. Given that accounting databases are used worldwide, particularly by non-technical people, there is an imminent need to develop models that could help extract information from accounting databases via natural language queries. In this resource paper, we aim to fill this gap by proposing a new large-scale Text-to-SQL dataset for the accounting and financial domain: BookSQL. The dataset consists of 100k natural language queries-SQL pairs, and accounting databases of 1 million records. We experiment with and analyze existing state-of-the-art models (including GPT-4) for the Text-to-SQL task on BookSQL. We find significant performance gaps, thus pointing towards developing more focused models for this domain.", "arxiv_url": "https://arxiv.org/abs/2406.07860", "authors": ["Rahul Kumar", "Amar Raja Dibbu", "Shrutendra Harsola", "Vignesh Subrahmaniam", "Ashutosh Modi"], "first_author": "Rahul Kumar", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Accounting-domain Text-to-SQL", "Expert-curated accounting queries", "Seven-table double-entry transaction schema", "Large-scale NL→SQL pairs (100k) over 1M records", "Complex SQL constructs: nested queries, aggregations, GROUP BY, ORDER BY, DISTINCT", "Cross-business schema variability (27 businesses)", "Evaluation of domain generalization gaps for SOTA models", "Benchmark statistics and dataset release"], "summary": "本文构建并公开了面向会计与财务领域的大规模Text-to-SQL基准数据集（100k问-SQL对、1M记录、七表会计模式），并验证现有SOTA模型在该领域存在显著泛化性能缺陷。", "quality": "High", "conference": "NAACL 2024", "pdf_url": "https://arxiv.org/pdf/2406.07860v1", "published": "2024-06-12", "update_time": "2024-06-12", "download_time": "2025-12-12 22:22:52"}
{"id": "2409.02038", "title": "BEAVER: An Enterprise Benchmark for Text-to-SQL", "abstract": "Existing text-to-SQL benchmarks have largely been constructed from web tables with human-generated question-SQL pairs. LLMs typically show strong results on these benchmarks, leading to a belief that LLMs are effective at text-to-SQL tasks. However, how these results transfer to enterprise settings is unclear because tables in enterprise databases might differ substantially from web tables in structure and content. To contend with this problem, we introduce a new dataset BEAVER, the first enterprise text-to-SQL benchmark sourced from real private enterprise data warehouses. This dataset includes natural language queries and their correct SQL statements, which we collected from actual query logs. We then benchmark off-the-shelf LLMs on this dataset. LLMs perform poorly, even when augmented with standard prompt engineering and RAG techniques. We identify three main reasons for the poor performance: (1) schemas of enterprise tables are more complex than the schemas in public data, resulting in SQL-generation tasks intrinsically harder; (2) business-oriented questions are often more complex, requiring joins over multiple tables, aggregations, and nested queries; (3) public LLMs cannot train on private enterprise data warehouses that are not publicly accessible, and therefore it is difficult for the model to learn to solve (1) and (2). We believe BEAVER will facilitate future research in building text-to-SQL systems that perform better in enterprise settings.", "arxiv_url": "https://arxiv.org/abs/2409.02038", "authors": ["Peter Baile Chen", "Fabian Wenz", "Yi Zhang", "Devin Yang", "Justin Choi", "Nesime Tatbul", "Michael Cafarella", "Çağatay Demiralp", "Michael Stonebraker"], "first_author": "Peter Baile Chen", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Enterprise data warehouse schemas", "Anonymized real-user SQL query logs", "Column-to-question mapping annotation", "Table retrieval / selection for SQL generation", "Query complexity analysis (joins, aggregations, nesting)", "Retrieval-augmented generation (RAG) evaluation", "Error analysis of SQL generation (incorrect columns/values)", "Challenges from private-data domain shift"], "summary": "本文提出BEAVER——来自真实私有企业数据仓库的首个企业级Text-to-SQL基准数据集，并通过对现成模型的大规模评测与误差分析揭示了企业场景中架构复杂性、复杂业务查询与私有数据不可访问性导致的性能崩溃问题。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2409.02038v2", "published": "2024-09-03", "update_time": "2025-01-20", "download_time": "2025-12-12 22:23:35"}
{"id": "2410.11076", "title": "PRACTIQ: A Practical Conversational Text-to-SQL dataset with Ambiguous and Unanswerable Queries", "abstract": "Previous text-to-SQL datasets and systems have primarily focused on user questions with clear intentions that can be answered. However, real user questions can often be ambiguous with multiple interpretations or unanswerable due to a lack of relevant data. In this work, we construct a practical conversational text-to-SQL dataset called PRACTIQ, consisting of ambiguous and unanswerable questions inspired by real-world user questions. We first identified four categories of ambiguous questions and four categories of unanswerable questions by studying existing text-to-SQL datasets. Then, we generate conversations with four turns: the initial user question, an assistant response seeking clarification, the user's clarification, and the assistant's clarified SQL response with the natural language explanation of the execution results. For some ambiguous queries, we also directly generate helpful SQL responses, that consider multiple aspects of ambiguity, instead of requesting user clarification. To benchmark the performance on ambiguous, unanswerable, and answerable questions, we implemented large language model (LLM)-based baselines using various LLMs. Our approach involves two steps: question category classification and clarification SQL prediction. Our experiments reveal that state-of-the-art systems struggle to handle ambiguous and unanswerable questions effectively. We will release our code for data generation and experiments on GitHub.", "arxiv_url": "https://arxiv.org/abs/2410.11076", "authors": ["Mingwen Dong", "Nischal Ashok Kumar", "Yiqun Hu", "Anuj Chauhan", "Chung-Wei Hang", "Shuaichen Chang", "Lin Pan", "Wuwei Lan", "Henghui Zhu", "Jiarong Jiang", "Patrick Ng", "Zhiguo Wang"], "first_author": "Mingwen Dong", "category": ["Benchmark", "Empirical"], "field": "Natural Language Interfaces to Databases (Text-to-SQL)", "task": "Conversational Text-to-SQL with Ambiguity and Unanswerability", "tags": ["Conversational text-to-SQL", "Ambiguity taxonomy for NL queries", "Unanswerable query categorization", "Clarification question generation", "Helpful multi-SQL responses", "Database/schema modification for data generation", "Human annotation for conversation quality", "Prompt-based LLM baselines", "Question category classification", "Clarification SQL prediction"], "summary": "本文提出 PRACTIQ，一个面向对话式 text-to-SQL 的数据集，包含细化的歧义与不可回答查询类别、基于模式的生成与人工注释流程，并用提示式 LLM 基线评测显示现有模型在处理实务性歧义/不可回答问题上仍有显著不足。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2410.11076v1", "published": "2024-10-14", "update_time": "2024-10-14", "download_time": "2025-12-12 22:24:35"}
{"id": "2410.22925", "title": "BIS: NL2SQL Service Evaluation Benchmark for Business Intelligence Scenarios", "abstract": "NL2SQL (Natural Language to Structured Query Language) transformation has seen wide adoption in Business Intelligence (BI) applications in recent years. However, existing NL2SQL benchmarks are not suitable for production BI scenarios, as they are not designed for common business intelligence questions. To address this gap, we have developed a new benchmark focused on typical NL questions in industrial BI scenarios. We discuss the challenges of constructing a BI-focused benchmark and the shortcomings of existing benchmarks. Additionally, we introduce question categories in our benchmark that reflect common BI inquiries. Lastly, we propose two novel semantic similarity evaluation metrics for assessing NL2SQL capabilities in BI applications and services.", "arxiv_url": "https://arxiv.org/abs/2410.22925", "authors": ["Bora Caglayan", "Mingxue Wang", "John D. Kelleher", "Shen Fei", "Gui Tong", "Jiandong Ding", "Puchao Zhang"], "first_author": "Bora Caglayan", "category": ["Benchmark"], "field": "Natural Language Interfaces to Databases", "task": "BI-focused NL2SQL Benchmarking and Evaluation", "tags": ["Business Intelligence query patterns", "Temporal / time-series queries", "Schema irregularities and synonym mapping", "Multi-table joins and join resolution", "SQL semantic similarity metric", "Partial SQL result similarity metric", "No-code BI query interfaces"], "summary": "该论文提出了面向商业智能场景的NL2SQL基准并引入了两种用于评估查询语义相似性和结果部分相似性的评估指标，以更真实地衡量NL2SQL在BI服务中的表现。", "quality": "High", "conference": "ICSOC (International Conference on Service-Oriented Computing) 2024", "pdf_url": "https://arxiv.org/pdf/2410.22925v1", "published": "2024-10-30", "update_time": "2024-10-30", "download_time": "2025-12-12 22:25:38"}
{"id": "2411.07763", "title": "Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows", "abstract": "Real-world enterprise text-to-SQL workflows often involve complex cloud or local data across various database systems, multiple SQL queries in various dialects, and diverse operations from data transformation to analytics. We introduce Spider 2.0, an evaluation framework comprising 632 real-world text-to-SQL workflow problems derived from enterprise-level database use cases. The databases in Spider 2.0 are sourced from real data applications, often containing over 1,000 columns and stored in local or cloud database systems such as BigQuery and Snowflake. We show that solving problems in Spider 2.0 frequently requires understanding and searching through database metadata, dialect documentation, and even project-level codebases. This challenge calls for models to interact with complex SQL workflow environments, process extremely long contexts, perform intricate reasoning, and generate multiple SQL queries with diverse operations, often exceeding 100 lines, which goes far beyond traditional text-to-SQL challenges. Our evaluations indicate that based on o1-preview, our code agent framework successfully solves only 21.3% of the tasks, compared with 91.2% on Spider 1.0 and 73.0% on BIRD. Our results on Spider 2.0 show that while language models have demonstrated remarkable performance in code generation -- especially in prior text-to-SQL benchmarks -- they require significant improvement in order to achieve adequate performance for real-world enterprise usage. Progress on Spider 2.0 represents crucial steps towards developing intelligent, autonomous, code agents for real-world enterprise settings. Our code, baseline models, and data are available at https://spider2-sql.github.io", "arxiv_url": "https://arxiv.org/abs/2411.07763", "authors": ["Fangyu Lei", "Jixuan Chen", "Yuxiao Ye", "Ruisheng Cao", "Dongchan Shin", "Hongjin Su", "Zhaoqing Suo", "Hongcheng Gao", "Wenjing Hu", "Pengcheng Yin", "Victor Zhong", "Caiming Xiong", "Ruoxi Sun", "Qian Liu", "Sida Wang", "Tao Yu"], "first_author": "Fangyu Lei", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "tags": ["Enterprise text-to-SQL benchmark", "Agentic code execution", "Multi-step SQL workflows", "Large-scale schemas and schema linking", "SQL dialect handling (BigQuery/Snowflake/etc.)", "Execution-feedback and environment interaction", "Data transformation / DBT pipelines", "Project codebase grounding"], "summary": "Spider 2.0 提出一个面向真实企业场景的文本到 SQL 基准（含632个复杂工作流），要求模型在大规模模式、多方言和多步代理交互下生成并执行复杂 SQL 与数据转换，评估显示当前 LLMs 在此现实任务上性能仍显不足。", "quality": "High", "conference": "ICLR 2025", "pdf_url": "https://arxiv.org/pdf/2411.07763v2", "published": "2024-11-12", "update_time": "2025-03-17", "download_time": "2025-12-12 22:27:52"}
{"id": "2505.18744", "title": "LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Complex Reasoning", "abstract": "Text-to-SQL is a critical task in natural language processing that aims to transform natural language questions into accurate and executable SQL queries. In real-world scenarios, these reasoning tasks are often accompanied by complex mathematical computations, domain knowledge, and hypothetical reasoning scenarios. However, existing large-scale Text-to-SQL datasets typically focus on business logic and task logic, neglecting critical factors such as vertical domain knowledge, complex mathematical reasoning, and hypothetical reasoning, which are essential for realistically reflecting the reasoning demands in practical applications and completing data querying and analysis. To bridge this gap, we introduce LogicCat, the first Text-to-SQL benchmark dataset specifically designed for complex reasoning and chain-of-thought parsing, encompassing physics, arithmetic, commonsense, and hypothetical reasoning scenarios. LogicCat comprises 4,038 English questions paired 12,114 detailed chain-of-thought reasoning steps, spanning 45 databases across diverse domains, significantly surpassing existing datasets in complexity. Experimental results demonstrate that LogicCat substantially increases the task difficulty for current state-of-the-art models to at most 33.20% execution accuracy, indicating that this task remains exceptionally challenging. The advancement of LogicCat represents a crucial step toward developing systems suitable for real-world enterprise data analysis and autonomous query generation. We have released our dataset code at https://github.com/Ffunkytao/LogicCat.", "arxiv_url": "https://arxiv.org/abs/2505.18744", "authors": ["Tao Liu", "Xutao Mao", "Hongying Zan", "Dixuan Zhang", "Yifan Li", "Haixin Liu", "Lulu Kong", "Jiaming Hou", "Rui Li", "YunLong Li", "aoze zheng", "Zhiqiang Zhang", "Luo Zhewei", "Kunli Zhang", "Min Peng"], "first_author": "Tao Liu", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Text-to-SQL benchmark", "Chain-of-Thought annotation", "Complex mathematical reasoning", "Physical domain knowledge", "Hypothetical scenario reasoning", "Cross-domain database schemas", "Execution-based SQL validation", "Multi-step reasoning decomposition", "Formula grounding in queries"], "summary": "本文提出LogicCat，一个包含4,038个问题、12,114步链式推理注释并覆盖45个领域的Text-to-SQL基准，专注于数学、物理与假设性复杂推理，实验证明现有模型在该基准上的执行准确率显著下降（最高约33%）。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2505.18744v3", "published": "2025-05-24", "update_time": "2025-09-09", "download_time": "2025-12-12 22:30:01"}
{"id": "2505.20321", "title": "BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases", "abstract": "Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However, current text-to-SQL systems often struggle to map qualitative scientific questions into executable SQL, particularly when implicit domain reasoning is required. We introduce BiomedSQL, the first benchmark explicitly designed to evaluate scientific reasoning in text-to-SQL generation over a real-world biomedical knowledge base. BiomedSQL comprises 68,000 question/SQL query/answer triples generated from templates and grounded in a harmonized BigQuery knowledge base that integrates gene-disease associations, causal inference from omics data, and drug approval records. Each question requires models to infer domain-specific criteria, such as genome-wide significance thresholds, effect directionality, or trial phase filtering, rather than rely on syntactic translation alone. We evaluate a range of open- and closed-source LLMs across prompting strategies and interaction paradigms. Our results reveal a substantial performance gap: GPT-o3-mini achieves 59.0% execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%, both well below the expert baseline of 90.0%. BiomedSQL provides a new foundation for advancing text-to-SQL systems capable of supporting scientific discovery through robust reasoning over structured biomedical knowledge bases. Our dataset is publicly available at https://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source at https://github.com/NIH-CARD/biomedsql.", "arxiv_url": "https://arxiv.org/abs/2505.20321", "authors": ["Mathew J. Koretsky", "Maya Willey", "Adi Asija", "Owen Bianchi", "Chelsea X. Alvarado", "Tanay Nayak", "Nicole Kuznetsov", "Sungwon Kim", "Mike A. Nalls", "Daniel Khashabi", "Faraz Faghri"], "first_author": "Mathew J. Koretsky", "category": ["Benchmark", "Empirical", "Technical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Text-to-SQL Benchmark", "Scientific Reasoning Evaluation", "Harmonized BigQuery Biomedical Schema", "Genome-wide significance thresholds", "Multi-omic causal inference queries", "Drug approval and trial-phase filtering", "Template-based question augmentation", "Expert-authored gold SQL annotations", "Multi-step agent orchestration", "Execution-based accuracy evaluation"], "summary": "本文提出了面向生物医学科学推理的BiomedSQL基准（68,000条问/SQL/答案三元组）并发布了对应的BigQuery知识库与评测工具，评估多种LLM与自定义多步代理在生成可执行SQL并据此回答科研问题时的表现，揭示了显著的性能差距。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2505.20321v3", "published": "2025-05-23", "update_time": "2025-10-09", "download_time": "2025-12-12 22:31:39"}
{"id": "2509.23338", "title": "PARROT: A Benchmark for Evaluating LLMs in Cross-System SQL Translation", "abstract": "Large language models (LLMS) have shown increasing effectiveness in Text-to-SQL tasks. However, another closely related problem, Cross-System SQL Translation (a.k.a., SQL-to-SQL), which adapts a query written for one database system (e.g., MySQL) into its equivalent one for another system (e.g., ClickHouse), is of great practical importance but remains underexplored. Existing SQL benchmarks are not well-suited for SQL-to-SQL evaluation, which (1) focus on a limited set of database systems (often just SQLite) and (2) cannot capture many system-specific SQL dialects (e.g., customized functions, data types, and syntax rules). Thus, in this paper, we introduce PARROT, a Practical And Realistic BenchmaRk for CrOss-System SQL Translation. PARROT comprises 598 translation pairs from 38 open-source benchmarks and real-world business services, specifically prepared to challenge system-specific SQL understanding (e.g., LLMS achieve lower than 38.53% accuracy on average). We also provide multiple benchmark variants, including PARROT-Diverse with 28,003 translations (for extensive syntax testing) and PARROT-Simple with 5,306 representative samples (for focused stress testing), covering 22 production-grade database systems. To promote future research, we release a public leaderboard and source code at: https://code4db.github.io/parrot-bench/.", "arxiv_url": "https://arxiv.org/abs/2509.23338", "authors": ["Wei Zhou", "Guoliang Li", "Haoyu Wang", "Yuxing Han", "Xufei Wu", "Fan Wu", "Xuanhe Zhou"], "first_author": "Wei Zhou", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Cross-system SQL translation", "SQL dialect diversity", "Execution-first evaluation", "Reference executors", "Schema normalization", "Unit-style challenge cases", "Manual curation from production workloads", "System-specific functions and types"], "summary": "该论文提出并公开了面向跨系统 SQL 翻译的实用基准与评测套件（含人工验证的翻译对、多种变体、执行优先度量、参照执行器和挑战集），并用其揭示现有大模型在方言适应性上的显著不足。", "quality": "High", "conference": "NeurIPS 2025", "pdf_url": "https://arxiv.org/pdf/2509.23338v1", "published": "2025-09-27", "update_time": "2025-09-27", "download_time": "2025-12-12 22:32:49"}
{"id": "2509.24405", "title": "Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents", "abstract": "Text-to-SQL enables natural access to databases, yet most benchmarks are English-only, limiting multilingual progress. We introduce MultiSpider 2.0, extending Spider 2.0 to eight languages (English, German, French, Spanish, Portuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's structural difficulty while adding linguistic and dialectal variability, demanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art LLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\\% execution accuracy when relying on intrinsic reasoning, versus 60\\% on MultiSpider 1.0. Therefore, we provide a collaboration-driven language agents baseline that iteratively refines queries, improving accuracy to 15\\%. These results reveal a substantial multilingual gap and motivate methods that are robust across languages and ready for real-world enterprise deployment. Our benchmark is available at https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL.", "arxiv_url": "https://arxiv.org/abs/2509.24405", "authors": ["Khanh Trinh Pham", "Thu Huong Nguyen", "Jun Jo", "Quoc Viet Hung Nguyen", "Thanh Tam Nguyen"], "first_author": "Khanh Trinh Pham", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Multilingual Text-to-SQL", "Enterprise-scale schemas", "SQL dialect diversity", "Schema localization and bilingual alignment", "Compositional and nested SQL", "Collaborative language agents", "Iterative query decomposition and refinement", "Schema linking and lexical ambiguity"], "summary": "本文提出了MultiSpider 2.0——一个覆盖八种语言、面向企业级复杂模式与多SQL方言的多语言Text-to-SQL基准，并引入基于协作语言智能体（COLA）的迭代分解与校正基线以提升多语言查询的执行准确率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2509.24405v1", "published": "2025-09-29", "update_time": "2025-09-29", "download_time": "2025-12-12 22:34:21"}
{"id": "2510.05318", "title": "BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions", "abstract": "Large language models (LLMs) have demonstrated remarkable performance on single-turn text-to-SQL tasks, but real-world database applications predominantly require multi-turn interactions to handle ambiguous queries, execution errors, and evolving user requirements. Existing multi-turn benchmarks fall short by treating conversation histories as static context or limiting evaluation to read-only operations, failing to reflect production-grade database assistant challenges. We introduce BIRD-INTERACT, a benchmark that restores this realism through: (1) a comprehensive interaction environment coupling each database with a hierarchical knowledge base, metadata files, and a function-driven user simulator, enabling models to solicit clarifications, retrieve knowledge, and recover from errors without human supervision; (2) two evaluation settings consisting of a pre-defined conversational protocol (c-Interact) and an open-ended agentic setting (a-Interact) where models autonomously decide when to query the user simulator or explore the environment; (3) a challenging task suite covering the full CRUD spectrum for business-intelligence and operational use cases, guarded by executable test cases. Each task features ambiguous and follow-up sub-tasks requiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600 tasks, up to 11,796 interactions) for comprehensive performance assessment, and BIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed behavioral analysis and rapid method development. Our empirical results highlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in c-Interact and 17.00% in a-Interact. Analysis via memory grafting and Interaction Test-time Scaling validates the importance of effective interaction for complex, dynamic text-to-SQL tasks.", "arxiv_url": "https://arxiv.org/abs/2510.05318", "authors": ["Nan Huo", "Xiaohan Xu", "Jinyang Li", "Per Jacobsson", "Shipei Lin", "Bowen Qin", "Binyuan Hui", "Xiaolong Li", "Ge Qu", "Shuzheng Si", "Linheng Han", "Edward Alexander", "Xintong Zhu", "Rui Qin", "Ruihan Yu", "Yiyao Jin", "Feige Zhou", "Weihao Zhong", "Yun Chen", "Hongyu Liu", "Chenhao Ma", "Fatma Ozcan", "Yannis Papakonstantinou", "Reynold Cheng"], "first_author": "Nan Huo", "category": ["Benchmark", "Empirical"], "field": "Natural Language Interfaces to Databases (NLIDB)", "task": "Interactive Text-to-SQL Evaluation", "tags": ["Function-driven user simulator", "Hierarchical knowledge base and metadata", "Protocol-guided (c-Interact) vs agentic (a-Interact) settings", "Executable test-case based correctness", "Full CRUD and schema/modification operations", "Controlled simulator to prevent ground-truth leakage", "Interaction Test-time Scaling (ITS)", "Memory grafting behavioral analysis"], "summary": "BIRD-INTERACT 提出一个面向动态多轮交互的文本到 SQL 基准，包含函数驱动的用户模拟器、层级知识库与可执行测试，并通过协议式与自治式两种评估设置衡量 LLM 在覆盖 CRUD 的交互式数据库任务中的表现与挑战。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2510.05318v2", "published": "2025-10-06", "update_time": "2025-10-08", "download_time": "2025-12-12 22:49:08"}
{"id": "2510.24762", "title": "Falcon: A Comprehensive Chinese Text-to-SQL Benchmark for Enterprise-Grade Evaluation", "abstract": "We introduce Falcon, a cross-domain Chinese text-to-SQL benchmark grounded in an enterprise-compatible dialect (MaxCompute/Hive). It contains 600 Chinese questions over 28 databases; 77% require multi-table reasoning and over half touch more than four tables. Each example is annotated along SQL-computation features and Chinese semantics. For evaluation, we release a robust execution comparator and an automated evaluation pipeline, under which all current state-of-the-art large-scale models (including Deepseek) achieve accuracies of at most 50%. Major errors originate from two sources: (1) schema linking in large enterprise landscapes - hundreds of tables, denormalized fields, ambiguous column names, implicit foreign-key relations and domain-specific synonyms that make correct join/column selection difficult; and (2) mapping concise, colloquial Chinese into the exact operators and predicates required for analytics - e.g., choosing the correct aggregation and group-by keys, expressing time windows and granularities, applying unit conversions, handling NULLs and data-quality rules, and formulating nested or windowed subqueries. Falcon therefore targets Chinese-specific semantics and enterprise dialects (abbreviations, business jargon, fuzzy entity references) and provides a reproducible middle ground before full production deployment by using realistic enterprise schemas, query templates, an execution comparator, and an automated evaluation pipeline for end-to-end validation.", "arxiv_url": "https://arxiv.org/abs/2510.24762", "authors": ["Wenzhen Luo", "Wei Guan", "Yifan Yao", "Yimin Pan", "Feng Wang", "Zhipeng Yu", "Zhe Wen", "Liang Chen", "Yihong Zhuang"], "first_author": "Wenzhen Luo", "category": ["Benchmark"], "field": "Natural Language Interfaces to Databases", "task": "Text-to-SQL (Chinese, enterprise dialects)", "tags": ["MaxCompute/Hive dialect support", "Enterprise-scale wide/denormalized schemas", "Multi-table join reasoning", "Schema linking and ambiguous column resolution", "Chinese ellipsis and business-jargon mapping", "Execution-based schema-aware SQL comparator", "Automated dialect-aware evaluation pipeline", "Synthetic enterprise case generation and annotation"], "summary": "该论文构建并公开了 Falcon——一个针对企业级中文文本到 SQL 的基准与评测工具，覆盖 MaxCompute/Hive 方言、复杂多表推理与中文商业术语并提供精细注释与可执行性比较器以衡量模型在真实企业场景下的性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2510.24762v1", "published": "2025-10-23", "update_time": "2025-10-23", "download_time": "2025-12-12 22:49:40"}
{"id": "2006.03511", "title": "Unsupervised Translation of Programming Languages", "abstract": "A transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is timeconsuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.", "arxiv_url": "https://arxiv.org/abs/2006.03511", "authors": ["Marie-Anne Lachaux", "Baptiste Roziere", "Lowik Chanussot", "Guillaume Lample"], "first_author": "Marie-Anne Lachaux", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Translation", "tags": ["Unsupervised code-to-code translation", "Back-translation for code", "Cross-language masked pretraining", "Shared multilingual seq2seq transformer", "Monolingual GitHub mining", "API/standard-library alignment", "Unit-test-based functional evaluation", "Type inference challenges"], "summary": "本文提出一种基于无监督机器翻译技术的神经源代码转译方法，仅使用单语代码（跨语言预训练+回译）在 C++/Java/Python 之间高精度翻译，并发布包含852个并行函数及单元测试的评测集，显著优于规则化基线。", "quality": "High", "conference": "NeurIPS 2020", "pdf_url": "https://arxiv.org/pdf/2006.03511v3", "published": "2020-06-05", "update_time": "2020-09-22", "download_time": "2025-12-12 22:50:22"}
{"id": "2512.10713", "title": "PACIFIC: a framework for generating benchmarks to check Precise Automatically Checked Instruction Following In Code", "abstract": "Large Language Model (LLM)-based code assistants have emerged as a powerful application of generative AI, demonstrating impressive capabilities in code generation and comprehension. A key requirement for these systems is their ability to accurately follow user instructions. We present Precise Automatically Checked Instruction Following In Code (PACIFIC), a novel framework designed to automatically generate benchmarks that rigorously assess sequential instruction-following and code dry-running capabilities in LLMs, while allowing control over benchmark difficulty. PACIFIC produces benchmark variants with clearly defined expected outputs, enabling straightforward and reliable evaluation through simple output comparisons. In contrast to existing approaches that often rely on tool usage or agentic behavior, our work isolates and evaluates the LLM's intrinsic ability to reason through code behavior step-by-step without execution (dry running) and to follow instructions. Furthermore, our framework mitigates training data contamination by facilitating effortless generation of novel benchmark variations. We validate our framework by generating a suite of benchmarks spanning a range of difficulty levels and evaluating multiple state-of-the-art LLMs. Our results demonstrate that PACIFIC can produce increasingly challenging benchmarks that effectively differentiate instruction-following and dry running capabilities, even among advanced models. Overall, our framework offers a scalable, contamination-resilient methodology for assessing core competencies of LLMs in code-related tasks.", "arxiv_url": "https://arxiv.org/abs/2512.10713", "authors": ["Itay Dreyfuss", "Antonio Abu Nassar", "Samuel Ackerman", "Axel Ben David", "Rami Katan", "Orna Raz", "Marcel Zalmanovici"], "first_author": "Itay Dreyfuss", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Automated benchmark generation", "Deterministic expected-result checking", "Code dry-running simulation", "Instruction concatenation pipelines", "Contamination-resistant variant generation", "Difficulty control via instruction count and output length", "Prompt vs chat mode evaluation"], "summary": "本文提出了PACIFIC框架，用可控的指令拼接与引用实现自动生成可确定性评估样本，以检测大型模型的代码逐步推理（干运行）与指令遵循能力，并通过可变难度和去污染的基准区分模型表现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10713v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-13 01:45:41"}
{"id": "2512.10493", "title": "Decoding Human-LLM Collaboration in Coding: An Empirical Study of Multi-Turn Conversations in the Wild", "abstract": "Large language models (LLMs) are increasingly acting as dynamic conversational interfaces, supporting multi-turn interactions that mimic human-like conversation and facilitate complex tasks like coding. While datasets such as LMSYS-Chat-1M and WildChat capture real-world user-LLM conversations, few studies systematically explore the mechanisms of human-LLM collaboration in coding scenarios. What tortuous paths do users experience during the interaction process? How well do the LLMs follow instructions? Are users satisfied? In this paper, we conduct an empirical analysis on human-LLM coding collaboration using LMSYS-Chat-1M and WildChat datasets to explore the human-LLM collaboration mechanism, LLMs' instruction following ability, and human satisfaction. This study yields interesting findings: 1) Task types shape interaction patterns(linear, star and tree), with code quality optimization favoring linear patterns, design-driven tasks leaning toward tree structures, and queries preferring star patterns; 2) Bug fixing and code refactoring pose greater challenges to LLMs' instruction following, with non-compliance rates notably higher than in information querying; 3) Code quality optimization and requirements-driven development tasks show lower user satisfaction, whereas structured knowledge queries and algorithm designs yield higher levels. These insights offer recommendations for improving LLM interfaces and user satisfaction in coding collaborations, while highlighting avenues for future research on adaptive dialogue systems. We believe this work broadens understanding of human-LLM synergies and supports more effective AI-assisted development.", "arxiv_url": "https://arxiv.org/abs/2512.10493", "authors": ["Binquan Zhang", "Li Zhang", "Haoyuan Zhang", "Fang Liu", "Song Wang", "Bo Shen", "An Fu", "Lin Shi"], "first_author": "Binquan Zhang", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Prompting", "tags": ["Multi-turn dialogue patterns", "Interaction topology (linear/star/tree)", "Instruction-following compliance", "User satisfaction trajectory", "Task taxonomy for coding interactions", "LLM-assisted data disentanglement", "Open card sorting annotation", "Statistical significance testing of interaction effects"], "summary": "本文基于真实多轮对话数据实证分析了编码场景下的人-LLM协作，归纳出五类任务与线性/星状/树状三种交互模式，评估模型指令遵循能力与用户满意度并提出改进建议。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10493v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-13 01:46:15"}
{"id": "2512.10789", "title": "Natural Language Interface for Firewall Configuration", "abstract": "This paper presents the design and prototype implementation of a natural language interface for configuring enterprise firewalls. The framework allows administrators to express access control policies in plain language, which are then translated into vendor specific configurations. A compact schema bound intermediate representation separates human intent from device syntax and in the current prototype compiles to Palo Alto PAN OS command line configuration while remaining extensible to other platforms. Large language models are used only as assistive parsers that generate typed intermediate representation objects, while compilation and enforcement remain deterministic. The prototype integrates three validation layers, namely a static linter that checks structural and vendor specific constraints, a safety gate that blocks overly permissive rules such as any to any allows, and a Batfish based simulator that validates configuration syntax and referential integrity against a synthetic device model. The paper describes the architecture, implementation, and test methodology on synthetic network context datasets and discusses how this approach can evolve into a scalable auditable and human centered workflow for firewall policy management.", "arxiv_url": "https://arxiv.org/abs/2512.10789", "authors": ["F. Taghiyev", "A. Aslanbayli"], "first_author": "F. Taghiyev", "category": ["Technical"], "field": "Network Security & Configuration", "task": "Natural Language to Firewall Configuration", "tags": ["Schema-bound intermediate representation", "Schema-constrained LLM parsing", "Resolver agent for entity grounding", "Vendor-agnostic compiler (Palo Alto backend prototype)", "IR linter (general and vendor-specific)", "Safety Gate preventing any-to-any and missing-fields", "Batfish-based configuration simulation", "Role-conditioned prompting and constrained decoding", "Least-privilege policy synthesis", "Audit logging and stage-only (side-effect free) compilation"], "summary": "本文提出并实现了一个原型系统，利用受约束的LLM将管理员的自然语言访问策略解析为类型化中间表示，并通过静态lint、安全门控和Batfish仿真将其无副作用地编译为供应商特定的防火墙配置以确保安全与可审计性。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10789v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-13 01:50:04"}
{"id": "2512.10563", "title": "NormCode: A Semi-Formal Language for Context-Isolated AI Planning", "abstract": "Multistep workflows that chain large language model (LLM) calls suffer from context pollution: as information accumulates across steps, models hallucinate, confuse intermediate outputs, and lose track of task constraints. We present NormCode, a semiformal language for constructing plans of inferences, structured decompositions where each step operates in data isolation and receives only explicitly passed inputs, which eliminates crossstep contamination by design. NormCode enforces a strict separation between semantic operations (LLMdriven reasoning, nondeterministic) and syntactic operations (deterministic data restructuring), enabling precise cost and reliability tracing. The language exists in three isomorphic formats: .ncds for human authoring, .ncd for machine execution, and .ncn for human verification, supporting progressive formalization from sketch to production. We validate NormCode through two demonstrations: (1) a base X addition algorithm achieving 100 percent accuracy on arbitrary length inputs, and (2) self hosted execution of NormCode's own five phase compiler pipeline. The working orchestrator provides dependency driven scheduling, SQLite backed checkpointing, and loop management, making AI workflows auditable by design and addressing a critical need for transparency in high stakes domains such as legal reasoning, medical decision making, and financial analysis.", "arxiv_url": "https://arxiv.org/abs/2512.10563", "authors": ["Xin Guan"], "first_author": "Xin Guan", "category": ["Technical"], "field": "Requirements & Design", "task": "Specification & Validation", "tags": ["Context Isolation", "Inference Decomposition", "Semi-Formal Intermediate Representation", "Semantic vs Syntactic Separation", "Progressive Formalization (.ncds/.ncd/.ncn)", "Auditable AI Workflows", "Dependency-Driven Orchestration", "Checkpointed Execution and Loop Management"], "summary": "NormCode提出一种半形式化语言与执行框架，通过在多步LLM推理中强制显式数据隔离、将语义（非确定性LLM推理）与句法（确定性数据重构）分离，并提供三种同构格式与可检查的编译/运行时支持，从而实现可审计、可靠的AI工作流编排与执行。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10563v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-13 01:53:27"}
{"id": "2512.10485", "title": "From Lab to Reality: A Practical Evaluation of Deep Learning Models and LLMs for Vulnerability Detection", "abstract": "Vulnerability detection methods based on deep learning (DL) have shown strong performance on benchmark datasets, yet their real-world effectiveness remains underexplored. Recent work suggests that both graph neural network (GNN)-based and transformer-based models, including large language models (LLMs), yield promising results when evaluated on curated benchmark datasets. These datasets are typically characterized by consistent data distributions and heuristic or partially noisy labels. In this study, we systematically evaluate two representative DL models-ReVeal and LineVul-across four representative datasets: Juliet, Devign, BigVul, and ICVul. Each model is trained independently on each respective dataset, and their code representations are analyzed using t-SNE to uncover vulnerability related patterns. To assess realistic applicability, we deploy these models along with four pretrained LLMs, Claude 3.5 Sonnet, GPT-o3-mini, GPT-4o, and GPT-5 on a curated dataset, VentiVul, comprising 20 recently (May 2025) fixed vulnerabilities from the Linux kernel. Our experiments reveal that current models struggle to distinguish vulnerable from non-vulnerable code in representation space and generalize poorly across datasets with differing distributions. When evaluated on VentiVul, our newly constructed time-wise out-of-distribution dataset, performance drops sharply, with most models failing to detect vulnerabilities reliably. These results expose a persistent gap between academic benchmarks and real-world deployment, emphasizing the value of our deployment-oriented evaluation framework and the need for more robust code representations and higher-quality datasets.", "arxiv_url": "https://arxiv.org/abs/2512.10485", "authors": ["Chaomeng Lu", "Bert Lagaisse"], "first_author": "Chaomeng Lu", "category": ["Empirical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "tags": ["Time-wise out-of-distribution evaluation", "Whole-file evaluation mode", "Function-pair evaluation mode", "Cross-dataset generalization", "Code representation clustering (t-SNE & centroid distance)", "Graph-based vs token-based code representations", "Zero-shot LLM vulnerability probing", "Linux CVE fix-based testbed", "Label quality and dataset bias analysis"], "summary": "本文构建了一个时间分离的真实漏洞测试集并提出部署导向的评估框架，系统性比较图/序列深度模型与LLM在漏洞检测上的表示能力与跨数据集泛化性，结果表明现有模型在真实场景中表现显著下降。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10485v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-14 01:58:04"}
{"id": "2512.10452", "title": "UniCoR: Modality Collaboration for Robust Cross-Language Hybrid Code Retrieval", "abstract": "Effective code retrieval is indispensable and it has become an important paradigm to search code in hybrid mode using both natural language and code snippets. Nevertheless, it remains unclear whether existing approaches can effectively leverage such hybrid queries, particularly in cross-language contexts. We conduct a comprehensive empirical study of representative code models and reveal three challenges: (1) insufficient semantic understanding; (2) inefficient fusion in hybrid code retrieval; and (3) weak generalization in cross-language scenarios. To address these challenges, we propose UniCoR, a novel self-supervised framework that learns Unified Code Representations framework designed to learn unified and robust code representations. Firstly, we design a multi-perspective supervised contrastive learning module to enhance semantic understanding and modality fusion. It aligns representations from multiple perspectives, including code-to-code, natural language-to-code, and natural language-to-natural language, enforcing the model to capture a semantic essence among modalities. Secondly, we introduce a representation distribution consistency learning module to improve cross-language generalization, which explicitly aligns the feature distributions of different programming languages, enabling language-agnostic representation learning. Extensive experiments on both empirical benchmark and large-scale benchmark show that UniCoR outperforms all baseline models, achieving an average improvement of 8.64% in MRR and 11.54% in MAP over the best-performing baseline. Furthermore, UniCoR exhibits stability in hybrid code retrieval and generalization capability in cross-language scenarios.", "arxiv_url": "https://arxiv.org/abs/2512.10452", "authors": ["Yang Yang", "Li Kuang", "Jiakun Liu", "Zhongxin Liu", "Yingjie Xia", "David Lo"], "first_author": "Yang Yang", "category": ["Technical", "Empirical", "Benchmark"], "field": "Coding Assistant", "task": "Code Retrieval", "tags": ["Hybrid query fusion (code + natural language)", "Cross-language alignment", "Multi-perspective supervised contrastive learning", "Representation distribution consistency", "Maximum Mean Discrepancy (MMD) for language-agnostic alignment", "Modality collaboration and fusion", "Functionally equivalent code pairing", "Automated natural-language query generation", "Self-supervised code representation learning", "Fusion strategies: input remix / vector concat / score weighting"], "summary": "本文提出UniCoR，一种结合多视角监督对比学习与表示分布一致性约束的自监督框架，通过强化模态协同与跨语言对齐，显著提升混合（代码+自然语言）跨语言代码检索的语义理解、融合效果与泛化能力。", "quality": "High", "conference": "IEEE/ACM International Conference on Software Engineering (ICSE 2026)", "pdf_url": "https://arxiv.org/pdf/2512.10452v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-14 01:58:31"}
{"id": "2512.10415", "title": "How to Trick Your AI TA: A Systematic Study of Academic Jailbreaking in LLM Code Evaluation", "abstract": "The use of Large Language Models (LLMs) as automatic judges for code evaluation is becoming increasingly prevalent in academic environments. But their reliability can be compromised by students who may employ adversarial prompting strategies in order to induce misgrading and secure undeserved academic advantages. In this paper, we present the first large-scale study of jailbreaking LLM-based automated code evaluators in academic context. Our contributions are: (i) We systematically adapt 20+ jailbreaking strategies for jailbreaking AI code evaluators in the academic context, defining a new class of attacks termed academic jailbreaking. (ii) We release a poisoned dataset of 25K adversarial student submissions, specifically designed for the academic code-evaluation setting, sourced from diverse real-world coursework and paired with rubrics and human-graded references, and (iii) In order to capture the multidimensional impact of academic jailbreaking, we systematically adapt and define three jailbreaking metrics (Jailbreak Success Rate, Score Inflation, and Harmfulness). (iv) We comprehensively evalulate the academic jailbreaking attacks using six LLMs. We find that these models exhibit significant vulnerability, particularly to persuasive and role-play-based attacks (up to 97% JSR). Our adversarial dataset and benchmark suite lay the groundwork for next-generation robust LLM-based evaluators in academic code assessment.", "arxiv_url": "https://arxiv.org/abs/2512.10415", "authors": ["Devanshu Sahoo", "Vasudev Majhi", "Arjun Neekhra", "Yash Sinha", "Murari Mandal", "Dhruv Kumar"], "first_author": "Devanshu Sahoo", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Understanding", "tags": ["Academic Jailbreaking", "Adversarial Prompt Injection", "Poisoned Student Submissions", "Rubric-aware Attack Design", "Jailbreak Metrics (JSR/SIR/Harmfulness)", "Persona/Role-play Attacks", "Token/Emoji-level Manipulation", "Cross-model Robustness Benchmark"], "summary": "本文首次系统研究了针对LLM自动代码评分器的“学术越狱”攻击，构建了约25K条对抗性学生提交数据集、提出专门的评价指标并在六种模型上进行基准评估，揭示了角色扮演与说服类攻击下的严重脆弱性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10415v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-14 02:06:36"}
{"id": "2512.10398", "title": "Confucius Code Agent: An Open-sourced AI Software Engineer at Industrial Scale", "abstract": "Real-world AI software engineering demands coding agents that can reason over massive repositories, maintain durable memory across and within long sessions, and robustly coordinate complex toolchains at test time. Existing open-source coding agents provide transparency but frequently fall short when pushed to these industrial-scale workloads, while proprietary coding agents offer strong practical performance but limited extensibility, interpretability, and controllability. We present the Confucius Code Agent (CCA), an open-sourced AI software engineer that can operate at an industrial scale. CCA is built atop the Confucius SDK, an open-sourced agent development platform designed around three complementary perspectives: Agent Experience (AX), User Experience (UX), and Developer Experience (DX). The SDK introduces a unified orchestrator with hierarchical working memory for long-context reasoning, a persistent note-taking system for cross-session continual learning, and a modular extension module for robust tool use. Moreover, a meta-agent automates the synthesis, evaluation, and refinement of agent configurations through a build-test-improve loop, enabling rapid agent development on new tasks, environments, and tool stacks. Instantiated on Confucius SDK with these mechanisms, CCA delivers strong performance on real-world software engineering tasks. On SWE-Bench-Pro, CCA achieves a state-of-the-art Resolve@1 performance of 54.3%, substantially improving over prior coding agents. Together, the Confucius SDK and CCA provide a transparent, extensible, and reproducible foundation for AI agents, bridge gaps between research prototypes and production-grade systems, and support agent development and deployment at industrial scale.", "arxiv_url": "https://arxiv.org/abs/2512.10398", "authors": ["Zhaodong Wang", "Zhenting Qi", "Sherman Wong", "Nathan Hu", "Samuel Lin", "Jun Ge", "Erwin Gao", "Yining Yang", "Ben Maurer", "Wenlin Chen", "David Recordon", "Yilun Du", "Minlan Yu", "Ying Zhang"], "first_author": "Zhaodong Wang", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Hierarchical working memory", "Adaptive context compression", "Persistent note-taking", "Cross-session continual learning", "Meta-agent build–test–improve loop", "Modular tool extensions with typed callbacks", "Agent orchestration for long-horizon tasks", "Developer observability and DX-focused tooling", "Industrial-scale repository navigation", "Ablation-driven agent evaluation"], "summary": "本文提出并开源了Confucius SDK及其实例化的Confucius Code Agent，通过层次化工作内存、自适应上下文压缩、持久化笔记和模块化扩展，并借助元代理的构建-测试-改进循环，实现面向工业规模代码库的长期推理与跨会话记忆，在多种软件工程任务上表现优异。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.10398v1", "published": "2025-12-11", "update_time": "2025-12-11", "download_time": "2025-12-14 02:07:20"}
