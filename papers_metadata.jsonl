{"id": "2411.04905", "title": "OpenCoder: The Open Cookbook for Top-Tier Code Large Language Models", "abstract": "Large language models (LLMs) for code have become indispensable in various domains, including code generation, reasoning tasks and agent systems. While open-access code LLMs are increasingly approaching the performance levels of proprietary models, high-quality code LLMs suitable for rigorous scientific investigation, particularly those with reproducible data processing pipelines and transparent training protocols, remain limited. The scarcity is due to various challenges, including resource constraints, ethical considerations, and the competitive advantages of keeping models advanced. To address the gap, we introduce OpenCoder, a top-tier code LLM that not only achieves performance comparable to leading models but also serves as an \"open cookbook\" for the research community. Unlike most prior efforts, we release not only model weights and inference code, but also the reproducible training data, complete data processing pipeline, rigorous experimental ablation results, and detailed training protocols for open scientific research. Through this comprehensive release, we identify the key ingredients for building a top-tier code LLM: (1) code optimized heuristic rules for data cleaning and methods for data deduplication, (2) recall of text corpus related to code and (3) high-quality synthetic data in both annealing and supervised fine-tuning stages. By offering this level of openness, we aim to broaden access to all aspects of a top-tier code LLM, with OpenCoder serving as both a powerful model and an open foundation to accelerate research, and enable reproducible advancements in code AI.", "arxiv_url": "https://arxiv.org/abs/2411.04905", "authors": ["Siming Huang", "Tianhao Cheng", "J. K. Liu", "Jiaran Hao", "Liuyihan Song", "Yang Xu", "J. Yang", "Jiaheng Liu", "Chenchen Zhang", "Linzheng Chai", "Ruifeng Yuan", "Zhaoxiang Zhang", "Jie Fu", "Qian Liu", "Ge Zhang", "Zili Wang", "Yuan Qi", "Yinghui Xu", "Wei Chu"], "first_author": "Siming Huang", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "OpenCoder提出并开源了一个顶级代码LLM，发布模型权重、可复现的数据与训练流水线，并通过数据清洗、去重与高质量合成数据等策略揭示构建高性能代码模型的关键要素。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.04905v3", "published": "2024-11-07", "update_time": "2025-03-20", "download_time": "2025-12-04 21:51:44"}
{"id": "2505.08503", "title": "ICVul: A Well-labeled C/C++ Vulnerability Dataset with Comprehensive Metadata and VCCs", "abstract": "Machine learning-based software vulnerability detection requires high-quality datasets, which is essential for training effective models. To address challenges related to data label quality, diversity, and comprehensiveness, we constructed ICVul, a dataset emphasizing data quality and enriched with comprehensive metadata, including Vulnerability-Contributing Commits (VCCs). We began by filtering Common Vulnerabilities and Exposures from the NVD, retaining only those linked to GitHub fix commits. Then we extracted functions and files along with relevant metadata from these commits and used the SZZ algorithm to trace VCCs. To further enhance label reliability, we developed the ESC (Eliminate Suspicious Commit) technique, ensuring credible data labels. The dataset is stored in a relational-like database for improved usability and data integrity. Both ICVul and its construction framework are publicly accessible on GitHub, supporting research in related field.", "arxiv_url": "https://arxiv.org/abs/2505.08503", "authors": ["Chaomeng Lu", "Tianyu Li", "Toon Dehaene", "Bert Lagaisse"], "first_author": "Chaomeng Lu", "category": ["Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "summary": "ICVul 提供了一个面向 C/C++ 的高质量漏洞数据集，包含详细的元数据、Vulnerability-Contributing-Commits (VCCs) 追溯以及用于去除噪声提交的 ESC 筛选技术，并公开了可复现的构建框架。", "quality": "High", "conference": "MSR 2025", "pdf_url": "https://arxiv.org/pdf/2505.08503v1", "published": "2025-05-13", "update_time": "2025-05-13", "download_time": "2025-12-04 23:08:14"}
{"id": "2409.12186", "title": "Qwen2.5-Coder Technical Report", "abstract": "In this report, we introduce the Qwen2.5-Coder series, a significant upgrade from its predecessor, CodeQwen1.5. This series includes six models: Qwen2.5-Coder-(0.5B/1.5B/3B/7B/14B/32B). As a code-specific model, Qwen2.5-Coder is built upon the Qwen2.5 architecture and continues pretrained on a vast corpus of over 5.5 trillion tokens. Through meticulous data cleaning, scalable synthetic data generation, and balanced data mixing, Qwen2.5-Coder demonstrates impressive code generation capabilities while retaining general and math skills. These models have been evaluated on a wide range of code-related tasks, achieving state-of-the-art (SOTA) performance across more than 10 benchmarks, including code generation, completion, reasoning, and repair, consistently outperforming larger models of the same model size. We believe that the release of the Qwen2.5-Coder series will advance research in code intelligence and, with its permissive licensing, support wider adoption by developers in real-world applications.", "arxiv_url": "https://arxiv.org/abs/2409.12186", "authors": ["Binyuan Hui", "Jian Yang", "Zeyu Cui", "Jiaxi Yang", "Dayiheng Liu", "Lei Zhang", "Tianyu Liu", "Jiajun Zhang", "Bowen Yu", "Keming Lu", "Kai Dang", "Yang Fan", "Yichang Zhang", "An Yang", "Rui Men", "Fei Huang", "Bo Zheng", "Yibo Miao", "Shanghaoran Quan", "Yunlong Feng", "Xingzhang Ren", "Xuancheng Ren", "Jingren Zhou", "Junyang Lin"], "first_author": "Binyuan Hui", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "本文介绍了Qwen2.5-Coder系列（0.5B–32B），在继承Qwen2.5架构的基础上用超过5.5万亿token的代码与相关文本进行大规模预训练并通过精心设计的指令微调与数据清洗策略显著提升代码生成、补全与推理任务的性能并开源发布。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2409.12186v3", "published": "2024-09-18", "update_time": "2024-11-12", "download_time": "2025-12-04 23:08:44"}
{"id": "2308.12950", "title": "Code Llama: Open Foundation Models for Code", "abstract": "We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67% and 65% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.", "arxiv_url": "https://arxiv.org/abs/2308.12950", "authors": ["Baptiste Rozière", "Jonas Gehring", "Fabian Gloeckle", "Sten Sootla", "Itai Gat", "Xiaoqing Ellen Tan", "Yossi Adi", "Jingyu Liu", "Romain Sauvestre", "Tal Remez", "Jérémy Rapin", "Artyom Kozhevnikov", "Ivan Evtimov", "Joanna Bitton", "Manish Bhatt", "Cristian Canton Ferrer", "Aaron Grattafiori", "Wenhan Xiong", "Alexandre Défossez", "Jade Copet", "Faisal Azhar", "Hugo Touvron", "Louis Martin", "Nicolas Usunier", "Thomas Scialom", "Gabriel Synnaeve"], "first_author": "Baptiste Rozière", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "本文发布并开源了Code Llama——一系列基于Llama 2 的代码专用大模型（多种规模与变体），支持中间填充、超长上下文与指令微调，并在多项代码基准上达到了开源模型的最先进表现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2308.12950v3", "published": "2023-08-24", "update_time": "2024-01-31", "download_time": "2025-12-04 23:09:20"}
{"id": "2305.06161", "title": "StarCoder: may the source be with you!", "abstract": "The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.", "arxiv_url": "https://arxiv.org/abs/2305.06161", "authors": ["Raymond Li", "Loubna Ben Allal", "Yangtian Zi", "Niklas Muennighoff", "Denis Kocetkov", "Chenghao Mou", "Marc Marone", "Christopher Akiki", "Jia Li", "Jenny Chim", "Qian Liu", "Evgenii Zheltonozhskii", "Terry Yue Zhuo", "Thomas Wang", "Olivier Dehaene", "Mishig Davaadorj", "Joel Lamy-Poirier", "João Monteiro", "Oleh Shliazhko", "Nicolas Gontier", "Nicholas Meade", "Armel Zebaze", "Ming-Ho Yee", "Logesh Kumar Umapathi", "Jian Zhu", "Benjamin Lipkin", "Muhtasham Oblokulov", "Zhiruo Wang", "Rudra Murthy", "Jason Stillerman", "Siva Sankalp Patel", "Dmitry Abulkhanov", "Marco Zocca", "Manan Dey", "Zhihan Zhang", "Nour Fahmy", "Urvashi Bhattacharyya", "Wenhao Yu", "Swayam Singh", "Sasha Luccioni", "Paulo Villegas", "Maxim Kunakov", "Fedor Zhdanov", "Manuel Romero", "Tony Lee", "Nadav Timor", "Jennifer Ding", "Claire Schlesinger", "Hailey Schoelkopf", "Jan Ebert", "Tri Dao", "Mayank Mishra", "Alex Gu", "Jennifer Robinson", "Carolyn Jane Anderson", "Brendan Dolan-Gavitt", "Danish Contractor", "Siva Reddy", "Daniel Fried", "Dzmitry Bahdanau", "Yacine Jernite", "Carlos Muñoz Ferrandis", "Sean Hughes", "Thomas Wolf", "Arjun Guha", "Leandro von Werra", "Harm de Vries"], "first_author": "Raymond Li", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "本文发布并评估了开源代码语言模型StarCoder与StarCoderBase（15.5B参数、8K上下文、填空能力与多查询注意力），在大规模许可代码语料上预训练并对Python进行微调，同时提供PII脱敏与归属追踪工具，且在多项基准上优于其他开源代码模型。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2305.06161v2", "published": "2023-05-09", "update_time": "2023-12-13", "download_time": "2025-12-04 23:09:51"}
{"id": "2307.14936", "title": "PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback", "abstract": "Large Language Models for Code (Code LLM) are flourishing. New and powerful models are released on a weekly basis, demonstrating remarkable performance on the code generation task. Various approaches have been proposed to boost the code generation performance of pre-trained Code LLMs, such as supervised fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we propose a novel RRTF (Rank Responses to align Test&Teacher Feedback) framework, which can effectively and efficiently boost pre-trained large language models for code generation. Under this framework, we present PanGu-Coder2, which achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through an extensive evaluation on CoderEval and LeetCode benchmarks, we show that PanGu-Coder2 consistently outperforms all previous Code LLMs.", "arxiv_url": "https://arxiv.org/abs/2307.14936", "authors": ["Bo Shen", "Jiaxin Zhang", "Taihong Chen", "Daoguang Zan", "Bing Geng", "An Fu", "Muhan Zeng", "Ailun Yu", "Jichuan Ji", "Jingyang Zhao", "Yuenan Guo", "Qianxiang Wang"], "first_author": "Bo Shen", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Instruction-Tuning", "summary": "本文提出了RRTF（Rank Responses to align Test&Teacher Feedback）框架，并基于该框架对StarCoder进行排序反馈式指令微调得到PanGu-Coder2，从而在HumanEval、CoderEval和LeetCode等基准上显著提升代码生成性能并达成新的SOTA。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2307.14936v1", "published": "2023-07-27", "update_time": "2023-07-27", "download_time": "2025-12-04 23:10:29"}
{"id": "2306.08568", "title": "WizardCoder: Empowering Code Large Language Models with Evol-Instruct", "abstract": "Code Large Language Models (Code LLMs), such as StarCoder, have demonstrated exceptional performance in code-related tasks. However, most existing models are solely pre-trained on extensive raw code data without instruction fine-tuning. In this paper, we introduce WizardCoder, which empowers Code LLMs with complex instruction fine-tuning, by adapting the Evol-Instruct method to the domain of code. Through comprehensive experiments on four prominent code generation benchmarks, namely HumanEval, HumanEval+, MBPP, and DS-1000, we unveil the exceptional capabilities of our model. It surpasses all other open-source Code LLMs by a substantial margin. Moreover, our model even outperforms the largest closed LLMs, Anthropic's Claude and Google's Bard, on HumanEval and HumanEval+. Our code, model weights, and data are public at https://github.com/nlpxucan/WizardLM", "arxiv_url": "https://arxiv.org/abs/2306.08568", "authors": ["Ziyang Luo", "Can Xu", "Pu Zhao", "Qingfeng Sun", "Xiubo Geng", "Wenxiang Hu", "Chongyang Tao", "Jing Ma", "Qingwei Lin", "Daxin Jiang"], "first_author": "Ziyang Luo", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Instruction-Tuning", "summary": "本文提出了面向代码领域的Code Evol-Instruct指令微调方法，并基于此训练出WizardCoder模型，在多项代码生成基准（HumanEval、MBPP等）上显著优于现有开源模型，部分规模也超越了若干闭源模型。", "quality": "High", "conference": "ICLR 2024", "pdf_url": "https://arxiv.org/pdf/2306.08568v2", "published": "2023-06-14", "update_time": "2025-05-27", "download_time": "2025-12-04 23:10:59"}
{"id": "2107.03374", "title": "Evaluating Large Language Models Trained on Code", "abstract": "We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.", "arxiv_url": "https://arxiv.org/abs/2107.03374", "authors": ["Mark Chen", "Jerry Tworek", "Heewoo Jun", "Qiming Yuan", "Henrique Ponde de Oliveira Pinto", "Jared Kaplan", "Harri Edwards", "Yuri Burda", "Nicholas Joseph", "Greg Brockman", "Alex Ray", "Raul Puri", "Gretchen Krueger", "Michael Petrov", "Heidy Khlaaf", "Girish Sastry", "Pamela Mishkin", "Brooke Chan", "Scott Gray", "Nick Ryder", "Mikhail Pavlov", "Alethea Power", "Lukasz Kaiser", "Mohammad Bavarian", "Clemens Winter", "Philippe Tillet", "Felipe Petroski Such", "Dave Cummings", "Matthias Plappert", "Fotios Chantzis", "Elizabeth Barnes", "Ariel Herbert-Voss", "William Hebgen Guss", "Alex Nichol", "Alex Paino", "Nikolas Tezak", "Jie Tang", "Igor Babuschkin", "Suchir Balaji", "Shantanu Jain", "William Saunders", "Christopher Hesse", "Andrew N. Carr", "Jan Leike", "Josh Achiam", "Vedant Misra", "Evan Morikawa", "Alec Radford", "Matthew Knight", "Miles Brundage", "Mira Murati", "Katie Mayer", "Peter Welinder", "Bob McGrew", "Dario Amodei", "Sam McCandlish", "Ilya Sutskever", "Wojciech Zaremba"], "first_author": "Mark Chen", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文介绍并评估了在公开代码上微调的GPT模型Codex，提出并发布了HumanEval数据集与pass@k评测以衡量从docstring合成Python函数的功能正确性，展示了模型性能、局限性及潜在影响。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2107.03374v2", "published": "2021-07-07", "update_time": "2021-07-14", "download_time": "2025-12-04 23:11:30"}
{"id": "2406.11931", "title": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence", "abstract": "We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained from an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion tokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially enhances the coding and mathematical reasoning capabilities of DeepSeek-V2, while maintaining comparable performance in general language tasks. Compared to DeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in various aspects of code-related tasks, as well as reasoning and general capabilities. Additionally, DeepSeek-Coder-V2 expands its support for programming languages from 86 to 338, while extending the context length from 16K to 128K. In standard benchmark evaluations, DeepSeek-Coder-V2 achieves superior performance compared to closed-source models such as GPT4-Turbo, Claude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.", "arxiv_url": "https://arxiv.org/abs/2406.11931", "authors": ["DeepSeek-AI", "Qihao Zhu", "Daya Guo", "Zhihong Shao", "Dejian Yang", "Peiyi Wang", "Runxin Xu", "Y. Wu", "Yukun Li", "Huazuo Gao", "Shirong Ma", "Wangding Zeng", "Xiao Bi", "Zihui Gu", "Hanwei Xu", "Damai Dai", "Kai Dong", "Liyue Zhang", "Yishi Piao", "Zhibin Gou", "Zhenda Xie", "Zhewen Hao", "Bingxuan Wang", "Junxiao Song", "Deli Chen", "Xin Xie", "Kang Guan", "Yuxiang You", "Aixin Liu", "Qiushi Du", "Wenjun Gao", "Xuan Lu", "Qinyu Chen", "Yaohui Wang", "Chengqi Deng", "Jiashi Li", "Chenggang Zhao", "Chong Ruan", "Fuli Luo", "Wenfeng Liang"], "first_author": "DeepSeek-AI", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "本文提出了开源的DeepSeek-Coder-V2——基于Mixture-of-Experts的大规模代码模型，通过在DeepSeek-V2上继续使用额外6万亿tokens的预训练并结合指令微调与GRPO对齐，使得236B参数模型在代码与数学推理基准上达到并在若干任务上超越GPT-4-Turbo等闭源模型，支持338种编程语言和128K上下文长度。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2406.11931v1", "published": "2024-06-17", "update_time": "2024-06-17", "download_time": "2025-12-04 23:11:56"}
{"id": "2402.19173", "title": "StarCoder 2 and The Stack v2: The Next Generation", "abstract": "The BigCode project, an open-scientific collaboration focused on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder2. In partnership with Software Heritage (SWH), we build The Stack v2 on top of the digital commons of their source code archive. Alongside the SWH repositories spanning 619 programming languages, we carefully select other high-quality data sources, such as GitHub pull requests, Kaggle notebooks, and code documentation. This results in a training set that is 4x larger than the first StarCoder dataset. We train StarCoder2 models with 3B, 7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate them on a comprehensive set of Code LLM benchmarks. We find that our small model, StarCoder2-3B, outperforms other Code LLMs of similar size on most benchmarks, and also outperforms StarCoderBase-15B. Our large model, StarCoder2- 15B, significantly outperforms other models of comparable size. In addition, it matches or outperforms CodeLlama-34B, a model more than twice its size. Although DeepSeekCoder- 33B is the best-performing model at code completion for high-resource languages, we find that StarCoder2-15B outperforms it on math and code reasoning benchmarks, as well as several low-resource languages. We make the model weights available under an OpenRAIL license and ensure full transparency regarding the training data by releasing the SoftWare Heritage persistent IDentifiers (SWHIDs) of the source code data.", "arxiv_url": "https://arxiv.org/abs/2402.19173", "authors": ["Anton Lozhkov", "Raymond Li", "Loubna Ben Allal", "Federico Cassano", "Joel Lamy-Poirier", "Nouamane Tazi", "Ao Tang", "Dmytro Pykhtar", "Jiawei Liu", "Yuxiang Wei", "Tianyang Liu", "Max Tian", "Denis Kocetkov", "Arthur Zucker", "Younes Belkada", "Zijian Wang", "Qian Liu", "Dmitry Abulkhanov", "Indraneil Paul", "Zhuang Li", "Wen-Ding Li", "Megan Risdal", "Jia Li", "Jian Zhu", "Terry Yue Zhuo", "Evgenii Zheltonozhskii", "Nii Osae Osae Dade", "Wenhao Yu", "Lucas Krauß", "Naman Jain", "Yixuan Su", "Xuanli He", "Manan Dey", "Edoardo Abati", "Yekun Chai", "Niklas Muennighoff", "Xiangru Tang", "Muhtasham Oblokulov", "Christopher Akiki", "Marc Marone", "Chenghao Mou", "Mayank Mishra", "Alex Gu", "Binyuan Hui", "Tri Dao", "Armel Zebaze", "Olivier Dehaene", "Nicolas Patry", "Canwen Xu", "Julian McAuley", "Han Hu", "Torsten Scholak", "Sebastien Paquet", "Jennifer Robinson", "Carolyn Jane Anderson", "Nicolas Chapados", "Mostofa Patwary", "Nima Tajbakhsh", "Yacine Jernite", "Carlos Muñoz Ferrandis", "Lingming Zhang", "Sean Hughes", "Thomas Wolf", "Arjun Guha", "Leandro von Werra", "Harm de Vries"], "first_author": "Anton Lozhkov", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "本文介绍了基于 Software Heritage 构建的 The Stack v2 数据集并在其上训练并公开了 StarCoder2（3B/7B/15B）系列代码大模型，同时发布训练数据标识与权重并在多项代码基准上进行全面评估以展示其性能优势。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2402.19173v1", "published": "2024-02-29", "update_time": "2024-02-29", "download_time": "2025-12-04 23:12:22"}
{"id": "2401.14196", "title": "DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence", "abstract": "The rapid development of large language models has revolutionized code intelligence in software development. However, the predominance of closed-source models has restricted extensive research and development. To address this, we introduce the DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion tokens. These models are pre-trained on a high-quality project-level code corpus and employ a fill-in-the-blank task with a 16K window to enhance code generation and infilling. Our extensive evaluations demonstrate that DeepSeek-Coder not only achieves state-of-the-art performance among open-source code models across multiple benchmarks but also surpasses existing closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models are under a permissive license that allows for both research and unrestricted commercial use.", "arxiv_url": "https://arxiv.org/abs/2401.14196", "authors": ["Daya Guo", "Qihao Zhu", "Dejian Yang", "Zhenda Xie", "Kai Dong", "Wentao Zhang", "Guanting Chen", "Xiao Bi", "Y. Wu", "Y. K. Li", "Fuli Luo", "Yingfei Xiong", "Wenfeng Liang"], "first_author": "Daya Guo", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "本文提出并开源DeepSeek-Coder系列（1.3B–33B），在包含仓库级语料的2万亿tokens上从零预训练并采用Fill-In-Middle训练与16K上下文，显著提升开源代码模型性能并在多项基准上超越现有开源及部分闭源模型。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2401.14196v2", "published": "2024-01-25", "update_time": "2024-01-26", "download_time": "2025-12-04 23:12:47"}
{"id": "2306.11644", "title": "Textbooks Are All You Need", "abstract": "We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality\" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45% on HumanEval.", "arxiv_url": "https://arxiv.org/abs/2306.11644", "authors": ["Suriya Gunasekar", "Yi Zhang", "Jyoti Aneja", "Caio César Teodoro Mendes", "Allie Del Giorno", "Sivakanth Gopi", "Mojan Javaheripi", "Piero Kauffmann", "Gustavo de Rosa", "Olli Saarikivi", "Adil Salim", "Shital Shah", "Harkirat Singh Behl", "Xin Wang", "Sébastien Bubeck", "Ronen Eldan", "Adam Tauman Kalai", "Yin Tat Lee", "Yuanzhi Li"], "first_author": "Suriya Gunasekar", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "本文提出 phi-1：一个仅1.3B参数、以精选“教科书质量”文本与合成编程练习训练的小型代码生成模型，能在HumanEval和MBPP上以极小规模取得接近或超越更大模型的高性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2306.11644v2", "published": "2023-06-20", "update_time": "2023-10-02", "download_time": "2025-12-04 23:13:17"}
{"id": "2305.07922", "title": "CodeT5+: Open Code Large Language Models for Code Understanding and Generation", "abstract": "Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretraining objectives to mitigate the pretrain-finetune discrepancy. These objectives cover span denoising, contrastive learning, text-code matching, and causal LM pretraining tasks, on both unimodal and bimodal multilingual code corpora. Furthermore, we propose to initialize CodeT5+ with frozen off-the-shelf LLMs without training from scratch to efficiently scale up our models, and explore instruction-tuning to align with natural language instructions. We extensively evaluate CodeT5+ on over 20 code-related benchmarks in different settings, including zero-shot, finetuning, and instruction-tuning. We observe state-of-the-art (SoTA) model performance on various code-related tasks, such as code generation and completion, math programming, and text-to-code retrieval tasks. Particularly, our instruction-tuned CodeT5+ 16B achieves new SoTA results on HumanEval code generation task against other open code LLMs.", "arxiv_url": "https://arxiv.org/abs/2305.07922", "authors": ["Yue Wang", "Hung Le", "Akhilesh Deepak Gotmare", "Nghi D. Q. Bui", "Junnan Li", "Steven C. H. Hoi"], "first_author": "Yue Wang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "本文提出CodeT5+，一种可灵活组合组件的encoder-decoder代码大模型族，通过混合预训练目标（span denoising、对比学习、文本-代码匹配与因果语言建模）、冻结大解码器并仅训练浅编码器与交叉注意力以高效扩展，并结合instruction-tuning，在多项代码理解与生成基准上取得显著提升并在HumanEval上达成新SOTA。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2305.07922v2", "published": "2023-05-13", "update_time": "2023-05-20", "download_time": "2025-12-04 23:13:45"}
{"id": "2305.02309", "title": "CodeGen2: Lessons for Training LLMs on Programming and Natural Languages", "abstract": "Large language models (LLMs) have demonstrated remarkable abilities in representation learning for program synthesis and understanding tasks. The quality of the learned representations appears to be dictated by the neural scaling laws as a function of the number of model parameters and observations, while imposing upper bounds on the model performance by the amount of available data and compute, which is costly.   In this study, we attempt to render the training of LLMs for program synthesis more efficient by unifying four key components: (1) model architectures, (2) learning methods, (3) infill sampling, and, (4) data distributions. Specifically, for the model architecture, we attempt to unify encoder and decoder-based models into a single prefix-LM. For learning methods, (i) causal language modeling, (ii) span corruption, (iii) infilling are unified into a simple learning algorithm. For infill sampling, we explore the claim of a \"free lunch\" hypothesis. For data distributions, the effect of a mixture distribution and multi-epoch training of programming and natural languages on model performance is explored.   We conduct a comprehensive series of empirical experiments on 1B LLMs, for which failures and successes of this exploration are distilled into five lessons. We will provide a final recipe for training and release CodeGen2 models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training framework as open-source: https://github.com/salesforce/CodeGen.", "arxiv_url": "https://arxiv.org/abs/2305.02309", "authors": ["Erik Nijkamp", "Hiroaki Hayashi", "Caiming Xiong", "Silvio Savarese", "Yingbo Zhou"], "first_author": "Erik Nijkamp", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "该工作通过对模型架构、学习目标、infill 采样和数据分布的大规模消融实验，提出并验证了用于代码与自然语言统一预训练的简洁混合目标与训练配方，并开源了 CodeGen2 模型与训练框架。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2305.02309v2", "published": "2023-05-03", "update_time": "2023-07-11", "download_time": "2025-12-04 23:14:12"}
{"id": "2303.17568", "title": "CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X", "abstract": "Large pre-trained code generation models, such as OpenAI Codex, can generate syntax- and function-correct code, making the coding of programmers more productive and our pursuit of artificial general intelligence closer. In this paper, we introduce CodeGeeX, a multilingual model with 13 billion parameters for code generation. CodeGeeX is pre-trained on 850 billion tokens of 23 programming languages as of June 2022. Our extensive experiments suggest that CodeGeeX outperforms multilingual code models of similar scale for both the tasks of code generation and translation on HumanEval-X. Building upon HumanEval (Python only), we develop the HumanEval-X benchmark for evaluating multilingual models by hand-writing the solutions in C++, Java, JavaScript, and Go. In addition, we build CodeGeeX-based extensions on Visual Studio Code, JetBrains, and Cloud Studio, generating 4.7 billion tokens for tens of thousands of active users per week. Our user study demonstrates that CodeGeeX can help to increase coding efficiency for 83.4% of its users. Finally, CodeGeeX is publicly accessible and in Sep. 2022, we open-sourced its code, model weights (the version of 850B tokens), API, extensions, and HumanEval-X at https://github.com/THUDM/CodeGeeX.", "arxiv_url": "https://arxiv.org/abs/2303.17568", "authors": ["Qinkai Zheng", "Xiao Xia", "Xu Zou", "Yuxiao Dong", "Shan Wang", "Yufei Xue", "Zihan Wang", "Lei Shen", "Andi Wang", "Yang Li", "Teng Su", "Zhilin Yang", "Jie Tang"], "first_author": "Qinkai Zheng", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "本文提出并开源了一个13B参数的多语言代码生成预训练模型CodeGeeX，同时构建了用于多语言函数正确性评估的HumanEval-X基准，并展示了模型在生成与翻译任务上的优势与实际IDE部署带来的用户增效。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2303.17568v2", "published": "2023-03-30", "update_time": "2024-07-10", "download_time": "2025-12-04 23:15:01"}
{"id": "2301.03988", "title": "SantaCoder: don't reach for the stars!", "abstract": "The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.", "arxiv_url": "https://arxiv.org/abs/2301.03988", "authors": ["Loubna Ben Allal", "Raymond Li", "Denis Kocetkov", "Chenghao Mou", "Christopher Akiki", "Carlos Munoz Ferrandis", "Niklas Muennighoff", "Mayank Mishra", "Alex Gu", "Manan Dey", "Logesh Kumar Umapathi", "Carolyn Jane Anderson", "Yangtian Zi", "Joel Lamy Poirier", "Hailey Schoelkopf", "Sergey Troshin", "Dmitry Abulkhanov", "Manuel Romero", "Michael Lappert", "Francesco De Toni", "Bernardo García del Río", "Qian Liu", "Shamik Bose", "Urvashi Bhattacharyya", "Terry Yue Zhuo", "Ian Yu", "Paulo Villegas", "Marco Zocca", "Sourab Mangrulkar", "David Lansky", "Huu Nguyen", "Danish Contractor", "Luis Villa", "Jia Li", "Dzmitry Bahdanau", "Yacine Jernite", "Sean Hughes", "Daniel Fried", "Arjun Guha", "Harm de Vries", "Leandro von Werra"], "first_author": "Loubna Ben Allal", "category": ["Technical", "Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "该技术报告介绍了BigCode社区训练的1.1B参数代码模型SantaCoder，包含PII删减管道、架构（MQA、FIM）与数据预处理的消融实验，并在MultiPL-E上优于之前开源多语言模型。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2301.03988v2", "published": "2023-01-09", "update_time": "2023-02-24", "download_time": "2025-12-04 23:15:33"}
{"id": "2203.13474", "title": "CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis", "abstract": "Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.", "arxiv_url": "https://arxiv.org/abs/2203.13474", "authors": ["Erik Nijkamp", "Bo Pang", "Hiroaki Hayashi", "Lifu Tu", "Huan Wang", "Yingbo Zhou", "Silvio Savarese", "Caiming Xiong"], "first_author": "Erik Nijkamp", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Pre-Training", "summary": "该论文训练并开源了多种规模的CODEGEN代码语言模型和训练库JAXFORMER，提出并评估了多轮程序合成范式并构建了Multi-Turn Programming Benchmark以验证多轮提示能显著提升代码生成性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2203.13474v5", "published": "2022-03-25", "update_time": "2023-02-27", "download_time": "2025-12-04 23:16:13"}
{"id": "2312.02120", "title": "Magicoder: Empowering Code Generation with OSS-Instruct", "abstract": "We introduce Magicoder, a series of fully open-source (code, weights, and data) Large Language Models (LLMs) for code that significantly closes the gap with top code models while having no more than 7B parameters. Magicoder models are trained on 75K synthetic instruction data using OSS-Instruct, a novel approach to enlightening LLMs with open-source code snippets to generate diverse instruction data for code. Our main motivation is to mitigate the inherent bias of the synthetic data generated by LLMs through the wealth of open-source references for the production of more realistic and controllable data. The orthogonality of OSS-Instruct and other data generation methods like Evol-Instruct further enables us to build an enhanced MagicoderS. Both Magicoder and MagicoderS substantially outperform state-of-the-art code models with similar or even larger sizes on a wide range of coding benchmarks. Notably, MagicoderS-CL-7B based on CodeLlama even surpasses the prominent ChatGPT on HumanEval+ (66.5 vs. 65.9 in pass@1 ). Overall, OSS-Instruct opens a new direction for crafting diverse synthetic instruction data for code using abundant open-source references.", "arxiv_url": "https://arxiv.org/abs/2312.02120", "authors": ["Yuxiang Wei", "Zhe Wang", "Jiawei Liu", "Yifeng Ding", "Lingming Zhang"], "first_author": "Yuxiang Wei", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Instruction-Tuning", "summary": "本文提出OSS-INSTRUCT方法从开源代码片段自动生成多样且可控的指令微调数据，并基于此训练并开源了Magicoder系列7B级代码生成模型，在多项代码生成基准上显著提升表现且部分模型超越ChatGPT。", "quality": "High", "conference": "ICML 2024", "pdf_url": "https://arxiv.org/pdf/2312.02120v2", "published": "2023-12-04", "update_time": "2024-06-07", "download_time": "2025-12-04 23:16:40"}
{"id": "2308.07124", "title": "OctoPack: Instruction Tuning Code Large Language Models", "abstract": "Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile CommitPack: 4 terabytes of Git commits across 350 programming languages. We benchmark CommitPack against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HumanEvalPack, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OctoCoder and OctoGeeX, achieve the best performance across HumanEvalPack among all permissive models, demonstrating CommitPack's benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack.", "arxiv_url": "https://arxiv.org/abs/2308.07124", "authors": ["Niklas Muennighoff", "Qian Liu", "Armel Zebaze", "Qinkai Zheng", "Binyuan Hui", "Terry Yue Zhuo", "Swayam Singh", "Xiangru Tang", "Leandro von Werra", "Shayne Longpre"], "first_author": "Niklas Muennighoff", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Instruction-Tuning", "summary": "本文发布了用于指令微调的COMMITPACK（4TB Git 提交数据）及其高质量子集COMMITPACKFT，构建了多语言的HUMANEVALPACK基准，并基于这些数据训练出OCTOCODER/OCTOGEEX，在多语言的代码合成、修复与解释任务上在可许可模型中取得最优表现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2308.07124v2", "published": "2023-08-14", "update_time": "2024-02-18", "download_time": "2025-12-04 23:17:10"}
{"id": "2411.12882", "title": "ProSec: Fortifying Code LLMs with Proactive Security Alignment", "abstract": "While recent code-specific large language models (LLMs) have greatly enhanced their code generation capabilities, the safety of these models remains under-explored, posing potential risks as insecure code generated by these models may introduce vulnerabilities into real-world systems. Existing methods collect security-focused datasets from real-world vulnerabilities for instruction tuning in order to mitigate such issues. However, they are largely constrained by the data sparsity of vulnerable code, and have limited applicability in the multi-stage post-training workflows of modern LLMs. In this paper, we propose ProSec, a novel proactive security alignment approach designed to align code LLMs with secure coding practices. ProSec systematically exposes the vulnerabilities in a code LLM by synthesizing vulnerability-inducing coding scenarios from Common Weakness Enumerations (CWEs) and generates fixes to vulnerable code snippets, allowing the model to learn secure practices through preference learning objectives. The scenarios synthesized by ProSec trigger 25x more vulnerable code than a normal instruction-tuning dataset, resulting in a security-focused alignment dataset 7x larger than the previous work. Experiments show that models trained with ProSec are 25.2% to 35.4% more secure compared to previous work without degrading models' utility.", "arxiv_url": "https://arxiv.org/abs/2411.12882", "authors": ["Xiangzhe Xu", "Zian Su", "Jinyao Guo", "Kaiyuan Zhang", "Zhenting Wang", "Xiangyu Zhang"], "first_author": "Xiangzhe Xu", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Alignment", "summary": "本文提出PROSEC，一种在后训练阶段通过基于CWE合成易触发漏洞的指令、从目标模型采样易受攻击与修复代码并构建偏好学习数据来对代码LLM进行主动安全对齐的方法，从而显著减少生成不安全代码而不损害模型实用性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.12882v3", "published": "2024-11-19", "update_time": "2025-06-06", "download_time": "2025-12-04 23:17:39"}
{"id": "2406.06887", "title": "$\\textbf{PLUM}$: Improving Code LMs with Execution-Guided On-Policy Preference Learning Driven By Synthetic Test Cases", "abstract": "Preference learning provides a promising solution to address the limitations of supervised fine-tuning (SFT) for code language models, where the model is not explicitly trained to differentiate between correct and incorrect code. Recent findings demonstrate that on-policy data is the key to successful preference learning, where the preference data is collected using the same policy LM being trained. Inspired by this, we propose PLUM, an on-policy $\\textbf{P}$reference $\\textbf{L}$earning framework A$\\textbf{u}$gmented with test cases for code L$\\textbf{M}$ s. The framework operates in three key stages: (1) automatic generation of test cases from natural language instructions, (2) creation of a preference data by evaluating candidate code solutions sampled from the policy, which can then be used to (3) train the policy LM. PLUM levitates the need to train reward models, allowing for large scale on-policy and online preference data collation. PLUM is evaluated on both standard benchmarks (HumanEval, MBPP) and more challenging ones (LiveCodeBench), delivering substantial improvements over original SFT'ed models and other execution-feedback-driven approaches. We show PLUM's benefits are consistent across various widely-used code LMs even they have been well-trained with SFT. For example, PLUM increases pass rates by up to 4.8% on average on standard benchmarks and 11.8% on LiveCodeBench, demonstrating its effectiveness and generalizability. We also demonstrate the benefits of on-policy and online preference learning by comprehensive experimentation.", "arxiv_url": "https://arxiv.org/abs/2406.06887", "authors": ["Dylan Zhang", "Shizhe Diao", "Xueyan Zou", "Hao Peng"], "first_author": "Dylan Zhang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Instruction-Tuning", "summary": "本文提出PLUM，一种通过自动从自然语言题目生成测试用例并对模型自采样候选解进行执行反馈的在线在策略偏好学习框架，以无需训练奖励模型的方式显著提升代码语言模型的正确率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2406.06887v4", "published": "2024-06-11", "update_time": "2024-10-12", "download_time": "2025-12-04 23:18:09"}
{"id": "2307.04349", "title": "RLTF: Reinforcement Learning from Unit Test Feedback", "abstract": "The goal of program synthesis, or code generation, is to generate executable code based on given descriptions. Recently, there has been an increasing number of studies employing reinforcement learning (RL) to improve the performance of large language models (LLMs) for code. However, current representative works either rely solely on offline frameworks, limiting the exploration of new sample spaces, or fall short in the utilization of unit test signals, not accounting for specific error locations within the code. To address these issues, we propose RLTF, i.e., Reinforcement Learning from Unit Test Feedback, a novel online RL framework with unit test feedback of multi-granularity for refining code LLMs. Our approach generates data in real-time during training and simultaneously utilizes fine-grained feedback signals to guide the model towards producing higher-quality code. Extensive experiments show that RLTF achieves state-of-the-art performance on the APPS and the MBPP benchmarks. Our code is available at: https://github.com/Zyq-scut/RLTF.", "arxiv_url": "https://arxiv.org/abs/2307.04349", "authors": ["Jiate Liu", "Yiqin Zhu", "Kaiwen Xiao", "Qiang Fu", "Xiao Han", "Wei Yang", "Deheng Ye"], "first_author": "Jiate Liu", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "summary": "本文提出 RLTF，一种基于在线强化学习并利用多粒度单元测试反馈（细粒度与自适应反馈）的框架，用以细化代码大模型并在 APPS 与 MBPP 基准上取得显著提升。", "quality": "High", "conference": "TMLR 2023", "pdf_url": "https://arxiv.org/pdf/2307.04349v2", "published": "2023-07-10", "update_time": "2023-11-13", "download_time": "2025-12-04 23:18:42"}
{"id": "2301.13816", "title": "Execution-based Code Generation using Deep Reinforcement Learning", "abstract": "The utilization of programming language (PL) models, pre-trained on large-scale code corpora, as a means of automating software engineering processes has demonstrated considerable potential in streamlining various code generation tasks such as code completion, code translation, and program synthesis. However, current approaches mainly rely on supervised fine-tuning objectives borrowed from text generation, neglecting unique sequence-level characteristics of code, including but not limited to compilability as well as syntactic and functional correctness. To address this limitation, we propose PPOCoder, a new framework for code generation that synergistically combines pre-trained PL models with Proximal Policy Optimization (PPO) which is a widely used deep reinforcement learning technique. By utilizing non-differentiable feedback from code execution and structure alignment, PPOCoder seamlessly integrates external code-specific knowledge into the model optimization process. It's important to note that PPOCoder is a task-agnostic and model-agnostic framework that can be used across different code generation tasks and PLs. Extensive experiments on three code generation tasks demonstrate the effectiveness of our proposed approach compared to SOTA methods, achieving significant improvements in compilation success rates and functional correctness across different PLs.", "arxiv_url": "https://arxiv.org/abs/2301.13816", "authors": ["Parshin Shojaee", "Aneesh Jain", "Sindhu Tipirneni", "Chandan K. Reddy"], "first_author": "Parshin Shojaee", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "该论文提出PPOCoder——一种将预训练代码模型与PPO强化学习结合、利用编译器执行和代码结构对齐作为不可微奖励来优化代码生成以提高可编译性和功能正确性的通用框架，并在多种编程语言与任务上显著优于现有方法。", "quality": "High", "conference": "TMLR 2023", "pdf_url": "https://arxiv.org/pdf/2301.13816v4", "published": "2023-01-31", "update_time": "2023-07-19", "download_time": "2025-12-04 23:19:17"}
{"id": "2207.01780", "title": "CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning", "abstract": "Program synthesis or code generation aims to generate a program that satisfies a problem specification. Recent approaches using large-scale pretrained language models (LMs) have shown promising results, yet they have some critical limitations. In particular, they often follow a standard supervised fine-tuning procedure to train a code generation model only from the pairs of natural-language problem descriptions and ground-truth programs. Such paradigm largely ignores some important but potentially useful signals in the problem specification such as unit tests, which thus often results in poor performance when solving complex unseen coding tasks. To address the limitations, we propose \"CodeRL\", a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL). Specifically, during training, we treat the code-generating LM as an actor network, and introduce a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor. During inference, we introduce a new generation procedure with a critical sampling strategy that allows a model to automatically regenerate programs based on feedback from example unit tests and critic scores. For the model backbones, we extended the encoder-decoder architecture of CodeT5 with enhanced learning objectives, larger model sizes, and better pretraining data. Our method not only achieves new SOTA results on the challenging APPS benchmark, but also shows strong zero-shot transfer capability with new SOTA results on the simpler MBPP benchmark.", "arxiv_url": "https://arxiv.org/abs/2207.01780", "authors": ["Hung Le", "Yue Wang", "Akhilesh Deepak Gotmare", "Silvio Savarese", "Steven C. H. Hoi"], "first_author": "Hung Le", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "summary": "本文提出CodeRL框架，将预训练代码语言模型作为actor并引入critic与基于单元测试的奖励和关键采样策略，通过深度强化学习优化代码生成与自动修复，在APPS和MBPP上取得新的SOTA表现。", "quality": "High", "conference": "NeurIPS 2022", "pdf_url": "https://arxiv.org/pdf/2207.01780v3", "published": "2022-07-05", "update_time": "2022-11-03", "download_time": "2025-12-04 23:19:47"}
{"id": "2410.01215", "title": "From Code to Correctness: Closing the Last Mile of Code Generation with Hierarchical Debugging", "abstract": "While large language models have made significant strides in code generation, the pass rate of the generated code is bottlenecked on subtle errors, often requiring human intervention to pass tests, especially for complex problems. Existing LLM-based debugging systems treat generated programs as monolithic units, failing to address bugs at multiple levels of granularity, from low-level syntax errors to high-level algorithmic flaws. In this paper, we introduce Multi-Granularity Debugger (MGDebugger), a hierarchical code debugger by isolating, identifying, and resolving bugs at various levels of granularity. MGDebugger decomposes problematic code into a hierarchical tree structure of subfunctions, with each level representing a particular granularity of error. During debugging, it analyzes each subfunction and iteratively resolves bugs in a bottom-up manner. To effectively test each subfunction, we propose an LLM-simulated Python executor, which traces code execution and tracks important variable states to pinpoint errors accurately. Extensive experiments demonstrate that MGDebugger outperforms existing debugging systems, achieving an 18.9% improvement in accuracy over seed generations in HumanEval and a 97.6% repair success rate in HumanEvalFix. Furthermore, MGDebugger effectively fixes bugs across different categories and difficulty levels, demonstrating its robustness and effectiveness.", "arxiv_url": "https://arxiv.org/abs/2410.01215", "authors": ["Yuling Shi", "Songsong Wang", "Chengcheng Wan", "Min Wang", "Xiaodong Gu"], "first_author": "Yuling Shi", "category": ["Technical"], "field": "Quality Management", "task": "Bug Repair", "summary": "提出MGDebugger，一种将代码分解为子函数层次、为子函数生成测试并利用LLM模拟执行自底向上定位与修复错误的分层调试方法，显著提升对LLM生成代码与真实软件缺陷的修复率。", "quality": "High", "conference": "ICSE 2026", "pdf_url": "https://arxiv.org/pdf/2410.01215v4", "published": "2024-10-02", "update_time": "2025-11-22", "download_time": "2025-12-04 23:20:17"}
{"id": "2406.18294", "title": "Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs", "abstract": "Some recently developed code large language models (Code LLMs) have been pre-trained on repository-level code data (Repo-Code LLMs), enabling these models to recognize repository structures and utilize cross-file information for code completion. However, in real-world development scenarios, simply concatenating the entire code repository often exceeds the context window limits of these Repo-Code LLMs, leading to significant performance degradation. In this study, we conducted extensive preliminary experiments and analyses on six Repo-Code LLMs. The results indicate that maintaining the topological dependencies of files and increasing the code file content in the completion prompts can improve completion accuracy; pruning the specific implementations of functions in all dependent files does not significantly reduce the accuracy of completions. Based on these findings, we proposed a strategy named Hierarchical Context Pruning (HCP) to construct completion prompts with high informational code content. The HCP models the code repository at the function level, maintaining the topological dependencies between code files while removing a large amount of irrelevant code content, significantly reduces the input length for repository-level code completion. We applied the HCP strategy in experiments with six Repo-Code LLMs, and the results demonstrate that our proposed method can significantly enhance completion accuracy while substantially reducing the length of input. Our code and data are available at https://github.com/Hambaobao/HCP-Coder.", "arxiv_url": "https://arxiv.org/abs/2406.18294", "authors": ["Lei Zhang", "Yunshui Li", "Jiaming Li", "Xiaobo Xia", "Jiaxi Yang", "Run Luo", "Minzheng Wang", "Longze Chen", "Junhao Liu", "Min Yang"], "first_author": "Lei Zhang", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出了分层上下文剪枝（HCP）策略，在函数级别保留文件间拓扑依赖并裁剪无关实现，从而大幅减少仓库级预训练代码LLM的输入长度并显著提升代码补全准确率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2406.18294v2", "published": "2024-06-26", "update_time": "2024-06-27", "download_time": "2025-12-04 23:20:47"}
{"id": "2402.16906", "title": "Debug like a Human: A Large Language Model Debugger via Verifying Runtime Execution Step-by-step", "abstract": "Large language models (LLMs) are leading significant progress in code generation. Beyond one-pass code generation, recent works further integrate unit tests and program verifiers into LLMs to iteratively refine the generated programs. However, these works consider the generated programs as an indivisible entity, which falls short for LLMs in debugging the programs, especially when the programs contain complex logic flows and data operations. In contrast, when human developers debug programs, they typically set breakpoints and selectively examine runtime execution information. The execution flow and the intermediate variables play a crucial role in the debugging process, yet they are underutilized in the existing literature on code generation. In this study, we introduce Large Language Model Debugger (LDB), a novel debugging framework that enables LLMs to refine their generated programs with the runtime execution information. Specifically, LDB segments the programs into basic blocks and tracks the values of intermediate variables after each block throughout the runtime execution. This allows LLMs to concentrate on simpler code units within the overall execution flow, verify their correctness against the task description block by block, and efficiently pinpoint any potential errors. Experiments demonstrate that LDB consistently enhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and TransCoder benchmarks, archiving new state-of-the-art performance in code debugging for various LLM selections.", "arxiv_url": "https://arxiv.org/abs/2402.16906", "authors": ["Li Zhong", "Zilong Wang", "Jingbo Shang"], "first_author": "Li Zhong", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Editing", "summary": "本文提出LDB，一种通过将程序分解为基本块并采集每块的运行时中间变量，让LLM逐步验证每个代码块并定位错误以迭代修复生成代码的方法，在HumanEval、MBPP和TransCoder上显著提升了调试与代码生成性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2402.16906v6", "published": "2024-02-25", "update_time": "2024-06-06", "download_time": "2025-12-04 23:21:15"}
{"id": "2306.02907", "title": "SelfEvolve: A Code Evolution Framework via Large Language Models", "abstract": "Large language models (LLMs) have already revolutionized code generation, after being pretrained on publicly available code data. However, while various methods have been proposed to augment LLMs with retrieved knowledge and enhance the quality of code generation, the performance of these retrieval-based methods is limited by the strength of the retrievers used. In addition, while LLMs show great emergent ability, they still struggle to produce the correct code in one turn. To address these challenges, we propose a novel two-step pipeline, called \\autoknow, that leverages LLMs as both knowledge providers and self-reflective programmers. Unlike retrieval-based methods, \\autoknow~obtains the knowledge from input prompts and generates intermediate code based on the generated knowledge. After that, \\autoknow~asks LLM to act as an expert programmer to perform debugging for the generated code. This is achieved by receiving the error message from the interpreter, without requiring special test cases for correctness verification. We evaluate \\autoknow~on three code generation datasets, including DS-1000 for data science code, HumanEval for software engineering code, and TransCoder for C++-to-Python translation. Our empirical experiments show that \\autoknow~outperforms strong baselines by a significant margin on all datasets. We also conduct exhaustive analytical experiments to validate the effectiveness of the two stages of \\autoknow, and find that both are superior to other prompting-based methods. Further scalability analysis demonstrates that \\autoknow~can be adapted to other more advanced models, such as GPT-4, and bring consistent efficacy improvement.", "arxiv_url": "https://arxiv.org/abs/2306.02907", "authors": ["Shuyang Jiang", "Yuhao Wang", "Yu Wang"], "first_author": "Shuyang Jiang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Prompting", "summary": "本文提出SELFEVOLVE，一种两阶段的LLM驱动代码演化框架：先让模型自生成与题目相关的知识再生成代码，随后基于解释器错误信息对代码进行自我调试与迭代，从而在多套数据集上显著提升执行正确率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2306.02907v1", "published": "2023-06-05", "update_time": "2023-06-05", "download_time": "2025-12-04 23:21:45"}
{"id": "2306.09896", "title": "Is Self-Repair a Silver Bullet for Code Generation?", "abstract": "Large language models have shown remarkable aptitude in code generation, but still struggle to perform complex tasks. Self-repair -- in which the model debugs and repairs its own code -- has recently become a popular way to boost performance in these settings. However, despite its increasing popularity, existing studies of self-repair have been limited in scope; in many settings, its efficacy thus remains poorly understood. In this paper, we analyze Code Llama, GPT-3.5 and GPT-4's ability to perform self-repair on problems taken from HumanEval and APPS. We find that when the cost of carrying out repair is taken into account, performance gains are often modest, vary a lot between subsets of the data, and are sometimes not present at all. We hypothesize that this is because self-repair is bottlenecked by the model's ability to provide feedback on its own code; using a stronger model to artificially boost the quality of the feedback, we observe substantially larger performance gains. Similarly, a small-scale study in which we provide GPT-4 with feedback from human participants suggests that even for the strongest models, self-repair still lags far behind what can be achieved with human-level debugging.", "arxiv_url": "https://arxiv.org/abs/2306.09896", "authors": ["Theo X. Olausson", "Jeevana Priya Inala", "Chenglong Wang", "Jianfeng Gao", "Armando Solar-Lezama"], "first_author": "Theo X. Olausson", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Editing", "summary": "本文在HumanEval和APPS数据集上系统评估了Code Llama、GPT-3.5和GPT-4的自我修复能力，发现考虑修复计算成本后收益常常有限且随数据子集波动较大，而提升反馈质量（由更强模型或人工提供）能显著提高修复效果。", "quality": "High", "conference": "ICLR 2024", "pdf_url": "https://arxiv.org/pdf/2306.09896v5", "published": "2023-06-16", "update_time": "2024-02-02", "download_time": "2025-12-04 23:22:09"}
{"id": "2304.05128", "title": "Teaching Large Language Models to Self-Debug", "abstract": "Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.", "arxiv_url": "https://arxiv.org/abs/2304.05128", "authors": ["Xinyun Chen", "Maxwell Lin", "Nathanael Schärli", "Denny Zhou"], "first_author": "Xinyun Chen", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Editing", "summary": "本文提出 SELF-DEBUGGING，一种通过少量示例提示让大语言模型执行其生成的代码、生成自然语言解释并基于执行结果迭代自我修复程序的调试方法，在多项代码生成基准上显著提升了准确率与样本效率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2304.05128v2", "published": "2023-04-11", "update_time": "2023-10-05", "download_time": "2025-12-04 23:22:35"}
{"id": "2302.08468", "title": "LEVER: Learning to Verify Language-to-Code Generation with Execution", "abstract": "The advent of large language models trained on code (code LLMs) has led to significant progress in language-to-code generation. State-of-the-art approaches in this area combine LLM decoding with sample pruning and reranking using test cases or heuristics based on the execution results. However, it is challenging to obtain test cases for many real-world language-to-code applications, and heuristics cannot well capture the semantic features of the execution results, such as data type and value range, which often indicates the correctness of the program. In this work, we propose LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results. Specifically, we train verifiers to determine whether a program sampled from the LLMs is correct or not based on the natural language input, the program itself and its execution results. The sampled programs are reranked by combining the verification score with the LLM generation probability, and marginalizing over programs with the same execution results. On four datasets across the domains of table QA, math QA and basic Python programming, LEVER consistently improves over the base code LLMs(4.6% to 10.9% with code-davinci-002) and achieves new state-of-the-art results on all of them.", "arxiv_url": "https://arxiv.org/abs/2302.08468", "authors": ["Ansong Ni", "Srini Iyer", "Dragomir Radev", "Ves Stoyanov", "Wen-tau Yih", "Sida I. Wang", "Xi Victoria Lin"], "first_author": "Ansong Ni", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "summary": "本文提出LEVER，一种通过对代码LLM生成的候选程序结合其执行结果训练判别器并据此重排序的方法，以提高语言到代码生成的执行准确率，在四个基准上显著优于基线并达成SOTA。", "quality": "High", "conference": "ICML 2023", "pdf_url": "https://arxiv.org/pdf/2302.08468v3", "published": "2023-02-16", "update_time": "2023-09-01", "download_time": "2025-12-04 23:22:58"}
{"id": "2211.16490", "title": "Coder Reviewer Reranking for Code Generation", "abstract": "Sampling diverse programs from a code language model and reranking with model likelihood is a popular method for code generation but it is prone to preferring degenerate solutions. Inspired by collaborative programming, we propose Coder-Reviewer reranking. We augment Coder language models from past work, which generate programs given language instructions, with Reviewer models, which evaluate the likelihood of the instruction given the generated programs. We perform an extensive study across six datasets with eight models from three model families. Experimental results show that Coder-Reviewer reranking leads to consistent and significant improvement (up to 17% absolute accuracy gain) over reranking with the Coder model only. When combined with executability filtering, Coder-Reviewer reranking can often outperform the minimum Bayes risk method. Coder-Reviewer reranking is easy to implement by prompting, can generalize to different programming languages, and works well with off-the-shelf hyperparameters.", "arxiv_url": "https://arxiv.org/abs/2211.16490", "authors": ["Tianyi Zhang", "Tao Yu", "Tatsunori B. Hashimoto", "Mike Lewis", "Wen-tau Yih", "Daniel Fried", "Sida I. Wang"], "first_author": "Tianyi Zhang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出Coder-Reviewer重排方法：通过prompt将生成的程序放在前并估计p(x|y)，与原始p(y|x)的乘积用于重排候选代码，从而减少退化解并在多数据集、多模型上显著提升代码生成准确率且无需额外训练。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2211.16490v1", "published": "2022-11-29", "update_time": "2022-11-29", "download_time": "2025-12-04 23:23:28"}
{"id": "2207.10397", "title": "CodeT: Code Generation with Generated Tests", "abstract": "The task of generating code solutions for a given programming problem can benefit from the use of pre-trained language models such as Codex, which can produce multiple diverse samples. However, a major challenge for this task is to select the most appropriate solution from the multiple samples generated by the pre-trained language models. A natural way to evaluate the quality and correctness of a code solution is to run it against a set of test cases, but the manual creation of such test cases is often costly and time-consuming. In this paper, we propose a novel method, CodeT, that leverages the same pre-trained language models to automatically generate test cases for the code samples, thus reducing the human effort and increasing the coverage of the test scenarios. CodeT then executes the code samples using the generated test cases, and performs a dual execution agreement, which considers both the consistency of the outputs against the generated test cases and the agreement of the outputs with other code samples. We conduct comprehensive experiments on four benchmarks, HumanEval, MBPP, APPS and CodeContests, using five different pre-trained language models with varying sizes and capabilities. Our results show that CodeT can significantly improve the performance of code solution selection over previous methods, achieving remarkable and consistent gains across different models and benchmarks. For instance, CodeT improves the pass@1 metric on HumanEval to 65.8%, which represents an absolute improvement of 18.8% over the code-davinci-002 model, and an absolute improvement of more than 20% over the previous state-of-the-art results.", "arxiv_url": "https://arxiv.org/abs/2207.10397", "authors": ["Bei Chen", "Fengji Zhang", "Anh Nguyen", "Daoguang Zan", "Zeqi Lin", "Jian-Guang Lou", "Weizhu Chen"], "first_author": "Bei Chen", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "论文提出CODET方法，利用同一预训练模型自动生成测试用例并通过双重执行一致性（基于生成测试的输出匹配和候选程序间的一致性）从多个候选解中选出最佳代码，从而在HumanEval、MBPP、APPS等基准上显著提升了pass@1性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2207.10397v2", "published": "2022-07-21", "update_time": "2022-11-23", "download_time": "2025-12-04 23:24:02"}
{"id": "2412.05210", "title": "Evaluating and Aligning CodeLLMs on Human Preference", "abstract": "Code large language models (codeLLMs) have made significant strides in code generation. Most previous code-related benchmarks, which consist of various programming exercises along with the corresponding test cases, are used as a common measure to evaluate the performance and capabilities of code LLMs. However, the current code LLMs focus on synthesizing the correct code snippet, ignoring the alignment with human preferences, where the query should be sampled from the practical application scenarios and the model-generated responses should satisfy the human preference. To bridge the gap between the model-generated response and human preference, we present a rigorous human-curated benchmark CodeArena to emulate the complexity and diversity of real-world coding tasks, where 397 high-quality samples spanning 40 categories and 44 programming languages, carefully curated from user queries. Further, we propose a diverse synthetic instruction corpus SynCode-Instruct (nearly 20B tokens) by scaling instructions from the website to verify the effectiveness of the large-scale synthetic instruction fine-tuning, where Qwen2.5-SynCoder totally trained on synthetic instruction data can achieve top-tier performance of open-source code LLMs. The results find performance differences between execution-based benchmarks and CodeArena. Our systematic experiments of CodeArena on 40+ LLMs reveal a notable performance gap between open SOTA code LLMs (e.g. Qwen2.5-Coder) and proprietary LLMs (e.g., OpenAI o1), underscoring the importance of the human preference alignment.\\footnote{\\url{https://codearenaeval.github.io/ }}", "arxiv_url": "https://arxiv.org/abs/2412.05210", "authors": ["Jian Yang", "Jiaxi Yang", "Ke Jin", "Yibo Miao", "Lei Zhang", "Liqun Yang", "Zeyu Cui", "Yichang Zhang", "Binyuan Hui", "Junyang Lin"], "first_author": "Jian Yang", "category": ["Benchmark", "Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Alignment", "summary": "本文提出了面向人类偏好的代码评估基准 CodeArena 与大规模合成指令语料 SynCode-Instruct，并基于此训练与评测多款 codeLLM，揭示开放模型与闭源模型在人类偏好对齐上的差距。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2412.05210v1", "published": "2024-12-06", "update_time": "2024-12-06", "download_time": "2025-12-04 23:24:39"}
{"id": "2412.00535", "title": "FullStack Bench: Evaluating LLMs as Full Stack Coders", "abstract": "As the capabilities of code large language models (LLMs) continue to expand, their applications across diverse code intelligence domains are rapidly increasing. However, most existing datasets only evaluate limited application domains. To address this gap, we have developed a comprehensive code evaluation dataset FullStack Bench focusing on full-stack programming, which encompasses a wide range of application domains (e.g., basic programming, data analysis, software engineering, mathematics, and machine learning). Besides, to assess multilingual programming capabilities, in FullStack Bench, we design real-world instructions and corresponding unit test cases from 16 widely-used programming languages to reflect real-world usage scenarios rather than simple translations. Moreover, we also release an effective code sandbox execution tool (i.e., SandboxFusion) supporting various programming languages and packages to evaluate the performance of our FullStack Bench efficiently. Comprehensive experimental results on our FullStack Bench demonstrate the necessity and effectiveness of our FullStack Bench and SandboxFusion.", "arxiv_url": "https://arxiv.org/abs/2412.00535", "authors": ["Bytedance-Seed-Foundation-Code-Team", ":", "Yao Cheng", "Jianfeng Chen", "Jie Chen", "Li Chen", "Liyu Chen", "Wentao Chen", "Zhengyu Chen", "Shijie Geng", "Aoyan Li", "Bo Li", "Bowen Li", "Linyi Li", "Boyi Liu", "Jiaheng Liu", "Kaibo Liu", "Qi Liu", "Shukai Liu", "Siyao Liu", "Tianyi Liu", "Tingkai Liu", "Yongfei Liu", "Rui Long", "Jing Mai", "Guanghan Ning", "Z. Y. Peng", "Kai Shen", "Jiahao Su", "Jing Su", "Tao Sun", "Yifan Sun", "Yunzhe Tao", "Guoyin Wang", "Siwei Wang", "Xuwu Wang", "Yite Wang", "Zihan Wang", "Jinxiang Xia", "Liang Xiang", "Xia Xiao", "Yongsheng Xiao", "Chenguang Xi", "Shulin Xin", "Jingjing Xu", "Shikun Xu", "Hongxia Yang", "Jack Yang", "Yingxiang Yang", "Jianbo Yuan", "Jun Zhang", "Yufeng Zhang", "Yuyu Zhang", "Shen Zheng", "He Zhu", "Ming Zhu"], "first_author": "Bytedance-Seed-Foundation-Code-Team", "category": ["Benchmark", "Technical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出了FullStack Bench——一个覆盖多领域、16种编程语言且基于真实使用场景与单元测试的全栈代码评测基准，并发布了支持多语言与依赖管理的执行沙箱SandboxFusion以自动化评估LLM的全栈编程能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2412.00535v6", "published": "2024-11-30", "update_time": "2025-05-12", "download_time": "2025-12-04 23:25:08"}
{"id": "2411.05830", "title": "GitChameleon: Unmasking the Version-Switching Capabilities of Code Generation Models", "abstract": "The rapid evolution of software libraries presents a significant challenge for code generation models, which must adapt to frequent version updates while maintaining compatibility with previous versions. Existing code completion benchmarks often overlook this dynamic aspect, and the one that does consider it relies on static code prediction tasks without execution-based evaluation, offering a limited perspective on a model's practical usability. To address this gap, we introduce \\textbf{\\GitChameleon{}}, a novel, manually curated dataset comprising 116 Python code completion problems, each conditioned on specific library versions and accompanied by executable unit tests. \\GitChameleon{} is designed to rigorously assess the ability of modern large language models (LLMs) to generate version-specific code that is not only syntactically correct but also functionally accurate upon execution. Our comprehensive evaluations reveal that state-of-the-art LLMs struggle with this task; for instance, \\textbf{GPT-4o} achieves a pass@10 of only 39.9\\% (43.7\\% when provided with error feedback), highlighting the complexity of the problem and the limitations of current models. By providing an execution-based benchmark that emphasizes the dynamic nature of code libraries, \\GitChameleon{} serves as a critical tool to advance the development of more adaptable and reliable code generation models. For facilitation for further exploration of version-conditioned code generation, we make our code repository publicly accessible at \\url{https://github.com/NizarIslah/GitChameleon}.", "arxiv_url": "https://arxiv.org/abs/2411.05830", "authors": ["Nizar Islah", "Justine Gehring", "Diganta Misra", "Eilif Muller", "Irina Rish", "Terry Yue Zhuo", "Massimo Caccia"], "first_author": "Nizar Islah", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出了GitChameleon，一个包含116个带可执行单元测试的Python库版本条件代码补全基准，用于评估LLM在库版本变化下生成语法与功能正确代码的能力，并展示了当前模型在该任务上的显著不足。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2411.05830v1", "published": "2024-11-05", "update_time": "2024-11-05", "download_time": "2025-12-04 23:25:36"}
{"id": "2408.06450", "title": "Evaluating Language Models for Efficient Code Generation", "abstract": "We introduce Differential Performance Evaluation (DPE), a framework designed to reliably evaluate Large Language Models (LLMs) for efficient code generation. Traditional coding benchmarks often fail to provide reliable insights into code efficiency, due to their reliance on simplistic test inputs and the absence of effective compound metrics. DPE addresses these issues by focusing on efficiency-demanding programming tasks and establishing an insightful compound metric for performance evaluation. DPE operates in two phases: To curate efficiency datasets, it selects efficiency-demanding tasks from existing coding benchmarks and generates computationally expensive inputs to stress the efficiency of LLM solutions. To assess the code efficiency, DPE profiles the new solution and compares it globally against a set of reference solutions that exhibit distinct efficiency levels, where the matched level defines its efficiency score. As a proof of concept, we use DPE to create EvalPerf, a benchmark with 121 performance-challenging coding tasks. Our comprehensive evaluation draws interesting findings on the efficiency impact of model sizes, instruction tuning, and prompting. For example, while the scaling law fails to account for code efficiency, general instruction tuning benefits both code correctness and efficiency. We also evaluate the evaluation by examining the effectiveness of DPE, showing that EvalPerf is reliable and convenient to use even across platforms.", "arxiv_url": "https://arxiv.org/abs/2408.06450", "authors": ["Jiawei Liu", "Songrun Xie", "Junhao Wang", "Yuxiang Wei", "Yifeng Ding", "Lingming Zhang"], "first_author": "Jiawei Liu", "category": ["Empirical", "Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出差异化性能评估（DPE）框架并构建了 EVALPERF 基准，通过自动合成性能测试输入、性能聚类与参考解比较，可靠地评估和量化大语言模型生成代码的运行效率，并基于此对模型规模、指令微调和提示进行了实证分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2408.06450v1", "published": "2024-08-12", "update_time": "2024-08-12", "download_time": "2025-12-04 23:26:06"}
{"id": "2403.07974", "title": "LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code", "abstract": "Large Language Models (LLMs) applied to code-related applications have emerged as a prominent field, attracting significant interest from both academia and industry. However, as new and improved LLMs are developed, existing evaluation benchmarks (e.g., HumanEval, MBPP) are no longer sufficient for assessing their capabilities. In this work, we propose LiveCodeBench, a comprehensive and contamination-free evaluation of LLMs for code, which continuously collects new problems over time from contests across three competition platforms, namely LeetCode, AtCoder, and CodeForces. Notably, our benchmark also focuses on a broader range of code related capabilities, such as self-repair, code execution, and test output prediction, beyond just code generation. Currently, LiveCodeBench hosts four hundred high-quality coding problems that were published between May 2023 and May 2024. We have evaluated 18 base LLMs and 34 instruction-tuned LLMs on LiveCodeBench. We present empirical findings on contamination, holistic performance comparisons, potential overfitting in existing benchmarks as well as individual model comparisons. We will release all prompts and model completions for further community analysis, along with a general toolkit for adding new scenarios and model", "arxiv_url": "https://arxiv.org/abs/2403.07974", "authors": ["Naman Jain", "King Han", "Alex Gu", "Wen-Ding Li", "Fanjia Yan", "Tianjun Zhang", "Sida Wang", "Armando Solar-Lezama", "Koushik Sen", "Ion Stoica"], "first_author": "Naman Jain", "category": ["Empirical", "Benchmark"], "field": "Coding Assistant", "task": "Holistic Benchmarking & Contamination-free Evaluation", "summary": "本文提出 LiveCodeBench，一个持续更新且防止训练数据污染的综合性代码评测基准，收集 2023–2024 年竞赛题并在代码生成、自我修复、代码执行与测试输出预测等多种场景上对多款 LLM 进行广泛评估与污染分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2403.07974v2", "published": "2024-03-12", "update_time": "2024-06-06", "download_time": "2025-12-04 23:26:44"}
{"id": "2403.08604", "title": "Prompting Large Language Models to Tackle the Full Software Development Lifecycle: A Case Study", "abstract": "Recent advancements in large language models (LLMs) have significantly enhanced their coding capabilities. However, existing benchmarks predominantly focused on simplified or isolated aspects of coding, such as single-file code generation or repository issue debugging, falling short of measuring the full spectrum of challenges raised by real-world programming activities. In this case study, we explore the performance of LLMs across the entire software development lifecycle with DevEval, encompassing stages including software design, environment setup, implementation, acceptance testing, and unit testing. DevEval features four programming languages, multiple domains, high-quality data collection, and carefully designed and verified metrics for each task. Empirical studies show that current LLMs, including GPT-4, fail to solve the challenges presented within DevEval. Our findings offer actionable insights for the future development of LLMs toward real-world programming applications.", "arxiv_url": "https://arxiv.org/abs/2403.08604", "authors": ["Bowen Li", "Wenhan Wu", "Ziwei Tang", "Lin Shi", "John Yang", "Jinyang Li", "Shunyu Yao", "Chen Qian", "Binyuan Hui", "Qicheng Zhang", "Zhiyin Yu", "He Du", "Ping Yang", "Dahua Lin", "Chao Peng", "Kai Chen"], "first_author": "Bowen Li", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "summary": "本文提出DevEval，一个覆盖软件设计、环境搭建、实现与验收/单元测试等软件全生命周期任务的基准，并通过对多种LLM（含GPT‑4系列）的评估发现其在仓库级实现与测试上仍存在显著不足。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2403.08604v3", "published": "2024-03-13", "update_time": "2024-12-14", "download_time": "2025-12-04 23:27:14"}
{"id": "2310.06770", "title": "SWE-bench: Can Language Models Resolve Real-World GitHub Issues?", "abstract": "Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of $2,294$ software engineering problems drawn from real GitHub issues and corresponding pull requests across $12$ popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere $1.96$% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.", "arxiv_url": "https://arxiv.org/abs/2310.06770", "authors": ["Carlos E. Jimenez", "John Yang", "Alexander Wettig", "Shunyu Yao", "Kexin Pei", "Ofir Press", "Karthik Narasimhan"], "first_author": "Carlos E. Jimenez", "category": ["Benchmark", "Empirical", "Technical"], "field": "Coding Assistant", "task": "Code Editing", "summary": "本文提出SWE-bench——一个由真实GitHub issue及其修复PR构成的库级代码编辑基准（2,294个实例），并评估多种大型语言模型、发布训练集SWE-bench-train及微调模型SWE-Llama，结果显示现有模型仅能解决极少数实际问题。", "quality": "High", "conference": "ICLR 2024", "pdf_url": "https://arxiv.org/pdf/2310.06770v3", "published": "2023-10-10", "update_time": "2024-11-11", "download_time": "2025-12-04 23:27:45"}
{"id": "2306.03091", "title": "RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems", "abstract": "Large Language Models (LLMs) have greatly advanced code auto-completion systems, with a potential for substantial productivity enhancements for developers. However, current benchmarks mainly focus on single-file tasks, leaving an assessment gap for more complex, real-world, multi-file programming scenarios. To fill this gap, we introduce RepoBench, a new benchmark specifically designed for evaluating repository-level code auto-completion systems. RepoBench supports both Python and Java and consists of three interconnected evaluation tasks: RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P (Pipeline). Each task respectively measures the system's ability to retrieve the most relevant code snippets from other files as cross-file context, predict the next line of code with cross-file and in-file context, and handle complex tasks that require a combination of both retrieval and next-line prediction. RepoBench aims to facilitate a more complete comparison of performance and encouraging continuous improvement in auto-completion systems. RepoBench is publicly available at https://github.com/Leolty/repobench.", "arxiv_url": "https://arxiv.org/abs/2306.03091", "authors": ["Tianyang Liu", "Canwen Xu", "Julian McAuley"], "first_author": "Tianyang Liu", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出RepoBench，一个面向仓库级别的代码自动补全基准（含检索、下一行补全与端到端流水线三项任务），并提供覆盖Python/Java的数据集与评测分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2306.03091v2", "published": "2023-06-05", "update_time": "2023-10-04", "download_time": "2025-12-04 23:28:06"}
{"id": "2306.14893", "title": "LongCoder: A Long-Range Pre-trained Language Model for Code Completion", "abstract": "In this paper, we introduce a new task for code completion that focuses on handling long code input and propose a sparse Transformer model, called LongCoder, to address this task. LongCoder employs a sliding window mechanism for self-attention and introduces two types of globally accessible tokens - bridge tokens and memory tokens - to improve performance and efficiency. Bridge tokens are inserted throughout the input sequence to aggregate local information and facilitate global interaction, while memory tokens are included to highlight important statements that may be invoked later and need to be memorized, such as package imports and definitions of classes, functions, or structures. We conduct experiments on a newly constructed dataset that contains longer code context and the publicly available CodeXGLUE benchmark. Experimental results demonstrate that LongCoder achieves superior performance on code completion tasks compared to previous models while maintaining comparable efficiency in terms of computational resources during inference. All the codes and data are available at https://github.com/microsoft/CodeBERT.", "arxiv_url": "https://arxiv.org/abs/2306.14893", "authors": ["Daya Guo", "Canwen Xu", "Nan Duan", "Jian Yin", "Julian McAuley"], "first_author": "Daya Guo", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出LongCoder——一种用于长代码上下文的稀疏预训练Transformer，采用滑动窗口注意力并引入桥接（bridge）与记忆（memory）令牌以实现全局信息访问，同时构建并发布长代码补全数据集LCC，实验证明在长/常规代码补全上性能优于既有模型且推理效率相当。", "quality": "High", "conference": "ICML 2023", "pdf_url": "https://arxiv.org/pdf/2306.14893v1", "published": "2023-06-26", "update_time": "2023-06-26", "download_time": "2025-12-04 23:28:30"}
{"id": "2308.10335", "title": "Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability of Large Language Model Code Generation", "abstract": "Recently, the large language models (LLMs) have shown extraordinary ability in understanding natural language and generating programming code. It has been a common practice of software engineers to consult LLMs when encountering coding questions. Although efforts have been made to avoid syntax errors and align the code with the intended semantics, the reliability and robustness of the code generationfrom LLMs have not yet been thoroughly studied. The executable code is not equivalent to the reliable and robust code, especially in the context of real-world software development. The misuse of APIs in the generated code could lead to severe problem, such as resource leaks, program crashes. To make things worse, the users of LLM code generation services are actually the developers that are most vulnerable to these code that seems right -- They are always novice developers that are not familiar with the APIs that LLMs generate code for them. Therefore, they could hardly tell the misuse in the code generated by LLMs, which further facilitates the incorrect code applied in real-world software. Existing code evaluation benchmark and datasets focus on crafting small tasks such as programming questions in coding interviews, which however deviates from the problem that developers would ask LLM for real-world coding help. To fill the missing piece, in this work, we propose a dataset RobustAPI for evaluating the reliability and robustness of code generated by LLMs. We collect 1208 coding questions from StackOverflow on 24 representative Java APIs. We summarize thecommon misuse patterns of these APIs and evaluate them oncurrent popular LLMs. The evaluation results show that evenfor GPT-4, 62% of the generated code contains API misuses,which would cause unexpected consequences if the code isintroduced into real-world software.", "arxiv_url": "https://arxiv.org/abs/2308.10335", "authors": ["Li Zhong", "Zilong Wang"], "first_author": "Li Zhong", "category": ["Benchmark", "Empirical", "Technical"], "field": "Coding Assistant", "task": "Code Prompting", "summary": "本文提出了面向Java API误用的基准数据集ROBUSTAPI及基于AST的API使用检测器，收集了1208个Stack Overflow问题并评估多款主流LLM，发现即使GPT-4也存在大量API误用（约62%）。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2308.10335v5", "published": "2023-08-20", "update_time": "2024-01-27", "download_time": "2025-12-04 23:28:56"}
{"id": "2308.16458", "title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Large Language Models", "abstract": "Pre-trained large language models (LLMs) have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks and to be appropriately specialized to particular domains. Here, we target bioinformatics due to the amount of domain knowledge, algorithms, and data operations this discipline requires. We present BioCoder, a benchmark developed to evaluate LLMs in generating bioinformatics-specific code. BioCoder spans much of the field, covering cross-file dependencies, class declarations, and global variables. It incorporates 1,026 Python functions and 1,243 Java methods extracted from GitHub, along with 253 examples from the Rosalind Project, all pertaining to bioinformatics. Using topic modeling, we show that the overall coverage of the included code is representative of the full spectrum of bioinformatics calculations. BioCoder incorporates a fuzz-testing framework for evaluation. We have applied it to evaluate various models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+, GPT-3.5, and GPT- 4. Furthermore, we fine-tuned one model (StarCoder), demonstrating that our training dataset can enhance the performance on our testing benchmark (by >15% in terms of Pass@K under certain prompt configurations and always >3%). The results highlight two key aspects of successful models: (1) Successful models accommodate a long prompt (> 2,600 tokens) with full context, including functional dependencies. (2) They contain domain-specific knowledge of bioinformatics, beyond just general coding capability. This is evident from the performance gain of GPT-3.5/4 compared to the smaller models on our benchmark (50% vs. up to 25%). Availability and implementation: Code is available at: https://github.com/gersteinlab/biocoder and https://biocoder-benchmark. github.io/.", "arxiv_url": "https://arxiv.org/abs/2308.16458", "authors": ["Xiangru Tang", "Bill Qian", "Rick Gao", "Jiakang Chen", "Xinyun Chen", "Mark Gerstein"], "first_author": "Xiangru Tang", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "BioCoder 提出一个面向生物信息学的大规模代码生成基准（共 2,269 个问题），从 GitHub 与 Rosalind 提取并清洗函数，提供多文件上下文、依赖解析、执行式模糊测试与 Docker 化评测工具，用以评估并分析不同 LLM 在生物信息学代码生成中的性能和领域知识需求。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2308.16458v5", "published": "2023-08-31", "update_time": "2024-05-20", "download_time": "2025-12-04 23:29:24"}
{"id": "2305.01210", "title": "Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation", "abstract": "Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code. Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis. However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus -- a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies. While EvalPlus is general, we extend the test-cases of the popular HumanEval benchmark by 80x to build HumanEval+. Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that test insufficiency can lead to mis-ranking. For example, both WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+, while none of them could on HumanEval. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis, but also opens up a new direction to improve such programming benchmarks through automated testing. We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.", "arxiv_url": "https://arxiv.org/abs/2305.01210", "authors": ["Jiawei Liu", "Chunqiu Steven Xia", "Yuyao Wang", "Lingming Zhang"], "first_author": "Jiawei Liu", "category": ["Technical", "Benchmark"], "field": "Software Testing", "task": "Test Generation", "summary": "本文提出EvalPlus，通过结合LLM驱动的种子输入生成与类型感知变异的大规模测试输入扩充并将HUMANEVAL扩展为HUMANEVAL+，从而更严格地评估LLM生成代码的功能正确性并揭示大量先前未被检测的错误与模型排名变化。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2305.01210v3", "published": "2023-05-02", "update_time": "2023-10-30", "download_time": "2025-12-04 23:29:49"}
{"id": "2305.18584", "title": "Coeditor: Leveraging Contextual Changes for Multi-round Code Auto-editing", "abstract": "Developers often dedicate significant time to maintaining and refactoring existing code. However, most prior work on generative models for code focuses solely on creating new code, overlooking the distinctive needs of editing existing code. In this work, we explore a multi-round code auto-editing setting, aiming to predict edits to a code region based on recent changes within the same codebase. Our model, Coeditor, is a fine-tuned language model specifically designed for code editing tasks. We represent code changes using a line diff format and employ static analysis to form large customized model contexts, ensuring the availability of appropriate information for prediction. We collect a code editing dataset from the commit histories of 1650 open-source Python projects for training and evaluation. In a simplified single-round, single-edit task, Coeditor significantly outperforms GPT-3.5 and SOTA open-source code completion models (bringing exact-match accuracy from 34.7 up to 60.4), demonstrating the benefits of incorporating editing history for code completion. In a multi-round, multi-edit setting, we observe substantial gains by iteratively conditioning on additional user edits. We have open-sourced our code, data, and model weights to encourage future research and have released a VSCode extension powered by our model for interactive IDE usage.", "arxiv_url": "https://arxiv.org/abs/2305.18584", "authors": ["Jiayi Wei", "Greg Durrett", "Isil Dillig"], "first_author": "Jiayi Wei", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Editing", "summary": "该论文提出Coeditor——一种基于repo级行diff编码、扩展CodeT5并采用块稀疏注意力的多轮代码自动编辑模型，同时构建并开源了PYCOMMITS数据集与评测框架，并在多轮编辑任务上显著优于现有基线。", "quality": "High", "conference": "The Twelfth International Conference on Learning Representations 2024", "pdf_url": "https://arxiv.org/pdf/2305.18584v2", "published": "2023-05-29", "update_time": "2024-04-28", "download_time": "2025-12-04 23:30:18"}
{"id": "2211.11501", "title": "DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation", "abstract": "We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from StackOverflow. Second, our automatic evaluation is highly specific (reliable) -- across all Codex-002-predicted solutions that our evaluation accept, only 1.8% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original StackOverflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.", "arxiv_url": "https://arxiv.org/abs/2211.11501", "authors": ["Yuhang Lai", "Chengxi Li", "Yiming Wang", "Tianyi Zhang", "Ruiqi Zhong", "Luke Zettlemoyer", "Scott Wen-tau Yih", "Daniel Fried", "Sida Wang", "Tao Yu"], "first_author": "Yuhang Lai", "category": ["Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出并发布了DS-1000——一个包含1000个来自StackOverflow的真实数据科学Python问题的基准，配备可执行的多准则自动评测并通过扰动问题防止模型记忆化，以可靠评估代码生成模型性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2211.11501v1", "published": "2022-11-18", "update_time": "2022-11-18", "download_time": "2025-12-04 23:30:49"}
{"id": "2208.08227", "title": "MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation", "abstract": "Large language models have demonstrated the ability to generate both natural language and programming language text. Such models open up the possibility of multi-language code generation: could code generation models generalize knowledge from one language to another? Although contemporary code generation models can generate semantically correct Python code, little is known about their abilities with other languages. We propose MultiPL-E, a system for translating unit test-driven code generation benchmarks to new languages. We create the first massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages.   We use MultiPL-E to extend the HumanEval benchmark and MBPP benchmark to 18 languages that encompass a range of programming paradigms and popularity. Using these new parallel benchmarks, we evaluate the multi-language performance of three state-of-the-art code generation models: Codex, CodeGen, and InCoder. We find that Codex matches or even exceeds its performance on Python for several other languages. The range of programming languages represented in MultiPL-E allow us to explore the impact of language frequency and language features on model performance. Finally, the MultiPL-E approach of compiling code generation benchmarks to new programming languages is both scalable and extensible, making it straightforward to evaluate new models, benchmarks, and languages.", "arxiv_url": "https://arxiv.org/abs/2208.08227", "authors": ["Federico Cassano", "John Gouwar", "Daniel Nguyen", "Sydney Nguyen", "Luna Phipps-Costin", "Donald Pinckney", "Ming-Ho Yee", "Yangtian Zi", "Carolyn Jane Anderson", "Molly Q Feldman", "Arjun Guha", "Michael Greenberg", "Abhinav Jangda"], "first_author": "Federico Cassano", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出 MultiPL-E，将 HumanEval 和 MBPP 翻译并扩展到 19 种编程语言，构建首个大规模多语言并行代码生成基准并用其评估多款模型以分析语言特性、流行度、类型系统和提示敏感性对生成性能的影响。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2208.08227v4", "published": "2022-08-17", "update_time": "2022-12-19", "download_time": "2025-12-04 23:31:19"}
{"id": "2108.07732", "title": "Program Synthesis with Large Language Models", "abstract": "This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6 percent of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8 percent accuracy. Going further, we study the model's ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model's initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input.", "arxiv_url": "https://arxiv.org/abs/2108.07732", "authors": ["Jacob Austin", "Augustus Odena", "Maxwell Nye", "Maarten Bosma", "Henryk Michalewski", "David Dohan", "Ellen Jiang", "Carrie Cai", "Michael Terry", "Quoc Le", "Charles Sutton"], "first_author": "Jacob Austin", "category": ["Empirical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文引入了两个用于Python程序合成的新数据集（MBPP和MathQA-Python），并实证评估了不同规模的大型语言模型在few-shot与微调情形下从自然语言描述生成短Python程序的性能、与人类交互以修复代码的能力及模型的语义归属限制。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2108.07732v1", "published": "2021-08-16", "update_time": "2021-08-16", "download_time": "2025-12-04 23:31:49"}
{"id": "2105.09938", "title": "Measuring Coding Challenge Competence With APPS", "abstract": "While programming is one of the most broadly applicable skills in modern society, modern machine learning models still cannot code solutions to basic problems. Despite its importance, there has been surprisingly little work on evaluating code generation, and it can be difficult to accurately assess code generation performance rigorously. To meet this challenge, we introduce APPS, a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code. Similar to how companies assess candidate software developers, we then evaluate models by checking their generated code on test cases. Our benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges. We fine-tune large language models on both GitHub and our training set, and we find that the prevalence of syntax errors is decreasing exponentially as models improve. Recent models such as GPT-Neo can pass approximately 20% of the test cases of introductory problems, so we find that machine learning models are now beginning to learn how to code. As the social significance of automatic code generation increases over the coming years, our benchmark can provide an important measure for tracking advancements.", "arxiv_url": "https://arxiv.org/abs/2105.09938", "authors": ["Dan Hendrycks", "Steven Basart", "Saurav Kadavath", "Mantas Mazeika", "Akul Arora", "Ethan Guo", "Collin Burns", "Samir Puranik", "Horace He", "Dawn Song", "Jacob Steinhardt"], "first_author": "Dan Hendrycks", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "summary": "本文提出并发布了APPS基准——一个包含10000道自然语言编程题及超过130,000条测试用例的Python代码生成数据集，并通过测试展示和分析了多种大规模语言模型在不同难度题目上的生成性能与错误模式。", "quality": "High", "conference": "NeurIPS 2021", "pdf_url": "https://arxiv.org/pdf/2105.09938v3", "published": "2021-05-20", "update_time": "2021-11-08", "download_time": "2025-12-04 23:32:17"}
{"id": "2511.17368", "title": "Exploring Scientific Debt: Harnessing AI for SATD Identification in Scientific Software", "abstract": "Developers often leave behind clues in their code, admitting where it falls short, known as Self-Admitted Technical Debt (SATD). In the world of Scientific Software (SSW), where innovation moves fast and collaboration is key, such debt is not just common but deeply impactful. As research relies on accurate and reproducible results, accumulating SATD can threaten the very foundations of scientific discovery. Yet, despite its significance, the relationship between SATD and SSW remains largely unexplored, leaving a crucial gap in understanding how to manage SATD in this critical domain. This study explores SATD in SSW repositories, comparing SATD in scientific versus general-purpose open-source software and evaluating transformer-based models for SATD identification. We analyzed SATD in 27 scientific and general-purpose repositories across multiple domains and languages. We fine-tuned and compared 10 transformer-based models (100M-7B parameters) on 67,066 labeled code comments. SSW contains 9.25x more Scientific Debt and 4.93x more SATD than general-purpose software due to complex computations, domain constraints, and evolving research needs. Furthermore, our best model outperforms existing ones. This study uncovers how SATD in SSW differs from general software, revealing its impact on quality and scientific validity. By recognizing these challenges, developers and researchers can adopt smarter strategies to manage debt and safeguard the integrity of scientific discovery.", "arxiv_url": "https://arxiv.org/abs/2511.17368", "authors": ["Eric L. Melin", "Ahmed Musa Awon", "Nasir U. Eisty", "Neil A. Ernst", "Shurui Zhou"], "first_author": "Eric L. Melin", "category": ["Technical", "Benchmark", "Experience"], "field": "Technical Debt", "task": "SATD Identification and Classification (including Scientific Debt) in Scientific Software", "summary": "本文构建并扩充了面向科学软件的SATD数据集、微调并比较了10种Transformer/大型语言模型用于自动识别与分类包括新定义的“Scientific Debt”在内的自认技术债务，并发现科学软件中的SATD显著多于通用软件且所提模型优于现有基线。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.17368v1", "published": "2025-11-21", "update_time": "2025-11-21", "download_time": "2025-12-04 23:58:50"}
{"id": "2511.17330", "title": "Agentic Program Verification", "abstract": "Automatically generated code is gaining traction recently, owing to the prevalence of Large Language Models (LLMs). Further, the AlphaProof initiative has demonstrated the possibility of using AI for general mathematical reasoning. Reasoning about computer programs (software) can be accomplished via general mathematical reasoning; however, it tends to be more structured and richer in contexts. This forms an attractive proposition, since then AI agents can be used to reason about voluminous code that gets generated by AI.   In this work, we present a first LLM agent, AutoRocq, for conducting program verification. Unlike past works, which rely on extensive training of LLMs on proof examples, our agent learns on-the-fly and improves the proof via an iterative refinement loop. The iterative improvement of the proof is achieved by the proof agent communicating with the Rocq (formerly Coq) theorem prover to get additional context and feedback. The final result of the iteration is a proof derivation checked by the Rocq theorem prover. In this way, our proof construction involves autonomous collaboration between the proof agent and the theorem prover. This autonomy facilitates the search for proofs and decision-making in deciding on the structure of the proof tree.   Experimental evaluation on SV-COMP benchmarks and on Linux kernel modules shows promising efficacy in achieving automated program verification. As automation in code generation becomes more widespread, we posit that our proof agent can be potentially integrated with AI coding agents to achieve a generate and validate loop, thus moving closer to the vision of trusted automatic programming.", "arxiv_url": "https://arxiv.org/abs/2511.17330", "authors": ["Haoxin Tu", "Huan Zhao", "Yahui Song", "Mehtab Zafar", "Ruijie Meng", "Abhik Roychoudhury"], "first_author": "Haoxin Tu", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "summary": "提出AutoRocq——一个与Rocq/Coq交互的agent式LLM证明代理，通过迭代地从定理证明器获取上下文与反馈并精炼策略，自动生成可被证明器检验的程序证明，在SV-COMP基准与Linux内核模块上显著提高了自动化程序验证效果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.17330v1", "published": "2025-11-21", "update_time": "2025-11-21", "download_time": "2025-12-04 23:59:18"}
{"id": "2511.17262", "title": "SlsReuse: LLM-Powered Serverless Function Reuse", "abstract": "Serverless computing has rapidly emerged as a popular cloud computing paradigm. It enables developers to implement function-level tasks, i.e., serverless functions, without managing infrastructure. While reducing operational overhead, it poses challenges, especially for novice developers. Developing functions from scratch requires adapting to heterogeneous, platform-specific programming styles, making the process time-consuming and error-prone. Function reuse offers a promising solution to address these challenges. However, research on serverless computing lacks a dedicated approach for function recommendation. Existing techniques from traditional contexts remain insufficient due to the semantic gap between task descriptions and heterogeneous function implementations. Advances in large language models (LLMs), pre-trained on large-scale corpora, create opportunities to bridge this gap by aligning developer requirements with function semantics.   This paper presents SlsReuse, the first LLM-powered framework for serverless function reuse. Specifically, SlsReuse first constructs a reusable function repository serving as a foundational knowledge base. Then, it learns unified semantic-enhanced representations of heterogeneous functions through effective prompt engineering with few-shot prompting, capturing implicit code intent, target platforms, programming languages, and cloud services. Finally, given a natural language task query, SlsReuse performs intent-aware discovery combined with a multi-level pruning strategy and similarity matching. We evaluate SlsReuse on a curated dataset of 110 task queries. Built on ChatGPT-4o, one of the most representative LLMs, SlsReuse achieves Recall@10 of 91.20%, exceeding the state-of-the-art baseline by 24.53 percentage points.", "arxiv_url": "https://arxiv.org/abs/2511.17262", "authors": ["Jinfeng Wen", "Yuehan Sun"], "first_author": "Jinfeng Wen", "category": ["Technical", "Benchmark"], "field": "Serverless Computing", "task": "Function Recommendation / Reuse", "summary": "本文提出SlsReuse——一个基于大型语言模型的无服务器函数重用框架，利用少样本提示工程构建语义增强表示并通过多级剪枝与相似度匹配实现意图感知的函数发现与推荐，在自建的500个函数与110个查询数据集上显著优于现有基线。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.17262v1", "published": "2025-11-21", "update_time": "2025-11-21", "download_time": "2025-12-04 23:59:50"}
{"id": "2511.17027", "title": "ReVul-CoT: Towards Effective Software Vulnerability Assessment with Retrieval-Augmented Generation and Chain-of-Thought Prompting", "abstract": "Context: Software Vulnerability Assessment (SVA) plays a vital role in evaluating and ranking vulnerabilities in software systems to ensure their security and reliability. Objective: Although Large Language Models (LLMs) have recently shown remarkable potential in SVA, they still face two major limitations. First, most LLMs are trained on general-purpose corpora and thus lack domain-specific knowledge essential for effective SVA. Second, they tend to rely on shallow pattern matching instead of deep contextual reasoning, making it challenging to fully comprehend complex code semantics and their security implications. Method: To alleviate these limitations, we propose a novel framework ReVul-CoT that integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (COT) prompting. In ReVul-CoT, the RAG module dynamically retrieves contextually relevant information from a constructed local knowledge base that consolidates vulnerability data from authoritative sources (such as NVD and CWE), along with corresponding code snippets and descriptive information. Building on DeepSeek-V3.1, CoT prompting guides the LLM to perform step-by-step reasoning over exploitability, impact scope, and related factors Results: We evaluate ReVul-CoT on a dataset of 12,070 vulnerabilities. Experimental results show that ReVul-CoT outperforms state-of-the-art SVA baselines by 16.50%-42.26% in terms of MCC, and outperforms the best baseline by 10.43%, 15.86%, and 16.50% in Accuracy, F1-score, and MCC, respectively. Our ablation studies further validate the contributions of considering dynamic retrieval, knowledge integration, and CoT-based reasoning. Conclusion: Our results demonstrate that combining RAG with CoT prompting significantly enhances LLM-based SVA and points out promising directions for future research.", "arxiv_url": "https://arxiv.org/abs/2511.17027", "authors": ["Zhijie Chen", "Xiang Chen", "Ziming Li", "Jiacheng Xue", "Chaoyang Gao"], "first_author": "Zhijie Chen", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Assessment (Severity Classification)", "summary": "本文提出ReVul-CoT框架，将检索增强生成(RAG)与链式思维(CoT)提示结合，利用从NVD/CWE构建的本地知识库对12,070条漏洞进行动态检索与逐步推理，从而显著提升基于LLM的CVSS v3漏洞严重性评估效果并公开数据与代码。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.17027v1", "published": "2025-11-21", "update_time": "2025-11-21", "download_time": "2025-12-05 00:00:20"}
{"id": "2511.16858", "title": "Is the Cure Still Worse Than the Disease? Test Overfitting by LLMs in Automated Program Repair", "abstract": "Automated program repair has been shown to be susceptible to generating repaired code that passes on seen tests but fails on a hold-out set of hidden tests. This problem, dubbed test overfitting, has been identified and studied before the rise of large language models. We experimentally study how much test overfitting is still a problem today, using repository-level SWE-bench tasks.", "arxiv_url": "https://arxiv.org/abs/2511.16858", "authors": ["Toufique Ahmed", "Jatin Ganhotra", "Avraham Shinnar", "Martin Hirzel"], "first_author": "Toufique Ahmed", "category": ["Empirical"], "field": "Software Testing", "task": "Test Generation", "summary": "本文在仓库级别的自动程序修复任务上，使用Agentless与e-Otter++并基于Claude-3.7-Sonnet与GPT-4o实证量化了LLM生成补丁对白盒测试的过拟合程度，评估了基于白盒测试的补丁改进对过拟合的影响并探索了缓解手段。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16858v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-05 00:00:46"}
{"id": "2511.16787", "title": "NALA_MAINZ at BLP-2025 Task 2: A Multi-agent Approach for Bangla Instruction to Python Code Generation", "abstract": "This paper presents JGU Mainz's winning system for the BLP-2025 Shared Task on Code Generation from Bangla Instructions. We propose a multi-agent-based pipeline. First, a code-generation agent produces an initial solution from the input instruction. The candidate program is then executed against the provided unit tests (pytest-style, assert-based). Only the failing cases are forwarded to a debugger agent, which reruns the tests, extracts error traces, and, conditioning on the error messages, the current program, and the relevant test cases, generates a revised solution. Using this approach, our submission achieved first place in the shared task with a $Pass@1$ score of 95.4. We also make our code public.", "arxiv_url": "https://arxiv.org/abs/2511.16787", "authors": ["Hossain Shaikh Saadi", "Faria Alam", "Mario Sanz-Guerrero", "Minh Duc Bui", "Manuel Mager", "Katharina von der Wense"], "first_author": "Hossain Shaikh Saadi", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Prompting", "summary": "本文提出了一种用于孟加拉语指令到Python代码生成的多智能体流水线：先由生成代理产生初始程序，运行pytest单元测试仅将失败样例连同错误追踪交给调试代理进行最小化修复，最终在BLP-2025共享任务中以95.4% Pass@1获胜并开源代码。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16787v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-05 00:01:29"}
{"id": "2511.16395", "title": "CorrectHDL: Agentic HDL Design with LLMs Leveraging High-Level Synthesis as Reference", "abstract": "Large Language Models (LLMs) have demonstrated remarkable potential in hardware front-end design using hardware description languages (HDLs). However, their inherent tendency toward hallucination often introduces functional errors into the generated HDL designs. To address this issue, we propose the framework CorrectHDL that leverages high-level synthesis (HLS) results as functional references to correct potential errors in LLM-generated HDL designs.The input to the proposed framework is a C/C++ program that specifies the target circuit's functionality. The program is provided to an LLM to directly generate an HDL design, whose syntax errors are repaired using a Retrieval-Augmented Generation (RAG) mechanism. The functional correctness of the LLM-generated circuit is iteratively improved by comparing its simulated behavior with an HLS reference design produced by conventional HLS tools, which ensures the functional correctness of the result but can lead to suboptimal area and power efficiency. Experimental results demonstrate that circuits generated by the proposed framework achieve significantly better area and power efficiency than conventional HLS designs and approach the quality of human-engineered circuits. Meanwhile, the correctness of the resulting HDL implementation is maintained, highlighting the effectiveness and potential of agentic HDL design leveraging the generative capabilities of LLMs and the rigor of traditional correctness-driven IC design flows.", "arxiv_url": "https://arxiv.org/abs/2511.16395", "authors": ["Kangwei Xu", "Grace Li Zhang", "Ulf Schlichtmann", "Bing Li"], "first_author": "Kangwei Xu", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Translation", "summary": "提出CorrectHDL框架，利用HLS生成的HDL作为功能参考，结合LLM、RAG与差分验证迭代修复LLM生成的HDL，从而实现功能正确且在面积与功耗上显著优于传统HLS的硬件设计。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16395v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-05 09:51:26"}
{"id": "2511.16383", "title": "An Agent-Based Framework for the Automatic Validation of Mathematical Optimization Models", "abstract": "Recently, using Large Language Models (LLMs) to generate optimization models from natural language descriptions has became increasingly popular. However, a major open question is how to validate that the generated models are correct and satisfy the requirements defined in the natural language description. In this work, we propose a novel agent-based method for automatic validation of optimization models that builds upon and extends methods from software testing to address optimization modeling . This method consists of several agents that initially generate a problem-level testing API, then generate tests utilizing this API, and, lastly, generate mutations specific to the optimization model (a well-known software testing technique assessing the fault detection power of the test suite). In this work, we detail this validation framework and show, through experiments, the high quality of validation provided by this agent ensemble in terms of the well-known software testing measure called mutation coverage.", "arxiv_url": "https://arxiv.org/abs/2511.16383", "authors": ["Alexander Zadorojniy", "Segev Wasserkrug", "Eitan Farchi"], "first_author": "Alexander Zadorojniy", "category": ["Technical"], "field": "Software Testing", "task": "Test Generation", "summary": "本文提出一种基于多代理的自动化验证框架，利用LLM生成问题级测试接口、测试用例和针对优化模型的变异体，并通过变异覆盖率评估来自自然语言描述的线性规划模型的正确性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16383v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-05 09:51:51"}
{"id": "2511.16224", "title": "Beyond Code Similarity: Benchmarking the Plausibility, Efficiency, and Complexity of LLM-Generated Smart Contracts", "abstract": "Smart Contracts are critical components of blockchain ecosystems, with Solidity as the dominant programming language. While LLMs excel at general-purpose code generation, the unique constraints of Smart Contracts, such as gas consumption, security, and determinism, raise open questions about the reliability of LLM-generated Solidity code. Existing studies lack a comprehensive evaluation of these critical functional and non-functional properties. We benchmark four state-of-the-art models under zero-shot and retrieval-augmented generation settings across 500 real-world functions. Our multi-faceted assessment employs code similarity metrics, semantic embeddings, automated test execution, gas profiling, and cognitive and cyclomatic complexity analysis. Results show that while LLMs produce code with high semantic similarity to real contracts, their functional correctness is low: only 20% to 26% of zero-shot generations behave identically to ground-truth implementations under testing. The generated code is consistently simpler, with significantly lower complexity and gas consumption, often due to omitted validation logic. Retrieval-Augmented Generation markedly improves performance, boosting functional correctness by up to 45% and yielding more concise and efficient code. Our findings reveal a significant gap between semantic similarity and functional plausibility in LLM-generated Smart Contracts. We conclude that while RAG is a powerful enhancer, achieving robust, production-ready code generation remains a substantial challenge, necessitating careful expert validation.", "arxiv_url": "https://arxiv.org/abs/2511.16224", "authors": ["Francesco Salzano", "Simone Scalabrino", "Rocco Oliveto", "Remo Pareschi"], "first_author": "Francesco Salzano", "category": ["Empirical", "Benchmark"], "field": "Coding Assistant", "task": "Code Prompting", "summary": "本文系统评估了四款大型模型在从自然语言描述生成Solidity智能合约函数时的语义相似度、功能可行性、gas效率与代码复杂度，发现尽管语义相似但功能正确率较低，而检索增强生成（RAG）能显著提升表现并降低gas消耗。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16224v2", "published": "2025-11-20", "update_time": "2025-11-21", "download_time": "2025-12-05 09:56:09"}
{"id": "2511.16123", "title": "Domain-constrained Synthesis of Inconsistent Key Aspects in Textual Vulnerability Descriptions", "abstract": "Textual Vulnerability Descriptions (TVDs) are crucial for security analysts to understand and address software vulnerabilities. However, the key aspect inconsistencies in TVDs from different repositories pose challenges for achieving a comprehensive understanding of vulnerabilities. Existing approaches aim to mitigate inconsistencies by aligning TVDs with external knowledge bases, but they often discard valuable information and fail to synthesize comprehensive representations. In this paper, we propose a domain-constrained LLM-based synthesis framework for unifying key aspects of TVDs. Our framework consists of three stages: 1) Extraction, guided by rule-based templates to ensure all critical details are captured; 2) Self-evaluation, using domain-specific anchor words to assess semantic variability across sources; and 3) Fusion, leveraging information entropy to reconcile inconsistencies and prioritize relevant details. This framework improves synthesis performance, increasing the F1 score for key aspect augmentation from 0.82 to 0.87, while enhancing comprehension and efficiency by over 30\\%. We further develop Digest Labels, a practical tool for visualizing TVDs, which human evaluations show significantly boosts usability.", "arxiv_url": "https://arxiv.org/abs/2511.16123", "authors": ["Linyi Han", "Shidong Pan", "Zhenchang Xing", "Sofonias Yitagesu", "Xiaowang Zhang", "Zhiyong Feng", "Jiamou Sun", "Qing Huang"], "first_author": "Linyi Han", "category": ["Technical"], "field": "Vulnerability Analysis & Documentation", "task": "Textual Vulnerability Description Synthesis", "summary": "本文提出一种域约束的大模型合成框架（包含提取、基于领域锚词的自评估和基于信息熵的融合）及 Digest Labels 可视化工具，用以统一并保全来自不同漏洞库中不一致的关键要素，从而提高漏洞描述的完整性与分析效率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16123v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-05 09:56:31"}
{"id": "2511.16092", "title": "The Future of Development Environments with AI Foundation Models: NII Shonan Meeting 222 Report", "abstract": "Generative Artificial Intelligence (GenAI) models are achieving remarkable performance in various tasks, including code generation, testing, code review, and program repair. The ability to increase the level of abstraction away from writing code has the potential to change the Human-AI interaction within the integrated development environment (IDE). To explore the impact of GenAI on IDEs, 33 experts from the Software Engineering, Artificial Intelligence, and Human-Computer Interaction domains gathered to discuss challenges and opportunities at Shonan Meeting 222. This is the report", "arxiv_url": "https://arxiv.org/abs/2511.16092", "authors": ["Xing Hu", "Raula Gaikovina Kula", "Christoph Treude"], "first_author": "Xing Hu", "category": ["Survey"], "field": "Requirements & Design", "task": "Analysis", "summary": "本文为一次Shonan会议报告，综述并讨论了AI基础模型对软件开发环境（IDE）的影响、可能带来的挑战与风险，并提出若干未来研究问题与方向。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16092v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-05 09:56:54"}
{"id": "2511.16708", "title": "Multi-Agent Code Verification via Information Theory", "abstract": "LLMs generate buggy code: 29.6% of SWE-bench solved patches fail, 62% of BaxBench solutions have vulnerabilities, and existing   tools only catch 65% of bugs with 35% false positives. We built CodeX-Verify, a multi-agent system that uses four specialized   agents to detect different types of bugs. We prove mathematically that combining agents with different detection patterns   finds more bugs than any single agent when the agents look for different problems, using submodularity of mutual information   under conditional independence. Measuring agent correlation of rho = 0.05 to 0.25 confirms they detect different bugs. Testing   on 99 code samples with verified labels shows our system catches 76.1% of bugs, matching the best existing method (Meta   Prompt Testing: 75%) while running faster and without test execution. We tested all 15 agent combinations and found that using   multiple agents improves accuracy by 39.7 percentage points (from 32.8% to 72.4%) compared to single agents, with diminishing   returns of +14.9pp, +13.5pp, and +11.2pp for agents 2, 3, and 4, validating our theoretical model. The best two-agent   combination (Correctness + Performance) reaches 79.3% accuracy. Testing on 300 real patches from Claude Sonnet 4.5 runs in   under 200ms per sample, making this practical for production use.", "arxiv_url": "https://arxiv.org/abs/2511.16708", "authors": ["Shreshth Rajan"], "first_author": "Shreshth Rajan", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Defect Prediction", "summary": "本文提出CodeX-Verify，一种由正确性、 安全、 性能与风格四个专用智能体组成的多智能体静态代码验证系统，并基于条件独立下互信息的次模性质证明组合智能体可提高缺陷检测率，实验在99个有验证标签样本上达到76.1%检出率并开源数据集。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16708v2", "published": "2025-11-20", "update_time": "2025-12-02", "download_time": "2025-12-05 09:57:21"}
{"id": "2511.16005", "title": "InfCode-C++: Intent-Guided Semantic Retrieval and AST-Structured Search for C++ Issue Resolution", "abstract": "Large language model (LLM) agents have recently shown strong performance on repository-level issue resolution, but existing systems are almost exclusively designed for Python and rely heavily on lexical retrieval and shallow code navigation. These approaches transfer poorly to C++ projects, where overloaded identifiers, nested namespaces, template instantiations, and deep control-flow structures make context retrieval and fault localization substantially more difficult. As a result, state-of-the-art Python-oriented agents show a drastic performance drop on the C++ subset of MultiSWE-bench. We introduce INFCODE-C++, the first C++-aware autonomous system for end-to-end issue resolution. The system combines two complementary retrieval mechanisms -- semantic code-intent retrieval and deterministic AST-structured querying -- to construct accurate, language-aware context for repair.These components enable precise localization and robust patch synthesis in large, statically typed C++ repositories. Evaluated on the \\texttt{MultiSWE-bench-CPP} benchmark, INFCODE-C++ achieves a resolution rate of 25.58\\%, outperforming the strongest prior agent by 10.85 percentage points and more than doubling the performance of MSWE-agent. Ablation and behavioral studies further demonstrate the critical role of semantic retrieval, structural analysis, and accurate reproduction in C++ issue resolution. INFCODE-C++ highlights the need for language-aware reasoning in multi-language software agents and establishes a foundation for future research on scalable, LLM-driven repair for complex, statically typed ecosystems.", "arxiv_url": "https://arxiv.org/abs/2511.16005", "authors": ["Qingao Dong", "Mengfei Wang", "Hengzhi Zhang", "Zhichao Li", "Yuan Yuan", "Mu Li", "Xiang Gao", "Hailong Sun", "Chunming Hu", "Weifeng Lv"], "first_author": "Qingao Dong", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Bug Repair", "summary": "本文提出InfCode-C++，结合意图引导的语义检索与基于AST的结构化查询，为复杂C++代码库构建语言感知的上下文检索与修复流程，在MultiSWE-bench-CPP上将问题解决率提升至25.58%，显著优于现有代理。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16005v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-05 09:57:47"}
{"id": "2511.16004", "title": "InfCode: Adversarial Iterative Refinement of Tests and Patches for Reliable Software Issue Resolution", "abstract": "Large language models have advanced software engineering automation, yet resolving real-world software issues remains difficult because it requires repository-level reasoning, accurate diagnostics, and strong verification signals. Existing agent-based and pipeline-based methods often rely on insufficient tests, which can lead to patches that satisfy verification but fail to fix the underlying defect. We present InfCode, an adversarial multi-agent framework for automated repository-level issue resolution. InfCode iteratively refines both tests and patches through adversarial interaction between a Test Patch Generator and a Code Patch Generator, while a Selector agent identifies the most reliable fix. The framework runs inside a containerized environment that supports realistic repository inspection, modification, and validation. Experiments on SWE-bench Lite and SWE-bench Verified using models such as DeepSeek-V3 and Claude 4.5 Sonnet show that InfCode consistently outperforms strong baselines. It achieves 79.4% performance on SWE-bench Verified, establishing a new state-of-the-art. We have released InfCode as an open-source project at https://github.com/Tokfinity/InfCode.", "arxiv_url": "https://arxiv.org/abs/2511.16004", "authors": ["KeFan Li", "Mengfei Wang", "Hengzhi Zhang", "Zhichao Li", "Yuan Yuan", "Mu Li", "Xiang Gao", "Hailong Sun", "Chunming Hu", "Weifeng Lv"], "first_author": "KeFan Li", "category": ["Technical"], "field": "Quality Management", "task": "Bug Repair", "summary": "本文提出 InfCode —— 一个在容器化环境中通过测试生成器与代码补丁生成器的对抗迭代、多代理协作来强化仓库级别缺陷定位与修复的框架，并在 SWE-bench 验证集上取得了新的 SOTA（79.4%）。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.16004v1", "published": "2025-11-20", "update_time": "2025-11-20", "download_time": "2025-12-05 09:58:13"}
{"id": "2511.15817", "title": "A Causal Perspective on Measuring, Explaining and Mitigating Smells in LLM-Generated Code", "abstract": "Recent advances in large language models (LLMs) have accelerated their adoption in software engineering contexts. However, concerns persist about the structural quality of the code they produce. In particular, LLMs often replicate poor coding practices, introducing code smells (i.e., patterns that hinder readability, maintainability, or design integrity). Although prior research has examined the detection or repair of smells, we still lack a clear understanding of how and when these issues emerge in generated code.   This paper addresses this gap by systematically measuring, explaining and mitigating smell propensity in LLM-generated code. We build on the Propensity Smelly Score (PSC), a probabilistic metric that estimates the likelihood of generating particular smell types, and establish its robustness as a signal of structural quality. Using PSC as an instrument for causal analysis, we identify how generation strategy, model size, model architecture and prompt formulation shape the structural properties of generated code. Our findings show that prompt design and architectural choices play a decisive role in smell propensity and motivate practical mitigation strategies that reduce its occurrence. A user study further demonstrates that PSC helps developers interpret model behavior and assess code quality, providing evidence that smell propensity signals can support human judgement. Taken together, our work lays the groundwork for integrating quality-aware assessments into the evaluation and deployment of LLMs for code.", "arxiv_url": "https://arxiv.org/abs/2511.15817", "authors": ["Alejandro Velasco", "Daniel Rodriguez-Cardenas", "Dipin Khati", "David N. Palacio", "Luftar Rahman Alif", "Denys Poshyvanyk"], "first_author": "Alejandro Velasco", "category": ["Empirical", "Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Prompting", "summary": "本文从因果视角验证并应用PSC指标，系统测量、解释并缓解LLM生成代码中的code smells，量化解码策略、模型架构与提示设计的影响并发布基准与数据集。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.15817v2", "published": "2025-11-19", "update_time": "2025-11-21", "download_time": "2025-12-05 09:58:39"}
{"id": "2511.15293", "title": "A Viable Paradigm of Software Automation: Iterative End-to-End Automated Software Development", "abstract": "Software development automation is a long-term goal in software engineering. With the development of artificial intelligence (AI), more and more researchers are exploring approaches to software automation. They view AI systems as tools or assistants in software development, still requiring significant human involvement. Another initiative is ``vibe coding'', where AI systems write and repeatedly revise most (or even all) of the code. We foresee these two development paths will converge towards the same destination: AI systems participate in throughout the software development lifecycle, expanding boundaries of full-stack software development. In this paper, we present a vision of an iterative end-to-end automated software development paradigm AutoSW. It operates in an analyze-plan-implement-deliver loop, where AI systems as human partners become first-class actors, translating human intentions expressed in natural language into executable software. We explore a lightweight prototype across the paradigm and initially execute various representative cases. The results indicate that AutoSW can successfully deliver executable software, providing a feasible direction for truly end-to-end automated software development.", "arxiv_url": "https://arxiv.org/abs/2511.15293", "authors": ["Jia Li", "Zhi Jin", "Huangzhao Zhang", "Kechi Zhang", "Jiaru Qian", "Tiankuo Zhao"], "first_author": "Jia Li", "category": ["Technical"], "field": "Requirements & Design", "task": "Elicitation", "summary": "本文提出了名为AutoSW的迭代端到端自动化软件开发范式，基于协同代理在分析-规划-实现-交付的循环中将AI作为第一类参与者，从自然语言意图出发自动化产生可测试、可部署的软件并支持可追溯的演化。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.15293v2", "published": "2025-11-19", "update_time": "2025-11-23", "download_time": "2025-12-05 09:59:31"}
{"id": "2511.15757", "title": "Rethinking Kernel Program Repair: Benchmarking and Enhancing LLMs with RGym", "abstract": "Large Language Models (LLMs) have revolutionized automated program repair (APR) but current benchmarks like SWE-Bench predominantly focus on userspace applications and overlook the complexities of kernel-space debugging and repair. The Linux kernel poses unique challenges due to its monolithic structure, concurrency, and low-level hardware interactions. Prior efforts such as KGym and CrashFixer have highlighted the difficulty of APR in this domain, reporting low success rates or relying on costly and complex pipelines and pricey cloud infrastructure. In this work, we introduce RGym, a lightweight, platform-agnostic APR evaluation framework for the Linux kernel designed to operate on local commodity hardware. Built on RGym, we propose a simple yet effective APR pipeline leveraging specialized localization techniques (e.g., call stacks and blamed commits) to overcome the unrealistic usage of oracles in KGym. We test on a filtered and verified dataset of 143 bugs. Our method achieves up to a 43.36% pass rate with GPT-5 Thinking while maintaining a cost of under $0.20 per bug. We further conduct an ablation study to analyze contributions from our proposed localization strategy, prompt structure, and model choice, and demonstrate that feedback-based retries can significantly enhance success rates.", "arxiv_url": "https://arxiv.org/abs/2511.15757", "authors": ["Kareem Shehada", "Yifan Wu", "Wyatt D. Feng", "Adithya Iyer", "Gryphon Kumfert", "Yangruibo Ding", "Zhiyun Qian"], "first_author": "Kareem Shehada", "category": ["Technical", "Benchmark"], "field": "Quality Management", "task": "Bug Repair", "summary": "本文提出 RGym——一个可在本地运行的轻量级内核APR评估框架，发布并验证了包含143个KASAN内核漏洞的数据集，并设计了基于bug-inducing commit与调用栈的现实定位策略与简单高效的LLM补丁流水线，实现在低成本下显著提升修复通过率并进行了消融分析。", "quality": "High", "conference": "NeurIPS 2025 Workshop: Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent Abilities, and Scaling 2025", "pdf_url": "https://arxiv.org/pdf/2511.15757v1", "published": "2025-11-19", "update_time": "2025-11-19", "download_time": "2025-12-05 09:59:57"}
{"id": "2511.15168", "title": "Finetuning LLMs for Automatic Form Interaction on Web-Browser in Selenium Testing Framework", "abstract": "Automated web application testing is a critical component of modern software development, with frameworks like Selenium widely adopted for validating functionality through browser automation. Among the essential aspects of such testing is the ability to interact with and validate web forms, a task that requires syntactically correct, executable scripts with high coverage of input fields. Despite its importance, this task remains underexplored in the context of large language models (LLMs), and no public benchmark or dataset exists to evaluate LLMs on form interaction generation systematically. This paper introduces a novel method for training LLMs to generate high-quality test cases in Selenium, specifically targeting form interaction testing. We curate both synthetic and human-annotated datasets for training and evaluation, covering diverse real-world forms and testing scenarios. We define clear metrics for syntax correctness, script executability, and input field coverage. Our empirical study demonstrates that our approach significantly outperforms strong baselines, including GPT-4o and other popular LLMs, across all evaluation metrics. Our work lays the groundwork for future research on LLM-based web testing and provides resources to support ongoing progress in this area.", "arxiv_url": "https://arxiv.org/abs/2511.15168", "authors": ["Nguyen-Khang Le", "Hiep Nguyen", "Ngoc-Minh Nguyen", "Son T. Luu", "Trung Vo", "Quan Minh Bui", "Shoshin Nomura", "Le-Minh Nguyen"], "first_author": "Nguyen-Khang Le", "category": ["Technical", "Benchmark"], "field": "Software Testing", "task": "Test Generation", "summary": "本文提出一种针对Selenium表单交互的LLM微调方法并构建并发布首个表单交互测试数据集，通过合成与人工标注数据训练模型生成语法正确、可执行且覆盖率高的Selenium测试脚本，实验证明在语法正确性、脚本可执行性和输入字段覆盖率上优于多种强基线（含GPT‑4o）。", "quality": "High", "conference": "Proceedings of KSE 2025", "pdf_url": "https://arxiv.org/pdf/2511.15168v2", "published": "2025-11-19", "update_time": "2025-11-20", "download_time": "2025-12-05 10:00:56"}
{"id": "2511.15755", "title": "Multi-Agent LLM Orchestration Achieves Deterministic, High-Quality Decision Support for Incident Response", "abstract": "Large language models (LLMs) promise to accelerate incident response in production systems, yet single-agent approaches generate vague, unusable recommendations. We present MyAntFarm.ai, a reproducible containerized framework demonstrating that multi-agent orchestration fundamentally transforms LLM-based incident response quality. Through 348 controlled trials comparing single-agent copilot versus multi-agent systems on identical incident scenarios, we find that multi-agent orchestration achieves 100% actionable recommendation rate versus 1.7% for single-agent approaches, an 80 times improvement in action specificity and 140 times improvement in solution correctness. Critically, multi-agent systems exhibit zero quality variance across all trials, enabling production SLA commitments impossible with inconsistent single-agent outputs. Both architectures achieve similar comprehension latency (approx.40s), establishing that the architectural value lies in deterministic quality, not speed. We introduce Decision Quality (DQ), a novel metric capturing validity, specificity, and correctness properties essential for operational deployment that existing LLM metrics do not address. These findings reframe multi-agent orchestration from a performance optimization to a production-readiness requirement for LLM-based incident response. All code, Docker configurations, and trial data are publicly available for reproduction.", "arxiv_url": "https://arxiv.org/abs/2511.15755", "authors": ["Philip Drammeh"], "first_author": "Philip Drammeh", "category": ["Technical", "Empirical"], "field": "AIOps", "task": "Incident Response / Decision Support", "summary": "本文通过可复现的容器化框架 MyAntFarm.ai，在348次受控试验中比较单体 LLM 与分工多智能体编排，提出多维决策质量（Decision Quality）指标并证明多智能体在可执行性、具体性与正确性上显著优于单体方案，使其在事故响应场景中实现确定性、高质量的决策支持。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.15755v1", "published": "2025-11-19", "update_time": "2025-11-19", "download_time": "2025-12-05 10:01:17"}
{"id": "2511.19427", "title": "Prompt Less, Smile More: MTP with Semantic Engineering in Lieu of Prompt Engineering", "abstract": "AI-Integrated programming is emerging as a foundational paradigm for building intelligent systems with large language models (LLMs). Recent approaches such as Meaning Typed Programming (MTP) automate prompt generation by leveraging the semantics already present in code. However, many real-world applications depend on contextual cues, developer intent, and domain-specific reasoning that extend beyond what static code semantics alone can express. To address this limitation, we introduce Semantic Engineering, a lightweight method for enriching program semantics so that LLM-based systems can more accurately reflect developer intent without requiring full manual prompt design. We present Semantic Context Annotations (SemTexts), a language-level mechanism that allows developers to embed natural-language context directly into program constructs. Integrated into the Jac programming language, Semantic Engineering extends MTP to incorporate these enriched semantics during prompt generation. We further introduce a benchmark suite designed to reflect realistic AI-Integrated application scenarios. Our evaluation shows that Semantic Engineering substantially improves prompt fidelity, achieving performance comparable to Prompt Engineering while requiring significantly less developer effort.", "arxiv_url": "https://arxiv.org/abs/2511.19427", "authors": ["Jayanaka L. Dantanarayana", "Savini Kashmira", "Thakee Nathees", "Zichen Zhang", "Krisztian Flautner", "Lingjia Tang", "Jason Mars"], "first_author": "Jayanaka L. Dantanarayana", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Prompting", "summary": "本文提出了语义工程并引入语言级的语义上下文注释SemText，将自然语言意图绑定到程序构造中以扩展MTP的中间表示，从而在显著减少开发者手工提示成本的同时将自动生成的提示性能提升到接近或优于传统Prompt Engineering，并通过在Jac中实现及一套更贴近实际场景的基准评测验证其有效性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.19427v1", "published": "2025-11-24", "update_time": "2025-11-24", "download_time": "2025-12-05 10:01:39"}
{"id": "2511.19132", "title": "LLMs-Powered Real-Time Fault Injection: An Approach Toward Intelligent Fault Test Cases Generation", "abstract": "A well-known testing method for the safety evaluation and real-time validation of automotive software systems (ASSs) is Fault Injection (FI). In accordance with the ISO 26262 standard, the faults are introduced artificially for the purpose of analyzing the safety properties and verifying the safety mechanisms during the development phase. However, the current FI method and tools have a significant limitation in that they require manual identification of FI attributes, including fault type, location and time. The more complex the system, the more expensive, time-consuming and labour-intensive the process. To address the aforementioned challenge, a novel Large Language Models (LLMs)-assisted fault test cases (TCs) generation approach for utilization during real-time FI tests is proposed in this paper. To this end, considering the representativeness and coverage criteria, the applicability of various LLMs to create fault TCs from the functional safety requirements (FSRs) has been investigated. Through the validation results of LLMs, the superiority of the proposed approach utilizing gpt-4o in comparison to other state-of-the-art models has been demonstrated. Specifically, the proposed approach exhibits high performance in terms of FSRs classification and fault TCs generation with F1-score of 88% and 97.5%, respectively. To illustrate the proposed approach, the generated fault TCs were executed in real time on a hardware-in-the-loop system, where a high-fidelity automotive system model served as a case study. This novel approach offers a means of optimizing the real-time testing process, thereby reducing costs while simultaneously enhancing the safety properties of complex safety-critical ASSs.", "arxiv_url": "https://arxiv.org/abs/2511.19132", "authors": ["Mohammad Abboush", "Ahmad Hatahet", "Andreas Rausch"], "first_author": "Mohammad Abboush", "category": ["Technical", "Empirical"], "field": "Software Testing", "task": "Test Generation", "summary": "本文提出一种基于大型语言模型（如GPT‑4o）的实时故障注入测试用例生成方法，能够从功能安全需求自动进行传感器/执行器分类并生成单点及并发故障测试用例，并在硬件在环（HIL）仿真中验证，取得分类F1=88%与测试用例生成F1=97.5%的优异结果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.19132v1", "published": "2025-11-24", "update_time": "2025-11-24", "download_time": "2025-12-05 10:02:07"}
{"id": "2511.19422", "title": "SLMFix: Leveraging Small Language Models for Error Fixing with Reinforcement Learning", "abstract": "Recent advancements in large language models (LLMs) have shown very impressive capabilities in code generation across many programming languages. However, even state-of-the-art LLMs generate programs that contains syntactic errors and fail to complete the given tasks, especially for low-resource programming languages (LRPLs). In addition, high training cost makes finetuning LLMs unaffordable with constrained computational resources, further undermining the effectiveness of LLMs for code generation. In this work, we propose SLMFix, a novel code generation pipeline that leverages a small language model (SLM) finetuned using reinforcement learning (RL) techniques to fix syntactic errors in LLM-generated programs to improve the quality of LLM-generated programs for domain-specific languages (DSLs). In specific, we applied RL on the SLM for the program repair task using a reward calculated using both a static validator and a static semantic similarity metric. Our experimental results demonstrate the effectiveness and generalizability of our approach across multiple DSLs, achieving more than 95% pass rate on the static validator. Notably, SLMFix brings substantial improvement to the base model and outperforms supervised finetuning approach even for 7B models on a LRPL, showing the potential of our approach as an alternative to traditional finetuning approaches.", "arxiv_url": "https://arxiv.org/abs/2511.19422", "authors": ["David Jiahao Fu", "Aryan Gupta", "Aaron Councilman", "David Grove", "Yu-Xiong Wang", "Vikram Adve"], "first_author": "David Jiahao Fu", "category": ["Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Editing", "summary": "本文提出SLMFix：在不微调大型模型的前提下，利用经强化学习微调的小型语言模型修复LLM生成的DSL代码中的静态错误，并构建了Ansible数据集以验证方法在低资源语言上的有效性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.19422v1", "published": "2025-11-24", "update_time": "2025-11-24", "download_time": "2025-12-05 10:02:30"}
{"id": "2511.19130", "title": "Can LLMs Recover Program Semantics? A Systematic Evaluation with Symbolic Execution", "abstract": "Obfuscation poses a persistent challenge for software engineering tasks such as program comprehension, maintenance, testing, and vulnerability detection. While compiler optimizations and third-party code often introduce transformations that obscure program intent, existing analysis tools and large language models (LLMs) struggle to recover the original semantics. In this work, we investigate whether LLMs, when fine-tuned with symbolic execution artifacts, can effectively deobfuscate programs and restore analyzability. We construct a benchmark by applying four widely studied transformations-control-flow flattening, opaque predicates, arithmetic encoding, and branch encoding-across diverse C programs from TUM Obfuscation Benchmarks, the LLVM test suite, and algorithmic repositories. We then compare three state-of-the-art LLMs under two training configurations: baseline fine-tuning on obfuscated/original code pairs, and enhanced fine-tuning with additional KLEE artifacts such as SMT constraints, path statistics, and test cases. Our evaluation examines syntactic correctness (compilation success), semantic fidelity (behavioral equivalence under symbolic execution), and code quality (readability and structure). Results show that GPT-4.1-mini achieves the strongest deobfuscation overall, and that incorporating KLEE artifacts consistently improves semantic preservation and compilation success across models. These findings highlight deobfuscation as a broader software engineering concern, demonstrating that combining LLMs with symbolic execution can strengthen automated testing, static analysis, and program comprehension in the presence of obfuscation.", "arxiv_url": "https://arxiv.org/abs/2511.19130", "authors": ["Rong Feng", "Suman Saha"], "first_author": "Rong Feng", "category": ["Technical", "Benchmark", "Empirical"], "field": "Program Analysis & Reverse Engineering", "task": "Deobfuscation / Program Semantics Recovery", "summary": "本文构建包含四种常见混淆变换的数据集，并提出将 KLEE 符号执行产生的约束、路径统计和测试用例等工件与 LLM 微调相结合的混合方法以恢复被混淆程序的可编译性与语义一致性，实验证明该方法显著提升了语法正确性、语义保真度和代码可读性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.19130v1", "published": "2025-11-24", "update_time": "2025-11-24", "download_time": "2025-12-05 10:02:56"}
{"id": "2511.20403", "title": "LLMs for Automated Unit Test Generation and Assessment in Java: The AgoneTest Framework", "abstract": "Unit testing is an essential but resource-intensive step in software development, ensuring individual code units function correctly. This paper introduces AgoneTest, an automated evaluation framework for Large Language Model-generated (LLM) unit tests in Java. AgoneTest does not aim to propose a novel test generation algorithm; rather, it supports researchers and developers in comparing different LLMs and prompting strategies through a standardized end-to-end evaluation pipeline under realistic conditions. We introduce the Classes2Test dataset, which maps Java classes under test to their corresponding test classes, and a framework that integrates advanced evaluation metrics, such as mutation score and test smells, for a comprehensive assessment. Experimental results show that, for the subset of tests that compile, LLM-generated tests can match or exceed human-written tests in terms of coverage and defect detection. Our findings also demonstrate that enhanced prompting strategies contribute to test quality. AgoneTest clarifies the potential of LLMs in software testing and offers insights for future improvements in model design, prompt engineering, and testing practices.", "arxiv_url": "https://arxiv.org/abs/2511.20403", "authors": ["Andrea Lops", "Fedelucio Narducci", "Azzurra Ragone", "Michelantonio Trizio", "Claudio Bartolini"], "first_author": "Andrea Lops", "category": ["Technical", "Benchmark", "Empirical"], "field": "Software Testing", "task": "Test Generation", "summary": "本文提出AGONETEST——一个用于端到端自动化评估LLM生成的Java类级别单元测试的框架，并发布了映射类到测试类的CLASSES2TEST数据集，结合覆盖率、变异分数和测试味等指标对生成测试进行综合评估，实验证明在可编译测试子集上LLM生成的测试在覆盖率和缺陷检测能力上可与人工测试媲美。", "quality": "High", "conference": "ASE 2025", "pdf_url": "https://arxiv.org/pdf/2511.20403v2", "published": "2025-11-25", "update_time": "2025-11-26", "download_time": "2025-12-05 10:03:31"}
{"id": "2511.21382", "title": "Large Language Models for Unit Test Generation: Achievements, Challenges, and the Road Ahead", "abstract": "Unit testing is an essential yet laborious technique for verifying software and mitigating regression risks. Although classic automated methods effectively explore program structures, they often lack the semantic information required to produce realistic inputs and assertions. Large Language Models (LLMs) address this limitation by utilizing by leveraging their data-driven knowledge of code semantics and programming patterns. To analyze the state of the art in this domain, we conducted a systematic literature review of 115 publications published between May 2021 and August 2025. We propose a unified taxonomy based on the unit test generation lifecycle that treats LLMs as stochastic generators requiring systematic engineering constraints. This framework analyzes the literature regarding core generative strategies and a set of enhancement techniques ranging from pre-generation context enrichment to post-generation quality assurance. Our analysis reveals that prompt engineering has emerged as the dominant utilization strategy and accounts for 89% of the studies due to its flexibility. We find that iterative validation and repair loops have become the standard mechanism to ensure robust usability and lead to significant improvements in compilation and execution pass rates. However, critical challenges remain regarding the weak fault detection capabilities of generated tests and the lack of standardized evaluation benchmarks. We conclude with a roadmap for future research that emphasizes the progression towards autonomous testing agents and hybrid systems combining LLMs with traditional software engineering tools. This survey provides researchers and practitioners with a comprehensive perspective on converting the potential of LLMs into industrial-grade testing solutions.", "arxiv_url": "https://arxiv.org/abs/2511.21382", "authors": ["Bei Chu", "Yang Feng", "Kui Liu", "Zifan Nan", "Zhaoqiang Guo", "Baowen Xu"], "first_author": "Bei Chu", "category": ["Survey"], "field": "Software Testing", "task": "Test Generation", "summary": "本文对2021–2025年间115篇关于利用大语言模型生成单元测试的研究进行了系统综述，提出基于生成生命周期的统一分类，梳理了核心策略与增强技术，指出了评估基准不足与缺陷检测能力薄弱等关键挑战并给出未来路线图。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.21382v1", "published": "2025-11-26", "update_time": "2025-11-26", "download_time": "2025-12-05 10:03:51"}
{"id": "2511.21380", "title": "Multi-Agent Systems for Dataset Adaptation in Software Engineering: Capabilities, Limitations, and Future Directions", "abstract": "Automating the adaptation of software engineering (SE) research artifacts across datasets is essential for scalability and reproducibility, yet it remains largely unstudied. Recent advances in large language model (LLM)-based multi-agent systems, such as GitHub Copilot's agent mode, promise to automate complex development workflows through coordinated reasoning, code generation, and tool interaction. This paper presents the first empirical study on how state-of-the-art multi-agent systems perform in dataset adaptation tasks. We evaluate Copilot, backed by GPT-4.1 and Claude Sonnet 4, on adapting SE research artifacts from benchmark repositories including ROCODE and LogHub2.0. Through a five-stage evaluation pipeline (file comprehension, code editing, command generation, validation, and final execution), we measure success rates, analyze failure patterns, and assess prompt-based interventions designed to enhance agent performance. Results show that current systems can identify key files and generate partial adaptations but rarely produce functionally correct implementations. Prompt-level interventions, especially providing execution error messages and reference code, substantially improve structural similarity to ground truth (from 7.25% to 67.14%), highlighting the importance of contextual and feedback-driven guidance. Our findings reveal both the promise and limitations of today's multi-agent LLM systems for dataset adaptation, and suggest concrete directions for building more reliable, self-correcting agents in future SE research.", "arxiv_url": "https://arxiv.org/abs/2511.21380", "authors": ["Jingyi Chen", "Xiaoyan Guo", "Songqiang Chen", "Shing-Chi Cheung", "Jiasi Shen"], "first_author": "Jingyi Chen", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Editing", "summary": "本文首次对基于大型语言模型的多智能体系统（以GitHub Copilot agent为代表）在软件工程研究工件的数据集适配任务上进行了系统实证评估，提出五阶段评估流程并发现当前系统能识别关键文件并生成部分适配代码但很少产生功能正确的实现，且提供执行错误信息与参考代码等提示能显著提升生成代码的结构相似度。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.21380v1", "published": "2025-11-26", "update_time": "2025-11-26", "download_time": "2025-12-05 10:04:12"}
{"id": "2511.21022", "title": "Lightweight Model Editing for LLMs to Correct Deprecated API Recommendations", "abstract": "Pre-trained or fine-tuned on large code corpora, Large Language Models (LLMs) have demonstrated strong performance in code completion tasks. However, their embedded knowledge is constrained by the timeliness of training data, which often includes code using deprecated APIs. Consequently, LLMs frequently generate deprecated APIs that will no longer be supported in future versions of third-party libraries. While retraining LLMs on updated codebases could refresh their API knowledge, this approach is computationally expensive. Recently, lightweight model editing methods have emerged to efficiently correct specific knowledge in LLMs. However, it remains unclear whether these methods can effectively update deprecated API knowledge and enable edited models to generate up-to-date APIs. To address this gap, we conduct the first systematic study applying 10 state-of-the-art model editing techniques to update deprecated API knowledge in three LLMs: Qwen2.5-Coder, StarCoder2, and DeepSeek-Coder. We introduce EDAPIBench, a dedicated benchmark featuring over 70 deprecated APIs from 8 popular Python libraries, with more than 3,000 editing instances. Our results show that the parameter-efficient fine-tuning method AdaLoRA achieves the best performance in enabling edited models to generate correct, up-to-date APIs, but falls short in Specificity (i.e., the editing influences untargeted knowledge). To resolve this, we propose AdaLoRA-L, which defines \"Common API Layers\" (layers within the LLMs with high importance across all APIs, storing general knowledge and excluded from editing) and restricts edits exclusively to \"Specific API Layers\" (layers with high importance only for the target API, storing the API-specific knowledge). Experimental results demonstrate that AdaLoRA-L significantly improves Specificity while maintaining comparable performance across other evaluation metrics.", "arxiv_url": "https://arxiv.org/abs/2511.21022", "authors": ["Guancheng Lin", "Xiao Yu", "Jacky Keung", "Xing Hu", "Xin Xia", "Alex X. Liu"], "first_author": "Guancheng Lin", "category": ["Empirical", "Technical", "Benchmark"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文构建了用于评估过时API知识编辑的基准EDAPIBench、比较了10种轻量级模型编辑方法在三款代码模型上的表现，并提出AdaLoRA-L通过区分“通用API层”和“特定API层”显著提高编辑的特异性同时保持效果与泛化能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.21022v1", "published": "2025-11-26", "update_time": "2025-11-26", "download_time": "2025-12-05 10:04:39"}
{"id": "2511.20933", "title": "Hierarchical Evaluation of Software Design Capabilities of Large Language Models of Code", "abstract": "Large language models (LLMs) are being increasingly adopted in the software engineering domain, yet the robustness of their grasp on core software design concepts remains unclear. We conduct an empirical study to systematically evaluate their understanding of cohesion (intra-module) and coupling (inter-module). We programmatically generate poorly designed code fragments and test the DeepSeek-R1 model family ($14$B, $32$B, $70$B) under varying levels of guidance, from simple \\textit{Verification} to \\textit{Guided} and \\textit{Open-ended Generation}, while varying contextual noise by injecting distractor elements. While models exhibit a solid baseline understanding of both concepts in ideal conditions, their practical knowledge is fragile and highly asymmetrical. Reasoning about coupling proves brittle; performance collapses in noisy, open-ended scenarios, with F1 scores dropping by over $50\\%$. In contrast, the models' analysis of cohesion is remarkably robust to internal noise in guided tasks, showing little performance degradation. However, this resilience also fails when all guidance is removed. Reasoning-trace analysis confirms these failure modes, revealing \\textit{cognitive shortcutting} for coupling versus a more exhaustive (yet still failing) analysis for cohesion. To summarize, while LLMs can provide reliable assistance for recognizing design flaws, their ability to reason autonomously in noisy, realistic contexts is limited, highlighting the critical need for more scalable and robust program understanding capabilities.", "arxiv_url": "https://arxiv.org/abs/2511.20933", "authors": ["Mootez Saad", "Boqi Chen", "José Antonio Hernández López", "Dániel Varró", "Tushar Sharma"], "first_author": "Mootez Saad", "category": ["Benchmark", "Empirical"], "field": "Requirements & Design", "task": "Analysis", "summary": "本文提出了一个可控的分层评估基准与方法，系统生成带有内聚与耦合缺陷的代码片段并在不同引导级别与噪声条件下评测DeepSeek‑R1系列模型，发现在噪声与开放式任务下耦合推理高度脆弱而内聚分析相对稳健。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.20933v1", "published": "2025-11-25", "update_time": "2025-11-25", "download_time": "2025-12-05 10:05:01"}
{"id": "2511.21197", "title": "Bug Detective and Quality Coach: Developers' Mental Models of AI-Assisted IDE Tools", "abstract": "AI-assisted tools support developers in performing cognitively demanding tasks such as bug detection and code readability assessment. Despite the advancements in the technical characteristics of these tools, little is known about how developers mentally model them and how mismatches affect trust, control, and adoption. We conducted six co-design workshops with 58 developers to elicit their mental models about AI-assisted bug detection and readability features. It emerged that developers conceive bug detection tools as \\textit{bug detectives}, which warn users only in case of critical issues, guaranteeing transparency, actionable feedback, and confidence cues. Readability assessment tools, on the other hand, are envisioned as \\textit{quality coaches}, which provide contextual, personalized, and progressive guidance. Trust, in both tasks, depends on the clarity of explanations, timing, and user control. A set of design principles for Human-Centered AI in IDEs has been distilled, aiming to balance disruption with support, conciseness with depth, and automation with human agency.", "arxiv_url": "https://arxiv.org/abs/2511.21197", "authors": ["Paolo Buono", "Mary Cerullo", "Stefano Cirillo", "Giuseppe Desolda", "Francesco Greco", "Emanuela Guglielmi", "Grazia Margarella", "Giuseppe Polese", "Simone Scalabrino", "Cesare Tucci"], "first_author": "Paolo Buono", "category": ["Empirical"], "field": "Quality Management", "task": "Defect Prediction", "summary": "本文通过六场涵盖58名开发者的共创工作坊，探讨了开发者对IDE中AI辅助缺陷检测与代码可读性评估工具的心智模型，提出“Bug Detective”和“Quality Coach”两种隐喻并归纳出以解释清晰、时机与可控性为核心的设计原则以增强信任与协作。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.21197v1", "published": "2025-11-26", "update_time": "2025-11-26", "download_time": "2025-12-05 10:05:26"}
{"id": "2511.19875", "title": "CodeFuse-CommitEval: Towards Benchmarking LLM's Power on Commit Message and Code Change Inconsistency Detection", "abstract": "Version control relies on commit messages to convey the rationale for code changes, but these messages are often low quality and, more critically, inconsistent with their diffs-known as message-code inconsistency (MCI). MCIs mislead reviewers, hinder maintenance, contaminate research datasets, and may obscure security patches. Yet, no dedicated benchmark exists to evaluate models for MCI detection. We introduce CODEFUSE-COMMITEVAL, the first benchmark designed for MCI detection using large language models (LLMs). Built on the ApacheCM dataset for diversity and quality, we generate seven types of inconsistent messages through rule-guided mutations of originally consistent commits and apply two-fold validation to verify both positive and negative samples. Using this labeled dataset of message-diff pairs, we evaluate six state-of-the-art open-source LLMs under a vanilla setting and with three augmentation strategies: few-shot prompting, chain-of-thought, and extended context. Results show models detect inconsistent commits more reliably than consistent ones (average Recall 85.95%, Precision 80.28%, Specificity 63.8%); gpt-oss-20B performs best overall but uses over twice the tokens of others. Augmentation effects vary: adjacent context helps larger models but adds noise for smaller ones; few-shot improves accuracy and reduces token use, yet increases universally incorrect predictions; chain-of-thought boosts precision and specificity at the cost of recall and higher token consumption. Type-wise analysis reveals higher detectability for component, file-path, and operation inconsistencies, but lower accuracy and higher token cost for intent-level \"purpose\" inconsistencies. CODEFUSE-COMMITEVAL provides a rigorous foundation for measuring, comparing, and advancing MCI detection, highlighting the need for richer context and balanced data to capture high-level semantic gaps.", "arxiv_url": "https://arxiv.org/abs/2511.19875", "authors": ["Qingyu Zhang", "Puzhuo Liu", "Peng Di", "Chenxiong Qian"], "first_author": "Qingyu Zhang", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Alignment", "summary": "本文提出了CODEFUSE-COMMITEVAL——基于ApacheCM并通过规则化突变与双重验证构建的首个用于提交信息与代码差异不一致（MCI）检测的基准数据集，并在六个开源大模型与多种增强策略下进行了系统评估与类型分析。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.19875v1", "published": "2025-11-25", "update_time": "2025-11-25", "download_time": "2025-12-05 10:05:50"}
{"id": "2511.20709", "title": "DUALGUAGE: Automated Joint Security-Functionality Benchmarking for Secure Code Generation", "abstract": "Large language models (LLMs) and autonomous coding agents are increasingly used to generate software across a wide range of domains. Yet a core requirement remains unmet: ensuring that generated code is secure without compromising its functional correctness. Existing benchmarks and evaluations for secure code generation fall short-many measure only vulnerability reduction, disregard correctness preservation, or evaluate security and functionality on separate datasets, violating the fundamental need for simultaneous joint evaluation. We present DUALGAUGE, the first fully automated benchmarking framework designed to rigorously evaluate the security and correctness of LLM-generated code in unison. Given the lack of datasets enabling joint evaluation of secure code generation, we also present DUALGAUGE-BENCH, a curated benchmark suite of diverse coding tasks, each paired with manually validated test suites for both security and functionality, designed for full coverage of specification requirements. At the core of DUALGAUGE is an agentic program executor, which runs a program against given tests in sandboxed environments, and an LLM-based evaluator, which assesses both correctness and vulnerability behavior against expected outcomes. We rigorously evaluated and ensured the quality of DUALGAUGE-BENCH and the accuracy of DUALGAUGE, and applied DUALGAUGE to benchmarking ten leading LLMs on DUALGAUGE-BENCH across thousands of test scenarios. Our results reveal critical gaps in correct and secure code generation by these LLMs, for which our open-source system and datasets help accelerate progress via reproducible, scalable, and rigorous evaluation.", "arxiv_url": "https://arxiv.org/abs/2511.20709", "authors": ["Abhijeet Pathak", "Suvadra Barua", "Dinesh Gudimetla", "Rupam Patir", "Jiawei Guo", "Hongxin Hu", "Haipeng Cai"], "first_author": "Abhijeet Pathak", "category": ["Technical", "Benchmark", "Empirical"], "field": "Software Testing", "task": "Testing automation", "summary": "本文提出DUALGAUGE及DUALGAUGE-BENCH，一个自动化的联合功能与安全性基准测试框架与覆盖驱动的测试套件，通过沙箱执行、依赖自动解析和基于LLM的语义评估来同时评估LLM生成代码的正确性与可被利用的漏洞，并对多款主流模型进行了大规模测试揭示关键缺陷。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.20709v1", "published": "2025-11-24", "update_time": "2025-11-24", "download_time": "2025-12-05 10:06:15"}
{"id": "2511.19635", "title": "Agint: Agentic Graph Compilation for Software Engineering Agents", "abstract": "LLM-based coding agents are increasingly common but still face challenges in context management, latency, reliability, reproducibility, and scalability. We present Agint, an agentic graph compiler, interpreter, and runtime that incrementally and hierarchically converts natural-language instructions into typed, effect-aware code DAGs. Agint introduces explicit type floors (text to data to spec to code) grounded in semantic graph transformations and a hybrid LLM and function-based JIT runtime. This enables dynamic graph refinement, reproducible and optimizable execution, speculative evaluation, and interoperability with existing developer tools. Agint's typed graph bindings improve reliability and allow concurrent composition of concurrent codebases by construction, supporting accelerated development with smaller and faster models, lower latency, efficient context utilization, and higher throughput. Hierarchical compilation allows scalable graph edits, while the graph structure supports reproducibility and efficient parallel generation. Agint provides a composable unix-style toolchain: dagify (DAG compiler), dagent (hybrid JIT runtime), schemagin (schema generator), and datagin (data transformer) for realtime, low-latency code and dataflow creation. Human developers and coding agents refine graphs through the Agint CLI, while non-technical users use Agint Flow GUI for visual editing, conversational refinement, and debugging to promote prototype agentic workflows to production code. This continuous co-creation model allows teams to prototype quickly, refine seamlessly, and deploy reliably, bridging natural language, compiler methods, and developer tooling to enable a new generation of composable, team-centric coding agents at scale.", "arxiv_url": "https://arxiv.org/abs/2511.19635", "authors": ["Abhi Chivukula", "Jay Somasundaram", "Vijay Somasundaram"], "first_author": "Abhi Chivukula", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出Agint——一种将自然语言意图逐步编译为类型化、具效应感知的代码DAG并通过混合LLM/函数JIT运行时进行增量解析、并行执行与可重现部署的代理图编译器与运行时，旨在提高代码生成的可靠性、低延迟和可组合性。", "quality": "High", "conference": "NeurIPS Workshop: Deep Learning for Code in the Agentic Era 2025", "pdf_url": "https://arxiv.org/pdf/2511.19635v1", "published": "2025-11-24", "update_time": "2025-11-24", "download_time": "2025-12-05 10:06:46"}
{"id": "2511.23408", "title": "Evaluating LLMs for One-Shot Patching of Real and Artificial Vulnerabilities", "abstract": "Automated vulnerability patching is crucial for software security, and recent advancements in Large Language Models (LLMs) present promising capabilities for automating this task. However, existing research has primarily assessed LLMs using publicly disclosed vulnerabilities, leaving their effectiveness on related artificial vulnerabilities largely unexplored. In this study, we empirically evaluate the patching effectiveness and complementarity of several prominent LLMs, such as OpenAI's GPT variants, LLaMA, DeepSeek, and Mistral models, using both real and artificial vulnerabilities. Our evaluation employs Proof-of-Vulnerability (PoV) test execution to concretely assess whether LLM-generated source code successfully patches vulnerabilities. Our results reveal that LLMs patch real vulnerabilities more effectively compared to artificial ones. Additionally, our analysis reveals significant variability across LLMs in terms of overlapping (multiple LLMs patching the same vulnerabilities) and complementarity (vulnerabilities patched exclusively by a single LLM), emphasizing the importance of selecting appropriate LLMs for effective vulnerability patching.", "arxiv_url": "https://arxiv.org/abs/2511.23408", "authors": ["Aayush Garg", "Zanis Ali Khan", "Renzo Degiovanni", "Qiang Tang"], "first_author": "Aayush Garg", "category": ["Empirical"], "field": "Quality Management", "task": "Vulnerability Repair", "summary": "本文通过对14款主流LLM在15个真实漏洞及41个对应人工漏洞上进行一次性补丁生成并以PoV测试执行验证，发现LLM对真实漏洞的修复效果优于人工漏洞且不同模型在修复上存在显著的互补性与重叠性。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.23408v1", "published": "2025-11-28", "update_time": "2025-11-28", "download_time": "2025-12-05 10:07:12"}
{"id": "2511.23321", "title": "Chart2Code-MoLA: Efficient Multi-Modal Code Generation via Adaptive Expert Routing", "abstract": "Chart-to-code generation is a critical task in automated data visualization, translating complex chart structures into executable programs. While recent Multi-modal Large Language Models (MLLMs) improve chart representation, existing approaches still struggle to achieve cross-type generalization, memory efficiency, and modular design. To address these challenges, this paper proposes C2C-MoLA, a multimodal framework that synergizes Mixture of Experts (MoE) with Low-Rank Adaptation (LoRA). The MoE component uses a complexity-aware routing mechanism with domain-specialized experts and load-balanced sparse gating, dynamically allocating inputs based on learnable structural metrics like element count and chart complexity. LoRA enables parameter-efficient updates for resource-conscious tuning, further supported by a tailored training strategy that aligns routing stability with semantic accuracy. Experiments on Chart2Code-160k show that the proposed model improves generation accuracy by up to 17%, reduces peak GPU memory by 18%, and accelerates convergence by 20%, when compared to standard fine-tuning and LoRA-only baselines, particularly on complex charts. Ablation studies validate optimal designs, such as 8 experts and rank-8 LoRA, and confirm scalability for real-world multimodal code generation.", "arxiv_url": "https://arxiv.org/abs/2511.23321", "authors": ["Yifei Wang", "Jacky Keung", "Zhenyu Mao", "Jingyu Zhang", "Yuchen Cao"], "first_author": "Yifei Wang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Translation", "summary": "本文提出C2C-MoLA，一种将Mixture of Experts与LoRA结合的多模态图表到代码生成框架，通过复杂度感知的动态路由、参数高效微调与语法语义联合训练在Chart2Code-160k上显著提升生成准确率、降低显存占用并加速收敛。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.23321v1", "published": "2025-11-28", "update_time": "2025-11-28", "download_time": "2025-12-05 10:07:36"}
{"id": "2401.12554", "title": "Can Large Language Models Write Parallel Code?", "abstract": "Large language models are increasingly becoming a popular tool for software development. Their ability to model and generate source code has been demonstrated in a variety of contexts, including code completion, summarization, translation, and lookup. However, they often struggle to generate code for complex programs. In this paper, we study the capabilities of state-of-the-art language models to generate parallel code. In order to evaluate language models, we create a benchmark, ParEval, consisting of prompts that represent 420 different coding tasks related to scientific and parallel computing. We use ParEval to evaluate the effectiveness of several state-of-the-art open- and closed-source language models on these tasks. We introduce novel metrics for evaluating the performance of generated code, and use them to explore how well each large language model performs for 12 different computational problem types and six different parallel programming models.", "arxiv_url": "https://arxiv.org/abs/2401.12554", "authors": ["Daniel Nichols", "Joshua H. Davis", "Zhaojun Xie", "Arjun Rajaram", "Abhinav Bhatele"], "first_author": "Daniel Nichols", "category": ["Benchmark", "Empirical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出了用于并行/HPC代码生成与翻译的基准ParEval、以及新的性能评估指标（speedup_n@k 和 efficiency_n@k），并用该基准评估多种开源与闭源大模型，发现模型普遍难以生成高质量并行代码，尤其在MPI和稀疏/非结构化问题上表现最差。", "quality": "High", "conference": "HPDC 2024", "pdf_url": "https://arxiv.org/pdf/2401.12554v3", "published": "2024-01-23", "update_time": "2024-05-14", "download_time": "2025-12-05 10:08:05"}
{"id": "2401.01062", "title": "Experimenting a New Programming Practice with LLMs", "abstract": "The recent development on large language models makes automatically constructing small programs possible. It thus has the potential to free software engineers from low-level coding and allow us to focus on the perhaps more interesting parts of software development, such as requirement engineering and system testing. In this project, we develop a prototype named AISD (AI-aided Software Development), which is capable of taking high-level (potentially vague) user requirements as inputs, generates detailed use cases, prototype system designs, and subsequently system implementation. Different from existing attempts, AISD is designed to keep the user in the loop, i.e., by repeatedly taking user feedback on use cases, high-level system designs, and prototype implementations through system testing. AISD has been evaluated with a novel benchmark of non-trivial software projects. The experimental results suggest that it might be possible to imagine a future where software engineering is reduced to requirement engineering and system testing only.", "arxiv_url": "https://arxiv.org/abs/2401.01062", "authors": ["Simiao Zhang", "Jiaping Wang", "Guoliang Dong", "Jun Sun", "Yueling Zhang", "Geguang Pu"], "first_author": "Simiao Zhang", "category": ["Technical", "Benchmark"], "field": "Requirements & Design", "task": "Elicitation", "summary": "本文提出AISD，一个以保持用户全程参与的LLM驱动软件开发框架，并构建了用于评估自动化软件开发能力的新基准CAASD，实验证明在用例生成、设计迭代与原型测试下对比现有方法有显著提升。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2401.01062v1", "published": "2024-01-02", "update_time": "2024-01-02", "download_time": "2025-12-05 10:08:32"}
{"id": "2512.01010", "title": "Chain of Unit-Physics: A Primitive-Centric Approach to Scientific Code Synthesis", "abstract": "Agentic large language models are proposed as autonomous code generators for scientific computing, yet their reliability in high-stakes problems remains unclear. Developing computational scientific software from natural-language queries remains challenging broadly due to (a) sparse representation of domain codes during training and (b) the limited feasibility of RLHF with a small expert community. To address these limitations, this work conceptualizes an inverse approach to code design, embodied in the Chain of Unit-Physics framework: a first-principles (or primitives)-centric, multi-agent system in which human expert knowledge is encoded as unit-physics tests that explicitly constrain code generation. The framework is evaluated on a nontrivial combustion task, used here as a representative benchmark for scientific problem with realistic physical constraints. Closed-weight systems and code-focused agentic variants fail to produce correct end-to-end solvers, despite tool and web access, exhibiting four recurrent error classes: interface (syntax/API) hallucinations, overconfident assumptions, numerical/physical incoherence, and configuration fragility. Open-weight models with chain-of-thought (CoT) decoding reduce interface errors but still yield incorrect solutions. On the benchmark task, the proposed framework converges within 5-6 iterations, matches the human-expert implementation (mean error of $3.1\\times10^{-3}$ %), with a $\\sim$33.4 % faster runtime and a $\\sim$30 % efficient memory usage at a cost comparable to mid-sized commercial APIs, yielding a practical template for physics-grounded scientific code generation. As datasets and models evolve, zero-shot code accuracy will improve; however, the Chain of Unit-Physics framework goes further by embedding first-principles analysis that is foundational to scientific codes.", "arxiv_url": "https://arxiv.org/abs/2512.01010", "authors": ["Vansh Sharma", "Venkat Raman"], "first_author": "Vansh Sharma", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Reasoning", "summary": "本文提出 Chain of Unit-Physics 框架，通过人类专家编写的基于第一性原理的“单元物理”可测试约束与多智能体迭代生成/验证流程，显著提升基于LLM的科学计算代码合成的物理一致性与可靠性，并在一个12自由度燃烧求解器任务上达到接近人工实现的精度同时提升性能。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.01010v1", "published": "2025-11-30", "update_time": "2025-11-30", "download_time": "2025-12-05 10:08:59"}
{"id": "2512.00867", "title": "The AI Attribution Paradox: Transparency as Social Strategy in Open-Source Software Development", "abstract": "AI coding assistants have transformed software development, raising questions about transparency and attribution practices. We examine the \"AI attribution paradox\": how developers strategically balance acknowledging AI assistance with managing community scrutiny. Analyzing 14,300 GitHub commits across 7,393 repositories from 2023-2025, we investigated attribution strategies and community responses across eight major AI tools. Results reveal widespread AI usage (95.2% of commits) but strategic attribution: only 29.5% employ explicit disclosure, with dramatic tool variation (Claude 80.5% versus Copilot 9.0%). Explicit attribution triggers modest scrutiny (23% more questions and 21% more comments) but tool choice matters 20-30 times more for predicting reception. Community sentiment remains neutral regardless of attribution type, suggesting curiosity rather than hostility. Temporal analyses show rapid norm evolution: explicit attribution increased from near-zero in early 2024 to 40% by late 2025, indicating community adaptation. These findings illuminate attribution as strategic communication rather than simple transparency, advancing understanding of algorithmic accountability and norm formation during technological transitions. We discuss implications for developers navigating disclosure decisions, platforms designing attribution mechanisms, and researchers studying emergent practices in AI-augmented collaborative work.", "arxiv_url": "https://arxiv.org/abs/2512.00867", "authors": ["Obada Kraishan"], "first_author": "Obada Kraishan", "category": ["Empirical"], "field": "Human-AI Collaboration", "task": "Attribution & Transparency in Open-Source Software", "summary": "基于对2023–2025年14,300次GitHub提交的实证分析，本文揭示了开发者在开源项目中对AI生成代码的归属披露是一种战略性沟通（“AI归属悖论”），并比较了不同工具、社区响应及其随时间的演进。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.00867v1", "published": "2025-11-30", "update_time": "2025-11-30", "download_time": "2025-12-05 10:09:31"}
{"id": "2512.01939", "title": "An Empirical Study of Agent Developer Practices in AI Agent Frameworks", "abstract": "The rise of large language models (LLMs) has sparked a surge of interest in agents, leading to the rapid growth of agent frameworks. Agent frameworks are software toolkits and libraries that provide standardized components, abstractions, and orchestration mechanisms to simplify agent development. Despite widespread use of agent frameworks, their practical applications and how they influence the agent development process remain underexplored. Different agent frameworks encounter similar problems during use, indicating that these recurring issues deserve greater attention and call for further improvements in agent framework design. Meanwhile, as the number of agent frameworks continues to grow and evolve, more than 80% of developers report difficulties in identifying the frameworks that best meet their specific development requirements. In this paper, we conduct the first empirical study of LLM-based agent frameworks, exploring real-world experiences of developers in building AI agents. To compare how well the agent frameworks meet developer needs, we further collect developer discussions for the ten previously identified agent frameworks, resulting in a total of 11,910 discussions. Finally, by analyzing these discussions, we compare the frameworks across five dimensions: development efficiency, functional abstraction, learning cost, performance optimization, and maintainability, which refers to how easily developers can update and extend both the framework itself and the agents built upon it over time. Our comparative analysis reveals significant differences among frameworks in how they meet the needs of agent developers. Overall, we provide a set of findings and implications for the LLM-driven AI agent framework ecosystem and offer insights for the design of future LLM-based agent frameworks and agent developers.", "arxiv_url": "https://arxiv.org/abs/2512.01939", "authors": ["Yanlin Wang", "Xinyi Xu", "Jiachi Chen", "Tingting Bi", "Wenchao Gu", "Zibin Zheng"], "first_author": "Yanlin Wang", "category": ["Empirical"], "field": "Requirements & Design", "task": "Management", "summary": "本文通过对1,575个LLM代理项目及11,910条开发者讨论进行大规模挖掘与分析，提出了基于SDLC的代理开发挑战分类并比较了十个主流代理框架在学习成本、开发效率、功能抽象、性能优化与可维护性五个维度的差异与优劣。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.01939v1", "published": "2025-12-01", "update_time": "2025-12-01", "download_time": "2025-12-05 10:10:06"}
{"id": "2512.01690", "title": "Generating REST API Tests With Descriptive Names", "abstract": "Automated test generation has become a key technique for ensuring software quality, particularly in modern API-based architectures. However, automatically generated test cases are typically assigned non-descriptive names (e.g., test0, test1), which reduces their readability and hinders their usefulness during comprehension and maintenance. In this work, we present three novel deterministic techniques to generate REST API test names. We then compare eight techniques in total for generating descriptive names for REST API tests automatically produced by the fuzzer EvoMaster, using 10 test cases generated for 9 different open-source APIs. The eight techniques include rule-based heuristics and large language model (LLM)-based approaches. Their effectiveness was empirically evaluated through two surveys (involving up to 39 people recruited via LinkedIn). Our results show that a rule-based approach achieves the highest clarity ratings among deterministic methods, performs on par with state-of-the-art LLM-based models such as Gemini and GPT-4o, and significantly outperforms GPT-3.5.   To further evaluate the practical impact of our results, an industrial case study was carried out with practitioners who actively use EvoMaster at Volkswagen AG. A developer questionnaire was then carried out based on the use of EvoMaster on four different APIs by four different users, for a total of 74 evaluated test cases. Feedback from practitioners further confirms that descriptive names produced by this approach improve test suite readability.   These findings highlight that lightweight, deterministic techniques can serve as effective alternatives to computationally expensive and security-sensitive LLM-based approaches for automated system-level test naming, providing a practical step toward more developer-friendly API test generation.", "arxiv_url": "https://arxiv.org/abs/2512.01690", "authors": ["Philip Garrett", "Juan P. Galeotti", "Andrea Arcuri", "Alexander Poth", "Olsi Rrjolli"], "first_author": "Philip Garrett", "category": ["Technical", "Empirical"], "field": "Software Testing", "task": "Test Generation", "summary": "本文提出三种确定性方法为自动生成的REST API测试用例生成描述性名称，并通过用户调研（39名参与者）与大众汽车的工业案例比较规则与LLM方法，结果表明基于规则的方法在可读性上与先进LLM（如Gemini、GPT-4o）相当且已集成至EvoMaster作为默认策略。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.01690v1", "published": "2025-12-01", "update_time": "2025-12-01", "download_time": "2025-12-05 10:10:33"}
{"id": "2503.01449", "title": "Benchmarking Large Language Models for Multi-Language Software Vulnerability Detection", "abstract": "Recent advancements in generative AI have led to the widespread adoption of large language models (LLMs) in software engineering, addressing numerous long-standing challenges. However, a comprehensive study examining the capabilities of LLMs in software vulnerability detection (SVD), a crucial aspect of software security, is currently lacking. Existing research primarily focuses on evaluating LLMs using C/C++ datasets. It typically explores only one or two strategies among prompt engineering, instruction tuning, and sequence classification fine-tuning for open-source LLMs. Consequently, there is a significant knowledge gap regarding the effectiveness of diverse LLMs in detecting vulnerabilities across various programming languages. To address this knowledge gap, we present a comprehensive empirical study evaluating the performance of LLMs on the SVD task. We have compiled a comprehensive dataset comprising 8,260 vulnerable functions in Python, 7,505 in Java, and 28,983 in JavaScript. We assess five open-source LLMs using multiple approaches, including prompt engineering, instruction tuning, and sequence classification fine-tuning. These LLMs are benchmarked against five fine-tuned small language models and two open-source static application security testing tools. Furthermore, we explore two avenues to improve LLM performance on SVD: a) Data perspective: Retraining models using downsampled balanced datasets. b) Model perspective: Investigating ensemble learning methods that combine predictions from multiple LLMs. Our comprehensive experiments demonstrate that SVD remains a challenging task for LLMs. This study provides a thorough understanding of the role of LLMs in SVD and offers practical insights for future advancements in leveraging generative AI to enhance software security practices.", "arxiv_url": "https://arxiv.org/abs/2503.01449", "authors": ["Ting Zhang", "Chengran Yang", "Yindu Su", "Martin Weyssow", "Hung Nguyen", "Tan Bui", "Hong Jin Kang", "Yikun Li", "Eng Lieh Ouh", "Lwin Khin Shar", "David Lo"], "first_author": "Ting Zhang", "category": ["Empirical", "Benchmark"], "field": "Quality Management", "task": "Vulnerability Detection", "summary": "本文构建了包含 Python、Java、JavaScript 函数级漏洞的大规模数据集，并对五个开源大模型在提示工程、指令微调和序列分类微调下进行全面基准评测，比较其与小型模型和静态扫描工具的表现，同时探索通过均衡重采样与模型集成提升检测效果。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2503.01449v1", "published": "2025-03-03", "update_time": "2025-03-03", "download_time": "2025-12-05 10:10:55"}
{"id": "2512.01609", "title": "GPTrace: Effective Crash Deduplication Using LLM Embeddings", "abstract": "Fuzzing is a highly effective method for uncovering software vulnerabilities, but analyzing the resulting data typically requires substantial manual effort. This is amplified by the fact that fuzzing campaigns often find a large number of crashing inputs, many of which share the same underlying bug. Crash deduplication is the task of finding such duplicate crashing inputs and thereby reducing the data that needs to be examined. Many existing deduplication approaches rely on comparing stack traces or other information that is collected when a program crashes. Although various metrics for measuring the similarity of such pieces of information have been proposed, many do not yield satisfactory deduplication results. In this work, we present GPTrace, a deduplication workflow that leverages a large language model to evaluate the similarity of various data sources associated with crashes by computing embedding vectors and supplying those as input to a clustering algorithm. We evaluate our approach on over 300 000 crashing inputs belonging to 50 ground truth labels from 14 different targets. The deduplication results produced by GPTrace show a noticeable improvement over hand-crafted stack trace comparison methods and even more complex state-of-the-art approaches that are less flexible.", "arxiv_url": "https://arxiv.org/abs/2512.01609", "authors": ["Patrick Herter", "Vincent Ahlrichs", "Ridvan Açilan", "Julian Horsch"], "first_author": "Patrick Herter", "category": ["Technical"], "field": "Software Testing", "task": "Testing automation", "summary": "GPTrace提出一种利用大语言模型生成堆栈跟踪和ASan报告的嵌入向量并对其进行聚类的崩溃去重工作流，从而在多目标大规模模糊测试崩溃集上显著优于现有基于栈跟踪的去重方法。", "quality": "High", "conference": "ICSE 2026", "pdf_url": "https://arxiv.org/pdf/2512.01609v1", "published": "2025-12-01", "update_time": "2025-12-01", "download_time": "2025-12-05 10:11:28"}
{"id": "2512.01396", "title": "BackportBench: A Multilingual Benchmark for Automated Backporting of Patches", "abstract": "Many modern software projects evolve rapidly to incorporate new features and security patches. It is important for users to update their dependencies to safer versions, but many still use older, vulnerable package versions because upgrading can be difficult and may break their existing codebase. Software developers can mitigate this problem by backporting security patches to older releases. However, manually backporting is time-consuming and error-prone. The effectiveness of existing automated backporting techniques on general software remains unclear since they typically target only code-hunk or function-level patch porting scenarios and are evaluated with imperfect metrics.   To facilitate the development and evaluation of automated backporting techniques, we introduce BackportBench, the first comprehensive benchmark suite for patch backporting problem. BackportBench is a multilingual benchmark that contains 202 patch backporting problems from PyPI, Maven, and npm, each with executable Docker environments and relevant test cases. We evaluated existing patch porting methods and LLM-based techniques that have the potential to adapt to this task using BackportBench. The results show that the agentic method has outperformed traditional patch porting methods, especially on cases that require logical and structural changes. However, the performance varies across different programming languages. Based on the findings, we draw several implications for researchers and software practitioners in future work on automated backporting.", "arxiv_url": "https://arxiv.org/abs/2512.01396", "authors": ["Zhiqing Zhong", "Jiaming Huang", "Pinjia He"], "first_author": "Zhiqing Zhong", "category": ["Benchmark", "Empirical"], "field": "Maintenance", "task": "Patch Backporting", "summary": "本文提出 BackportBench——一个包含来自 PyPI、Maven 和 npm 的 202 个多语言可执行补丁回移任务的基准，并用该基准评估现有补丁移植方法与多种 LLM 技术以分析其在不同语言与场景下的表现差异。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.01396v1", "published": "2025-12-01", "update_time": "2025-12-01", "download_time": "2025-12-05 10:11:49"}
{"id": "2512.01356", "title": "LAURA: Enhancing Code Review Generation with Context-Enriched Retrieval-Augmented LLM", "abstract": "Code review is critical for ensuring software quality and maintainability. With the rapid growth in software scale and complexity, code review has become a bottleneck in the development process because of its time-consuming and knowledge-intensive nature and the shortage of experienced developers willing to review code. Several approaches have been proposed for automatically generating code reviews based on retrieval, neural machine translation, pre-trained models, or large language models (LLMs). These approaches mainly leverage historical code changes and review comments. However, a large amount of crucial information for code review, such as the context of code changes and prior review knowledge, has been overlooked. This paper proposes an LLM-based review knowledge-augmented, context-aware framework for code review generation, named LAURA. The framework integrates review exemplar retrieval, context augmentation, and systematic guidance to enhance the performance of ChatGPT-4o and DeepSeek v3 in generating code review comments. Besides, given the extensive low-quality reviews in existing datasets, we also constructed a high-quality dataset. Experimental results show that for both models, LAURA generates review comments that are either completely correct or at least helpful to developers in 42.2% and 40.4% of cases, respectively, significantly outperforming SOTA baselines. Furthermore, our ablation studies demonstrate that all components of LAURA contribute positively to improving comment quality.", "arxiv_url": "https://arxiv.org/abs/2512.01356", "authors": ["Yuxin Zhang", "Yuxia Zhang", "Zeyu Sun", "Yanjie Jiang", "Hui Liu"], "first_author": "Yuxin Zhang", "category": ["Technical", "Benchmark"], "field": "Maintenance", "task": "Code Review", "summary": "本文提出LAURA框架，通过上下文增强、相似评审示例检索与系统化引导对LLM进行检索增强生成代码评审，并构建了包含301,256条高质量diff‑comment序列的数据集，从而显著提升了ChatGPT-4o和DeepSeek v3的评审质量。", "quality": "High", "conference": "ASE 2025", "pdf_url": "https://arxiv.org/pdf/2512.01356v1", "published": "2025-12-01", "update_time": "2025-12-01", "download_time": "2025-12-05 10:12:12"}
{"id": "2512.01255", "title": "Large Language Models Cannot Reliably Detect Vulnerabilities in JavaScript: The First Systematic Benchmark and Evaluation", "abstract": "Researchers have proposed numerous methods to detect vulnerabilities in JavaScript, especially those assisted by Large Language Models (LLMs). However, the actual capability of LLMs in JavaScript vulnerability detection remains questionable, necessitating systematic evaluation and comprehensive benchmarks. Unfortunately, existing benchmarks suffer from three critical limitations: (1) incomplete coverage, such as covering a limited subset of CWE types; (2) underestimation of LLM capabilities caused by unreasonable ground truth labeling; and (3) overestimation due to unrealistic cases such as using isolated vulnerable files rather than complete projects.   In this paper, we introduce, for the first time, three principles for constructing a benchmark for JavaScript vulnerability detection that directly address these limitations: (1) comprehensiveness, (2) no underestimation, and (3) no overestimation. Guided by these principles, we propose FORGEJS, the first automatic benchmark generation framework for evaluating LLMs' capability in JavaScript vulnerability detection. Then, we use FORGEJS to construct ARENAJS-the first systematic benchmark for LLM-based JavaScript vulnerability detection-and further propose JUDGEJS, an automatic evaluation framework.   We conduct the first systematic evaluation of LLMs for JavaScript vulnerability detection, leveraging JUDGEJS to assess seven popular commercial LLMs on ARENAJS. The results show that LLMs not only exhibit limited reasoning capabilities, but also suffer from severe robustness defects, indicating that reliable JavaScript vulnerability detection with LLMs remains an open challenge.", "arxiv_url": "https://arxiv.org/abs/2512.01255", "authors": ["Qingyuan Fei", "Xin Liu", "Song Li", "Shujiang Wu", "Jianwei Hou", "Ping Chen", "Zifeng Kang"], "first_author": "Qingyuan Fei", "category": ["Benchmark", "Empirical", "Technical"], "field": "Quality Management", "task": "Vulnerability Detection", "summary": "本文提出了面向JavaScript漏洞检测的自动基准生成框架FORGEJS，构建了系统性基准ARENAJS和自动评估框架JUDGEJS，并使用其对多款主流商用LLM进行系统评估，揭示了模型在推理能力、鲁棒性和可部署性方面的显著缺陷。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.01255v1", "published": "2025-12-01", "update_time": "2025-12-01", "download_time": "2025-12-05 10:12:36"}
{"id": "2512.02795", "title": "Towards Observation Lakehouses: Living, Interactive Archives of Software Behavior", "abstract": "Code-generating LLMs are trained largely on static artifacts (source, comments, specifications) and rarely on materializations of run-time behavior. As a result, they readily internalize buggy or mislabeled code. Since non-trivial semantic properties are undecidable in general, the only practical way to obtain ground-truth functionality is by dynamic observation of executions. In prior work, we addressed representation with Sequence Sheets, Stimulus-Response Matrices (SRMs), and Stimulus-Response Cubes (SRCs) to capture and compare behavior across tests, implementations, and contexts. These structures make observation data analyzable offline and reusable, but they do not by themselves provide persistence, evolution, or interactive analytics at scale. In this paper, therefore, we introduce observation lakehouses that operationalize continual SRCs: a tall, append-only observations table storing every actuation (stimulus, response, context) and SQL queries that materialize SRC slices on demand. Built on Apache Parquet + Iceberg + DuckDB, the lakehouse ingests data from controlled pipelines (LASSO) and CI pipelines (e.g., unit test executions), enabling n-version assessment, behavioral clustering, and consensus oracles without re-execution. On a 509-problem benchmark, we ingest $\\approx$8.6M observation rows ($<$51MiB) and reconstruct SRM/SRC views and clusters in $<$100ms on a laptop, demonstrating that continual behavior mining is practical without a distributed cluster of machines. This makes behavioral ground truth first-class alongside other run-time data and provides an infrastructure path toward behavior-aware evaluation and training. The Observation Lakehouse, together with the accompanying dataset, is publicly available as an open-source project on GitHub: https://github.com/SoftwareObservatorium/observation-lakehouse", "arxiv_url": "https://arxiv.org/abs/2512.02795", "authors": ["Marcus Kessel"], "first_author": "Marcus Kessel", "category": ["Technical", "Benchmark"], "field": "Software Testing", "task": "Testing automation", "summary": "本文提出 Observation Lakehouse —— 一个基于 Parquet + Iceberg + DuckDB 的可持续、可交互的观测数据湖屋，用于以细粒度调用步骤记录运行时行为并按需重构 SRM/SRC，从而实现无需重执行即可进行 n 版评估、行为聚类与共识 Oracle 并开源了相应数据集与实现。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.02795v1", "published": "2025-12-02", "update_time": "2025-12-02", "download_time": "2025-12-05 10:13:02"}
{"id": "2512.02750", "title": "\"Can you feel the vibes?\": An exploration of novice programmer engagement with vibe coding", "abstract": "Emerging alongside generative AI and the broader trend of AI-assisted coding, the term \"vibe coding\" refers to creating software via natural language prompts rather than direct code authorship. This approach promises to democratize software development, but its educational implications remain underexplored. This paper reports on a one-day educational hackathon investigating how novice programmers and mixed-experience teams engage with vibe coding. We organized an inclusive event at a Brazilian public university with 31 undergraduate participants from computing and non-computing disciplines, divided into nine teams. Through observations, an exit survey, and semi-structured interviews, we examined creative processes, tool usage patterns, collaboration dynamics, and learning outcomes. Findings reveal that vibe coding enabled rapid prototyping and cross-disciplinary collaboration, with participants developing prompt engineering skills and delivering functional demonstrations within time constraints. However, we observed premature convergence in ideation, uneven code quality requiring rework, and limited engagement with core software engineering practices. Teams adopted sophisticated workflows combining multiple AI tools in pipeline configurations, with human judgment remaining essential for critical refinement. The short format (9 hours) proved effective for confidence-building among newcomers while accommodating participants with limited availability. We conclude that vibe coding hackathons can serve as valuable low-stakes learning environments when coupled with explicit scaffolds for divergent thinking, critical evaluation of AI outputs, and realistic expectations about production quality.", "arxiv_url": "https://arxiv.org/abs/2512.02750", "authors": ["Kiev Gama", "Filipe Calegario", "Victoria Jackson", "Alexander Nolte", "Luiz Augusto Morais", "Vinicius Garcia"], "first_author": "Kiev Gama", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Prompting", "summary": "本文报道了一次在巴西公立大学举办的一日“vibe coding”教育黑客松，通过观察、问卷与访谈分析31名本科生在以自然语言提示驱动的软件开发中的协作、工具使用与学习成果，发现其有利于快速原型和跨学科合作但存在早期思路收敛、代码质量不均与对工程实践参与有限等问题，建议通过引导发散思维与批判性评估来提升教学效果。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.02750v1", "published": "2025-12-02", "update_time": "2025-12-02", "download_time": "2025-12-05 10:13:27"}
{"id": "2511.15665", "title": "Quantum-Guided Test Case Minimization for LLM-Based Code Generation", "abstract": "Precisely controlling Large Language Models (LLMs) to generate efficient and concise code is a central challenge in software engineering. We introduce a framework based on Test-Driven Development (TDD) that transforms code specification into a combinatorial optimization task. The framework first prompts an LLM to generate a test suite, then formulates the Test Case Minimization (TCM) problem as a Quadratic Unconstrained Binary Optimization (QUBO) model. This QUBO paradigm is compatible with both classical solvers and emerging hardware such as quantum annealers. Experimentally, quantum annealing solves the core TCM task 16 times faster than simulated annealing. This performance underpins our end-to-end framework, which reduces total token consumption by 36.5\\% and significantly improves code quality. This work demonstrates a powerful synergy between generative AI and combinatorial optimization in software engineering, highlighting the critical importance of precise model formulation.", "arxiv_url": "https://arxiv.org/abs/2511.15665", "authors": ["Huixiang Zhang", "Mahzabeen Emu"], "first_author": "Huixiang Zhang", "category": ["Technical"], "field": "Software Testing", "task": "Test Generation", "summary": "本文提出一个结合TDD与QUBO优化的端到端框架：先用LLM生成带功能标签的冗余测试集，再将测试用例最小化问题建模为QUBO并使用量子退火（或经典求解器）求解，以显著减少token消耗并提升LLM生成代码的质量与效率；实验证明量子退火在核心优化上比模拟退火快约16倍，整体token减少36.5%、代码复杂度降低26.1%、生成时间缩短54.3%。", "quality": "Middle", "conference": "IEEE CASCON 2025", "pdf_url": "https://arxiv.org/pdf/2511.15665v1", "published": "2025-11-19", "update_time": "2025-11-19", "download_time": "2025-12-05 16:54:31"}
{"id": "2511.17417", "title": "CREST: Improving Interpretability and Effectiveness of Troubleshooting at Ericsson through Criterion-Specific Trouble Report Retrieval", "abstract": "The rapid evolution of the telecommunication industry necessitates efficient troubleshooting processes to maintain network reliability, software maintainability, and service quality. Trouble Reports (TRs), which document issues in Ericsson's production system, play a critical role in facilitating the timely resolution of software faults. However, the complexity and volume of TR data, along with the presence of diverse criteria that reflect different aspects of each fault, present challenges for retrieval systems. Building on prior work at Ericsson, which utilized a two-stage workflow, comprising Initial Retrieval (IR) and Re-Ranking (RR) stages, this study investigates different TR observation criteria and their impact on the performance of retrieval models. We propose \\textbf{CREST} (\\textbf{C}riteria-specific \\textbf{R}etrieval via \\textbf{E}nsemble of \\textbf{S}pecialized \\textbf{T}R models), a criterion-driven retrieval approach that leverages specialized models for different TR fields to improve both effectiveness and interpretability, thereby enabling quicker fault resolution and supporting software maintenance. CREST utilizes specialized models trained on specific TR criteria and aggregates their outputs to capture diverse and complementary signals. This approach leads to enhanced retrieval accuracy, better calibration of predicted scores, and improved interpretability by providing relevance scores for each criterion, helping users understand why specific TRs were retrieved. Using a subset of Ericsson's internal TRs, this research demonstrates that criterion-specific models significantly outperform a single model approach across key evaluation metrics. This highlights the importance of all targeted criteria used in this study for optimizing the performance of retrieval systems.", "arxiv_url": "https://arxiv.org/abs/2511.17417", "authors": ["Soroush Javdan", "Pragash Krishnamoorthy", "Olga Baysal"], "first_author": "Soroush Javdan", "category": ["Technical", "Empirical"], "field": "Maintenance", "task": "Trouble Report Retrieval", "summary": "本文提出 CREST，一种基于准则的故障报告检索方法，通过为故障报告中不同的观察准则训练专门模型并融合其输出，提高检索准确性、得分校准与可解释性，从而加速 Ericsson 的故障排查与维护流程。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2511.17417v1", "published": "2025-11-21", "update_time": "2025-11-21", "download_time": "2025-12-05 17:24:08"}
{"id": "2512.03421", "title": "Exploring the Potential and Limitations of Large Language Models for Novice Program Fault Localization", "abstract": "Novice programmers often face challenges in fault localization due to their limited experience and understanding of programming syntax and logic. Traditional methods like Spectrum-Based Fault Localization (SBFL) and Mutation-Based Fault Localization (MBFL) help identify faults but often lack the ability to understand code context, making them less effective for beginners. In recent years, Large Language Models (LLMs) have shown promise in overcoming these limitations by utilizing their ability to understand program syntax and semantics. LLM-based fault localization provides more accurate and context-aware results than traditional techniques. This study evaluates six closed-source and seven open-source LLMs using the Codeflaws, Condefects, and BugT datasets, with BugT being a newly constructed dataset specifically designed to mitigate data leakage concerns. Advanced models with reasoning capabilities, such as OpenAI o3 and DeepSeekR1, achieve superior accuracy with minimal reliance on prompt engineering. In contrast, models without reasoning capabilities, like GPT-4, require carefully designed prompts to maintain performance. While LLMs perform well in simple fault localization, their accuracy decreases as problem difficulty increases, though top models maintain robust performance in the BugT dataset. Over-reasoning is another challenge, where some models generate excessive explanations that hinder fault localization clarity. Additionally, the computational cost of deploying LLMs remains a significant barrier for real-time debugging. LLM's explanations demonstrate significant value for novice programmer assistance, with one-year experience participants consistently rating them highly. Our findings demonstrate the potential of LLMs to improve debugging efficiency while stressing the need for further refinement in their reasoning and computational efficiency for practical adoption.", "arxiv_url": "https://arxiv.org/abs/2512.03421", "authors": ["Hexiang Xu", "Hengyuan Liu", "Yonghao Wu", "Xiaolan Kang", "Xiang Chen", "Yong Liu"], "first_author": "Hexiang Xu", "category": ["Empirical", "Benchmark"], "field": "Quality Management", "task": "Bug Localization", "summary": "本文构建了新的BugT数据集并在Codeflaws、Condefects与BugT上系统评估多种开源与闭源LLM，实证分析了它们在新手程序故障定位中的性能、难度影响、推理过度与计算成本，并通过用户研究验证了LLM解释对新手的教学价值。", "quality": "High", "conference": "The Journal of Systems & Software", "pdf_url": "https://arxiv.org/pdf/2512.03421v1", "published": "2025-12-03", "update_time": "2025-12-03", "download_time": "2025-12-05 17:25:17"}
{"id": "2512.03420", "title": "HarnessAgent: Scaling Automatic Fuzzing Harness Construction with Tool-Augmented LLM Pipelines", "abstract": "Large language model (LLM)-based techniques have achieved notable progress in generating harnesses for program fuzzing. However, applying them to arbitrary functions (especially internal functions) \\textit{at scale} remains challenging due to the requirement of sophisticated contextual information, such as specification, dependencies, and usage examples. State-of-the-art methods heavily rely on static or incomplete context provisioning, causing failure of generating functional harnesses. Furthermore, LLMs tend to exploit harness validation metrics, producing plausible yet logically useless code. % Therefore, harness generation across large and diverse projects continues to face challenges in reliable compilation, robust code retrieval, and comprehensive validation.   To address these challenges, we present HarnessAgent, a tool-augmented agentic framework that achieves fully automated, scalable harness construction over hundreds of OSS-Fuzz targets. HarnessAgent introduces three key innovations: 1) a rule-based strategy to identify and minimize various compilation errors; 2) a hybrid tool pool for precise and robust symbol source code retrieval; and 3) an enhanced harness validation pipeline that detects fake definitions. We evaluate HarnessAgent on 243 target functions from OSS-Fuzz projects (65 C projects and 178 C++ projects). It improves the three-shot success rate by approximately 20\\% compared to state-of-the-art techniques, reaching 87\\% for C and 81\\% for C++. Our one-hour fuzzing results show that more than 75\\% of the harnesses generated by HarnessAgent increase the target function coverage, surpassing the baselines by over 10\\%. In addition, the hybrid tool-pool system of HarnessAgent achieves a response rate of over 90\\% for source code retrieval, outperforming Fuzz Introspector by more than 30\\%.", "arxiv_url": "https://arxiv.org/abs/2512.03420", "authors": ["Kang Yang", "Yunhang Zhang", "Zichuan Li", "GuanHong Tao", "Jun Xu", "XiaoJing Liao"], "first_author": "Kang Yang", "category": ["Technical", "Empirical"], "field": "Software Testing", "task": "Test Generation", "summary": "本文提出HarnessAgent，一种工具增强的agent框架，通过编译错误分流、混合检索工具池与强化验证流水线，实现对大型C/C++代码库中内部函数的自动化模糊测试驱动（harness）构建，并在243个OSS-Fuzz目标上显著提高生成成功率与模糊覆盖率。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.03420v1", "published": "2025-12-03", "update_time": "2025-12-03", "download_time": "2025-12-05 17:25:58"}
{"id": "2512.05073", "title": "David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?", "abstract": "Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated agentic AI framework on NVIDIA's Comprehensive Verilog Design Problems(CVDP) benchmark. Results show that agentic workflows: through task decomposition, iterative feedback, and correction - not only unlock near-LLM performance at a fraction of the cost but also create learning opportunities for agents, paving the way for efficient, adaptive solutions in complex design tasks.", "arxiv_url": "https://arxiv.org/abs/2512.05073", "authors": ["Shashwat Shankar", "Subhranshu Pandey", "Innocent Dengkhw Mochahari", "Bhabesh Mali", "Animesh Basak Chowdhury", "Sukanta Bhattacharjee", "Chandan Karfa"], "first_author": "Shashwat Shankar", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Prompting", "summary": "本文提出并在NVIDIA CVDP基准上评估了一个为小型语言模型（SLM）定制的多智能体 agentic AI 框架，通过任务分解、SLM 优化提示、迭代验证与回滚机制，使低成本 SLM 在 Verilog 硬件设计与理解任务中接近或匹配大模型性能并显著降低能耗与成本。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.05073v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-06 23:50:18"}
{"id": "2512.04680", "title": "Generative AI for Self-Adaptive Systems: State of the Art and Research Roadmap", "abstract": "Self-adaptive systems (SASs) are designed to handle changes and uncertainties through a feedback loop with four core functionalities: monitoring, analyzing, planning, and execution. Recently, generative artificial intelligence (GenAI), especially the area of large language models, has shown impressive performance in data comprehension and logical reasoning. These capabilities are highly aligned with the functionalities required in SASs, suggesting a strong potential to employ GenAI to enhance SASs. However, the specific benefits and challenges of employing GenAI in SASs remain unclear. Yet, providing a comprehensive understanding of these benefits and challenges is complex due to several reasons: limited publications in the SAS field, the technological and application diversity within SASs, and the rapid evolution of GenAI technologies. To that end, this paper aims to provide researchers and practitioners a comprehensive snapshot that outlines the potential benefits and challenges of employing GenAI's within SAS. Specifically, we gather, filter, and analyze literature from four distinct research fields and organize them into two main categories to potential benefits: (i) enhancements to the autonomy of SASs centered around the specific functions of the MAPE-K feedback loop, and (ii) improvements in the interaction between humans and SASs within human-on-the-loop settings. From our study, we outline a research roadmap that highlights the challenges of integrating GenAI into SASs. The roadmap starts with outlining key research challenges that need to be tackled to exploit the potential for applying GenAI in the field of SAS. The roadmap concludes with a practical reflection, elaborating on current shortcomings of GenAI and proposing possible mitigation strategies.", "arxiv_url": "https://arxiv.org/abs/2512.04680", "authors": ["Jialong Li", "Mingyue Zhang", "Nianyu Li", "Danny Weyns", "Zhi Jin", "Kenji Tei"], "first_author": "Jialong Li", "category": ["Survey"], "field": "Self-Adaptive Systems", "task": "GenAI for MAPE-K (Monitoring, Analysis, Planning, Execution) and Human-on-the-loop Interaction", "summary": "本文综述了将生成式AI（尤其是大型语言模型）应用于自适应系统的最新进展，分析其在增强MAPE‑K各功能和改善人机协作中的潜力与挑战，并提出了面向研究与实践的路线图与缓解策略。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04680v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-06 23:50:52"}
{"id": "2512.04702", "title": "POLARIS: Is Multi-Agentic Reasoning the Next Wave in Engineering Self-Adaptive Systems?", "abstract": "The growing scale, complexity, interconnectivity, and autonomy of modern software ecosystems introduce unprecedented uncertainty, challenging the foundations of traditional self-adaptation. Existing approaches, typically rule-driven controllers or isolated learning components, struggle to generalize to novel contexts or coordinate responses across distributed subsystems, leaving them ill-equipped for emergent unknown unknowns. Recent discussions on Self-Adaptation 2.0 emphasize an equal partnership between AI and adaptive systems, merging learning-driven intelligence with adaptive control for predictive and proactive behavior. Building on this foundation, we introduce POLARIS, a three-layer multi-agentic self-adaptation framework that advances beyond reactive adaptation. POLARIS integrates: (1) a low-latency Adapter layer for monitoring and safe execution, (2) a transparent Reasoning layer that generates and verifies plans using tool-aware, explainable agents, and (3) a Meta layer that records experiences and meta-learns improved adaptation policies over time. Through shared knowledge and predictive models, POLARIS handles uncertainty, learns from past actions, and evolves its strategies, enabling systems that anticipate change and maintain resilient, goal-directed behavior. Preliminary evaluation on two self-adaptive exemplars, SWIM and SWITCH, shows that POLARIS consistently outperforms state-of-the-art baselines. We argue this marks a shift toward Self-Adaptation 3.0, akin to Software 3.0: a paradigm where systems not only learn from their environment but also reason about and evolve their own adaptation processes, continuously improving to meet novel challenges.", "arxiv_url": "https://arxiv.org/abs/2512.04702", "authors": ["Divyansh Pandey", "Vyakhya Gupta", "Prakhar Singhal", "Karthik Vaidhyanathan"], "first_author": "Divyansh Pandey", "category": ["Technical"], "field": "Self-Adaptive Systems", "task": "Multi-Agentic Runtime Adaptation (LLM-based reasoning & meta-learning)", "summary": "提出POLARIS——一个三层多智能体自适应框架，结合低延迟适配器、可解释的推理代理和元学习来实现预测性、可验证且能随经验持续演化的自适应系统，并在SWIM和SWITCH示例上显示出优于现有基线的性能提升。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04702v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-07 01:57:21"}
{"id": "2512.04673", "title": "Cross-Task Benchmarking and Evaluation of General-Purpose and Code-Specific Large Language Models", "abstract": "Large Language Models (LLMs) have revolutionized both general natural language processing and domain-specific applications such as code synthesis, legal reasoning, and finance. However, while prior studies have explored individual model capabilities, a systematic cross-domain comparison that unifies linguistic, reasoning, and code understanding abilities remains underexplored. In this work, we present a comprehensive evaluation of five general-purpose and three code-specific state-of-the-art LLMs across six diverse benchmarks encompassing linguistic competence, mathematical reasoning, and trustworthiness. Additionally, we analyze model behavior on the CoNaLa dataset for code explanation, comparing natural language and code-specialized LLMs. Our findings reveal that models optimized for code (e.g., CodeLLaMA variants) exhibit strong reasoning and syntactic precision, that even for non-coding tasks can show measurable performance gains, in contrast to general-purpose models like Mistral-7B and Llama-3-8B.", "arxiv_url": "https://arxiv.org/abs/2512.04673", "authors": ["Gunjan Das", "Paheli Bhattacharya", "Rishabh Gupta"], "first_author": "Gunjan Das", "category": ["Empirical"], "field": "Coding Assistant", "task": "Code Summarization", "summary": "本文对多款通用与代码专用大型语言模型在六个自然语言与推理基准及CoNaLa代码解释任务上进行了系统的横向评估与对比，发现代码优化模型在推理能力和语法精确性上具有显著优势。", "quality": "Middle", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04673v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-07 01:57:56"}
{"id": "2512.05100", "title": "Structured Document Translation via Format Reinforcement Learning", "abstract": "Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \\textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.", "arxiv_url": "https://arxiv.org/abs/2512.05100", "authors": ["Haiyue Song", "Johannes Eschbach-Dymanus", "Hour Kaing", "Sumire Honda", "Hideki Tanaka", "Bianka Buschbeck", "Masao Utiyama"], "first_author": "Haiyue Song", "category": ["Technical"], "field": "Structured Document Translation", "task": "XML/HTML Structured Document Translation", "summary": "本文提出FORMATRL，一种基于Group Relative Policy Optimization的强化学习方法，通过TreeSim和Node-chrF等结构感知奖励以及StrucAUC评估，将结构信息直接纳入模型优化以提升带XML/HTML标记的软件文档翻译的结构保真度与翻译质量。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.05100v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-07 01:59:06"}
{"id": "2512.04785", "title": "ASTRIDE: A Security Threat Modeling Platform for Agentic-AI Applications", "abstract": "AI agent-based systems are becoming increasingly integral to modern software architectures, enabling autonomous decision-making, dynamic task execution, and multimodal interactions through large language models (LLMs). However, these systems introduce novel and evolving security challenges, including prompt injection attacks, context poisoning, model manipulation, and opaque agent-to-agent communication, that are not effectively captured by traditional threat modeling frameworks. In this paper, we introduce ASTRIDE, an automated threat modeling platform purpose-built for AI agent-based systems. ASTRIDE extends the classical STRIDE framework by introducing a new threat category, A for AI Agent-Specific Attacks, which encompasses emerging vulnerabilities such as prompt injection, unsafe tool invocation, and reasoning subversion, unique to agent-based applications. To automate threat modeling, ASTRIDE combines a consortium of fine-tuned vision-language models (VLMs) with the OpenAI-gpt-oss reasoning LLM to perform end-to-end analysis directly from visual agent architecture diagrams, such as data flow diagrams(DFDs). LLM agents orchestrate the end-to-end threat modeling automation process by coordinating interactions between the VLM consortium and the reasoning LLM. Our evaluations demonstrate that ASTRIDE provides accurate, scalable, and explainable threat modeling for next-generation intelligent systems. To the best of our knowledge, ASTRIDE is the first framework to both extend STRIDE with AI-specific threats and integrate fine-tuned VLMs with a reasoning LLM to fully automate diagram-driven threat modeling in AI agent-based applications.", "arxiv_url": "https://arxiv.org/abs/2512.04785", "authors": ["Eranga Bandara", "Amin Hass", "Ross Gore", "Sachin Shetty", "Ravi Mukkamala", "Safdar H. Bouk", "Xueping Liang", "Ng Wee Keong", "Kasun De Zoysa", "Aruna Withanage", "Nilaan Loganathan"], "first_author": "Eranga Bandara", "category": ["Technical", "Benchmark"], "field": "Requirements & Design", "task": "Analysis", "summary": "本文提出ASTRIDE——一个面向智能体式AI应用的自动化威胁建模平台，扩展STRIDE引入AI特有威胁类别并结合微调的视觉-语言模型与推理LLM从架构图端到端自动生成可解释的威胁模型。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04785v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-07 02:04:42"}
{"id": "2512.04611", "title": "PBFuzz: Agentic Directed Fuzzing for PoV Generation", "abstract": "Proof-of-Vulnerability (PoV) input generation is a critical task in software security and supports downstream applications such as path generation and validation. Generating a PoV input requires solving two sets of constraints: (1) reachability constraints for reaching vulnerable code locations, and (2) triggering constraints for activating the target vulnerability. Existing approaches, including directed greybox fuzzing and LLM-assisted fuzzing, struggle to efficiently satisfy these constraints. This work presents an agentic method that mimics human experts. Human analysts iteratively study code to extract semantic reachability and triggering constraints, form hypotheses about PoV triggering strategies, encode them as test inputs, and refine their understanding using debugging feedback. We automate this process with an agentic directed fuzzing framework called PBFuzz. PBFuzz tackles four challenges in agentic PoV generation: autonomous code reasoning for semantic constraint extraction, custom program-analysis tools for targeted inference, persistent memory to avoid hypothesis drift, and property-based testing for efficient constraint solving while preserving input structure. Experiments on the Magma benchmark show strong results. PBFuzz triggered 57 vulnerabilities, surpassing all baselines, and uniquely triggered 17 vulnerabilities not exposed by existing fuzzers. PBFuzz achieved this within a 30-minute budget per target, while conventional approaches use 24 hours. Median time-to-exposure was 339 seconds for PBFuzz versus 8680 seconds for AFL++ with CmpLog, giving a 25.6x efficiency improvement with an API cost of 1.83 USD per vulnerability.", "arxiv_url": "https://arxiv.org/abs/2512.04611", "authors": ["Haochen Zeng", "Andrew Bao", "Jiajun Cheng", "Chengyu Song"], "first_author": "Haochen Zeng", "category": ["Technical"], "field": "Software Testing", "task": "Test Generation", "summary": "本文提出PBFuzz——一种基于LLM代理的定向模糊测试框架，通过自主演绎代码约束、工具编排、持久化记忆与基于属性的测试生成PoV输入，并在Magma基准上显著优于现有方法。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04611v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-08 01:51:09"}
{"id": "2512.04538", "title": "Completion by Comprehension: Guiding Code Generation with Multi-Granularity Understanding", "abstract": "As code completion task from function-level to repository-level, leveraging contextual information from large-scale codebases becomes a core challenge. However, existing retrieval-augmented generation (RAG) methods typically treat code as plain natural language, relying primarily on shallow semantic matching while overlooking structural semantics and code-specific dependencies. This limits their ability to capture control flow and underlying intent, ultimately constraining the quality of generated code. Therefore, we propose CoCo, a novel framework that enables code Completion by Comprehension of multi-granularity context from large-scale code repositories. CoCo employs static code analysis to extract structured context at the function, file, and project levels, capturing execution logic and semantic dependencies. It then adopts an graph-based multi-granularity context selection mechanism to filter out redundant information and remove noise. Consequently, the information is converted into natural language in a consistent manner, thereby functioning as explicit contextual prompts to guide subsequent code completion. Additionally, a structure-aware code re-ranker mechanism ensures alignment at both semantic and structural levels. Extensive experiments on CrossCodeEval and RepoEval benchmarks demonstrate that CoCo consistently surpasses state-of-the-art baselines, achieving up to 20.2% gains in EM. Moreover, the framework is model-agnostic and can be seamlessly integrated into existing methods, leading to significant performance.", "arxiv_url": "https://arxiv.org/abs/2512.04538", "authors": ["Xinkui Zhao", "Rongkai Liu", "Yifan Zhang", "Chen Zhi", "Lufei Zhang", "Guanjie Cheng", "Yueshen Xu", "Shuiguang Deng", "Jianwei Yin"], "first_author": "Xinkui Zhao", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Completion", "summary": "本文提出CoCo框架，通过静态代码分析提取函数、文件和仓库三级结构化上下文，使用图式多粒度上下文选择与结构感知重排序将关键信息转为自然语言提示，从而指导检索增强的模型进行仓库级代码补全并在基准上显著优于现有方法。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04538v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-08 01:51:40"}
{"id": "2512.04738", "title": "OsmT: Bridging OpenStreetMap Queries and Natural Language with Open-source Tag-aware Language Models", "abstract": "Bridging natural language and structured query languages is a long-standing challenge in the database community. While recent advances in language models have shown promise in this direction, existing solutions often rely on large-scale closed-source models that suffer from high inference costs, limited transparency, and lack of adaptability for lightweight deployment. In this paper, we present OsmT, an open-source tag-aware language model specifically designed to bridge natural language and Overpass Query Language (OverpassQL), a structured query language for accessing large-scale OpenStreetMap (OSM) data. To enhance the accuracy and structural validity of generated queries, we introduce a Tag Retrieval Augmentation (TRA) mechanism that incorporates contextually relevant tag knowledge into the generation process. This mechanism is designed to capture the hierarchical and relational dependencies present in the OSM database, addressing the topological complexity inherent in geospatial query formulation. In addition, we define a reverse task, OverpassQL-to-Text, which translates structured queries into natural language explanations to support query interpretation and improve user accessibility. We evaluate OsmT on a public benchmark against strong baselines and observe consistent improvements in both query generation and interpretation. Despite using significantly fewer parameters, our model achieves competitive accuracy, demonstrating the effectiveness of open-source pre-trained language models in bridging natural language and structured query languages within schema-rich geospatial environments.", "arxiv_url": "https://arxiv.org/abs/2512.04738", "authors": ["Zhuoyue Wan", "Wentao Hu", "Chen Jason Zhang", "Yuanfeng Song", "Shuaimin Li", "Ruiqiang Xiao", "Xiao-Yong Wei", "Raymond Chi-Wing Wong"], "first_author": "Zhuoyue Wan", "category": ["Technical"], "field": "Natural Language Interfaces for Structured Geospatial Data", "task": "Text-to-OverpassQL and OverpassQL-to-Text (bidirectional NL ↔ OverpassQL translation)", "summary": "本文提出OSMT，一种开源的标签感知预训练语言模型，并通过Tag Retrieval Augmentation和混合预训练策略实现对自然语言与OverpassQL的双向翻译，在参数量较小的情况下仍能在Text-to-OverpassQL和OverpassQL-to-Text任务上取得有竞争力的性能与更好的结构化生成质量。", "quality": "High", "conference": "42nd IEEE International Conference on Data Engineering (ICDE 2026)", "pdf_url": "https://arxiv.org/pdf/2512.04738v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-08 02:00:39"}
{"id": "2512.04419", "title": "Solving LLM Repetition Problem in Production: A Comprehensive Study of Multiple Solutions", "abstract": "The repetition problem, where Large Language Models (LLMs) continuously generate repetitive content without proper termination, poses a critical challenge in production deployments, causing severe performance degradation and system stalling. This paper presents a comprehensive investigation and multiple practical solutions for the repetition problem encountered in real-world batch code interpretation tasks.   We identify three distinct repetition patterns: (1) business rule generation repetition, (2) method call relationship analysis repetition, and (3) PlantUML diagram syntax generation repetition. Through rigorous theoretical analysis based on Markov models, we establish that the root cause lies in greedy decoding's inability to escape repetitive loops, exacerbated by self-reinforcement effects.   Our comprehensive experimental evaluation demonstrates three viable solutions: (1) Beam Search decoding with early_stopping=True serves as a universal post-hoc mechanism that effectively resolves all three repetition patterns; (2) presence_penalty hyperparameter provides an effective solution specifically for BadCase 1; and (3) Direct Preference Optimization (DPO) fine-tuning offers a universal model-level solution for all three BadCases.   The primary value of this work lies in combining first-hand production experience with extensive experimental validation. Our main contributions include systematic theoretical analysis of repetition mechanisms, comprehensive evaluation of multiple solutions with task-specific applicability mapping, identification of early_stopping as the critical parameter for Beam Search effectiveness, and practical production-ready solutions validated in real deployment environments.", "arxiv_url": "https://arxiv.org/abs/2512.04419", "authors": ["Weiwei Wang", "Weijie Zou", "Jiyong Min"], "first_author": "Weiwei Wang", "category": ["Technical", "Empirical"], "field": "Coding Assistant", "task": "Code Prompting", "summary": "本文结合生产环境中的一手经验、理论分析与大规模实验，提出并验证了三类可行的解决方案（启用early_stopping的Beam Search、presence_penalty参数调整和基于DPO的微调）以在批量代码解析任务中根治或缓解LLM的重复生成问题。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.04419v1", "published": "2025-12-04", "update_time": "2025-12-04", "download_time": "2025-12-08 02:07:41"}
{"id": "2512.05908", "title": "Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures", "abstract": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.", "arxiv_url": "https://arxiv.org/abs/2512.05908", "authors": ["Amirkia Rafiei Oskooei", "S. Selcan Yukcu", "Mehmet Cevheri Bozoglan", "Mehmet S. Aktas"], "first_author": "Amirkia Rafiei Oskooei", "category": ["Technical", "Empirical"], "field": "Quality Management", "task": "Bug Localization", "summary": "本文提出将多仓库微服务代码库转化为分层的自然语言摘要，并采用先将错误报告路由到相关仓库、再自上而下进行目录与文件级搜索的两阶段 LLM 推理流程，实现对大规模工业系统的可解释缺陷定位，显著优于检索与 RAG 基线。", "quality": "High", "conference": "LLM4Code Workshop, ICSE 2026", "pdf_url": "https://arxiv.org/pdf/2512.05908v1", "published": "2025-12-05", "update_time": "2025-12-05", "download_time": "2025-12-09 01:48:46"}
{"id": "2512.05887", "title": "Bootstrapping Fuzzers for Compilers of Low-Resource Language Dialects Using Language Models", "abstract": "Modern extensible compiler frameworks-such as MLIR-enable rapid creation of domain-specific language dialects. This flexibility, however, makes correctness harder to ensure as the same extensibility that accelerates development also complicates maintaining the testing infrastructure. Extensible languages require automated test generation that is both dialect-agnostic (works across dialects without manual adaptation) and dialect-effective (targets dialect-specific features to find bugs). Existing approaches typically sacrifice one of these goals by either requiring manually constructed seed corpora for each dialect, or by failing to be effective. We present a dialect-agnostic and dialect-effective grammar-based and coverage-guided fuzzing approach for extensible compilers that combines two key insights from existing work: (i) the grammars of dialects, which already encode the structural and type constraints, can often be extracted automatically from the dialect specification; and (ii) these grammars can be used in combination with pre-trained large language models to automatically generate representative and diverse seed inputs from the full dialect space without requiring any manual input or training data. These seeds can then be used to bootstrap coverage-guided fuzzers. We built this approach into a tool, Germinator. When evaluated on six MLIR projects spanning 91 dialects, Germinator generated seeds improve line coverage by 10-120% over grammar-based baselines. We compare against grammar-based baselines because they are the only class of existing automatic seed generators that can be applied uniformly across MLIR's heterogeneous dialect ecosystem. Germinator discovers 88 previously unknown bugs (40 confirmed), including 23 in dialects with no prior automated test generators, demonstrating effective and controllable testing of low-resource dialects at scale.", "arxiv_url": "https://arxiv.org/abs/2512.05887", "authors": ["Sairam Vaidya", "Marcel Böhme", "Loris D'Antoni"], "first_author": "Sairam Vaidya", "category": ["Technical", "Empirical"], "field": "Software Testing", "task": "Test Generation", "summary": "本文提出 Germinator：一种从 MLIR 方言的 TableGen 自动提取语法并结合语法约束的大型语言模型生成多样化种子以引导覆盖驱动的模糊测试，从而在 91 个方言上显著提升覆盖率并发现大量未知缺陷。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.05887v1", "published": "2025-12-05", "update_time": "2025-12-05", "download_time": "2025-12-09 01:49:12"}
{"id": "2512.05962", "title": "Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity", "abstract": "Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the \"mode-seeking\" or \"zero-forcing\" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $α$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.", "arxiv_url": "https://arxiv.org/abs/2512.05962", "authors": ["Germán Kruszewski", "Pierre Erbacher", "Jos Rozen", "Marc Dymetman"], "first_author": "Germán Kruszewski", "category": ["Technical", "Empirical"], "field": "Reasoning & Verification", "task": "Theorem Proving / Proof Search", "summary": "本文提出了DMVR框架与α-DPG方法，通过以验证器过滤构造显式目标分布并利用α-散度在mode-seeking与mass-covering之间插值，从而可控地在精确性与多样性之间权衡，并在Lean定理证明基准上取得覆盖率-精度前沿上的最优表现，显著缓解了RLVR类方法导致的多样性下降。 ", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2512.05962v1", "published": "2025-12-05", "update_time": "2025-12-05", "download_time": "2025-12-09 01:50:02"}
{"id": "2512.05908", "title": "Natural Language Summarization Enables Multi-Repository Bug Localization by LLMs in Microservice Architectures", "abstract": "Bug localization in multi-repository microservice architectures is challenging due to the semantic gap between natural language bug reports and code, LLM context limitations, and the need to first identify the correct repository. We propose reframing this as a natural language reasoning task by transforming codebases into hierarchical NL summaries and performing NL-to-NL search instead of cross-modal retrieval. Our approach builds context-aware summaries at file, directory, and repository levels, then uses a two-phase search: first routing bug reports to relevant repositories, then performing top-down localization within those repositories. Evaluated on DNext, an industrial system with 46 repositories and 1.1M lines of code, our method achieves Pass@10 of 0.82 and MRR of 0.50, significantly outperforming retrieval baselines and agentic RAG systems like GitHub Copilot and Cursor. This work demonstrates that engineered natural language representations can be more effective than raw source code for scalable bug localization, providing an interpretable repository -> directory -> file search path, which is vital for building trust in enterprise AI tools by providing essential transparency.", "arxiv_url": "https://arxiv.org/abs/2512.05908", "authors": ["Amirkia Rafiei Oskooei", "S. Selcan Yukcu", "Mehmet Cevheri Bozoglan", "Mehmet S. Aktas"], "first_author": "Amirkia Rafiei Oskooei", "category": ["Technical", "Empirical"], "field": "Quality Management", "task": "Bug Localization", "summary": "本文将多仓库微服务代码转换为层次化的自然语言摘要，并通过仓库路由与目录/文件自上而下的两阶段NL‑to‑NL搜索，利用LLM实现可解释且可扩展的多仓库缺陷定位，在工业级DNext系统上显著优于RAG与检索基线。", "quality": "High", "conference": "LLM4Code Workshop, ICSE 2026", "pdf_url": "https://arxiv.org/pdf/2512.05908v1", "published": "2025-12-05", "update_time": "2025-12-05", "download_time": "2025-12-09 01:52:21"}
{"id": "2311.07989", "title": "Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code", "abstract": "In this work we systematically review the recent advancements in software engineering with language models, covering 70+ models, 40+ evaluation tasks, 180+ datasets, and 900 related works. Unlike previous works, we integrate software engineering (SE) with natural language processing (NLP) by discussing the perspectives of both sides: SE applies language models for development automation, while NLP adopts SE tasks for language model evaluation. We break down code processing models into general language models represented by the GPT family and specialized models that are specifically pretrained on code, often with tailored objectives. We discuss the relations and differences between these models, and highlight the historical transition of code modeling from statistical models and RNNs to pretrained Transformers and LLMs, which is exactly the same course that had been taken by NLP. We also go beyond programming and review LLMs' application in other software engineering activities including requirement engineering, testing, deployment, and operations in an endeavor to provide a global view of NLP in SE, and identify key challenges and potential future directions in this domain. We keep the survey open and updated on GitHub at https://github.com/codefuse-ai/Awesome-Code-LLM.", "arxiv_url": "https://arxiv.org/abs/2311.07989", "authors": ["Ziyin Zhang", "Chaoyu Chen", "Bingchang Liu", "Cong Liao", "Zi Gong", "Hang Yu", "Jianguo Li", "Rui Wang"], "first_author": "Ziyin Zhang", "category": ["Survey"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["Taxonomy of Code LMs", "SE–NLP Integration", "Code Modeling History", "Full Software Lifecycle Coverage"], "summary": "该论文系统综述代码语言模型及其在软件工程全生命周期中的应用，并统一了NLP与SE两个社区的视角。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2311.07989v7", "published": "2023-11-14", "update_time": "2024-06-26", "download_time": "2025-12-10 15:34:24"}
{"id": "2509.14856", "title": "CodeFuse-CR-Bench: A Comprehensiveness-aware Benchmark for End-to-End Code Review Evaluation in Python Projects", "abstract": "Automated code review (CR) is a key application for Large Language Models (LLMs), but progress is hampered by a \"reality gap\": existing benchmarks evaluate models on isolated sub-tasks using simplified, context-poor data. This fails to reflect the holistic context-rich nature of real-world CR. To bridge this gap, we introduce CodeFuse-CR-Bench, the first comprehensiveness-aware benchmark for repository-level CR evaluation. CodeFuse-CR-Bench comprises 601 high-quality instances from 70 Python projects covering nine Pull-Request (PR) problem domains, where each instance provides rich, multi-faceted context including the associated issue, PR details, and repository state, enabling end-to-end evaluation. Beyond superficial metrics, we also propose a novel evaluation framework that combines rule-based checks for location and syntax with model-based judgments of review quality. We present the first large-scale assessment of state-of-the-art LLMs on this comprehensive CR task. Our results establish crucial baselines and reveal that (1) no single LLM dominates all aspects of CR; (2) Gemini 2.5 Pro achieves the highest comprehensive performance; and (3) different LLMs exhibit varying robustness to redundant context. These findings highlight the necessity of holistic, multi-dimensional evaluation and provide actionable insights for advancing truly intelligent yet practical CR assistants.", "arxiv_url": "https://arxiv.org/abs/2509.14856", "authors": ["Hanyang Guo", "Xunjin Zheng", "Zihan Liao", "Hang Yu", "Peng DI", "Ziyin Zhang", "Hong-Ning Dai"], "first_author": "Hanyang Guo", "category": ["Benchmark"], "field": "Maintenance", "task": "Code Review", "tags": ["Repository-level Context", "End-to-End Evaluation", "Comprehensive Code Review", "Holistic Evaluation Framework"], "summary": "该论文提出首个面向端到端代码审查的全面性基准，通过提供丰富仓库级上下文并结合规则与模型评估框架，更真实地衡量LLM在实际代码审查任务中的能力。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2509.14856v3", "published": "2025-09-18", "update_time": "2025-10-23", "download_time": "2025-12-10 15:34:47"}
{"id": "2505.16901", "title": "Code Graph Model (CGM): A Graph-Integrated Large Language Model for Repository-Level Software Engineering Tasks", "abstract": "Recent advances in Large Language Models (LLMs) have shown promise in function-level code generation, yet repository-level software engineering tasks remain challenging. Current solutions predominantly rely on proprietary LLM agents, which introduce unpredictability and limit accessibility, raising concerns about data privacy and model customization. This paper investigates whether open-source LLMs can effectively address repository-level tasks without requiring agent-based approaches. We demonstrate this is possible by enabling LLMs to comprehend functions and files within codebases through their semantic information and structural dependencies. To this end, we introduce Code Graph Models (CGMs), which integrate repository code graph structures into the LLM's attention mechanism and map node attributes to the LLM's input space using a specialized adapter. When combined with an agentless graph RAG framework, our approach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark using the open-source Qwen2.5-72B model. This performance ranks first among open weight models, second among methods with open-source systems, and eighth overall, surpassing the previous best open-source model-based method by 12.33%.", "arxiv_url": "https://arxiv.org/abs/2505.16901", "authors": ["Hongyuan Tao", "Ying Zhang", "Zhenhao Tang", "Hongen Peng", "Xukun Zhu", "Bingchang Liu", "Yingguang Yang", "Ziyin Zhang", "Zhaogui Xu", "Haipeng Zhang", "Linchao Zhu", "Rui Wang", "Hang Yu", "Jianguo Li", "Peng Di"], "first_author": "Hongyuan Tao", "category": ["Technical"], "field": "Quality Management", "task": "Bug Repair", "tags": ["Repository-level Issue Fixing", "Graph-Integrated Attention", "Code Graph Representation", "Agentless RAG"], "summary": "该论文提出通过将代码图结构融入LLM并结合无代理的图检索框架，实现开源模型在仓库级缺陷修复任务中的大幅性能提升。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2505.16901v4", "published": "2025-05-22", "update_time": "2025-06-23", "download_time": "2025-12-10 15:35:12"}
{"id": "2409.04183", "title": "GALLa: Graph Aligned Large Language Models for Improved Source Code Understanding", "abstract": "Programming languages possess rich semantic information - such as data flow - that is represented by graphs and not available from the surface form of source code. Recent code language models have scaled to billions of parameters, but model source code solely as text tokens while ignoring any other structural information. Conversely, models that do encode structural information of code make modifications to the Transformer architecture, limiting their scale and compatibility with pretrained LLMs. In this work, we take the best of both worlds with GALLa - Graph Aligned Large Language Models. GALLa utilizes graph neural networks and cross-modal alignment technologies to inject the structural information of code into LLMs as an auxiliary task during finetuning. This framework is both model-agnostic and task-agnostic, as it can be applied to any code LLM for any code downstream task, and requires the structural graph data only at training time from a corpus unrelated to the finetuning data, while incurring no cost at inference time over the baseline LLM. Experiments on five code tasks with seven different baseline LLMs ranging in size from 350M to 14B validate the effectiveness of GALLa, demonstrating consistent improvement over the baseline, even for powerful models such as LLaMA3 and Qwen2.5-Coder.", "arxiv_url": "https://arxiv.org/abs/2409.04183", "authors": ["Ziyin Zhang", "Hang Yu", "Shijie Li", "Peng Di", "Jianguo Li", "Rui Wang"], "first_author": "Ziyin Zhang", "category": ["Technical"], "field": "Coding Assistant", "task": "Code Reasoning", "tags": ["Graph Alignment", "AST Integration", "DFG Integration", "GNN Adapters", "Cross‑Modal Representation"], "summary": "该论文提出GALLa框架利用图结构信息对代码LLM进行对齐，从而在多种代码理解与生成任务中提升模型性能。", "quality": "High", "conference": "ACL 2025", "pdf_url": "https://arxiv.org/pdf/2409.04183v4", "published": "2024-09-06", "update_time": "2025-09-23", "download_time": "2025-12-10 15:35:38"}
{"id": "2212.09420", "title": "Large Language Models Meet NL2Code: A Survey", "abstract": "The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are \"Large Size, Premium Data, Expert Tuning\". In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.", "arxiv_url": "https://arxiv.org/abs/2212.09420", "authors": ["Daoguang Zan", "Bei Chen", "Fengji Zhang", "Dianjie Lu", "Bingchao Wu", "Bei Guan", "Yongji Wang", "Jian-Guang Lou"], "first_author": "Daoguang Zan", "category": ["Survey"], "field": "Coding Assistant", "task": "NL2Code", "tags": ["NL2Code", "Code Generation", "Model Comparison", "Benchmark Analysis"], "summary": "该论文系统综述了27种面向NL2Code任务的大语言模型，总结其技术特征、性能表现与未来挑战。", "quality": "High", "conference": "ACL 2023", "pdf_url": "https://arxiv.org/pdf/2212.09420v2", "published": "2022-12-19", "update_time": "2023-05-08", "download_time": "2025-12-10 15:35:55"}
{"id": "2212.10079", "title": "A Survey on Pretrained Language Models for Neural Code Intelligence", "abstract": "As the complexity of modern software continues to escalate, software engineering has become an increasingly daunting and error-prone endeavor. In recent years, the field of Neural Code Intelligence (NCI) has emerged as a promising solution, leveraging the power of deep learning techniques to tackle analytical tasks on source code with the goal of improving programming efficiency and minimizing human errors within the software industry. Pretrained language models have become a dominant force in NCI research, consistently delivering state-of-the-art results across a wide range of tasks, including code summarization, generation, and translation. In this paper, we present a comprehensive survey of the NCI domain, including a thorough review of pretraining techniques, tasks, datasets, and model architectures. We hope this paper will serve as a bridge between the natural language and programming language communities, offering insights for future research in this rapidly evolving field.", "arxiv_url": "https://arxiv.org/abs/2212.10079", "authors": ["Yichen Xu", "Yanqiao Zhu"], "first_author": "Yichen Xu", "category": ["Survey"], "field": "Coding Assistant", "task": "Code Pre-Training", "tags": ["Tokenization Strategies", "Structure Extraction", "Pretraining Paradigms", "Neural Code Intelligence"], "summary": "该论文系统综述了面向神经代码智能的预训练语言模型在预处理、模型设计与下游任务中的方法与进展。", "quality": "High", "conference": null, "pdf_url": "https://arxiv.org/pdf/2212.10079v1", "published": "2022-12-20", "update_time": "2022-12-20", "download_time": "2025-12-10 15:36:12"}
{"id": "2302.04026", "title": "An Empirical Comparison of Pre-Trained Models of Source Code", "abstract": "While a large number of pre-trained models of source code have been successfully developed and applied to a variety of software engineering (SE) tasks in recent years, our understanding of these pre-trained models is arguably fairly limited. With the goal of advancing our understanding of these models, we perform the first systematic empirical comparison of 19 recently-developed pre-trained models of source code on 13 SE tasks. To gain additional insights into these models, we adopt a recently-developed 4-dimensional categorization of pre-trained models, and subsequently investigate whether there are correlations between different categories of pre-trained models and their performances on different SE tasks.", "arxiv_url": "https://arxiv.org/abs/2302.04026", "authors": ["Changan Niu", "Chuanyi Li", "Vincent Ng", "Dongxiao Chen", "Jidong Ge", "Bin Luo"], "first_author": "Changan Niu", "category": ["Empirical"], "field": "New Field: Model Evaluation for SE", "task": "New Task: Pre-trained Model Comparison", "tags": ["Empirical Comparison", "Code Pre-trained Models", "Cross-task Evaluation", "Model Categorization"], "summary": "该论文系统性地对19种源码预训练模型在13项软件工程任务上的表现进行了实证比较，并分析其类别与性能之间的关联。", "quality": "High", "conference": "ICSE 2023", "pdf_url": "https://arxiv.org/pdf/2302.04026v1", "published": "2023-02-08", "update_time": "2023-02-08", "download_time": "2025-12-10 15:36:31"}
